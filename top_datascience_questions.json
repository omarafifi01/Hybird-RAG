[
  {
    "tags": [
      "multiclass-classification",
      "model-evaluations"
    ],
    "owner": {
      "account_id": 5794180,
      "reputation": 3855,
      "user_id": 13518,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/14f7bd035afecfefb4000c5542937fc7?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "SHASHANK GUPTA",
      "link": "https://datascience.stackexchange.com/users/13518/shashank-gupta"
    },
    "is_answered": true,
    "view_count": 394864,
    "answer_count": 8,
    "score": 295,
    "last_activity_date": 1648891704,
    "creation_date": 1483033147,
    "last_edit_date": 1579464176,
    "question_id": 15989,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin",
    "title": "Micro Average vs Macro average Performance in a Multiclass classification setting",
    "body": "<p>I am trying out a multiclass classification setting with 3 classes. The class distribution is skewed with most of the data falling in 1 of the 3 classes. (class labels being 1,2,3, with 67.28% of the data falling in class label 1, 11.99% data in class 2, and remaining in class 3)</p>\n\n<p>I am training a multiclass classifier on this dataset and I am getting the following performance:</p>\n\n<pre><code>                    Precision           Recall           F1-Score\nMicro Average       0.731               0.731            0.731\nMacro Average       0.679               0.529            0.565\n</code></pre>\n\n<p>I am not sure why all Micro average performances are equal and also Macro average performances are low compared to Micro average.</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "convolutional-neural-network",
      "convolution"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 282479,
    "accepted_answer_id": 12110,
    "answer_count": 12,
    "score": 285,
    "last_activity_date": 1655507837,
    "creation_date": 1434189405,
    "last_edit_date": 1655507837,
    "question_id": 6107,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/6107/what-are-deconvolutional-layers",
    "title": "What are deconvolutional layers?",
    "body": "<p>I recently read <a href=\"http://arxiv.org/abs/1411.4038\">Fully Convolutional Networks for Semantic Segmentation</a> by Jonathan Long, Evan Shelhamer, Trevor Darrell. I don't understand what \"deconvolutional layers\" do / how they work.</p>\n\n<p>The relevant part is</p>\n\n<blockquote>\n  <p>3.3. Upsampling is backwards strided convolution</p>\n  \n  <p>Another way to connect coarse outputs to dense pixels\n  is interpolation. For instance, simple bilinear interpolation\n  computes each output $y_{ij}$ from the nearest four inputs by a\n  linear map that depends only on the relative positions of the\n  input and output cells.<br/>\n  In a sense, upsampling with factor $f$ is convolution with\n  a fractional input stride of 1/f. So long as $f$ is integral, a\n  natural way to upsample is therefore backwards convolution\n  (sometimes called deconvolution) with an output stride of\n  $f$. Such an operation is trivial to implement, since it simply\n  reverses the forward and backward passes of convolution.<br/>\n  Thus upsampling is performed in-network for end-to-end\n  learning by backpropagation from the pixelwise loss.<br/>\n  Note that the deconvolution filter in such a layer need not\n  be fixed (e.g., to bilinear upsampling), but can be learned.\n  A stack of deconvolution layers and activation functions can\n  even learn a nonlinear upsampling.<br/>\n  In our experiments, we find that in-network upsampling\n  is fast and effective for learning dense prediction. Our best\n  segmentation architecture uses these layers to learn to upsample\n  for refined prediction in Section 4.2.</p>\n</blockquote>\n\n<p>I don't think I really understood how convolutional layers are trained. </p>\n\n<p>What I think I've understood is that convolutional layers with a kernel size $k$ learn filters of size $k \\times k$. The output of a convolutional layer with kernel size $k$, stride $s \\in \\mathbb{N}$ and $n$ filters is of dimension $\\frac{\\text{Input dim}}{s^2} \\cdot n$. However, I don't know how the learning of convolutional layers works. (I understand how simple MLPs learn with gradient descent, if that helps).</p>\n\n<p>So if my understanding of convolutional layers is correct, I have no clue how this can be reversed.</p>\n\n<p>Could anybody please help me to understand deconvolutional layers?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "classification",
      "keras",
      "weighted-data"
    ],
    "owner": {
      "account_id": 8700074,
      "reputation": 8747,
      "user_id": 21560,
      "user_type": "registered",
      "accept_rate": 36,
      "profile_image": "https://www.gravatar.com/avatar/349cff1d621bb8779315322f202fba5b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Hendrik",
      "link": "https://datascience.stackexchange.com/users/21560/hendrik"
    },
    "is_answered": true,
    "view_count": 444305,
    "protected_date": 1653238822,
    "accepted_answer_id": 13496,
    "answer_count": 10,
    "score": 266,
    "last_activity_date": 1626366288,
    "creation_date": 1471426545,
    "last_edit_date": 1609412923,
    "question_id": 13490,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras",
    "title": "How to set class weights for imbalanced classes in Keras?",
    "body": "<p>I know that there is a possibility in Keras with the <code>class_weights</code> parameter dictionary at fitting, but I couldn't find any example. Would somebody so kind to provide one?</p>\n\n<p>By the way, in this case the appropriate praxis is simply to weight up the minority class proportionally to its underrepresentation?</p>\n"
  },
  {
    "tags": [
      "python",
      "scikit-learn"
    ],
    "owner": {
      "account_id": 7545308,
      "reputation": 2967,
      "user_id": 15064,
      "user_type": "registered",
      "accept_rate": 75,
      "profile_image": "https://www.gravatar.com/avatar/9cdd4d966d919fd8105714d75ce1f27f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Kaggle",
      "link": "https://datascience.stackexchange.com/users/15064/kaggle"
    },
    "is_answered": true,
    "view_count": 359506,
    "protected_date": 1618181089,
    "accepted_answer_id": 12346,
    "answer_count": 10,
    "score": 249,
    "last_activity_date": 1625731749,
    "creation_date": 1466503508,
    "last_edit_date": 1625731749,
    "question_id": 12321,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/12321/whats-the-difference-between-fit-and-fit-transform-in-scikit-learn-models",
    "title": "What&#39;s the difference between fit and fit_transform in scikit-learn models?",
    "body": "<p>I do not understand the difference between the <code>fit</code> and <code>fit_transform</code> methods in scikit-learn. Can anybody explain simply why we might need to transform data?</p>\n<p>What does it mean, fitting a model on training data and transforming to test data? Does it mean, for example, converting categorical variables into numbers in training and transforming the new feature set onto test data?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "scikit-learn",
      "cross-validation"
    ],
    "owner": {
      "account_id": 8700074,
      "reputation": 8747,
      "user_id": 21560,
      "user_type": "registered",
      "accept_rate": 36,
      "profile_image": "https://www.gravatar.com/avatar/349cff1d621bb8779315322f202fba5b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Hendrik",
      "link": "https://datascience.stackexchange.com/users/21560/hendrik"
    },
    "is_answered": true,
    "view_count": 449656,
    "accepted_answer_id": 15136,
    "answer_count": 18,
    "score": 205,
    "last_activity_date": 1718810695,
    "creation_date": 1479221704,
    "last_edit_date": 1670256784,
    "question_id": 15135,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn",
    "title": "Train/Test/Validation Set Splitting in Sklearn",
    "body": "<p>How could I randomly split a data matrix and the corresponding label vector into a <code>X_train</code>, <code>X_test</code>, <code>X_val</code>, <code>y_train</code>, <code>y_test</code>, <code>y_val</code> with scikit-learn?</p>\n<p>As far as I know, <code>sklearn.model_selection.train_test_split</code> is only capable of splitting into two not into three...</p>\n"
  },
  {
    "tags": [
      "open-source",
      "dataset"
    ],
    "owner": {
      "account_id": 210958,
      "reputation": 1393,
      "user_id": 227,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/bf9facac0cf9c57c1c4d1e02c63ab6f5?s=256&d=identicon&r=PG",
      "display_name": "Amir Ali Akbari",
      "link": "https://datascience.stackexchange.com/users/227/amir-ali-akbari"
    },
    "is_answered": true,
    "view_count": 33523,
    "protected_date": 1528317241,
    "accepted_answer_id": 158,
    "answer_count": 35,
    "community_owned_date": 1458123130,
    "score": 203,
    "last_activity_date": 1656655070,
    "creation_date": 1400438738,
    "last_edit_date": 1480977233,
    "question_id": 155,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/155/publicly-available-datasets",
    "title": "Publicly Available Datasets",
    "body": "<p>One of the common problems in data science is gathering data from various sources in a somehow cleaned (semi-structured) format and combining metrics from various sources for making a higher level analysis. Looking at the other people's effort, especially other questions on this site, it appears that many people in this field are doing somewhat repetitive work. For example analyzing tweets, facebook posts, Wikipedia articles etc. is a part of a lot of big data problems.</p>\n\n<p>Some of these data sets are accessible using public APIs provided by the provider site, but usually, some valuable information or metrics are missing from these APIs and everyone has to do the same analyses again and again. For example, although clustering users may depend on different use cases and selection of features, but having a base clustering of Twitter/Facebook users can be useful in many Big Data applications, which is neither provided by the API nor available publicly in independent data sets.</p>\n\n<p>Is there any index or publicly available data set hosting site containing valuable data sets that can be reused in solving other big data problems? I mean something like GitHub (or a group of sites/public datasets or at least a comprehensive listing) for the data science. If not, what are the reasons for not having such a platform for data science? The commercial value of data, need to frequently update data sets, ...? Can we not have an open-source model for sharing data sets devised for data scientists?</p>\n"
  },
  {
    "tags": [
      "data-mining",
      "clustering",
      "octave",
      "k-means",
      "categorical-data"
    ],
    "owner": {
      "account_id": 241216,
      "reputation": 5474,
      "user_id": 97,
      "user_type": "registered",
      "accept_rate": 77,
      "profile_image": "https://www.gravatar.com/avatar/68acdc33e278ab19debf230662a7ad96?s=256&d=identicon&r=PG",
      "display_name": "IgorS",
      "link": "https://datascience.stackexchange.com/users/97/igors"
    },
    "is_answered": true,
    "view_count": 316795,
    "accepted_answer_id": 24,
    "answer_count": 13,
    "score": 202,
    "last_activity_date": 1714727101,
    "creation_date": 1400047101,
    "last_edit_date": 1714727079,
    "question_id": 22,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data",
    "title": "K-Means clustering for mixed numeric and categorical data",
    "body": "<p>My data set contains a number of numeric attributes and one categorical.</p>\n<p>Say, <code>NumericAttr1, NumericAttr2, ..., NumericAttrN, CategoricalAttr</code>,</p>\n<p>where <code>CategoricalAttr</code> takes one of three possible values: <code>CategoricalAttrValue1</code>, <code>CategoricalAttrValue2</code> or <code>CategoricalAttrValue3</code>.</p>\n<p>I'm using default <a href=\"https://octave.sourceforge.io/statistics/function/kmeans.html\" rel=\"nofollow noreferrer\">k-means clustering algorithm implementation for Octave</a>.\nIt works with numeric data only.</p>\n<p>So my question: is it correct to split the categorical attribute <code>CategoricalAttr</code> into three numeric (binary) variables, like <code>IsCategoricalAttrValue1, IsCategoricalAttrValue2, IsCategoricalAttrValue3</code> ?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning"
    ],
    "owner": {
      "account_id": 2901835,
      "reputation": 4125,
      "user_id": 793,
      "user_type": "registered",
      "accept_rate": 60,
      "profile_image": "https://i.sstatic.net/N5kqP.jpg?s=256",
      "display_name": "tejaskhot",
      "link": "https://datascience.stackexchange.com/users/793/tejaskhot"
    },
    "is_answered": true,
    "view_count": 163947,
    "accepted_answer_id": 5734,
    "answer_count": 6,
    "score": 200,
    "last_activity_date": 1722921586,
    "creation_date": 1430971916,
    "last_edit_date": 1514349496,
    "question_id": 5706,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks",
    "title": "What is the &quot;dying ReLU&quot; problem in neural networks?",
    "body": "<p>Referring to the Stanford course notes on <a href=\"http://cs231n.github.io/neural-networks-1/#actfun\" rel=\"noreferrer\">Convolutional Neural Networks for Visual Recognition</a>, a paragraph says:</p>\n\n<blockquote>\n  <p>\"Unfortunately, ReLU units can be fragile during training and can\n  \"die\". For example, a large gradient flowing through a ReLU neuron\n  could cause the weights to update in such a way that the neuron will\n  never activate on any datapoint again. If this happens, then the\n  gradient flowing through the unit will forever be zero from that point\n  on. That is, the ReLU units can irreversibly die during training since\n  they can get knocked off the data manifold. For example, you may find\n  that as much as 40% of your network can be \"dead\" (i.e. neurons that\n  never activate across the entire training dataset) if the learning\n  rate is set too high. With a proper setting of the learning rate this\n  is less frequently an issue.\"</p>\n</blockquote>\n\n<p>What does dying of neurons here mean? </p>\n\n<p>Could you please provide an intuitive explanation in simpler terms.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "svm",
      "software-recommendation"
    ],
    "owner": {
      "account_id": 8286018,
      "reputation": 2509,
      "user_id": 20618,
      "user_type": "registered",
      "accept_rate": 29,
      "profile_image": "https://www.gravatar.com/avatar/7e70a2b9967260cc5fb6bd03a6f3677c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Muhammad Ali",
      "link": "https://datascience.stackexchange.com/users/20618/muhammad-ali"
    },
    "is_answered": true,
    "view_count": 390575,
    "protected_date": 1589309965,
    "accepted_answer_id": 130354,
    "answer_count": 7,
    "score": 200,
    "last_activity_date": 1727327355,
    "creation_date": 1478142624,
    "last_edit_date": 1540451693,
    "question_id": 14899,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/14899/how-to-draw-deep-learning-network-architecture-diagrams",
    "title": "How to draw Deep learning network architecture diagrams?",
    "body": "<p>I have built my model. Now I want to draw the network architecture diagram for my research paper. Example is shown below:</p>\n\n<p><a href=\"https://i.sstatic.net/zyIUI.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/zyIUI.png\" alt=\"enter image description here\"></a></p>\n\n<p><a href=\"https://i.sstatic.net/CHuCF.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/CHuCF.png\" alt=\"enter image description here\"></a></p>\n"
  },
  {
    "tags": [
      "python",
      "pandas",
      "dataframe"
    ],
    "owner": {
      "account_id": 12899103,
      "reputation": 2403,
      "user_id": 46740,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/2210822062268656/picture?type=large",
      "display_name": "Vaibhav Thakur",
      "link": "https://datascience.stackexchange.com/users/46740/vaibhav-thakur"
    },
    "is_answered": true,
    "view_count": 242222,
    "accepted_answer_id": 37879,
    "answer_count": 2,
    "score": 196,
    "last_activity_date": 1713743749,
    "creation_date": 1536228841,
    "last_edit_date": 1587477394,
    "question_id": 37878,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/37878/difference-between-isna-and-isnull-in-pandas",
    "title": "Difference between isna() and isnull() in pandas",
    "body": "<p>I have been using pandas for quite some time. But, I don't understand what's the difference between <code>isna()</code> and <code>isnull()</code>. And, more importantly, which one to use when identifying missing values in a dataframe.</p>\n\n<p>What is the basic underlying difference of how a value is detected as either <code>na</code> or <code>null</code>?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "visualization"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 271818,
    "answer_count": 21,
    "score": 185,
    "last_activity_date": 1721836167,
    "creation_date": 1468861697,
    "last_edit_date": 1516622478,
    "question_id": 12851,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/12851/how-do-you-visualize-neural-network-architectures",
    "title": "How do you visualize neural network architectures?",
    "body": "<p>When writing a paper / making a presentation about a topic which is about neural networks, one usually visualizes the networks architecture.</p>\n\n<p>What are good / simple ways to visualize common architectures automatically?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "deep-learning",
      "lstm",
      "gru"
    ],
    "owner": {
      "account_id": 8885637,
      "reputation": 2101,
      "user_id": 23710,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-_Ta3cjF1uhk/AAAAAAAAAAI/AAAAAAAAAk8/V1id8gyx0PE/s256-rj/photo.jpg",
      "display_name": "Sayali Sonawane",
      "link": "https://datascience.stackexchange.com/users/23710/sayali-sonawane"
    },
    "is_answered": true,
    "view_count": 188452,
    "protected_date": 1593517106,
    "answer_count": 6,
    "score": 184,
    "last_activity_date": 1661551715,
    "creation_date": 1476704865,
    "last_edit_date": 1575800188,
    "question_id": 14581,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/14581/when-to-use-gru-over-lstm",
    "title": "When to use GRU over LSTM?",
    "body": "<p>The key difference between a GRU and an LSTM is that a GRU has two gates (<em>reset</em> and <em>update</em> gates) whereas an LSTM has three gates (namely <em>input</em>, <em>output</em> and <em>forget</em> gates). </p>\n\n<p>Why do we make use of GRU when we clearly have more control on the network through the LSTM model (as we have three gates)? In which scenario GRU is preferred over LSTM? </p>\n"
  },
  {
    "tags": [
      "scikit-learn",
      "categorical-data",
      "feature-engineering"
    ],
    "owner": {
      "account_id": 160820,
      "reputation": 1893,
      "user_id": 10462,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0f87063cd5f4bfb15c83497e07b638f6?s=256&d=identicon&r=PG",
      "display_name": "anthr",
      "link": "https://datascience.stackexchange.com/users/10462/anthr"
    },
    "is_answered": true,
    "view_count": 128986,
    "protected_date": 1612445504,
    "accepted_answer_id": 9447,
    "answer_count": 4,
    "score": 177,
    "last_activity_date": 1645395476,
    "creation_date": 1450553435,
    "last_edit_date": 1545958239,
    "question_id": 9443,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor",
    "title": "When to use One Hot Encoding vs LabelEncoder vs DictVectorizor?",
    "body": "<p>I have been building models with categorical data for a while now and when in this situation I basically default to using scikit-learn's LabelEncoder function to transform this data prior to building a model.</p>\n\n<p>I understand the difference between <code>OHE</code>, <code>LabelEncoder</code> and <code>DictVectorizor</code> in terms of what they are doing to the data, but what is not clear to me is when you might choose to employ one technique over another.  </p>\n\n<p>Are there certain algorithms or situations in which one has advantages/disadvantages with respect to the others?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "tensorflow"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 166973,
    "answer_count": 6,
    "score": 153,
    "last_activity_date": 1631377364,
    "creation_date": 1449728568,
    "last_edit_date": 1524165448,
    "question_id": 9302,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9302/the-cross-entropy-error-function-in-neural-networks",
    "title": "The cross-entropy error function in neural networks",
    "body": "<p>In the <a href=\"https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html\" rel=\"noreferrer\">MNIST For ML Beginners</a> they define cross-entropy as</p>\n\n<p>$$H_{y'} (y) := - \\sum_{i} y_{i}' \\log (y_i)$$</p>\n\n<p>$y_i$ is the predicted probability value for class $i$ and $y_i'$ is the true probability for that class.</p>\n\n<h2>Question 1</h2>\n\n<p>Isn't it a problem that $y_i$ (in $\\log(y_i)$) could be 0? This would mean that we have a really bad classifier, of course. But think of an error in our dataset, e.g. an \"obvious\" <code>1</code> labeled as <code>3</code>. Would it simply crash? Does the model we chose (softmax activation at the end) basically never give the probability 0 for the correct class?</p>\n\n<h2>Question 2</h2>\n\n<p>I've learned that cross-entropy is defined as </p>\n\n<p>$$H_{y'}(y) := - \\sum_{i} ({y_i' \\log(y_i) + (1-y_i') \\log (1-y_i)})$$</p>\n\n<p>What is correct? Do you have any textbook references for either version? How do those functions differ in their properties (as error functions for neural networks)?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "neural-network"
    ],
    "owner": {
      "account_id": 3399912,
      "reputation": 1667,
      "user_id": 989,
      "user_type": "registered",
      "accept_rate": 29,
      "profile_image": "https://www.gravatar.com/avatar/4480675ad0ede0043e7caba04750d974?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "marcodena",
      "link": "https://datascience.stackexchange.com/users/989/marcodena"
    },
    "is_answered": true,
    "view_count": 126611,
    "protected_date": 1461824867,
    "accepted_answer_id": 695,
    "answer_count": 17,
    "community_owned_date": 1481558285,
    "score": 150,
    "last_activity_date": 1544571592,
    "creation_date": 1404760624,
    "last_edit_date": 1496079806,
    "question_id": 694,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/694/best-python-library-for-neural-networks",
    "title": "Best python library for neural networks",
    "body": "<p>I'm using Neural Networks to solve different Machine learning problems. I'm using Python and <a href=\"http://pybrain.org/\" rel=\"noreferrer\">pybrain</a> but this library is almost discontinued. Are there other good alternatives in Python?</p>\n"
  },
  {
    "tags": [
      "python",
      "pandas",
      "sql"
    ],
    "owner": {
      "account_id": 21138,
      "reputation": 611,
      "user_id": 13672,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/144ac0e81f73797307e5179df6ecd1f4?s=256&d=identicon&r=PG",
      "display_name": "vy32",
      "link": "https://datascience.stackexchange.com/users/13672/vy32"
    },
    "is_answered": true,
    "view_count": 101823,
    "accepted_answer_id": 34366,
    "answer_count": 13,
    "community_owned_date": 1550365048,
    "score": 149,
    "last_activity_date": 1735504102,
    "creation_date": 1531387551,
    "last_edit_date": 1598113761,
    "question_id": 34357,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/34357/why-do-people-prefer-pandas-to-sql",
    "title": "Why do people prefer Pandas to SQL?",
    "body": "<p>I've been using SQL since 1996, so I may be biased. I've used MySQL and SQLite 3 extensively, but have also used Microsoft SQL Server and Oracle.</p>\n\n<p>The vast majority of the operations I've seen done with Pandas can be done more easily with SQL. This includes filtering a dataset, selecting specific columns for display, applying a function to a values, and so on.</p>\n\n<p>SQL has the advantage of having an optimizer and data persistence. SQL also has error messages that are clear and understandable. Pandas has a somewhat cryptic API, in which sometimes it's appropriate to use a single <code>[ stuff ]</code>, other times you need <code>[[ stuff ]]</code>, and sometimes you need a <code>.loc</code>. Part of the complexity of Pandas arises from the fact that there is so much overloading going on. </p>\n\n<p>So I'm trying to understand why Pandas is so popular.</p>\n"
  },
  {
    "tags": [
      "r",
      "statistics",
      "correlation"
    ],
    "owner": {
      "account_id": 4179565,
      "reputation": 2078,
      "user_id": 1151,
      "user_type": "registered",
      "accept_rate": 0,
      "profile_image": "https://www.gravatar.com/avatar/b6c6c1670281f453f3cc9ccf3fbf7071?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "GeorgeOfTheRF",
      "link": "https://datascience.stackexchange.com/users/1151/georgeoftherf"
    },
    "is_answered": true,
    "view_count": 384808,
    "protected_date": 1506578680,
    "answer_count": 1,
    "score": 132,
    "last_activity_date": 1506523959,
    "creation_date": 1407071244,
    "last_edit_date": 1407454717,
    "question_id": 893,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab",
    "title": "How to get correlation between two categorical variable and a categorical variable and continuous variable?",
    "body": "<p>I am building a regression model and I need to calculate the below to check for correlations</p>\n\n<ol>\n<li>Correlation between 2 Multi level categorical variables</li>\n<li>Correlation between a Multi level categorical variable and\ncontinuous variable </li>\n<li>VIF(variance inflation factor) for a Multi\nlevel categorical variables</li>\n</ol>\n\n<p>I believe its wrong to use Pearson correlation coefficient for the above scenarios because Pearson only works for 2 continuous variables. </p>\n\n<p>Please answer the below questions</p>\n\n<ol>\n<li>Which correlation coefficient works best for the above cases ? </li>\n<li>VIF calculation only works for continuous data so what is the\nalternative? </li>\n<li>What are the assumptions I need to check before I use the correlation coefficient you suggest? </li>\n<li>How to implement them in SAS &amp; R?</li>\n</ol>\n"
  },
  {
    "tags": [
      "machine-learning",
      "r",
      "python"
    ],
    "owner": {
      "account_id": 4602394,
      "reputation": 159,
      "user_id": 721,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/614317946/picture?type=large",
      "display_name": "user721",
      "link": "https://datascience.stackexchange.com/users/721/user721"
    },
    "is_answered": true,
    "view_count": 121430,
    "protected_date": 1503557837,
    "answer_count": 14,
    "community_owned_date": 1437297375,
    "score": 128,
    "last_activity_date": 1657576249,
    "creation_date": 1402553088,
    "last_edit_date": 1560182218,
    "question_id": 326,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/326/python-vs-r-for-machine-learning",
    "title": "Python vs R for machine learning",
    "body": "<p>I'm just starting to develop a <a href=\"https://en.wikipedia.org/wiki/Machine_learning\" rel=\"noreferrer\">machine learning</a> application for academic purposes. I'm currently using <strong>R</strong> and training myself in it. However, in a lot of places, I have seen people using <strong>Python</strong>.</p>\n\n<p>What are people using in academia and industry, and what is the recommendation?</p>\n"
  },
  {
    "tags": [
      "python",
      "keras",
      "rnn",
      "training"
    ],
    "owner": {
      "account_id": 33264,
      "reputation": 1370,
      "user_id": 44256,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4c7cfbfaf33789fbea36bc5507c05c36?s=256&d=identicon&r=PG",
      "display_name": "Tac-Tics",
      "link": "https://datascience.stackexchange.com/users/44256/tac-tics"
    },
    "is_answered": true,
    "view_count": 117576,
    "answer_count": 2,
    "score": 125,
    "last_activity_date": 1573479034,
    "creation_date": 1515282080,
    "last_edit_date": 1545265933,
    "question_id": 26366,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/26366/training-an-rnn-with-examples-of-different-lengths-in-keras",
    "title": "Training an RNN with examples of different lengths in Keras",
    "body": "<p>I am trying to get started learning about RNNs and I'm using Keras. I understand the basic premise of vanilla RNN and LSTM layers, but I'm having trouble understanding a certain technical point for training.</p>\n\n<p>In the <a href=\"https://keras.io/layers/recurrent/\" rel=\"noreferrer\">keras documentation</a>, it says the input to an RNN layer must have shape <code>(batch_size, timesteps, input_dim)</code>. This suggests that all the training examples have a fixed sequence length, namely <code>timesteps</code>. </p>\n\n<p>But this is not especially typical, is it? I might want to have the RNN operate on sentences of varying lengths. When I train it on some corpus, I will feed it batches of sentences, all of different lengths.</p>\n\n<p>I suppose the obvious thing to do would be to find the max length of any sequence in the training set and zero pad it. But then does that mean I can't make predictions at test time with input length greater than that?</p>\n\n<p>This is a question about Keras's particular implementation, I suppose, but I'm also asking for what people typically do when faced with this kind of a problem in general.</p>\n"
  },
  {
    "tags": [
      "python",
      "svm",
      "scikit-learn"
    ],
    "owner": {
      "account_id": 2901835,
      "reputation": 4125,
      "user_id": 793,
      "user_type": "registered",
      "accept_rate": 60,
      "profile_image": "https://i.sstatic.net/N5kqP.jpg?s=256",
      "display_name": "tejaskhot",
      "link": "https://datascience.stackexchange.com/users/793/tejaskhot"
    },
    "is_answered": true,
    "view_count": 157892,
    "protected_date": 1612301570,
    "answer_count": 12,
    "score": 119,
    "last_activity_date": 1628677587,
    "creation_date": 1408358817,
    "last_edit_date": 1612729334,
    "question_id": 989,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/989/svm-using-scikit-learn-runs-endlessly-and-never-completes-execution",
    "title": "SVM using scikit learn runs endlessly and never completes execution",
    "body": "<p>I am trying to run SVR using scikit-learn (python) on a training dataset that has 595605 rows and 5 columns (features) while the test dataset has 397070 rows. The data has been pre-processed and regularized.</p>\n<p>I am able to successfully run the test examples, but on executing using my dataset and letting it run for over an hour, I could still not see any output or termination of the program. I tried executing using a different IDE and even from the terminal, but that does not seem to be the issue.\nI also tried changing the 'C' parameter value from 1 to 1e3.</p>\n<p>I am facing similar issues with all SVM implementations using scikit.</p>\n<p>Am I not waiting long enough for it to complete?\nHow much time should this execution take?</p>\n<p>From my experience, it should not require more than a few minutes.</p>\n<p>Here is my system configuration:\nUbuntu 14.04, 8GB RAM, lots of free memory, 4th gen i7 processor</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "optimization",
      "hyperparameter"
    ],
    "owner": {
      "account_id": 2360984,
      "reputation": 1854,
      "user_id": 890,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0c86e226759ee03937af84103fd1b1ab?s=256&d=identicon&r=PG",
      "display_name": "ragingSloth",
      "link": "https://datascience.stackexchange.com/users/890/ragingsloth"
    },
    "is_answered": true,
    "view_count": 129232,
    "accepted_answer_id": 414,
    "answer_count": 11,
    "score": 115,
    "last_activity_date": 1698128747,
    "creation_date": 1402942118,
    "last_edit_date": 1516201176,
    "question_id": 410,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/410/choosing-a-learning-rate",
    "title": "Choosing a learning rate",
    "body": "<p>I'm currently working on implementing Stochastic Gradient Descent, <code>SGD</code>, for neural nets using back-propagation, and while I understand its purpose I have some questions about how to choose values for the learning rate.</p>\n\n<ul>\n<li>Is the learning rate related to the shape of the error gradient, as it dictates the rate of descent?</li>\n<li>If so, how do you use this information to inform your decision about a value?</li>\n<li>If it's not what sort of values should I choose, and how should I choose them?</li>\n<li>It seems like you would want small values to avoid overshooting, but how do you choose one such that you don't get stuck in local minima or take to long to descend?</li>\n<li>Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?</li>\n</ul>\n\n<p>In short: How do I choose the learning rate for SGD?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "linear-regression",
      "loss-function"
    ],
    "owner": {
      "account_id": 1406232,
      "reputation": 1323,
      "user_id": 16148,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7ae9b432531de46073888f1a49c2391c?s=256&d=identicon&r=PG",
      "display_name": "Golo Roden",
      "link": "https://datascience.stackexchange.com/users/16148/golo-roden"
    },
    "is_answered": true,
    "view_count": 71997,
    "accepted_answer_id": 10192,
    "answer_count": 5,
    "score": 115,
    "last_activity_date": 1551092585,
    "creation_date": 1455141150,
    "last_edit_date": 1514827628,
    "question_id": 10188,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10188/why-do-cost-functions-use-the-square-error",
    "title": "Why do cost functions use the square error?",
    "body": "<p>I'm just getting started with some machine learning, and until now I have been dealing with linear regression over one variable.</p>\n\n<p>I have learnt that there is a hypothesis, which is:</p>\n\n<p>$h_\\theta(x)=\\theta_0+\\theta_1x$</p>\n\n<p>To find out good values for the parameters $\\theta_0$ and $\\theta_1$ we want to minimize the difference between the calculated result and the actual result of our test data. So we subtract</p>\n\n<p>$h_\\theta(x^{(i)})-y^{(i)}$</p>\n\n<p>for all $i$ from $1$ to $m$. Hence we calculate the sum over this difference and then calculate the average by multiplying the sum by $\\frac{1}{m}$. So far, so good. This would result in:</p>\n\n<p>$\\frac{1}{m}\\sum_{i=1}^mh_\\theta(x^{(i)})-y^{(i)}$</p>\n\n<p>But this is not what has been suggested. Instead the course suggests to take the square value of the difference, and to multiply by $\\frac{1}{2m}$. So the formula is:</p>\n\n<p>$\\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2$</p>\n\n<p>Why is that? Why do we use the square function here, and why do we multiply by $\\frac{1}{2m}$ instead of $\\frac{1}{m}$?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "decision-trees",
      "information-theory",
      "gini-index",
      "entropy"
    ],
    "owner": {
      "account_id": 5908135,
      "reputation": 1231,
      "user_id": 16236,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-T54Wrxm8Y_4/AAAAAAAAAAI/AAAAAAAAAHE/qSHB-FllIXY/s256-rj/photo.jpg",
      "display_name": "Krish Mahajan",
      "link": "https://datascience.stackexchange.com/users/16236/krish-mahajan"
    },
    "is_answered": true,
    "view_count": 142382,
    "answer_count": 9,
    "score": 113,
    "last_activity_date": 1689442760,
    "creation_date": 1455314741,
    "last_edit_date": 1689442760,
    "question_id": 10228,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/10228/when-should-i-use-gini-impurity-as-opposed-to-information-gain-entropy",
    "title": "When should I use Gini Impurity as opposed to Information Gain (Entropy)?",
    "body": "<p>Can someone practically explain the rationale behind <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity\" rel=\"noreferrer\">Gini impurity</a> vs <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain\" rel=\"noreferrer\">Information gain</a> (based on Entropy)?</p>\n\n<p>Which <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning#Metrics\" rel=\"noreferrer\">metric</a> is better to use in different scenarios while using decision trees?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "backpropagation"
    ],
    "owner": {
      "account_id": 4413633,
      "reputation": 1240,
      "user_id": 18632,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/QuNxD.jpg?s=256",
      "display_name": "shinvu",
      "link": "https://datascience.stackexchange.com/users/18632/shinvu"
    },
    "is_answered": true,
    "view_count": 89330,
    "accepted_answer_id": 11703,
    "answer_count": 6,
    "score": 111,
    "last_activity_date": 1723064982,
    "creation_date": 1463042292,
    "last_edit_date": 1463116197,
    "question_id": 11699,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/11699/backprop-through-max-pooling-layers",
    "title": "Backprop Through Max-Pooling Layers?",
    "body": "<p>This is a small conceptual question that's been nagging me for a while: How can we back-propagate through a max-pooling layer in a neural network?</p>\n\n<p>I came across max-pooling layers while going through <a href=\"https://github.com/soumith/cvpr2015/blob/master/Deep%20Learning%20with%20Torch.ipynb\" rel=\"noreferrer\">this tutorial</a> for Torch 7's nn library. The library abstracts the gradient calculation and forward passes for each layer of a deep network. I don't understand how the gradient calculation is done for a max-pooling layer.</p>\n\n<p>I know that if you have an input ${z_i}^l$ going into neuron $i$ of layer $l$, then ${\\delta_i}^l$ (defined as ${\\delta_i}^l = \\frac{\\partial E}{\\partial {z_i}^l}$) is given by:\n$$\n{\\delta_i}^l = \\theta^{'}({z_i}^l) \\sum_{j} {\\delta_j}^{l+1} w_{i,j}^{l,l+1}\n$$</p>\n\n<p>So, a max-pooling layer would receive the ${\\delta_j}^{l+1}$'s of the next layer as usual; but since the activation function for the max-pooling neurons takes in a vector of values (over which it maxes) as input, ${\\delta_i}^{l}$ isn't a single number anymore, but a vector ($\\theta^{'}({z_j}^l)$ would have to be replaced by $\\nabla \\theta(\\left\\{{z_j}^l\\right\\})$). Furthermore, $\\theta$, being the max function, isn't differentiable with respect to it's inputs. </p>\n\n<p>So....how should it work out exactly?</p>\n"
  },
  {
    "tags": [
      "nlp",
      "encoding",
      "attention-mechanism",
      "transformer"
    ],
    "owner": {
      "account_id": 9914032,
      "reputation": 1235,
      "user_id": 65027,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-6jIAe62Q10s/AAAAAAAAAAI/AAAAAAAAABw/kRBVixTmLmk/s256-rj/photo.jpg",
      "display_name": "Peyman",
      "link": "https://datascience.stackexchange.com/users/65027/peyman"
    },
    "is_answered": true,
    "view_count": 125136,
    "protected_date": 1619556984,
    "accepted_answer_id": 90038,
    "answer_count": 4,
    "score": 109,
    "last_activity_date": 1685622341,
    "creation_date": 1556462597,
    "last_edit_date": 1590380170,
    "question_id": 51065,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model",
    "title": "What is the positional encoding in the transformer model?",
    "body": "<p>I'm trying to read and understand the paper <a href=\"https://arxiv.org/abs/1706.03762\" rel=\"noreferrer\">Attention is all you need</a> and in it, there is a picture:</p>\n\n<p><a href=\"https://i.sstatic.net/BpxYv.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/BpxYv.png\" alt=\"enter image description here\"></a></p>\n\n<p>I don't know what <strong>positional encoding</strong> is. by listening to some youtube videos I've found out that it is an embedding having both meaning and position of a word in it and has something to do with <span class=\"math-container\">$sin(x)$</span> or <span class=\"math-container\">$cos(x)$</span></p>\n\n<p>but I couldn't understand what exactly it is and how exactly it is doing that. so I'm here for some help. thanks in advance.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "accuracy"
    ],
    "owner": {
      "account_id": 4568217,
      "reputation": 1093,
      "user_id": 2653,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/c1Tj8pgY.jpg?s=256",
      "display_name": "aidankmcl",
      "link": "https://datascience.stackexchange.com/users/2653/aidankmcl"
    },
    "is_answered": true,
    "view_count": 108465,
    "accepted_answer_id": 807,
    "answer_count": 4,
    "score": 99,
    "last_activity_date": 1683220581,
    "creation_date": 1406000600,
    "last_edit_date": 1683220581,
    "question_id": 806,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy",
    "title": "Advantages of AUC vs standard accuracy",
    "body": "<p>I was starting to look into area under curve(AUC) and am a little confused about its usefulness. When first explained to me, AUC seemed to be a great measure of performance but in my research I've found that some claim its advantage is mostly marginal in that it is best for catching 'lucky' models with high standard accuracy measurements and low AUC.</p>\n<p>So should I avoid relying on AUC for validating models or would a combination be best?</p>\n"
  },
  {
    "tags": [
      "python",
      "scikit-learn",
      "pandas",
      "random-forest",
      "python-3.x"
    ],
    "owner": {
      "account_id": 4962749,
      "reputation": 2785,
      "user_id": 17310,
      "user_type": "registered",
      "accept_rate": 25,
      "profile_image": "https://www.gravatar.com/avatar/6b2ddcbdf322159e5ff9fa987d2af7c9?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Edamame",
      "link": "https://datascience.stackexchange.com/users/17310/edamame"
    },
    "is_answered": true,
    "view_count": 446917,
    "protected_date": 1686756295,
    "accepted_answer_id": 11933,
    "answer_count": 10,
    "score": 96,
    "last_activity_date": 1677927899,
    "creation_date": 1464235984,
    "last_edit_date": 1631691764,
    "question_id": 11928,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/11928/valueerror-input-contains-nan-infinity-or-a-value-too-large-for-dtypefloat32",
    "title": "ValueError: Input contains NaN, infinity or a value too large for dtype(&#39;float32&#39;)",
    "body": "<p>I got ValueError when predicting test data using a RandomForest model.</p>\n\n<p>My code:</p>\n\n<pre><code>clf = RandomForestClassifier(n_estimators=10, max_depth=6, n_jobs=1, verbose=2)\nclf.fit(X_fit, y_fit)\n\ndf_test.fillna(df_test.mean())\nX_test = df_test.values  \ny_pred = clf.predict(X_test)\n</code></pre>\n\n<p>The error:</p>\n\n<pre><code>ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n</code></pre>\n\n<p>How do I find the bad values in the test dataset? Also, I do not want to drop these records, can I just replace them with the mean or median?</p>\n\n<p>Thanks.</p>\n"
  },
  {
    "tags": [
      "bigdata",
      "scalability",
      "efficiency",
      "performance"
    ],
    "owner": {
      "account_id": 1822136,
      "reputation": 4117,
      "user_id": 84,
      "user_type": "registered",
      "accept_rate": 82,
      "profile_image": "https://www.gravatar.com/avatar/5394d14f632e89b2dfc937e3660f0079?s=256&d=identicon&r=PG",
      "display_name": "Rubens",
      "link": "https://datascience.stackexchange.com/users/84/rubens"
    },
    "is_answered": true,
    "view_count": 20351,
    "protected_date": 1496218479,
    "accepted_answer_id": 37,
    "answer_count": 12,
    "score": 94,
    "last_activity_date": 1525179883,
    "creation_date": 1400039780,
    "last_edit_date": 1434053728,
    "question_id": 19,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/19/how-big-is-big-data",
    "title": "How big is big data?",
    "body": "<p>Lots of people use the term <em>big data</em> in a rather <em>commercial</em> way, as a means of indicating that large datasets are involved in the computation, and therefore potential solutions must have good performance. Of course, <em>big data</em> always carry associated terms, like scalability and efficiency, but what exactly defines a problem as a <em>big data</em> problem?</p>\n\n<p>Does the computation have to be related to some set of specific purposes, like data mining/information retrieval, or could an algorithm for general graph problems be labeled <em>big data</em> if the dataset was <em>big enough</em>? Also, how <em>big</em> is <em>big enough</em> (if this is possible to define)?</p>\n"
  },
  {
    "tags": [
      "classification"
    ],
    "owner": {
      "account_id": 109880,
      "reputation": 1023,
      "user_id": 2507,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/599c94001308cbe8e8b6d5e7ff8e9082?s=256&d=identicon&r=PG",
      "display_name": "ahoffer",
      "link": "https://datascience.stackexchange.com/users/2507/ahoffer"
    },
    "is_answered": true,
    "view_count": 97657,
    "protected_date": 1594903442,
    "accepted_answer_id": 773,
    "answer_count": 6,
    "score": 92,
    "last_activity_date": 1599234195,
    "creation_date": 1405459811,
    "question_id": 744,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/744/cosine-similarity-versus-dot-product-as-distance-metrics",
    "title": "Cosine similarity versus dot product as distance metrics",
    "body": "<p>It looks like the cosine similarity of two features is just their dot product scaled by the product of their magnitudes. When does cosine similarity make a better distance metric than the dot product? I.e. do the dot product and cosine similarity have different strengths or weaknesses in different situations?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "deep-learning",
      "normalization"
    ],
    "owner": {
      "account_id": 1641798,
      "reputation": 2236,
      "user_id": 9465,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b9700c79a6982daed72c137ed7091d7f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Rizky Luthfianto",
      "link": "https://datascience.stackexchange.com/users/9465/rizky-luthfianto"
    },
    "is_answered": true,
    "view_count": 90941,
    "accepted_answer_id": 13362,
    "answer_count": 1,
    "score": 91,
    "last_activity_date": 1694837143,
    "creation_date": 1469725949,
    "last_edit_date": 1493624864,
    "question_id": 13061,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/13061/when-to-use-he-or-glorot-normal-initialization-over-uniform-init-and-what-are",
    "title": "When to use (He or Glorot) normal initialization over uniform init? And what are its effects with Batch Normalization?",
    "body": "<p>I knew that Residual Network (ResNet) made He normal initialization popular. In ResNet, He normal initialization is used <s>, while the first layer uses He uniform initialization.</s></p>\n\n<p>I've looked through ResNet paper and \"Delving Deep into Rectifiers\" paper (He initialization paper), but I haven't found any mention on normal init vs uniform init.</p>\n\n<p>Also:</p>\n\n<blockquote>\n  <p>Batch Normalization allows us to use much higher learning rates and be less careful about initialization.</p>\n</blockquote>\n\n<p>In Batch Normalization paper's abstract, it is said that Batch Normalization allows us to be less careful about initialization.</p>\n\n<p><s>ResNet itself is still care on when to use normal init vs uniform init (rather than just go with the uniform init).</s></p>\n\n<p>So: </p>\n\n<ul>\n<li>When to use (He or Glorot) normal-distributed initialization over uniform initialization?</li>\n<li>What are normal-distributed initialization effects with Batch Normalization?</li>\n</ul>\n\n<p>Notes aside:</p>\n\n<ul>\n<li>It rhymes to use normal init with Batch Normalization, but I haven't found any paper to back this fact.</li>\n<li>I knew that ResNet uses He init over Glorot init because He init does better on a deep network.</li>\n<li>I've understood <a href=\"http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\" rel=\"noreferrer\">about Glorot init vs He init</a>.</li>\n<li>My question is about Normal vs Uniform init.</li>\n</ul>\n"
  },
  {
    "tags": [
      "machine-learning",
      "correlation"
    ],
    "owner": {
      "account_id": 6107354,
      "reputation": 1279,
      "user_id": 33386,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://i.sstatic.net/HzVLJ.jpg?s=256",
      "display_name": "Spider",
      "link": "https://datascience.stackexchange.com/users/33386/spider"
    },
    "is_answered": true,
    "view_count": 129713,
    "accepted_answer_id": 24453,
    "answer_count": 7,
    "score": 90,
    "last_activity_date": 1578475193,
    "creation_date": 1510065461,
    "last_edit_date": 1510183828,
    "question_id": 24452,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/24452/in-supervised-learning-why-is-it-bad-to-have-correlated-features",
    "title": "In supervised learning, why is it bad to have correlated features?",
    "body": "<p>I read somewhere that if we have features that are too correlated, we have to remove one, as this may worsen the model. It is clear that correlated features means that they bring the same information, so it is logical to remove one of them. But I can not understand why this can worsen the model. </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "scikit-learn",
      "random-forest",
      "decision-trees"
    ],
    "owner": {
      "account_id": 3597040,
      "reputation": 1015,
      "user_id": 8409,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/28ee736df1c6de2d9b5d45fb6623d4b9?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user3001408",
      "link": "https://datascience.stackexchange.com/users/8409/user3001408"
    },
    "is_answered": true,
    "view_count": 155714,
    "protected_date": 1603952351,
    "accepted_answer_id": 5229,
    "answer_count": 6,
    "score": 88,
    "last_activity_date": 1603952203,
    "creation_date": 1424826434,
    "last_edit_date": 1570026748,
    "question_id": 5226,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/5226/strings-as-features-in-decision-tree-random-forest",
    "title": "strings as features in decision tree/random forest",
    "body": "<p>I am doing some problems on an application of decision tree/random forest. I am trying to fit a problem which has numbers as well as strings (such as country name) as features. Now the library, <a href=\"http://scikit-learn.org\" rel=\"noreferrer\">scikit-learn</a> takes only numbers as parameters, but I want to inject the strings as well as they carry a significant amount of knowledge.</p>\n\n<p>How do I handle such a scenario?</p>\n\n<p>I can convert a string to numbers by some mechanism such as hashing in Python. But I would like to know the best practice on how strings are handled in decision tree problems.</p>\n"
  },
  {
    "tags": [
      "time-series",
      "deep-learning",
      "rnn",
      "prediction"
    ],
    "owner": {
      "account_id": 3367658,
      "reputation": 1085,
      "user_id": 20544,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e63c326aa3d4fa518d894063ed53318c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "ahajib",
      "link": "https://datascience.stackexchange.com/users/20544/ahajib"
    },
    "is_answered": true,
    "view_count": 69225,
    "answer_count": 8,
    "score": 87,
    "last_activity_date": 1691934210,
    "creation_date": 1468255521,
    "last_edit_date": 1469181187,
    "question_id": 12721,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/12721/time-series-prediction-using-arima-vs-lstm",
    "title": "Time series prediction using ARIMA vs LSTM",
    "body": "<p>The problem that I am dealing with is predicting time series values. I am looking at one time series at a time and based on for example 15% of the input data, I would like to predict its future values. So far I have come across two models:</p>\n\n<ul>\n<li><a href=\"http://deeplearning.net/tutorial/lstm.html\" rel=\"noreferrer\">LSTM</a> (long short term memory; a class of recurrent neural networks)</li>\n<li>ARIMA</li>\n</ul>\n\n<p>I have tried both and read some articles on them. Now I am trying to get a better sense on how to compare the two. What I have found so far:</p>\n\n<ol>\n<li>LSTM works better if we are dealing with huge amount of data and enough training data is available, while ARIMA is better for smaller datasets (is this correct?)</li>\n<li>ARIMA requires a series of parameters <code>(p,q,d)</code> which must be calculated based on data, while LSTM does not require setting such parameters. However, there are some hyperparameters we need to tune for LSTM.</li>\n<li><strong>EDIT:</strong> One major difference between the two that I noticed while reading a great article <a href=\"http://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/\" rel=\"noreferrer\">here</a>, is that ARIMA could only perform well on stationary time series (where there is no seasonality, trend and etc.) and you need to take care of that if want to use ARIMA</li>\n</ol>\n\n<p>Other than the above-mentioned properties, I could not find any other points or facts which could help me toward selecting the best model. I would be really grateful if someone could help me finding articles, papers or other stuff (had no luck so far, only some general opinions here and there and nothing based on experiments.)</p>\n\n<p>I have to mention that originally I am dealing with streaming data, however for now I am using <a href=\"https://github.com/numenta/NAB\" rel=\"noreferrer\">NAB datasets</a> which includes 50 datasets with the maximum size of 20k data points.</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "convolutional-neural-network"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 54881,
    "accepted_answer_id": 12833,
    "answer_count": 4,
    "score": 87,
    "last_activity_date": 1640974723,
    "creation_date": 1468761802,
    "last_edit_date": 1640974723,
    "question_id": 12830,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/12830/how-are-1x1-convolutions-the-same-as-a-fully-connected-layer",
    "title": "How are 1x1 convolutions the same as a fully connected layer?",
    "body": "<p>I recently read <a href=\"https://www.facebook.com/yann.lecun/posts/10152820758292143\" rel=\"noreferrer\">Yan LeCuns comment on 1x1 convolutions</a>:</p>\n<blockquote>\n<p><strong>In Convolutional Nets, there is no such thing as &quot;fully-connected layers&quot;. There are only convolution layers with 1x1 convolution kernels and a full connection table.</strong></p>\n<p>It's a too-rarely-understood fact that ConvNets don't need to have a fixed-size input.</p>\n<p>You can train them on inputs that happen to produce a single output vector (with no spatial extent), and then apply them to larger images.</p>\n<p>Instead of a single output vector, you then get a spatial map of output vectors.  Each vector sees input windows at different locations on the input.\n<strong>In that scenario, the &quot;fully connected layers&quot; really act as 1x1 convolutions.</strong></p>\n</blockquote>\n<p>I would like to see a simple example for this.</p>\n<h2>Example</h2>\n<p>Assume you have a fully connected network. It has only an input layer and an\noutput layer. The input layer has 3 nodes, the output layer has 2 nodes. This\nnetwork has <span class=\"math-container\">$3 \\cdot 2 = 6$</span> parameters. To make it even more concrete, lets say you have a ReLU activation function in the output layer and the weight matrix</p>\n<p><span class=\"math-container\">$$\n\\begin{align}\nW &amp;= \n\\begin{pmatrix}\n 0 &amp; 1 &amp; 1\\\\\n 2 &amp; 3 &amp; 5\\\\\n\\end{pmatrix} \\in \\mathbb{R}^{2 \\times 3}\\\\\nb &amp;= \\begin{pmatrix}8\\\\ 13\\end{pmatrix} \\in \\mathbb{R}^2\n\\end{align}\n$$</span></p>\n<p>So the network is <span class=\"math-container\">$f(x) = ReLU(W \\cdot x + b)$</span> with <span class=\"math-container\">$x \\in \\mathbb{R}^3$</span>.</p>\n<p><strong>How would the convolutional layer have to look like to be the same? What does LeCun mean with &quot;full connection table&quot;?</strong></p>\n<p>I guess to get an equivalent CNN it would have to have exactly the same number of parameters. The MLP from above has <span class=\"math-container\">$2 \\cdot 3 + 2 = 8$</span> parameters.</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "deep-learning",
      "convolution"
    ],
    "owner": {
      "account_id": 9650821,
      "reputation": 1003,
      "user_id": 27607,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e6f7455a9d4b2bc0d6b29a87f79092f5?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Aamir ",
      "link": "https://datascience.stackexchange.com/users/27607/aamir"
    },
    "is_answered": true,
    "view_count": 54361,
    "accepted_answer_id": 16084,
    "answer_count": 5,
    "score": 84,
    "last_activity_date": 1668704670,
    "creation_date": 1483519275,
    "last_edit_date": 1551677312,
    "question_id": 16060,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/16060/what-is-the-difference-between-equivariant-to-translation-and-invariant-to-tr",
    "title": "What is the difference between &quot;equivariant to translation&quot; and &quot;invariant to translation&quot;",
    "body": "<p>I'm having trouble understanding the difference between <em>equivariant to translation</em> and <em>invariant to translation</em>.</p>\n\n<p>In the book <a href=\"http://www.deeplearningbook.org/\" rel=\"noreferrer\">Deep Learning</a>. MIT Press, 2016 (I. Goodfellow, A. Courville, and Y. Bengio), one can find on the convolutional networks:</p>\n\n<ul>\n<li>[...] the particular form of parameter sharing causes the layer to have a property called <em>equivariance</em> to translation </li>\n<li>[...] pooling helps to make the representation become approximately <em>invariant</em> to small translations of the input</li>\n</ul>\n\n<p>Is there any difference between them or are the terms interchangeably used?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "algorithms",
      "xgboost",
      "ensemble-modeling",
      "gbm"
    ],
    "owner": {
      "account_id": 9079660,
      "reputation": 997,
      "user_id": 28834,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8b72b8a089377e38015d8a396ad8c3ad?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Aman ",
      "link": "https://datascience.stackexchange.com/users/28834/aman"
    },
    "is_answered": true,
    "view_count": 128901,
    "protected_date": 1586279046,
    "accepted_answer_id": 16908,
    "answer_count": 5,
    "score": 83,
    "last_activity_date": 1586267569,
    "creation_date": 1486843403,
    "last_edit_date": 1559717611,
    "question_id": 16904,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/16904/gbm-vs-xgboost-key-differences",
    "title": "GBM vs XGBOOST? Key differences?",
    "body": "<p>I am trying to understand the key differences between GBM and XGBOOST. I tried to google it, but could not find any good answers explaining the differences between the two algorithms and why xgboost almost always performs better than GBM. What makes XGBOOST so fast?</p>\n"
  },
  {
    "tags": [
      "machine-learning"
    ],
    "owner": {
      "account_id": 266740,
      "reputation": 4199,
      "user_id": 12515,
      "user_type": "registered",
      "accept_rate": 37,
      "profile_image": "https://i.sstatic.net/G0s1w.jpg?s=256",
      "display_name": "Ryan Zotti",
      "link": "https://datascience.stackexchange.com/users/12515/ryan-zotti"
    },
    "is_answered": true,
    "view_count": 32390,
    "protected_date": 1691655897,
    "answer_count": 9,
    "score": 82,
    "last_activity_date": 1691684309,
    "creation_date": 1519107304,
    "last_edit_date": 1519133270,
    "question_id": 28006,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/28006/data-scientist-vs-machine-learning-engineer",
    "title": "Data scientist vs machine learning engineer",
    "body": "<p>What are the differences, if any, between a \"data scientist\" and a \"machine learning engineer\"? </p>\n\n<p>Over the past year or so \"machine learning engineer\" has started to show up a lot in job postings. This is particularly noticeable in San Francisco, which is arguably where the term \"data scientist\" originated. At one point \"data scientist\" overtook \"statistician\", and I'm wondering if the same is now slowly beginning to happen to \"data scientist\".</p>\n\n<p>Career advice is listed as off-topic on this site, but I view my question as highly relevant since I'm asking about definitions; I'm not asking about recommendations given my own career trajectory or personal circumstances like other off-topic questions have. </p>\n\n<p>This question is on-topic because it might someday have significant implications for many users of this site. In fact, this stack-exchange site might not exist if the \"statistician\" vs \"data scientist\" evolution had not occurred. In that sense, this is a rather pertinent, potentially existential question. </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "softmax"
    ],
    "owner": {
      "account_id": 6871697,
      "reputation": 3091,
      "user_id": 15412,
      "user_type": "registered",
      "accept_rate": 86,
      "profile_image": "https://www.gravatar.com/avatar/2e213ca2cb638ccd192024a844956b15?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "enterML",
      "link": "https://datascience.stackexchange.com/users/15412/enterml"
    },
    "is_answered": true,
    "view_count": 157567,
    "protected_date": 1641888894,
    "accepted_answer_id": 20301,
    "answer_count": 6,
    "score": 80,
    "last_activity_date": 1733700780,
    "creation_date": 1499682399,
    "last_edit_date": 1608845112,
    "question_id": 20296,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation",
    "title": "Cross-entropy loss explanation",
    "body": "<p>Suppose I build a neural network for classification. The last layer is a dense layer with Softmax activation. I have five different classes to classify. Suppose for a single training example, the <code>true label</code> is <code>[1 0 0 0 0]</code> while the predictions be <code>[0.1 0.5 0.1 0.1 0.2]</code>. How would I calculate the cross entropy loss for this example?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "gradient-descent"
    ],
    "owner": {
      "account_id": 10594602,
      "reputation": 1109,
      "user_id": 57082,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b557ebdf54e104f505c0b86a7c5dc877?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Developer",
      "link": "https://datascience.stackexchange.com/users/57082/developer"
    },
    "is_answered": true,
    "view_count": 173314,
    "answer_count": 7,
    "score": 79,
    "last_activity_date": 1724392289,
    "creation_date": 1533364564,
    "last_edit_date": 1559965251,
    "question_id": 36450,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/36450/what-is-the-difference-between-gradient-descent-and-stochastic-gradient-descent",
    "title": "What is the difference between Gradient Descent and Stochastic Gradient Descent?",
    "body": "<p>What is the difference between Gradient Descent and Stochastic Gradient Descent? </p>\n\n<p>I am not very familiar with these, can you describe the difference with a short example?</p>\n"
  },
  {
    "tags": [
      "python",
      "anaconda"
    ],
    "owner": {
      "account_id": 8700074,
      "reputation": 8747,
      "user_id": 21560,
      "user_type": "registered",
      "accept_rate": 36,
      "profile_image": "https://www.gravatar.com/avatar/349cff1d621bb8779315322f202fba5b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Hendrik",
      "link": "https://datascience.stackexchange.com/users/21560/hendrik"
    },
    "is_answered": true,
    "view_count": 208350,
    "accepted_answer_id": 24096,
    "answer_count": 10,
    "score": 77,
    "last_activity_date": 1709801925,
    "creation_date": 1509021387,
    "last_edit_date": 1560990829,
    "question_id": 24093,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/24093/how-to-clone-python-working-environment-on-another-machine",
    "title": "How to clone Python working environment on another machine?",
    "body": "<p>I developed a machine learning model with Python (Anaconda + Flask) on my workstation and all goes well. Later, I tried to ship this program onto another machine where of course I tried to set up the same environment, but the program fails to run. I copied the program to other machines where it also runs smoothly.</p>\n\n<p>I cannot figure out what the problem is in the failed case (both the program code and the error message are copious so I am not able to present them here) but I'm almost certain that it is something with the different versions of the dependencies.</p>\n\n<p>So, my question is that given an environment where a certain program runs well, how can I clone it to another where it should run well also? Of course, without the cloning of the full system ;)</p>\n"
  },
  {
    "tags": [
      "pandas"
    ],
    "owner": {
      "account_id": 8532730,
      "reputation": 871,
      "user_id": 44204,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/10206893710564335/picture?type=large",
      "display_name": "Aravind Veluchamy",
      "link": "https://datascience.stackexchange.com/users/44204/aravind-veluchamy"
    },
    "is_answered": true,
    "view_count": 221013,
    "protected_date": 1633961873,
    "answer_count": 4,
    "score": 77,
    "last_activity_date": 1572468821,
    "creation_date": 1515177633,
    "last_edit_date": 1528224680,
    "question_id": 26333,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/26333/convert-a-list-of-lists-into-a-pandas-dataframe",
    "title": "Convert a list of lists into a Pandas Dataframe",
    "body": "<p>I am trying to convert a list of lists which looks like the following into a Pandas Dataframe</p>\n\n<pre><code>[['New York Yankees ', '\"Acevedo Juan\"  ', 900000, ' Pitcher\\n'], \n['New York Yankees ', '\"Anderson Jason\"', 300000, ' Pitcher\\n'], \n['New York Yankees ', '\"Clemens Roger\" ', 10100000, ' Pitcher\\n'], \n['New York Yankees ', '\"Contreras Jose\"', 5500000, ' Pitcher\\n']]\n</code></pre>\n\n<p>I am basically trying to convert each item in the array into a pandas data frame which has four columns. What would be the best approach to this as pd.Dataframe does not quite give me what I am looking for.</p>\n"
  },
  {
    "tags": [
      "nlp",
      "sentiment-analysis",
      "bert",
      "language-model",
      "text-classification"
    ],
    "owner": {
      "account_id": 4651817,
      "reputation": 967,
      "user_id": 42519,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d9893bdf312ee773e00fba1995def22b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user3768495",
      "link": "https://datascience.stackexchange.com/users/42519/user3768495"
    },
    "is_answered": true,
    "view_count": 118186,
    "accepted_answer_id": 66209,
    "answer_count": 4,
    "score": 75,
    "last_activity_date": 1624862789,
    "creation_date": 1578590410,
    "last_edit_date": 1613745012,
    "question_id": 66207,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/66207/what-is-purpose-of-the-cls-token-and-why-is-its-encoding-output-important",
    "title": "What is purpose of the [CLS] token and why is its encoding output important?",
    "body": "<p>I am reading <a href=\"http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\" rel=\"noreferrer\">this article on how to use BERT</a> by Jay Alammar and I understand things up until:</p>\n\n<blockquote>\n  <p>For sentence classification, were only only interested in BERTs output for the [CLS] token, so we select that slice of the cube and discard everything else.</p>\n</blockquote>\n\n<p>I have read <a href=\"https://datascience.stackexchange.com/questions/46312/what-is-the-vector-value-of-cls-sep-tokens-in-bert\">this topic</a>, but still have some questions:</p>\n\n<p>Isn't the [CLS] token at the very beginning of each sentence? Why is that \"we are only interested in BERT's output for the [CLS] token\"? Can anyone help me get my head around this? Thanks!</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "data-mining",
      "anomaly-detection",
      "library"
    ],
    "owner": {
      "account_id": 2579280,
      "reputation": 943,
      "user_id": 1406,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/gwbFC.jpg?s=256",
      "display_name": "ximiki",
      "link": "https://datascience.stackexchange.com/users/1406/ximiki"
    },
    "is_answered": true,
    "view_count": 83677,
    "answer_count": 7,
    "score": 74,
    "last_activity_date": 1603620465,
    "creation_date": 1437575218,
    "last_edit_date": 1603620465,
    "question_id": 6547,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/6547/open-source-anomaly-detection-in-python",
    "title": "Open source Anomaly Detection in Python",
    "body": "<p><strong>Problem Background:</strong>\nI am working on a project that involves log files similar to those found in the IT monitoring space (to my best understanding of IT space). These log files are time-series data, organized into hundreds/thousands of rows of various parameters. Each parameter is numeric (float) and there is a non-trivial/non-error value for each time point. My task is to monitor said log files for anomaly detection (spikes, falls, unusual patterns with some parameters being out of sync, strange 1st/2nd/etc. derivative behavior, etc.).</p>\n<p>On a similar assignment, I have tried Splunk with Prelert, but I am exploring open-source options at the moment.</p>\n<p><strong>Constraints:</strong>\nI am limiting myself to Python because I know it well, and would like to delay the switch to R and the associated learning curve. Unless there seems to be overwhelming support for R (or other languages/software), I would like to stick to Python for this task.</p>\n<p>Also, I am working in a Windows environment for the moment. I would like to continue to sandbox in Windows on small-sized log files but can move to Linux environment if needed.</p>\n<p><strong>Resources:</strong>\nI have checked out the following with dead-ends as results:</p>\n<ol>\n<li><p>Some info <a href=\"https://datascience.stackexchange.com/questions/5193/python-or-r-for-implementing-machine-learning-algorithms-for-fraud-detection\">here</a> is helpful, but unfortunately, I am struggling to find the right package because:</p>\n</li>\n<li><p>Twitter's &quot;AnomalyDetection&quot; is in R, and I want to stick to Python. Furthermore, the Python port <a href=\"https://github.com/nicolasmiller/pyculiarity\" rel=\"noreferrer\">pyculiarity</a> seems to cause issues in implementing in Windows environment for me.</p>\n</li>\n<li><p>Skyline, my next attempt, seems to have been pretty much discontinued (from <a href=\"https://github.com/etsy/skyline/issues/119\" rel=\"noreferrer\">github issues</a>). I haven't dived deep into this, given how little support there seems to be online.</p>\n</li>\n<li><p>scikit-learn I am still exploring, but this seems to be much more manual. The down-in-the-weeds approach is OK by me, but my background in learning tools is weak, so would like something like a black box for the technical aspects like algorithms, similar to Splunk+Prelert.</p>\n</li>\n</ol>\n<p><strong>Problem Definition and Questions:</strong>\nI am looking for open-source software that can help me with automating the process of anomaly detection from time-series log files in Python via packages or libraries.</p>\n<ol start=\"5\">\n<li>Do such things exist to assist with my immediate task, or are they imaginary in my mind?</li>\n<li>Can anyone assist with concrete steps to help me to my goal, including background fundamentals or concepts?</li>\n<li>Is this the best StackExchange community to ask in, or is Stats, Math, or even Security or Stackoverflow the better options?</li>\n</ol>\n<p><strong>EDIT [2015-07-23]</strong>\nNote that the latest update to <a href=\"https://github.com/nicolasmiller/pyculiarity\" rel=\"noreferrer\">pyculiarity</a> seems to be <a href=\"https://github.com/nicolasmiller/pyculiarity/issues/1\" rel=\"noreferrer\">fixed</a> for the Windows environment! I have yet to confirm, but should be another useful tool for the community.</p>\n<p><strong>EDIT [2016-01-19]</strong>\nA minor update. I had not time to work on this and research, but I am taking a step back to understand the fundamentals of this problem before continuing to research in specific details. For example, two concrete steps that I am taking are:</p>\n<ol>\n<li><p>Starting with the <a href=\"https://en.wikipedia.org/wiki/Anomaly_detection\" rel=\"noreferrer\">Wikipedia articles for anomaly detection</a>, understanding fully, and then either moving up or down in concept hierarchy of other linked Wikipedia articles, such as <a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\" rel=\"noreferrer\">this</a>, and then <a href=\"https://en.wikipedia.org/wiki/Machine_learning\" rel=\"noreferrer\">this</a>.</p>\n</li>\n<li><p>Exploring techniques in the great surveys done by Chandola et al 2009 <a href=\"http://www-users.cs.umn.edu/%7Ebanerjee/papers/09/anomaly.pdf\" rel=\"noreferrer\"><em>Anomaly Detection: A Survey</em></a> and Hodge et al 2004 <a href=\"http://eprints.whiterose.ac.uk/767/1/hodgevj4.pdf\" rel=\"noreferrer\"><em>A Survey of Outlier Detection Methodologies</em></a>.</p>\n</li>\n</ol>\n<p>Once the concepts are better understood (I hope to play around with toy examples as I go to develop the practical side as well), I hope to understand which open source Python tools are better suited for my problems.</p>\n<p><strong>EDIT [2020-02-04]</strong>\nIt has been a few years since I worked on this problem, and am no longer working on this project, so I will not be following or researching this area until further notice. Thank you very much to all for their input. I hope this discussion helps others that need guidance on anomaly detection work.</p>\n<p>FWIW, if I had to do the same project now with the same resources (few thousand USD in expenses), I would pursue the deep learning/neural network approach. The ability of the method to automatically learn structure and hierarchy via hidden layers would've been very appealing since we had lots of data and (now) could spend the money on cloud compute. I would still use Python though ;).</p>\n<p>Cheers!</p>\n"
  },
  {
    "tags": [
      "feature-selection",
      "feature-extraction",
      "dimensionality-reduction"
    ],
    "owner": {
      "account_id": 302279,
      "reputation": 2500,
      "user_id": 122,
      "user_type": "registered",
      "accept_rate": 62,
      "profile_image": "https://www.gravatar.com/avatar/0e9087f2672b0e4f28d91266acf9ce57?s=256&d=identicon&r=PG",
      "display_name": "alvas",
      "link": "https://datascience.stackexchange.com/users/122/alvas"
    },
    "is_answered": true,
    "view_count": 40355,
    "accepted_answer_id": 132,
    "answer_count": 11,
    "score": 72,
    "last_activity_date": 1612729953,
    "creation_date": 1400394375,
    "last_edit_date": 1612729953,
    "question_id": 130,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/130/what-is-dimensionality-reduction-what-is-the-difference-between-feature-selecti",
    "title": "What is dimensionality reduction? What is the difference between feature selection and extraction?",
    "body": "<p>From wikipedia:</p>\n<blockquote>\n<p>dimensionality reduction or dimension reduction is the process of\nreducing the number of random variables under consideration, and\ncan be divided into feature selection and feature extraction.</p>\n</blockquote>\n<p>What is the difference between feature selection and feature extraction?</p>\n<p>What is an example of dimensionality reduction in a Natural Language Processing task?</p>\n"
  },
  {
    "tags": [
      "data-mining",
      "svm",
      "state-of-the-art"
    ],
    "owner": {
      "account_id": 2274369,
      "reputation": 29368,
      "user_id": 836,
      "user_type": "registered",
      "accept_rate": 83,
      "profile_image": "https://www.gravatar.com/avatar/bd4f0d39f82be9c22f6f01c80d7ad800?s=256&d=identicon&r=PG",
      "display_name": "Neil Slater",
      "link": "https://datascience.stackexchange.com/users/836/neil-slater"
    },
    "is_answered": true,
    "view_count": 11922,
    "accepted_answer_id": 712,
    "answer_count": 2,
    "score": 71,
    "last_activity_date": 1597648294,
    "creation_date": 1404908542,
    "last_edit_date": 1597648294,
    "question_id": 711,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/711/are-support-vector-machines-still-considered-state-of-the-art-in-their-niche",
    "title": "Are Support Vector Machines still considered &quot;state of the art&quot; in their niche?",
    "body": "<p>This question is in response to a comment I saw on another question.</p>\n<p>The comment was regarding the Machine Learning course syllabus on Coursera, and along the lines of &quot;SVMs are not used so much nowadays&quot;.</p>\n<p>I have only just finished the relevant lectures myself, and my understanding of SVMs is that they are a robust and efficient learning algorithm for classification, and that when using a kernel, they have a &quot;niche&quot; covering number of features perhaps 10 to 1000 and number of training samples perhaps 100 to 10,000. The limit on training samples is because the core algorithm revolves around optimising results generated from a square matrix with dimensions based on number of training samples, not number of original features.</p>\n<p>So does the comment I saw refer some real change since the course was made, and if so, what is that change: A new algorithm that covers SVM's &quot;sweet spot&quot; just as well, better CPUs meaning SVM's computational advantages are not worth as much? Or is it perhaps opinion or personal experience of the commenter?</p>\n<p>I tried a search for e.g. &quot;are support vector machines out of fashion&quot; and found nothing to imply they were being dropped in favour of anything else.</p>\n<p>And Wikipedia has this: <a href=\"http://en.wikipedia.org/wiki/Support_vector_machine#Issues\" rel=\"noreferrer\">http://en.wikipedia.org/wiki/Support_vector_machine#Issues</a> . . . the main sticking point appears to be difficulty of interpreting the model. Which makes SVM fine for a black-box predicting engine, but not so good for generating insights. I don't see that as a major issue, just another minor thing to take into account when picking the right tool for the job (along with nature of the training data and learning task etc).</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "reinforcement-learning"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 91701,
    "accepted_answer_id": 10358,
    "answer_count": 7,
    "score": 71,
    "last_activity_date": 1732575468,
    "creation_date": 1453125085,
    "last_edit_date": 1512666069,
    "question_id": 9832,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9832/what-is-the-q-function-and-what-is-the-v-function-in-reinforcement-learning",
    "title": "What is the Q function and what is the V function in reinforcement learning?",
    "body": "<p>It seems to me that the $V$ function can be easily expressed by the $Q$ function and thus the $V$ function seems to be superfluous to me. However, I'm new to reinforcement learning so I guess I got something wrong.</p>\n\n<h2>Definitions</h2>\n\n<p>Q- and V-learning are in the context of <a href=\"https://en.wikipedia.org/wiki/Markov_decision_process#Definition\" rel=\"noreferrer\">Markov Decision Processes</a>. A <strong>MDP</strong> is a 5-tuple $(S, A, P, R, \\gamma)$ with</p>\n\n<ul>\n<li>$S$ is a set of states (typically finite)</li>\n<li>$A$ is a set of actions (typically finite)</li>\n<li>$P(s, s', a) = P(s_{t+1} = s' | s_t = s, a_t = a)$ is the probability to get from state $s$ to state $s'$ with action $a$.</li>\n<li>$R(s, s', a) \\in \\mathbb{R}$ is the immediate reward after going from state $s$ to state $s'$ with action $a$. (It seems to me that usually only $s'$ matters).</li>\n<li>$\\gamma \\in [0, 1]$ is called discount factor and determines if one focuses on immediate rewards ($\\gamma = 0$), the total reward ($\\gamma = 1$) or some trade-off.</li>\n</ul>\n\n<p>A <strong>policy $\\pi$</strong>, according to <a href=\"http://incompleteideas.net/book/the-book-2nd.html\" rel=\"noreferrer\">Reinforcement Learning: An Introduction</a> by Sutton and Barto is a function $\\pi: S \\rightarrow A$ (this could be probabilistic).</p>\n\n<p>According to <a href=\"http://www.cs.upc.edu/~mmartin/Ag4-4x.pdf\" rel=\"noreferrer\">Mario Martins slides</a>, the <strong>$V$ function</strong> is\n$$V^\\pi(s) = E_\\pi \\{R_t | s_t = s\\} = E_\\pi \\{\\sum_{k=0}^\\infty \\gamma^k r_{t+k+1} | s_t = s\\}$$\nand the <strong>Q function</strong> is\n$$Q^\\pi(s, a) = E_\\pi \\{R_t | s_t = s, a_t = a\\} = E_\\pi \\{\\sum_{k=0}^\\infty \\gamma^k r_{t+k+1} | s_t = s, a_t=a\\}$$</p>\n\n<h2>My thoughts</h2>\n\n<p>The $V$ function states what the expected overall value (not reward!) of a state $s$ under the policy $\\pi$ is.</p>\n\n<p>The $Q$ function states what the value of a state $s$ and an action $a$  under the policy $\\pi$ is.</p>\n\n<p>This means,\n$$Q^\\pi(s, \\pi(s)) = V^\\pi(s)$$</p>\n\n<p>Right? So why do we have the value function at all? (I guess I mixed up something)</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "mini-batch-gradient-descent"
    ],
    "owner": {
      "account_id": 8700074,
      "reputation": 8747,
      "user_id": 21560,
      "user_type": "registered",
      "accept_rate": 36,
      "profile_image": "https://www.gravatar.com/avatar/349cff1d621bb8779315322f202fba5b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Hendrik",
      "link": "https://datascience.stackexchange.com/users/21560/hendrik"
    },
    "is_answered": true,
    "view_count": 81674,
    "accepted_answer_id": 16818,
    "answer_count": 5,
    "score": 71,
    "last_activity_date": 1722472649,
    "creation_date": 1486471225,
    "last_edit_date": 1722472649,
    "question_id": 16807,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/16807/why-mini-batch-size-is-better-than-one-single-batch-with-all-training-data",
    "title": "Why mini batch size is better than one single &quot;batch&quot; with all training data?",
    "body": "<p>I often read that in case of Deep Learning models the usual practice is to apply mini batches (generally a small one, 32/64) over several training epochs. I cannot really fathom the reason behind this.</p>\n\n<p>Unless I'm mistaken, the batch size is the number of training instances let seen by the model during a training iteration; and epoch is a full turn when each of the training instances have been seen by the model. If so, I cannot see the advantage of iterate over an almost insignificant subset of the training instances several times in contrast with applying a \"max batch\" by expose all the available training instances in each turn to the model (assuming, of course, enough the memory). What is the advantage of this approach?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "time-series"
    ],
    "owner": {
      "account_id": 2618092,
      "reputation": 1235,
      "user_id": 26800,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://www.gravatar.com/avatar/e0436fe410b764e66a722f80275eb5e3?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Rjay155",
      "link": "https://datascience.stackexchange.com/users/26800/rjay155"
    },
    "is_answered": true,
    "view_count": 54559,
    "answer_count": 5,
    "score": 71,
    "last_activity_date": 1596623735,
    "creation_date": 1487715460,
    "last_edit_date": 1558022394,
    "question_id": 17099,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/17099/adding-features-to-time-series-model-lstm",
    "title": "Adding Features To Time Series Model LSTM",
    "body": "<p>have been reading up a bit on LSTM's and their use for time series and its been interesting but difficult at the same time. One thing I have had difficulties with understanding is the approach to adding additional features to what is already a list of time series features. Assuming you have your dataset up like this:</p>\n\n<p>t-3,t-2,t-1,Output</p>\n\n<p>Now lets say you know you have a feature that does affect the output but its not necessarily a time series feature, lets say its the weather outside. Is this something you can just add and the LSTM will be able to distinguish what is the time series aspect and what isnt?</p>\n"
  },
  {
    "tags": [
      "python",
      "pytorch"
    ],
    "owner": {
      "account_id": 8354153,
      "reputation": 1312,
      "user_id": 41058,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/98eef7d323b6ebdbce78c4239ac8c5cd?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "mausamsion",
      "link": "https://datascience.stackexchange.com/users/41058/mausamsion"
    },
    "is_answered": true,
    "view_count": 174456,
    "accepted_answer_id": 32654,
    "answer_count": 4,
    "score": 71,
    "last_activity_date": 1612731712,
    "creation_date": 1528186906,
    "last_edit_date": 1587550014,
    "question_id": 32651,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/32651/what-is-the-use-of-torch-no-grad-in-pytorch",
    "title": "What is the use of torch.no_grad in pytorch?",
    "body": "<p>I am new to pytorch and started with <a href=\"https://github.com/pytorch/tutorials/blob/master/beginner_source/examples_autograd/two_layer_net_autograd.py\" rel=\"noreferrer\">this</a> github code. I do not understand the comment in line 60-61 in the code <code>\"because weights have requires_grad=True, but we don't need to track this in autograd\"</code>. I understood that we mention <code>requires_grad=True</code> to the variables which we need to calculate the gradients for using autograd but what does it mean to be <code>\"tracked by autograd\"</code> ?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning"
    ],
    "owner": {
      "account_id": 6631713,
      "reputation": 14308,
      "user_id": 28175,
      "user_type": "registered",
      "accept_rate": 95,
      "profile_image": "https://i.sstatic.net/ojtVl.jpg?s=256",
      "display_name": "Green Falcon",
      "link": "https://datascience.stackexchange.com/users/28175/green-falcon"
    },
    "is_answered": true,
    "view_count": 107033,
    "accepted_answer_id": 24539,
    "answer_count": 11,
    "score": 70,
    "last_activity_date": 1681483198,
    "creation_date": 1510213335,
    "last_edit_date": 1559023013,
    "question_id": 24511,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/24511/why-should-the-data-be-shuffled-for-machine-learning-tasks",
    "title": "Why should the data be shuffled for machine learning tasks",
    "body": "<p>In machine learning tasks it is common to shuffle data and normalize it. The purpose of normalization is clear (for having same range of feature values). But, after struggling a lot, I did not find any valuable reason for shuffling data.</p>\n\n<p>I have read this post <a href=\"https://stats.stackexchange.com/a/180847/179078\">here</a> discussing when we need to shuffle data, but it is not obvious why we should shuffle the data. Furthermore, I have frequently seen in algorithms such as Adam or SGD where we need batch gradient descent (data should be separated to mini-batches and batch size has to be specified). It is vital according to this <a href=\"https://stats.stackexchange.com/questions/245502/shuffling-data-in-the-mini-batch-training-of-neural-network\">post</a> to shuffle data for each epoch to have different data for each batch. So, perhaps the data is shuffled and more importantly changed.</p>\n\n<p>Why do we do this?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "keras",
      "loss-function",
      "encoding"
    ],
    "owner": {
      "account_id": 8619416,
      "reputation": 803,
      "user_id": 63516,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7a0901ae0fc40244433e3df11ad097c2?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Master M",
      "link": "https://datascience.stackexchange.com/users/63516/master-m"
    },
    "is_answered": true,
    "view_count": 66647,
    "accepted_answer_id": 41923,
    "answer_count": 2,
    "score": 70,
    "last_activity_date": 1612731967,
    "creation_date": 1543645686,
    "last_edit_date": 1612731935,
    "question_id": 41921,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/41921/sparse-categorical-crossentropy-vs-categorical-crossentropy-keras-accuracy",
    "title": "Sparse_categorical_crossentropy vs categorical_crossentropy (keras, accuracy)",
    "body": "<p>Which is better for accuracy or are they the same?\nOf course, if you use <code>categorical_crossentropy</code> you use one hot encoding, and if you use <code>sparse_categorical_crossentropy</code> you encode as normal integers.\nAdditionally, when is one better than the other?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning"
    ],
    "owner": {
      "account_id": 8927925,
      "reputation": 793,
      "user_id": 39461,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/507855c7c792dd58cc97f4e48b37df0d?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Hans",
      "link": "https://datascience.stackexchange.com/users/39461/hans"
    },
    "is_answered": true,
    "view_count": 32840,
    "accepted_answer_id": 24112,
    "answer_count": 5,
    "score": 69,
    "last_activity_date": 1633956968,
    "creation_date": 1505886798,
    "last_edit_date": 1592305723,
    "question_id": 23159,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/23159/in-softmax-classifier-why-use-exp-function-to-do-normalization",
    "title": "In softmax classifier, why use exp function to do normalization?",
    "body": "<p><a href=\"https://stackoverflow.com/questions/17187507/why-use-softmax-as-opposed-to-standard-normalization\">Why use softmax as opposed to standard normalization?</a> In the comment area of the top answer of this question, @Kilian Batzner raised 2 questions which also confuse me a lot. It seems no one gives an explanation except numerical benefits.</p>\n<blockquote>\n<p>I get the reasons for using Cross-Entropy Loss, but how does that relate to the softmax? You said &quot;the softmax function can be seen as trying to minimize the cross-entropy between the predictions and the truth&quot;. Suppose, I would use standard / linear normalization, but still use the Cross-Entropy Loss. Then I would also try to minimize the Cross-Entropy. So how is the softmax linked to the Cross-Entropy except for the numerical benefits?</p>\n<p>As for the probabilistic view: what is the motivation for looking at log probabilities? The reasoning seems to be a bit like &quot;We use e^x in the softmax, because we interpret x as log-probabilties&quot;. With the same reasoning we could say, we use e^e^e^x in the softmax, because we interpret x as log-log-log-probabilities (Exaggerating here, of course). I get the numerical benefits of softmax, but what is the theoretical motivation for using it?</p>\n</blockquote>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "clustering",
      "k-means",
      "geospatial"
    ],
    "owner": {
      "account_id": 1317803,
      "reputation": 813,
      "user_id": 2533,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/bd81fb6370a901321913b1465efa8e51?s=256&d=identicon&r=PG",
      "display_name": "rokpoto.com",
      "link": "https://datascience.stackexchange.com/users/2533/rokpoto-com"
    },
    "is_answered": true,
    "view_count": 107838,
    "accepted_answer_id": 764,
    "answer_count": 9,
    "score": 67,
    "last_activity_date": 1673549407,
    "creation_date": 1405590641,
    "last_edit_date": 1494578340,
    "question_id": 761,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/761/clustering-geo-location-coordinates-lat-long-pairs",
    "title": "Clustering geo location coordinates (lat,long pairs)",
    "body": "<p>What is the right approach and clustering algorithm for geolocation clustering?</p>\n\n<p>I'm using the following code to cluster geolocation coordinates:</p>\n\n<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.vq import kmeans2, whiten\n\ncoordinates= np.array([\n           [lat, long],\n           [lat, long],\n            ...\n           [lat, long]\n           ])\nx, y = kmeans2(whiten(coordinates), 3, iter = 20)  \nplt.scatter(coordinates[:,0], coordinates[:,1], c=y);\nplt.show()\n</code></pre>\n\n<p>Is it right to use K-means for geolocation clustering, as it uses Euclidean distance, and not <a href=\"https://en.wikipedia.org/wiki/Haversine_formula\" rel=\"noreferrer\">Haversine formula</a> as a distance function?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "keras"
    ],
    "owner": {
      "account_id": 1375078,
      "reputation": 1200,
      "user_id": 17484,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://i.sstatic.net/RKmWz.jpg?s=256",
      "display_name": "hipoglucido",
      "link": "https://datascience.stackexchange.com/users/17484/hipoglucido"
    },
    "is_answered": true,
    "view_count": 79582,
    "accepted_answer_id": 12533,
    "answer_count": 4,
    "score": 66,
    "last_activity_date": 1699544509,
    "creation_date": 1467374054,
    "last_edit_date": 1560099044,
    "question_id": 12532,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/12532/does-batch-size-in-keras-have-any-effects-in-results-quality",
    "title": "Does batch_size in Keras have any effects in results&#39; quality?",
    "body": "<p>I am about to train a big LSTM network with 2-3 million articles and am struggling with Memory Errors (I use AWS EC2 g2x2large).</p>\n\n<p>I found out that one solution is to reduce the <code>batch_size</code>. However, I am not sure if this parameter is only related to memory efficiency issues or if it will effect my results. As a matter of fact, I also noticed that <code>batch_size</code> used in examples is usually as a power of two, which I don't understand either.</p>\n\n<p>I don't mind if my network takes longer to train, but I would like to know if reducing the <code>batch_size</code> will decrease the quality of my predictions.</p>\n\n<p>Thanks.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "classification",
      "keras"
    ],
    "owner": {
      "account_id": 1394768,
      "reputation": 1067,
      "user_id": 59446,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/49dbfd8834d69e780c017b8dc9e391e0?s=256&d=identicon&r=PG",
      "display_name": "ZelelB",
      "link": "https://datascience.stackexchange.com/users/59446/zelelb"
    },
    "is_answered": true,
    "view_count": 207577,
    "protected_date": 1617715533,
    "accepted_answer_id": 45166,
    "answer_count": 5,
    "score": 66,
    "last_activity_date": 1617674924,
    "creation_date": 1549459764,
    "question_id": 45165,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model",
    "title": "How to get accuracy, F1, precision and recall, for a keras model?",
    "body": "<p>I want to compute the precision, recall and F1-score for my binary KerasClassifier model, but don't find any solution. </p>\n\n<p>Here's my actual code: </p>\n\n<pre><code># Split dataset in train and test data \nX_train, X_test, Y_train, Y_test = train_test_split(normalized_X, Y, test_size=0.3, random_state=seed)\n\n# Build the model\nmodel = Sequential()\nmodel.add(Dense(23, input_dim=45, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n\ntensorboard = TensorBoard(log_dir=\"logs/{}\".format(time.time()))\n\ntime_callback = TimeHistory()\n\n# Fit the model\nhistory = model.fit(X_train, Y_train, validation_split=0.3, epochs=200, batch_size=5, verbose=1, callbacks=[tensorboard, time_callback]) \n</code></pre>\n\n<p>And then I am predicting on new test data, and getting the confusion matrix like this: </p>\n\n<pre><code>y_pred = model.predict(X_test)\ny_pred =(y_pred&gt;0.5)\nlist(y_pred)\n\ncm = confusion_matrix(Y_test, y_pred)\nprint(cm)\n</code></pre>\n\n<p>But is there any solution to get the accuracy-score, the F1-score, the precision, and the recall? (If not complicated, also the cross-validation-score, but not necessary for this answer)</p>\n\n<p>Thank you for any help! </p>\n"
  },
  {
    "tags": [
      "efficiency",
      "algorithms",
      "parameter"
    ],
    "owner": {
      "account_id": 216182,
      "reputation": 1932,
      "user_id": 158,
      "user_type": "registered",
      "accept_rate": 71,
      "profile_image": "https://www.gravatar.com/avatar/7e5775439438b07b6f14a697a17dc49b?s=256&d=identicon&r=PG",
      "display_name": "blunders",
      "link": "https://datascience.stackexchange.com/users/158/blunders"
    },
    "is_answered": true,
    "view_count": 43340,
    "answer_count": 6,
    "score": 65,
    "last_activity_date": 1587913393,
    "creation_date": 1402677869,
    "last_edit_date": 1492087841,
    "question_id": 361,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted",
    "title": "When is a Model Underfitted?",
    "body": "<p>Logic often states that by underfitting a model, it's capacity to generalize is increased. That said, clearly at some point underfitting a model cause models to become worse regardless of the complexity of data.</p>\n\n<p>How do you know when your model has struck the right balance and is not underfitting the data it seeks to model?</p>\n\n<hr>\n\n<p><strong>Note:</strong> This is a followup to my question, \"<a href=\"https://datascience.stackexchange.com/questions/61/why-is-overfitting-bad/\">Why Is Overfitting Bad?</a>\"</p>\n"
  },
  {
    "tags": [
      "python",
      "tools",
      "version-control"
    ],
    "owner": {
      "account_id": 1279,
      "reputation": 761,
      "user_id": 895,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/362e951a8c3d5494dcc652cc458f2ed6?s=256&d=identicon&r=PG",
      "display_name": "Yuval F",
      "link": "https://datascience.stackexchange.com/users/895/yuval-f"
    },
    "is_answered": true,
    "view_count": 10240,
    "accepted_answer_id": 759,
    "answer_count": 9,
    "score": 63,
    "last_activity_date": 1702650236,
    "creation_date": 1405541348,
    "last_edit_date": 1440175034,
    "question_id": 758,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/758/tools-and-protocol-for-reproducible-data-science-using-python",
    "title": "Tools and protocol for reproducible data science using Python",
    "body": "<p>I am working on a data science project using Python.\nThe project has several stages.\nEach stage comprises of taking a data set, using Python scripts, auxiliary data, configuration and parameters, and creating another data set.\nI store the code in git, so that part is covered.\nI would like to hear about:</p>\n\n<ol>\n<li>Tools for data version control.</li>\n<li>Tools enabling to reproduce stages and experiments.</li>\n<li>Protocol and suggested directory structure for such a project.</li>\n<li>Automated build/run tools.</li>\n</ol>\n"
  },
  {
    "tags": [
      "machine-learning",
      "time-series",
      "feature-selection"
    ],
    "owner": {
      "account_id": 963950,
      "reputation": 1071,
      "user_id": 88,
      "user_type": "registered",
      "accept_rate": 25,
      "profile_image": "https://www.gravatar.com/avatar/f244d83c6c44043718514221586862f7?s=256&d=identicon&r=PG",
      "display_name": "Igor Bobriakov",
      "link": "https://datascience.stackexchange.com/users/88/igor-bobriakov"
    },
    "is_answered": true,
    "view_count": 67920,
    "accepted_answer_id": 2370,
    "answer_count": 10,
    "score": 63,
    "last_activity_date": 1665740415,
    "creation_date": 1414560355,
    "question_id": 2368,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/2368/machine-learning-features-engineering-from-date-time-data",
    "title": "Machine learning - features engineering from date/time data",
    "body": "<p>What are the common/best practices to handle time data for machine learning application?</p>\n\n<p>For example, if in data set there is a column with timestamp of event, such as \"2014-05-05\", how you can extract useful features from this column if any?</p>\n\n<p>Thanks in advance!</p>\n"
  },
  {
    "tags": [
      "bigdata",
      "databases",
      "binary",
      "version-control"
    ],
    "migrated_from": {
      "other_site": {
        "aliases": [
          "https://academics.stackexchange.com"
        ],
        "styling": {
          "tag_background_color": "#FFF",
          "tag_foreground_color": "#000",
          "link_color": "#0077CC"
        },
        "related_sites": [
          {
            "relation": "meta",
            "api_site_parameter": "academia.meta",
            "site_url": "https://academia.meta.stackexchange.com",
            "name": "Academia Meta Stack Exchange"
          },
          {
            "relation": "chat",
            "site_url": "https://chat.stackexchange.com?tab=site&host=academia.stackexchange.com",
            "name": "Chat Stack Exchange"
          }
        ],
        "launch_date": 1398277239,
        "open_beta_date": 1329854452,
        "closed_beta_date": 1329246000,
        "site_state": "normal",
        "high_resolution_icon_url": "https://cdn.sstatic.net/Sites/academia/Img/apple-touch-icon@2.png",
        "favicon_url": "https://cdn.sstatic.net/Sites/academia/Img/favicon.ico",
        "icon_url": "https://cdn.sstatic.net/Sites/academia/Img/apple-touch-icon.png",
        "audience": "academics and those enrolled in higher education",
        "site_url": "https://academia.stackexchange.com",
        "api_site_parameter": "academia",
        "logo_url": "https://cdn.sstatic.net/Sites/academia/Img/logo.png",
        "name": "Academia",
        "site_type": "main_site"
      },
      "on_date": 1424263066,
      "question_id": 38813
    },
    "owner": {
      "account_id": 5782891,
      "reputation": 741,
      "user_id": 8320,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e70eb68f24986414a337227edf63173e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Johann",
      "link": "https://datascience.stackexchange.com/users/8320/johann"
    },
    "is_answered": true,
    "view_count": 31424,
    "accepted_answer_id": 6771,
    "answer_count": 11,
    "score": 63,
    "last_activity_date": 1721219014,
    "creation_date": 1423822165,
    "last_edit_date": 1424323892,
    "question_id": 5178,
    "link": "https://datascience.stackexchange.com/questions/5178/how-to-deal-with-version-control-of-large-amounts-of-binary-data",
    "title": "How to deal with version control of large amounts of (binary) data",
    "body": "<p>I am a PhD student of Geophysics and work with large amounts of image data  (hundreds of GB, tens of thousands of files). I know <code>svn</code> and <code>git</code> fairly well and come to value a project history, combined with the ability to easily work together and have protection against disk corruption. I find <code>git</code> also extremely helpful for having consistent backups but I know that git cannot handle large amounts of binary data efficiently.</p>\n\n<p>In my masters studies I worked on data sets of similar size (also images) and had a lot of problems keeping track of different version on different servers/devices. Diffing 100GB over the network really isn't fun, and cost me a lot of time and effort.</p>\n\n<p>I know that others in science seem to have similar problems, yet I couldn't find a good solution.</p>\n\n<p>I want to use the storage facilities of my institute, so I need something that can use a \"dumb\" server. I also would like to have an additional backup on a portable hard disk, because I would like to avoid transferring hundreds of GB over the network wherever possible. So, I need a tool that can handle more than one remote location.</p>\n\n<p>Lastly, I really need something that other researcher can use, so it does not need to be super simple, but should be learnable in a few hours.</p>\n\n<p>I have evaluated a lot of different solutions, but none seem to fit the bill:</p>\n\n<ul>\n<li><a href=\"http://subversion.apache.org/\">svn</a> is somewhat inefficient and needs a smart server</li>\n<li>hg <a href=\"http://mercurial.selenic.com/wiki/BigfilesExtension\">bigfile</a>/<a href=\"http://mercurial.selenic.com/wiki/LargefilesExtension\">largefile</a> can only use one remote</li>\n<li>git <a href=\"https://github.com/beenje/git-bigfile\">bigfile</a>/<a href=\"https://github.com/alebedev/git-media\">media</a> can also use only one remote, but is also not very efficient</li>\n<li><a href=\"https://attic-backup.org/\">attic</a> doesn't seem to have a log, or diffing capabilities</li>\n<li><a href=\"https://bup.github.io/\">bup</a> looks really good, but needs a \"smart\" server to work</li>\n</ul>\n\n<p>I've tried <code>git-annex</code>, which does everything I need it to do (and much more), but it is very difficult to use and not well documented. I've used it for several days and couldn't get my head around it, so I doubt any other coworker would be interested.</p>\n\n<p>How do researchers deal with large datasets, and what are other research groups using?</p>\n\n<p>To be clear, I am primarily interested in how other researchers deal with this situation, not just this specific dataset. It seems to me that almost everyone should have this problem, yet I don't know anyone who has solved it. Should I just keep a backup of the original data and forget all this version control stuff? Is that what everyone else is doing?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "predictive-modeling",
      "optimization",
      "training"
    ],
    "owner": {
      "account_id": 6165335,
      "reputation": 1803,
      "user_id": 13450,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://www.gravatar.com/avatar/7b4fe85f09404d96a10df58489d834bc?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "neural-nut",
      "link": "https://datascience.stackexchange.com/users/13450/neural-nut"
    },
    "is_answered": true,
    "view_count": 68247,
    "accepted_answer_id": 12763,
    "answer_count": 6,
    "score": 63,
    "last_activity_date": 1685526560,
    "creation_date": 1468407834,
    "last_edit_date": 1468504426,
    "question_id": 12761,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/12761/should-a-model-be-re-trained-if-new-observations-are-available",
    "title": "Should a model be re-trained if new observations are available?",
    "body": "<p>So, I have not been able to find any literature on this subject but it seems like something worth giving a thought:</p>\n\n<ul>\n<li><p>What are the best practices in model training and optimization if new observations are available?</p></li>\n<li><p>Is there any way to determine the period/frequency of re-training a model before the predictions begin to degrade?</p></li>\n<li><p>Is it over-fitting if the parameters are re-optimised for the aggregated data?</p></li>\n</ul>\n\n<p>Note that the learning may not <em>necessarily</em> be online. One may wish to upgrade an existing model after observing significant variance in more recent predictions.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "dataset",
      "training",
      "accuracy"
    ],
    "owner": {
      "account_id": 8249834,
      "reputation": 4030,
      "user_id": 52089,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Sa2qz.jpg?s=256",
      "display_name": "pcko1",
      "link": "https://datascience.stackexchange.com/users/52089/pcko1"
    },
    "is_answered": true,
    "view_count": 37628,
    "answer_count": 5,
    "score": 63,
    "last_activity_date": 1645403073,
    "creation_date": 1528797256,
    "question_id": 33008,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/33008/is-it-always-better-to-use-the-whole-dataset-to-train-the-final-model",
    "title": "Is it always better to use the whole dataset to train the final model?",
    "body": "<p>A common technique after training, validating and testing the Machine Learning model of preference is to use the complete dataset, including the testing subset, to train a <strong>final model</strong> to <strong>deploy</strong> it on, e.g. a product. </p>\n\n<blockquote>\n  <p>My question is: Is it always for the best to do so? What if the\n  performance actually deteriorates?</p>\n</blockquote>\n\n<p>For example, let us assume a case where the model scores around 65% in classifying the testing subset. This could mean that either the model is trained insufficiently OR that the testing subset consists of outliers. In the latter case, training the final model with them would decrease its performance and you find out only after deploying it. </p>\n\n<p>Re-phrasing my initial question:</p>\n\n<blockquote>\n  <p>If you had a <strong>one-time demonstration of a model</strong>, such as deploying it\n  on embedded electronics on-board an expensive rocket experiment, would you trust a\n  model that has been re-trained with the test subset in the final step without being\n  re-tested on its new performance?</p>\n</blockquote>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "scikit-learn",
      "preprocessing",
      "encoding"
    ],
    "owner": {
      "account_id": 13604003,
      "reputation": 773,
      "user_id": 59601,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/370195683469837/picture?type=large",
      "display_name": "Saurabh Singh",
      "link": "https://datascience.stackexchange.com/users/59601/saurabh-singh"
    },
    "is_answered": true,
    "view_count": 58198,
    "answer_count": 4,
    "score": 63,
    "last_activity_date": 1660297589,
    "creation_date": 1538938540,
    "last_edit_date": 1538966115,
    "question_id": 39317,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/39317/difference-between-ordinalencoder-and-labelencoder",
    "title": "Difference between OrdinalEncoder and LabelEncoder",
    "body": "<p>I was going through the official documentation of scikit-learn learn after going through a book on ML and came across the following thing:</p>\n\n<p>In the Documentation it is given about <code>sklearn.preprocessing.OrdinalEncoder()</code> whereas in the book it was given about <code>sklearn.preprocessing.LabelEncoder()</code>, when I checked their functionality it looked same to me. Can Someone please tell me the difference between the two please?</p>\n"
  },
  {
    "tags": [
      "nlp",
      "topic-model",
      "lda"
    ],
    "owner": {
      "account_id": 302279,
      "reputation": 2500,
      "user_id": 122,
      "user_type": "registered",
      "accept_rate": 62,
      "profile_image": "https://www.gravatar.com/avatar/0e9087f2672b0e4f28d91266acf9ce57?s=256&d=identicon&r=PG",
      "display_name": "alvas",
      "link": "https://datascience.stackexchange.com/users/122/alvas"
    },
    "is_answered": true,
    "view_count": 33521,
    "accepted_answer_id": 296,
    "answer_count": 6,
    "score": 62,
    "last_activity_date": 1612429856,
    "creation_date": 1400393452,
    "last_edit_date": 1400593559,
    "question_id": 128,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/128/latent-dirichlet-allocation-vs-hierarchical-dirichlet-process",
    "title": "Latent Dirichlet Allocation vs Hierarchical Dirichlet Process",
    "body": "<p><a href=\"http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\">Latent Dirichlet Allocation (LDA)</a> and <a href=\"http://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process\">Hierarchical Dirichlet Process (HDP)</a> are both topic modeling processes. The major difference is LDA requires the specification of the number of topics, and HDP doesn't. Why is that so? And what are the differences, pros, and cons of both topic modelling methods?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "pgm"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 19278,
    "accepted_answer_id": 9820,
    "answer_count": 9,
    "score": 62,
    "last_activity_date": 1670442760,
    "creation_date": 1453035897,
    "last_edit_date": 1453119389,
    "question_id": 9818,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9818/is-there-any-domain-where-bayesian-networks-outperform-neural-networks",
    "title": "Is there any domain where Bayesian Networks outperform neural networks?",
    "body": "<p>Neural networks get top results in Computer Vision tasks (see <a href=\"http://yann.lecun.com/exdb/mnist/\">MNIST</a>, <a href=\"http://www.image-net.org/challenges/LSVRC/\">ILSVRC</a>, <a href=\"http://blog.kaggle.com/2014/04/18/winning-the-galaxy-challenge-with-convnets/\">Kaggle Galaxy Challenge</a>).  They seem to outperform every other approach in Computer Vision. But there are also other tasks:</p>\n\n<ul>\n<li><a href=\"http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview/\">Kaggle Molecular Activity Challenge</a></li>\n<li>Regression: <a href=\"http://blog.kaggle.com/2016/01/04/how-much-did-it-rain-ii-winners-interview-1st-place-pupa-aka-aaron-sim/\">Kaggle Rain prediction</a>, also the <a href=\"http://blog.kaggle.com/2015/12/17/how-much-did-it-rain-ii-2nd-place-luis-andre-dutra-e-silva/\">2nd place</a></li>\n<li><a href=\"http://blog.kaggle.com/2015/09/29/grasp-and-lift-eeg-detection-winners-interview-2nd-place-daheimao/\">Grasp and Lift 2nd</a> also <a href=\"http://blog.kaggle.com/2015/10/05/grasp-and-lift-eeg-detection-winners-interview-3rd-place-team-hedj/\">third place</a> - Identify hand motions from EEG recordings</li>\n</ul>\n\n<p>I'm not too sure about ASR (automatic speech recognition) and machine translation, but I think I've also heard that (recurrent) neural networks (start to) outperform other approaches.</p>\n\n<p>I am currently learning about Bayesian Networks and I wonder in which cases those models are usually applied. So my question is:</p>\n\n<p><strong>Is there any challenge / (Kaggle) competition, where the state of the art are Bayesian Networks or at least very similar models?</strong></p>\n\n<p>(Side note: I've also seen <a href=\"http://blog.kaggle.com/2015/12/21/rossmann-store-sales-winners-interview-1st-place-gert/\">decision trees</a>, <a href=\"http://blog.kaggle.com/2015/11/09/profiling-top-kagglers-gilberto-titericz-new-1-in-the-world/\">2</a>, <a href=\"http://blog.kaggle.com/2015/10/30/dato-winners-interview-2nd-place-mortehu/\">3</a>, <a href=\"http://blog.kaggle.com/2015/10/21/recruit-coupon-purchase-winners-interview-2nd-place-halla-yang/\">4</a>, <a href=\"http://blog.kaggle.com/2015/10/20/caterpillar-winners-interview-3rd-place-team-shift-workers/\">5</a>, <a href=\"http://blog.kaggle.com/2015/09/28/liberty-mutual-property-inspection-winners-interview-qingchen-wang/\">6</a>, <a href=\"http://blog.kaggle.com/2015/09/22/caterpillar-winners-interview-1st-place-gilberto-josef-leustagos-mario/\">7</a> win in several recent Kaggle challenges)</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "beginner"
    ],
    "owner": {
      "account_id": 2022295,
      "reputation": 745,
      "user_id": 16244,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ecfb016e56d5df6d0622cc90953dce8d?s=256&d=identicon&r=PG",
      "display_name": "Larry Freeman",
      "link": "https://datascience.stackexchange.com/users/16244/larry-freeman"
    },
    "is_answered": true,
    "view_count": 75336,
    "accepted_answer_id": 11621,
    "answer_count": 5,
    "score": 62,
    "last_activity_date": 1606370281,
    "creation_date": 1462545380,
    "question_id": 11619,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/11619/rnn-vs-cnn-at-a-high-level",
    "title": "RNN vs CNN at a high level",
    "body": "<p>I've been thinking about the Recurrent Neural Networks (RNN) and their varieties and Convolutional Neural Networks (CNN) and their varieties.</p>\n\n<p>Would these two points be fair to say:</p>\n\n<ul>\n<li>Use CNNs to break a component (such as an image) into subcomponents (such as an object in an image, such as the outline of the object in the image, etc.)</li>\n<li>Use RNNs to create combinations of subcomponents (image captioning, text generation, language translation, etc.)</li>\n</ul>\n\n<p>I would appreciate if anyone wants to point out any inaccuracies in these statements.  My goal here is to get a more clearer foundation on the uses of CNNs and RNNs.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "neural-network",
      "activation-function",
      "difference"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 62525,
    "accepted_answer_id": 18589,
    "answer_count": 3,
    "score": 62,
    "last_activity_date": 1728736286,
    "creation_date": 1493121493,
    "last_edit_date": 1724511360,
    "question_id": 18583,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/18583/leakyrelu-vs-prelu",
    "title": "LeakyReLU vs PReLU",
    "body": "<p>I thought both, PReLU and Leaky ReLU are:\n<span class=\"math-container\">$$f(x) = \\max(x, \\alpha x) \\qquad \\text{ with } \\alpha \\in (0, 1)$$</span></p>\n<p>Keras, however, has both functions in <a href=\"https://keras.io/layers/advanced_activations/\" rel=\"nofollow noreferrer\">the docs</a>.</p>\n<h2>Leaky ReLU</h2>\n<p><a href=\"https://github.com/fchollet/keras/blob/master/keras/layers/advanced_activations.py#L41\" rel=\"nofollow noreferrer\">Source of LeakyReLU</a>:</p>\n<pre><code>return K.relu(inputs, alpha=self.alpha)\n</code></pre>\n<p>Hence (see <a href=\"https://github.com/fchollet/keras/blob/master/keras/backend/tensorflow_backend.py#L2529\" rel=\"nofollow noreferrer\">relu code</a>):\n<span class=\"math-container\">$$f_1(x) = \\max(0, x) - \\alpha \\max(0, -x)$$</span></p>\n<h2>PReLU</h2>\n<p><a href=\"https://github.com/keras-team/keras/blob/50d83f7ab80c04283236a986a1c99550cecc5140/keras/layers/advanced_activations.py#L121\" rel=\"nofollow noreferrer\">Source of PReLU</a>:</p>\n<pre><code>def call(self, inputs, mask=None):\n    pos = K.relu(inputs)\n    if K.backend() == 'theano':\n        neg = (K.pattern_broadcast(self.alpha, self.param_broadcast) *\n               (inputs - K.abs(inputs)) * 0.5)\n    else:\n        neg = -self.alpha * K.relu(-inputs)\n    return pos + neg\n</code></pre>\n<p>Hence:\n<span class=\"math-container\">$$f_2(x) = \\max(0, x) - \\alpha \\max(0, -x)$$</span></p>\n<h2>Question</h2>\n<p>Did I get something wrong? Aren't <span class=\"math-container\">$f_1$</span> and <span class=\"math-container\">$f_2$</span> equivalent to <span class=\"math-container\">$f$</span> (assuming <span class=\"math-container\">$\\alpha \\in (0, 1)$</span>?)</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "training"
    ],
    "owner": {
      "account_id": 7149567,
      "reputation": 1265,
      "user_id": 27616,
      "user_type": "registered",
      "accept_rate": 25,
      "profile_image": "https://www.gravatar.com/avatar/7ae925e98424ffd55234ebdff0c5b5c3?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "James Bond",
      "link": "https://datascience.stackexchange.com/users/27616/james-bond"
    },
    "is_answered": true,
    "view_count": 48462,
    "accepted_answer_id": 20193,
    "answer_count": 4,
    "score": 62,
    "last_activity_date": 1615794952,
    "creation_date": 1499233400,
    "question_id": 20179,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/20179/what-is-the-advantage-of-keeping-batch-size-a-power-of-2",
    "title": "What is the advantage of keeping batch size a power of 2?",
    "body": "<p>While training models in machine learning, why is it sometimes advantageous to keep the batch size to a power of 2? I thought it would be best to use a size that is the largest fit in your GPU memory / RAM.</p>\n\n<p>This <a href=\"https://datascience.stackexchange.com/questions/12532/does-batch-size-in-keras-have-any-effects-in-results-quality/12533#12533\">answer</a> claims that for some packages, a power of 2 is better as a batch size. Can someone provide a detailed explanation / link to a detailed explanation for this? Is this true for all optimisation algorithms (gradient descent, backpropagation, etc) or only some of them?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "deep-learning"
    ],
    "owner": {
      "account_id": 233741,
      "reputation": 733,
      "user_id": 2471,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/9fc97061330a2f8acd09644467e47f41?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "lithuak",
      "link": "https://datascience.stackexchange.com/users/2471/lithuak"
    },
    "is_answered": true,
    "view_count": 28386,
    "accepted_answer_id": 732,
    "answer_count": 3,
    "score": 61,
    "last_activity_date": 1493624782,
    "creation_date": 1405242279,
    "last_edit_date": 1493624782,
    "question_id": 731,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/731/how-to-fight-underfitting-in-a-deep-neural-net",
    "title": "How to fight underfitting in a deep neural net",
    "body": "<p>When I started with artificial neural networks (NN) I thought I'd have to fight overfitting as the main problem. But in practice I can't even get my NN to pass the 20% error rate barrier. I can't even beat my score on random forest!</p>\n\n<p>I'm seeking some very general or not so general advice on what should one do to make a NN start capturing trends in data.</p>\n\n<p>For implementing NN I use Theano Stacked Auto Encoder with <a href=\"https://github.com/lisa-lab/DeepLearningTutorials/blob/master/code/SdA.py\">the code from tutorial</a> that works great (less than 5% error rate) for classifying the MNIST dataset. It is a multilayer perceptron, with softmax layer on top with each hidden later being pre-trained as autoencoder (fully described at <a href=\"http://deeplearning.net/tutorial/deeplearning.pdf\">tutorial</a>, chapter 8). There are ~50 input features and ~10 output classes. The NN has sigmoid neurons and all data are normalized to [0,1]. I tried lots of different configurations: number of hidden layers and neurons in them (100->100->100, 60->60->60, 60->30->15, etc.), different learning and pre-train rates, etc.</p>\n\n<p>And the best thing I can get is a 20% error rate on the validation set and a 40% error rate on the test set.</p>\n\n<p>On the other hand, when I try to use Random Forest (from scikit-learn) I easily get a 12% error rate on the validation set and 25%(!) on the test set.</p>\n\n<p>How can it be that my deep NN with pre-training behaves so badly? What should I try? </p>\n"
  },
  {
    "tags": [
      "r",
      "tools",
      "rstudio",
      "programming"
    ],
    "owner": {
      "account_id": 241216,
      "reputation": 5474,
      "user_id": 97,
      "user_type": "registered",
      "accept_rate": 77,
      "profile_image": "https://www.gravatar.com/avatar/68acdc33e278ab19debf230662a7ad96?s=256&d=identicon&r=PG",
      "display_name": "IgorS",
      "link": "https://datascience.stackexchange.com/users/97/igors"
    },
    "is_answered": true,
    "view_count": 111201,
    "protected_date": 1633781470,
    "accepted_answer_id": 28853,
    "answer_count": 10,
    "score": 61,
    "last_activity_date": 1642481846,
    "creation_date": 1426678780,
    "last_edit_date": 1434441094,
    "question_id": 5345,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/5345/ide-alternatives-for-r-programming-rstudio-intellij-idea-eclipse-visual-stud",
    "title": "IDE alternatives for R programming (RStudio, IntelliJ IDEA, Eclipse, Visual Studio)",
    "body": "<p>I use RStudio for R programming. I remember about solid IDE-s from other technology stacks, like Visual Studio or Eclipse.</p>\n\n<p>I have two questions:</p>\n\n<ol>\n<li>What other IDE-s than RStudio are used (please consider providing some brief description on them).</li>\n<li>Does any of them have noticeable advantages over RStudio?</li>\n</ol>\n\n<p>I mostly mean debug/build/deploy features, besides coding itself (so text editors are probably not a solution).</p>\n"
  },
  {
    "tags": [
      "feature-selection",
      "correlation",
      "xgboost",
      "gbm"
    ],
    "owner": {
      "account_id": 6165335,
      "reputation": 1803,
      "user_id": 13450,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://www.gravatar.com/avatar/7b4fe85f09404d96a10df58489d834bc?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "neural-nut",
      "link": "https://datascience.stackexchange.com/users/13450/neural-nut"
    },
    "is_answered": true,
    "view_count": 67741,
    "accepted_answer_id": 12597,
    "answer_count": 6,
    "score": 61,
    "last_activity_date": 1623217666,
    "creation_date": 1467444613,
    "last_edit_date": 1467465411,
    "question_id": 12554,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/12554/does-xgboost-handle-multicollinearity-by-itself",
    "title": "Does XGBoost handle multicollinearity by itself?",
    "body": "<p>I'm currently using XGBoost on a data-set with 21 features (selected from list of some 150 features), then one-hot coded them to obtain ~98 features. A few of these 98 features are somewhat redundant, for example: a variable (feature) $A$ also appears as $\\frac{B}{A}$ and $\\frac{C}{A}$.</p>\n\n<p>My questions are :</p>\n\n<ul>\n<li>How (<em>If?</em>) do Boosted Decision Trees handle multicollinearity? </li>\n<li>How would the existence of multicollinearity affect prediction if it is not handled?</li>\n</ul>\n\n<p>From what I understand, the model is learning more than one tree and the final prediction is based on something like a \"weighted sum\" of the individual predictions. So if this is correct, then Boosted Decision Trees <em>should</em> be able to handle co-dependence between variables.</p>\n\n<p>Also, on a related note - how does the variable importance object in XGBoost work?</p>\n"
  },
  {
    "tags": [
      "feature-selection",
      "scikit-learn"
    ],
    "owner": {
      "account_id": 4852932,
      "reputation": 725,
      "user_id": 2854,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e8594eba3b2963d728533fc73219cdea?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Maksud",
      "link": "https://datascience.stackexchange.com/users/2854/maksud"
    },
    "is_answered": true,
    "view_count": 131072,
    "accepted_answer_id": 998,
    "answer_count": 8,
    "score": 60,
    "last_activity_date": 1629061711,
    "creation_date": 1407425623,
    "last_edit_date": 1628993250,
    "question_id": 937,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/937/does-scikit-learn-have-a-forward-selection-stepwise-regression-algorithm",
    "title": "Does scikit-learn have a forward selection/stepwise regression algorithm?",
    "body": "<p>I am working on a problem with too many features and training my models takes way too long. I implemented a forward selection algorithm to choose features.</p>\n<p>However, I was wondering does scikit-learn have a forward selection/stepwise regression algorithm?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "neural-network",
      "statistics",
      "tensorflow"
    ],
    "owner": {
      "account_id": 4025821,
      "reputation": 723,
      "user_id": 13809,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/RpemN.png?s=256",
      "display_name": "daniel451",
      "link": "https://datascience.stackexchange.com/users/13809/daniel451"
    },
    "is_answered": true,
    "view_count": 54066,
    "accepted_answer_id": 9870,
    "answer_count": 5,
    "score": 60,
    "last_activity_date": 1573620311,
    "creation_date": 1453204109,
    "question_id": 9850,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9850/neural-networks-which-cost-function-to-use",
    "title": "Neural networks: which cost function to use?",
    "body": "<p>I am using <a href=\"https://en.wikipedia.org/wiki/TensorFlow\">TensorFlow</a> for experiments mainly with neural networks. Although I have done quite some experiments (XOR-Problem, MNIST, some Regression stuff, ...) now, I struggle with choosing the \"correct\" cost function for specific problems because overall I could be considered a beginner.</p>\n\n<p>Before coming to TensorFlow I coded some fully-connected MLPs and some recurrent networks on my own with <a href=\"https://en.wikipedia.org/wiki/Python_(programming_language)\">Python</a> and <a href=\"https://en.wikipedia.org/wiki/NumPy\">NumPy</a> but mostly I had problems where a simple squared error and a simple gradient descient was sufficient.</p>\n\n<p>However, since TensorFlow offers quite a lot of cost functions itself as well as building custom cost functions, I would like to know if there is some kind of tutorial maybe specifically for cost functions on neural networks? (I've already done like half of the official TensorFlow tutorials but they're not really explaining <strong>why</strong> specific cost functions or learners are used for specific problems - at least not for beginners)</p>\n\n<p><strong>To give some examples:</strong></p>\n\n<pre><code>cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_output, y_train))\n</code></pre>\n\n<p>I guess it applies the softmax function on both inputs so that the sum of one vector equals 1. But what exactly is cross entropy with logits? I thought it sums up the values and calculates the cross entropy...so some metric measurement?! Wouldn't this be very much the same if I normalize the output, sum it up and take the squared error?\nAdditionally, why is this used e.g. for MNIST (or even much harder problems)? When I want to classify like 10 or maybe even 1000 classes, doesn't summing up the values completely destroy any information about <em>which</em> class actually was the output?</p>\n\n<p><code>cost = tf.nn.l2_loss(vector)</code></p>\n\n<p>What is this for? I thought l2 loss is pretty much the squared error but TensorFlow's API tells that it's input is just one tensor. Doesn't get the idea at all?!</p>\n\n<p>Besides I saw this for <strong>cross entropy</strong> pretty often:</p>\n\n<pre><code>cross_entropy = -tf.reduce_sum(y_train * tf.log(y_output))\n</code></pre>\n\n<p>...but why is this used? Isn't the loss in cross entropy mathematically this:</p>\n\n<pre><code>-1/n * sum(y_train * log(y_output) + (1 - y_train) * log(1 - y_output))\n</code></pre>\n\n<p>Where is the <code>(1 - y_train) * log(1 - y_output)</code> part in most TensorFlow examples? Isn't it missing?</p>\n\n<hr>\n\n<p><strong>Answers:</strong> I know this question is quite open, but I do not expect to get like 10 pages with every single problem/cost function listed in detail. I just need a short summary about when to use which cost function (in general or in TensorFlow, doesn't matter much to me) and some explanation about this topic. And/or some source(s) for beginners ;)</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning"
    ],
    "owner": {
      "account_id": 11188804,
      "reputation": 1167,
      "user_id": 50247,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/a8c56d24daf403c7208b0a7d7fb3b53b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Rajat",
      "link": "https://datascience.stackexchange.com/users/50247/rajat"
    },
    "is_answered": true,
    "view_count": 71976,
    "accepted_answer_id": 31045,
    "answer_count": 3,
    "score": 60,
    "last_activity_date": 1685950009,
    "creation_date": 1525100154,
    "question_id": 31041,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/31041/what-does-logits-in-machine-learning-mean",
    "title": "What does Logits in machine learning mean?",
    "body": "<p>\"One common mistake that I would make is adding a non-linearity to my logits output.\"</p>\n\n<p>What does the term \"logit\" means here or what does it represent ?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "dataset",
      "class-imbalance"
    ],
    "owner": {
      "account_id": 1357099,
      "reputation": 693,
      "user_id": 2661,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/9S4aW.png?s=256",
      "display_name": "pnp",
      "link": "https://datascience.stackexchange.com/users/2661/pnp"
    },
    "is_answered": true,
    "view_count": 16732,
    "accepted_answer_id": 811,
    "answer_count": 6,
    "score": 59,
    "last_activity_date": 1613863415,
    "creation_date": 1406032150,
    "last_edit_date": 1480416770,
    "question_id": 810,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/810/should-i-go-for-a-balanced-dataset-or-a-representative-dataset",
    "title": "Should I go for a &#39;balanced&#39; dataset or a &#39;representative&#39; dataset?",
    "body": "<p>My 'machine learning' task is of separating benign Internet traffic from malicious traffic. In the real world scenario, most (say 90% or more) of Internet traffic is benign. Thus I felt that I should choose a similar data setup for training my models as well. But I came across a research paper or two (in my area of work) which have used a \"class balancing\" data approach to training the models, implying an equal number of instances of benign and malicious traffic.</p>\n\n<p>In general, if I am building machine learning models, should I go for a dataset which is representative of the real world problem, or is a balanced dataset better suited for building the models (since certain classifiers do not behave well with class imbalance, or due to other reasons not known to me)?</p>\n\n<p>Can someone shed more light on the <em>pros</em> and <em>cons</em> of both the choices and how to decide which one to go choose?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "rnn"
    ],
    "owner": {
      "account_id": 3848020,
      "reputation": 1297,
      "user_id": 13686,
      "user_type": "registered",
      "accept_rate": 0,
      "profile_image": "https://www.gravatar.com/avatar/14f7bd035afecfefb4000c5542937fc7?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "wabbit",
      "link": "https://datascience.stackexchange.com/users/13686/wabbit"
    },
    "is_answered": true,
    "view_count": 82192,
    "answer_count": 5,
    "score": 59,
    "last_activity_date": 1595915156,
    "creation_date": 1457522060,
    "last_edit_date": 1457533631,
    "question_id": 10615,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10615/number-of-parameters-in-an-lstm-model",
    "title": "Number of parameters in an LSTM model",
    "body": "<p>How many parameters does a single stacked LSTM have? The number of parameters imposes a <strong>lower bound on the number of training examples</strong> required and also influences the training time. Hence knowing the number of parameters is useful for training models using LSTMs.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "keras",
      "cnn",
      "theano"
    ],
    "owner": {
      "account_id": 9478878,
      "reputation": 693,
      "user_id": 49697,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/a4838f34b45fce108a978fbd02d420bd?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Ermene",
      "link": "https://datascience.stackexchange.com/users/49697/ermene"
    },
    "is_answered": true,
    "view_count": 115369,
    "accepted_answer_id": 29726,
    "answer_count": 3,
    "score": 59,
    "last_activity_date": 1626137663,
    "creation_date": 1522392810,
    "last_edit_date": 1626137663,
    "question_id": 29719,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/29719/how-to-set-batch-size-steps-per-epoch-and-validation-steps",
    "title": "How to set batch_size, steps_per epoch, and validation steps?",
    "body": "<p>I am starting to learn CNNs using Keras. I am using the theano backend.</p>\n<p>I don't understand how to set values to:</p>\n<ul>\n<li><code>batch_size</code></li>\n<li><code>steps_per_epoch</code></li>\n<li><code>validation_steps</code></li>\n</ul>\n<p>What should be the value set to <code>batch_size</code>, <code>steps_per_epoch</code>, and <code>validation_steps</code>, if I have 240,000 samples in the training set and 80,000 in the test set?</p>\n"
  },
  {
    "tags": [
      "cross-validation",
      "model-evaluations"
    ],
    "owner": {
      "account_id": 5686358,
      "reputation": 1047,
      "user_id": 9011,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4dfba39b5ade8c489914ee647d4834e5?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Fredrik",
      "link": "https://datascience.stackexchange.com/users/9011/fredrik"
    },
    "is_answered": true,
    "view_count": 77287,
    "accepted_answer_id": 32276,
    "answer_count": 4,
    "score": 59,
    "last_activity_date": 1729507144,
    "creation_date": 1527513389,
    "question_id": 32264,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/32264/what-is-the-difference-between-bootstrapping-and-cross-validation",
    "title": "What is the difference between bootstrapping and cross-validation?",
    "body": "<p>I used to apply K-fold cross-validation for robust evaluation of my machine learning models. But I'm aware of the existence of the bootstrapping method for this purpose as well. However, I cannot see the main difference between them in terms of performance estimation.</p>\n\n<p>As far as I see, bootstrapping is also producing a certain number of random training+testing subsets (albeit in a different way) so what is the point, advantage for using this method over CV? The only thing I could figure out that in case of bootstrapping one could artificially produce virtually arbitrary number of such subsets while for CV the number of instances is a kind of limit for this. But this aspect seems to be a very little nuisance.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "xgboost"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user14204"
    },
    "is_answered": true,
    "view_count": 71887,
    "accepted_answer_id": 12594,
    "answer_count": 2,
    "score": 58,
    "last_activity_date": 1559136396,
    "creation_date": 1466488939,
    "last_edit_date": 1559136396,
    "question_id": 12318,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/12318/how-to-interpret-the-output-of-xgboost-importance",
    "title": "How to interpret the output of XGBoost importance?",
    "body": "<p>I ran a xgboost model. I don't exactly know how to interpret the output of <code>xgb.importance</code>.</p>\n\n<p>What is the meaning of Gain, Cover, and Frequency and how do we interpret them?</p>\n\n<p>Also, what does Split, RealCover, and RealCover% mean? I have some extra parameters <a href=\"https://raw.githubusercontent.com/pommedeterresautee/xgboost/master/R-package/vignettes/discoverYourData.Rmd\" rel=\"noreferrer\">here</a></p>\n\n<p>Are there any other parameters that can tell me more about feature importances?</p>\n\n<p>From the R documentation, I have some understanding that Gain is something similar to Information gain and Frequency is number of times a feature is used across all the trees. I have no idea what Cover is.</p>\n\n<p>I ran the example code given in the link (and also tried doing the same on the problem that I am working on), but the split definition given there did not match with the numbers that I calculated.</p>\n\n<pre><code>importance_matrix\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>           Feature         Gain        Cover    Frequence\n  1:            xxx 2.276101e-01 0.0618490331 1.913283e-02\n  2:           xxxx 2.047495e-01 0.1337406946 1.373710e-01\n  3:           xxxx 1.239551e-01 0.1032614896 1.319798e-01\n  4:           xxxx 6.269780e-02 0.0431682707 1.098646e-01\n  5:          xxxxx 6.004842e-02 0.0305611830 1.709108e-02\n\n214:     xxxxxxxxxx 4.599139e-06 0.0001551098 1.147052e-05\n215:     xxxxxxxxxx 4.500927e-06 0.0001665320 1.147052e-05\n216:   xxxxxxxxxxxx 3.899363e-06 0.0001536857 1.147052e-05\n217: xxxxxxxxxxxxxx 3.619348e-06 0.0001808504 1.147052e-05\n218:  xxxxxxxxxxxxx 3.429679e-06 0.0001792233 1.147052e-05\n</code></pre>\n"
  },
  {
    "tags": [
      "machine-learning",
      "predictive-modeling"
    ],
    "owner": {
      "account_id": 216182,
      "reputation": 1932,
      "user_id": 158,
      "user_type": "registered",
      "accept_rate": 71,
      "profile_image": "https://www.gravatar.com/avatar/7e5775439438b07b6f14a697a17dc49b?s=256&d=identicon&r=PG",
      "display_name": "blunders",
      "link": "https://datascience.stackexchange.com/users/158/blunders"
    },
    "is_answered": true,
    "view_count": 17957,
    "accepted_answer_id": 62,
    "answer_count": 8,
    "score": 57,
    "last_activity_date": 1505615251,
    "creation_date": 1400090941,
    "last_edit_date": 1492087841,
    "question_id": 61,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/61/why-is-overfitting-bad-in-machine-learning",
    "title": "Why Is Overfitting Bad in Machine Learning?",
    "body": "<p>Logic often states that by overfitting a model, its capacity to generalize is limited, though this might only mean that overfitting stops a model from improving after a certain complexity. Does overfitting cause models to become worse regardless of the complexity of data, and if so, why is this the case?</p>\n\n<hr>\n\n<p><strong>Related:</strong> Followup to the question above, \"<a href=\"https://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted\">When is a Model Underfitted?</a>\"</p>\n"
  },
  {
    "tags": [
      "beginner",
      "tools",
      "career",
      "reference-request"
    ],
    "owner": {
      "account_id": 3846039,
      "reputation": 875,
      "user_id": 13100,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://i.sstatic.net/FrYLK.jpg?s=256",
      "display_name": "StatguyUser",
      "link": "https://datascience.stackexchange.com/users/13100/statguyuser"
    },
    "is_answered": true,
    "view_count": 13820,
    "accepted_answer_id": 13514,
    "answer_count": 8,
    "score": 57,
    "last_activity_date": 1499620922,
    "creation_date": 1471496745,
    "last_edit_date": 1472180465,
    "question_id": 13513,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/13513/why-do-internet-companies-prefer-java-python-for-data-scientist-job",
    "title": "Why do internet companies prefer Java/Python for data scientist job?",
    "body": "<p>I see a many times in job description for data scientist asking for Python/Java experience and disregard R. Below is a personal email I received from chief data scientist of a company I applied for through linkedin.</p>\n\n<blockquote>\n  <p>X, Thanks for connecting and expressing interest. You do have good\n  Analytics Skills. However,  all our data scientists must have  good\n  programming skills in Java/Python as we are a internet/mobile\n  organisation and everything we do is online.</p>\n</blockquote>\n\n<p>While I respect the decision of the chief data scientist, I am unable to get a clear picture as to what are the tasks that Python can do that R cannot do. Can anyone care to elaborate? I am actually keen to learn Python/Java, provided I get a bit more detail.</p>\n\n<p>Edit: I found an interesting discussion on Quora.\n<a href=\"https://www.quora.com/Why-is-Python-a-language-of-choice-for-data-scientists\">Why is Python a language of choice for data scientists?</a></p>\n\n<p>Edit2: Blog from Udacity on <a href=\"http://blog.udacity.com/2016/04/languages-and-libraries-for-machine-learning.html\">Languages and Libraries for Machine Learning</a></p>\n"
  },
  {
    "tags": [
      "neural-network",
      "convolutional-neural-network"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 37528,
    "accepted_answer_id": 9384,
    "answer_count": 5,
    "score": 56,
    "last_activity_date": 1655406466,
    "creation_date": 1449093197,
    "last_edit_date": 1655406466,
    "question_id": 9175,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/9175/how-do-subsequent-convolution-layers-work",
    "title": "How do subsequent convolution layers work?",
    "body": "<p>This question boils down to \"how do convolution layers <em>exactly</em> work.</p>\n\n<p>Suppose I have an $n \\times m$ greyscale image. So the image has one channel.\nIn the first layer, I apply a $3\\times 3$ convolution with $k_1$ filters and padding. Then I have another convolution layer with $5 \\times 5$ convolutions and $k_2$ filters. How many feature maps do I have?</p>\n\n<h2>Type 1 convolution</h2>\n\n<p>The first layer gets executed. After that, I have $k_1$ feature maps (one for each filter). Each of those has the size $n \\times m$. Every single pixel was created by taking $3 \\cdot 3 = 9$ pixels from the padded input image.</p>\n\n<p>Then the second layer gets applied. Every single filter gets applied separately to <strong>each of the feature maps</strong>. This results in $k_2$ feature maps for every of the $k_1$ feature maps. So there are $k_1 \\times k_2$ feature maps after the second layer. Every single pixel of each of the new feature maps got created by taking $5 \\cdot 5 = 25$ \"pixels\" of the padded feature map from before.</p>\n\n<p>The system has to learn $k_1 \\cdot 3 \\cdot 3 + k_2 \\cdot 5 \\cdot 5$ parameters.</p>\n\n<h2>Type 2.1 convolution</h2>\n\n<p>Like before: The first layer gets executed. After that, I have $k_1$ feature maps (one for each filter). Each of those has the size $n \\times m$. Every single pixel was created by taking $3 \\cdot 3 = 9$ pixels from the padded input image.</p>\n\n<p>Unlike before: Then the second layer gets applied. Every single filter gets applied to the same region, but <strong>all feature maps</strong> from before. This results in $k_2$ feature maps in total after the second layer got executed. Every single pixel of each of the new feature maps got created by taking $k_2 \\cdot 5 \\cdot 5 = 25 \\cdot k_2$ \"pixels\" of the padded feature maps from before.</p>\n\n<p>The system has to learn $k_1 \\cdot 3 \\cdot 3 + k_2 \\cdot 5 \\cdot 5$ parameters.</p>\n\n<h2>Type 2.2 convolution</h2>\n\n<p>Like above, but instead of having $5 \\cdot 5 = 25$ parameters per filter which have to be learned and get simply copied for the other input feature maps, you have $k_1 \\cdot 3 \\cdot 3 + k_2 \\cdot k_1 \\cdot 5 \\cdot 5$ paramters which have to be learned.</p>\n\n<h2>Question</h2>\n\n<ol>\n<li>Is type 1 or type 2 typically used?</li>\n<li>Which type is used in <a href=\"http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf\" rel=\"noreferrer\">Alexnet</a>?</li>\n<li>Which type is used in <a href=\"http://arxiv.org/abs/1409.4842\" rel=\"noreferrer\">GoogLeNet</a>?\n\n<ul>\n<li>If you say type 1: Why do $1 \\times 1$ convolutions make any sense? Don't they only multiply the data with a constant?</li>\n<li>If you say type 2: Please explain the quadratic cost (\"For example, in a deep vision network, if two convolutional layers are chained,\nany uniform increase in the number of their filters results in a quadratic increase of computation\")</li>\n</ul></li>\n</ol>\n\n<p>For all answers, please give some evidence (papers, textbooks, documentation of frameworks) that your answer is correct.</p>\n\n<h2>Bonus question 1</h2>\n\n<p>Is the pooling applied always only per feature map or is it also done over multiple feature maps?</p>\n\n<h2>Bonus question 2</h2>\n\n<p>I'm relatively sure that type 1 is correct and I got something wrong with the GoogLe paper. But there a 3D convolutions, too. Lets say you have 1337 feature maps of size $42 \\times 314$ and you apply a $3 \\times 4 \\times 5$ filter. How do you slide the filter over the feature maps? (Left to right, top to bottom, first feature map to last feature map?) Does it matter as long as you do it consistantly?</p>\n\n<h2>My research</h2>\n\n<ul>\n<li>I've read the two papers from above, but I'm still not sure what is used.</li>\n<li>I've read the <a href=\"http://lasagne.readthedocs.org/en/latest/modules/layers/conv.html\" rel=\"noreferrer\">lasagne documentation</a></li>\n<li>I've read the <a href=\"http://deeplearning.net/software/theano/library/tensor/nnet/conv.html\" rel=\"noreferrer\">theano documentation</a></li>\n<li>I've read the answers on <a href=\"https://stats.stackexchange.com/q/85767/25741\">Understanding convolutional neural networks</a> (without following all links)</li>\n<li>I've read <a href=\"http://deeplearning.net/tutorial/lenet.html\" rel=\"noreferrer\">Convolutional Neural Networks (LeNet)</a>. Especially figure 1 makes me relatively sure that Type 2.1 is the right one. This would also fit to the \"quadratic cost\" comment in GoogLe Net and to some practical experience I had with Caffee.</li>\n</ul>\n"
  },
  {
    "tags": [
      "python",
      "scikit-learn",
      "sampling"
    ],
    "owner": {
      "account_id": 11266002,
      "reputation": 663,
      "user_id": 34253,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-KcMplteN91U/AAAAAAAAAAI/AAAAAAAAAj0/odq_HX83Uvs/s256-rj/photo.jpg",
      "display_name": "josh_gray",
      "link": "https://datascience.stackexchange.com/users/34253/josh-gray"
    },
    "is_answered": true,
    "view_count": 531976,
    "protected_date": 1568069176,
    "accepted_answer_id": 20203,
    "answer_count": 2,
    "score": 56,
    "last_activity_date": 1568060523,
    "creation_date": 1499318275,
    "last_edit_date": 1553582548,
    "question_id": 20199,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/20199/train-test-split-error-found-input-variables-with-inconsistent-numbers-of-sam",
    "title": "train_test_split() error: Found input variables with inconsistent numbers of samples",
    "body": "<p>Fairly new to Python but building out my first RF model based on some classification data. I've converted all of the labels into int64 numerical data and loaded into X and Y as a numpy array, but I am hitting an error when I am trying to train the models. </p>\n\n<p>Here is what my arrays look like:</p>\n\n<pre><code>&gt;&gt;&gt; X = np.array([[df.tran_cityname, df.tran_signupos, df.tran_signupchannel, df.tran_vmake, df.tran_vmodel, df.tran_vyear]])\n\n&gt;&gt;&gt; Y = np.array(df['completed_trip_status'].values.tolist())\n\n&gt;&gt;&gt; X\narray([[[   1,    1,    2,    3,    1,    1,    1,    1,    1,    3,    1,\n            3,    1,    1,    1,    1,    2,    1,    3,    1,    3,    3,\n            2,    3,    3,    1,    1,    1,    1],\n        [   0,    5,    5,    1,    1,    1,    2,    2,    0,    2,    2,\n            3,    1,    2,    5,    5,    2,    1,    2,    2,    2,    2,\n            2,    4,    3,    5,    1,    0,    1],\n        [   2,    2,    1,    3,    3,    3,    2,    3,    3,    2,    3,\n            2,    3,    2,    2,    3,    2,    2,    1,    1,    2,    1,\n            2,    2,    1,    2,    3,    1,    1],\n        [   0,    0,    0,   42,   17,    8,   42,    0,    0,    0,   22,\n            0,   22,    0,    0,   42,    0,    0,    0,    0,   11,    0,\n            0,    0,    0,    0,   28,   17,   18],\n        [   0,    0,    0,   70,  291,   88,  234,    0,    0,    0,  222,\n            0,  222,    0,    0,  234,    0,    0,    0,    0,   89,    0,\n            0,    0,    0,    0,   40,  291,  131],\n        [   0,    0,    0, 2016, 2016, 2006, 2014,    0,    0,    0, 2015,\n            0, 2015,    0,    0, 2015,    0,    0,    0,    0, 2015,    0,\n            0,    0,    0,    0, 2016, 2016, 2010]]])\n\n&gt;&gt;&gt; Y\narray(['NO', 'NO', 'NO', 'YES', 'NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'NO',\n       'NO', 'YES', 'NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',\n       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO'], \n      dtype='|S3')\n\n&gt;&gt;&gt; X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n</code></pre>\n\n<blockquote>\n  <p>Traceback (most recent call last):</p>\n\n<pre><code>  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/Library/Python/2.7/site-packages/sklearn/cross_validation.py\", line\n</code></pre>\n  \n  <p>2039, in train_test_split\n          arrays = indexable(*arrays)\n        File \"/Library/Python/2.7/site-packages/sklearn/utils/validation.py\", line\n  206, in indexable\n          check_consistent_length(*result)\n        File \"/Library/Python/2.7/site-packages/sklearn/utils/validation.py\", line\n  181, in check_consistent_length\n          \" samples: %r\" % [int(l) for l in lengths])</p>\n\n<pre><code>ValueError: Found input variables with inconsistent numbers of samples: [1, 29]\n</code></pre>\n</blockquote>\n"
  },
  {
    "tags": [
      "bigdata",
      "r"
    ],
    "owner": {
      "account_id": 3990511,
      "reputation": 723,
      "user_id": 136,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/3a675d8059ab4654b82e5165e54247f7?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "akellyirl",
      "link": "https://datascience.stackexchange.com/users/136/akellyirl"
    },
    "is_answered": true,
    "view_count": 10766,
    "accepted_answer_id": 44,
    "answer_count": 9,
    "score": 55,
    "last_activity_date": 1550921681,
    "creation_date": 1400066140,
    "last_edit_date": 1400072788,
    "question_id": 41,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/41/is-the-r-language-suitable-for-big-data",
    "title": "Is the R language suitable for Big Data",
    "body": "<p>R has many libraries which are aimed at Data Analysis (e.g. JAGS, BUGS, ARULES etc..), and is mentioned in popular textbooks such as: J.Krusche, Doing Bayesian Data Analysis; B.Lantz, \"Machine Learning with R\".</p>\n\n<p>I've seen a guideline of 5TB for a dataset to be considered as Big Data.</p>\n\n<p>My question is: Is R suitable for the amount of Data typically seen in Big Data problems? \nAre there strategies to be employed when using R with this size of dataset?</p>\n"
  },
  {
    "tags": [
      "pandas",
      "dataframe"
    ],
    "owner": {
      "account_id": 13763679,
      "reputation": 837,
      "user_id": 53575,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b8623828cc8f6de92695ec97993d539b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "a_a_a",
      "link": "https://datascience.stackexchange.com/users/53575/a-a-a"
    },
    "is_answered": true,
    "view_count": 422082,
    "accepted_answer_id": 33075,
    "answer_count": 9,
    "score": 55,
    "last_activity_date": 1688506053,
    "creation_date": 1528842840,
    "question_id": 33053,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/33053/how-do-i-compare-columns-in-different-data-frames",
    "title": "How do I compare columns in different data frames?",
    "body": "<p>I would like to compare one column of a df with other df's. The columns are names and last names. I'd like to check if a person in one data frame is in another one. </p>\n"
  },
  {
    "tags": [
      "neural-network",
      "optimization"
    ],
    "owner": {
      "account_id": 5170471,
      "reputation": 1656,
      "user_id": 41929,
      "user_type": "registered",
      "accept_rate": 75,
      "profile_image": "https://i.sstatic.net/J8cH3.png?s=256",
      "display_name": "PyRsquared",
      "link": "https://datascience.stackexchange.com/users/41929/pyrsquared"
    },
    "is_answered": true,
    "view_count": 103189,
    "accepted_answer_id": 30347,
    "answer_count": 2,
    "score": 54,
    "last_activity_date": 1728266964,
    "creation_date": 1523811334,
    "question_id": 30344,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/30344/why-not-always-use-the-adam-optimization-technique",
    "title": "Why not always use the ADAM optimization technique?",
    "body": "<p>It seems the <a href=\"http://ruder.io/optimizing-gradient-descent/\" rel=\"noreferrer\">Adaptive Moment Estimation (Adam)</a> optimizer nearly always works better (faster and more reliably reaching a global minimum) when minimising the cost function in training neural nets.</p>\n\n<p>Why not always use Adam? Why even bother using RMSProp or momentum optimizers?</p>\n"
  },
  {
    "tags": [
      "computer-vision"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 90469,
    "accepted_answer_id": 16813,
    "answer_count": 3,
    "score": 52,
    "last_activity_date": 1597949148,
    "creation_date": 1486458555,
    "question_id": 16797,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/16797/what-does-the-notation-map-5-95-mean",
    "title": "What does the notation mAP@[.5:.95] mean?",
    "body": "<p>For detection, a common way to determine if one object proposal was right is <em>Intersection over Union</em> (IoU, IU). This takes the set $A$ of proposed object pixels and the set of true object pixels $B$ and calculates:</p>\n\n<p>$$IoU(A, B) = \\frac{A \\cap B}{A \\cup B}$$</p>\n\n<p>Commonly, IoU > 0.5 means that it was a hit, otherwise it was a fail. For each class, one can calculate the</p>\n\n<ul>\n<li>True Positive ($TP(c)$): a proposal was made for class $c$ and there actually was an object of class $c$</li>\n<li>False Positive ($FP(c)$): a proposal was made for class $c$, but there is no object of class $c$</li>\n<li>Average Precision for class $c$: $\\frac{\\#TP(c)}{\\#TP(c) + \\#FP(c)}$</li>\n</ul>\n\n<p>The mAP (mean average precision) = $\\frac{1}{|classes|}\\sum_{c \\in classes} \\frac{\\#TP(c)}{\\#TP(c) + \\#FP(c)}$</p>\n\n<p>If one wants better proposals, one does increase the IoU from 0.5 to a higher value (up to 1.0 which would be perfect). One can denote this with mAP@p, where $p \\in (0, 1)$ is the IoU.</p>\n\n<p>But what does <code>mAP@[.5:.95]</code> (as found in <a href=\"https://arxiv.org/abs/1512.04412\" rel=\"noreferrer\">this paper</a>) mean?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "q-learning"
    ],
    "owner": {
      "account_id": 266740,
      "reputation": 4199,
      "user_id": 12515,
      "user_type": "registered",
      "accept_rate": 37,
      "profile_image": "https://i.sstatic.net/G0s1w.jpg?s=256",
      "display_name": "Ryan Zotti",
      "link": "https://datascience.stackexchange.com/users/12515/ryan-zotti"
    },
    "is_answered": true,
    "view_count": 84567,
    "accepted_answer_id": 20542,
    "answer_count": 3,
    "score": 52,
    "last_activity_date": 1675458311,
    "creation_date": 1500437722,
    "last_edit_date": 1543460437,
    "question_id": 20535,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits",
    "title": "What is &quot;experience replay&quot; and what are its benefits?",
    "body": "<p>I've been reading Google's DeepMind Atari <a href=\"http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\" rel=\"noreferrer\" title=\"RL\">paper</a> and I'm trying to understand the concept of \"experience replay\". Experience replay comes up in a lot of other reinforcement learning papers (particularly, the AlphaGo paper), so I want to understand how it works. Below are some excerpts.</p>\n\n<blockquote>\n  <p>First, we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution.</p>\n</blockquote>\n\n<p>The paper then elaborates as follows:</p>\n\n<blockquote>\n  <p>While other stable methods exist for training neural networks in the reinforcement learning setting, such as neural fitted Q-iteration, these\n  methods involve the repeated training of networks <em>de novo</em> hundreds\n  of iterations. Consequently, these methods, unlike our algorithm, are\n  too inefficient to be used successfully with large neural networks. We\n  parameterize an approximate value function <span class=\"math-container\">$Q(s, a; \\theta_i)$</span> using the deep\n  convolutional neural network shown in Fig. 1, in which <span class=\"math-container\">$\\theta_i$</span> are the parameters (that is, weights) of the Q-network at iteration <span class=\"math-container\">$i$</span>. To perform\n  experience replay, we store the agent's experiences <span class=\"math-container\">$e_t = (s_t, a_t, r_t, s_{t+1})$</span> at each time-step <span class=\"math-container\">$t$</span> in a data set <span class=\"math-container\">$D_t = \\{e_1, \\dots, e_t \\}$</span>. During learning, we apply Q-learning updates, on samples (or mini-batches) of experience <span class=\"math-container\">$(s, a, r, s') \\sim U(D)$</span>, drawn uniformly at random from the pool of stored samples. The Q-learning update at iteration <span class=\"math-container\">$i$</span> uses the following loss\n  function: </p>\n  \n  <p><span class=\"math-container\">$$\nL_i(\\theta_i) = \\mathbb{E}_{(s, a, r, s') \\sim U(D)} \\left[ \\left(r + \\gamma \\max_{a'} Q(s', a'; \\theta_i^-) - Q(s, a; \\theta_i)\\right)^2 \\right]\n$$</span></p>\n</blockquote>\n\n<p>What is experience replay, and what are its benefits, in laymen's terms?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "pytorch"
    ],
    "owner": {
      "account_id": 987187,
      "reputation": 827,
      "user_id": 67135,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/cc458e828b50a1f9329469ab1e5d98fd?s=256&d=identicon&r=PG",
      "display_name": "Muppet",
      "link": "https://datascience.stackexchange.com/users/67135/muppet"
    },
    "is_answered": true,
    "view_count": 51917,
    "accepted_answer_id": 48374,
    "answer_count": 3,
    "score": 52,
    "last_activity_date": 1740108138,
    "creation_date": 1554145204,
    "last_edit_date": 1554158222,
    "question_id": 48369,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/48369/what-loss-function-to-use-for-imbalanced-classes-using-pytorch",
    "title": "What loss function to use for imbalanced classes (using PyTorch)?",
    "body": "<p>I have a dataset with 3 classes with the following items:</p>\n\n<ul>\n<li>Class 1: 900 elements </li>\n<li>Class 2: 15000 elements </li>\n<li>Class 3: 800 elements</li>\n</ul>\n\n<p>I need to predict class 1 and class 3, which signal important deviations from the norm. Class 2 is the default normal case which I dont care about.</p>\n\n<p>What kind of loss function would I use here? I was thinking of using CrossEntropyLoss, but since there is a class imbalance, this would need to be weighted I suppose? How does that work in practice? Like this (using PyTorch)?</p>\n\n<pre><code>summed = 900 + 15000 + 800\nweight = torch.tensor([900, 15000, 800]) / summed\ncrit = nn.CrossEntropyLoss(weight=weight)\n</code></pre>\n\n<p>Or should the weight be inverted? i.e. 1 / weight?</p>\n\n<p>Is this the right approach to begin with or are there other / better methods I could use?</p>\n\n<p>Thanks</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "deep-learning"
    ],
    "owner": {
      "account_id": 1499381,
      "reputation": 1625,
      "user_id": 847,
      "user_type": "registered",
      "accept_rate": 67,
      "profile_image": "https://www.gravatar.com/avatar/4480675ad0ede0043e7caba04750d974?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Nitesh",
      "link": "https://datascience.stackexchange.com/users/847/nitesh"
    },
    "is_answered": true,
    "view_count": 47187,
    "accepted_answer_id": 5152,
    "answer_count": 8,
    "score": 51,
    "last_activity_date": 1734619888,
    "creation_date": 1416466140,
    "question_id": 2504,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/2504/deep-learning-vs-gradient-boosting-when-to-use-what",
    "title": "Deep Learning vs gradient boosting: When to use what?",
    "body": "<p>I have a big data problem with a large dataset (take for example 50 million rows and 200 columns). The dataset consists of about 100 numerical columns and 100 categorical columns and a response column that represents a binary class problem. The cardinality of each of the categorical columns is less than 50. </p>\n\n<p>I want to know a priori whether I should go for deep learning methods or ensemble tree based methods (for example gradient boosting, adaboost, or random forests). Are there some exploratory data analysis or some other techniques that can help me decide for one method over the other? </p>\n"
  },
  {
    "tags": [
      "scikit-learn",
      "random-forest",
      "multilabel-classification"
    ],
    "owner": {
      "account_id": 228179,
      "reputation": 913,
      "user_id": 20429,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4277666449f41474561f867f04f32b73?s=256&d=identicon&r=PG",
      "display_name": "Harpal",
      "link": "https://datascience.stackexchange.com/users/20429/harpal"
    },
    "is_answered": true,
    "view_count": 161206,
    "accepted_answer_id": 22821,
    "answer_count": 3,
    "score": 51,
    "last_activity_date": 1581518793,
    "creation_date": 1504263477,
    "last_edit_date": 1592305723,
    "question_id": 22762,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/22762/understanding-predict-proba-from-multioutputclassifier",
    "title": "Understanding predict_proba from MultiOutputClassifier",
    "body": "<p>I'm following this <a href=\"http://scikit-learn.org/stable/modules/multiclass.html#multioutput-classification\" rel=\"noreferrer\">example</a> on the scikit-learn website to perform a multioutput classification with a Random Forest model.</p>\n<pre><code>from sklearn.datasets import make_classification\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.utils import shuffle\nimport numpy as np\n\nX, y1 = make_classification(n_samples=5, n_features=5, n_informative=2, n_classes=2, random_state=1)\ny2 = shuffle(y1, random_state=1)\nY = np.vstack((y1, y2)).T\n\nforest = RandomForestClassifier(n_estimators=10, random_state=1)\nmulti_target_forest = MultiOutputClassifier(forest, n_jobs=-1)\nmulti_target_forest.fit(X, Y).predict(X)\n\nprint(multi_target_forest.predict_proba(X))\n</code></pre>\n<p>From this <code>predict_proba</code> I get a 2 5x2 arrays:</p>\n<pre><code>[array([[ 0.8,  0.2],\n       [ 0.4,  0.6],\n       [ 0.8,  0.2],\n       [ 0.9,  0.1],\n       [ 0.4,  0.6]]), array([[ 0.6,  0.4],\n       [ 0.1,  0.9],\n       [ 0.2,  0.8],\n       [ 0.9,  0.1],\n       [ 0.9,  0.1]])]\n</code></pre>\n<p>I was really expecting a <code>n_sample</code> by <code>n_classes</code> matrix. I'm struggling to understand how this relates to the probability of the classes present.</p>\n<p>The <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html#sklearn.multioutput.MultiOutputClassifier.predict_proba\" rel=\"noreferrer\">docs</a> for <code>predict_proba</code> states:</p>\n<blockquote>\n<p>array of shape = [n_samples, n_classes], or a list of n_outputs such arrays if n_outputs &gt; 1.</p>\n<p>The class probabilities of the input samples. The order of the classes corresponds to that in the attribute classes_.</p>\n</blockquote>\n<p>I'm guessing I have the latter in the description, but I'm still struggling to understand how this relates to my class probabilities.</p>\n<p>Furthermore, when I attempt to access the <code>classes_</code> attribute for the <code>forest</code> model I get an <code>AttributeError</code> and this attribute does not exist on the <code>MultiOutputClassifier</code>. How can I relate the classes to the output?</p>\n<pre><code>print(forest.classes_)\n\nAttributeError: 'RandomForestClassifier' object has no attribute 'classes_'\n</code></pre>\n"
  },
  {
    "tags": [
      "machine-learning",
      "scikit-learn",
      "preprocessing"
    ],
    "owner": {
      "account_id": 7842833,
      "reputation": 725,
      "user_id": 59087,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/211749f4cce532b80175e68f06d90e16?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "tsumaranaina",
      "link": "https://datascience.stackexchange.com/users/59087/tsumaranaina"
    },
    "is_answered": true,
    "view_count": 74535,
    "accepted_answer_id": 38441,
    "answer_count": 3,
    "score": 51,
    "last_activity_date": 1655605972,
    "creation_date": 1537238136,
    "last_edit_date": 1655605972,
    "question_id": 38395,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/38395/standardscaler-before-or-after-splitting-data-which-is-better",
    "title": "StandardScaler before or after splitting data - which is better?",
    "body": "<p>When I was reading about using <code>StandardScaler</code>, most of the recommendations were saying that you should use <code>StandardScaler</code> <em>before</em> splitting the data into train/test, but when i was checking some of the codes posted online (using sklearn) there were two major uses.</p>\n<p><strong>Case 1</strong>: Using <code>StandardScaler</code> on all the data. E.g.</p>\n<pre><code>from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_fit = sc.fit(X)\nX_std = X_fit.transform(X)\n</code></pre>\n<p>Or</p>\n<pre><code>from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit(X)\nX = sc.transform(X)\n</code></pre>\n<p>Or simply</p>\n<pre><code>from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_std = sc.fit_transform(X)\n</code></pre>\n<p><strong>Case 2</strong>: Using <code>StandardScaler</code> on split data.</p>\n<pre><code>from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform (X_test)\n</code></pre>\n<p>I would like to standardize my data, but I am confused which approach is the best!</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "classification"
    ],
    "owner": {
      "account_id": 6403827,
      "reputation": 1379,
      "user_id": 50534,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/GASUB.jpg?s=256",
      "display_name": "qmeeus",
      "link": "https://datascience.stackexchange.com/users/50534/qmeeus"
    },
    "is_answered": true,
    "view_count": 37771,
    "accepted_answer_id": 49594,
    "answer_count": 5,
    "score": 50,
    "last_activity_date": 1740510946,
    "creation_date": 1534767745,
    "last_edit_date": 1534858517,
    "question_id": 37186,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/37186/early-stopping-on-validation-loss-or-on-accuracy",
    "title": "Early stopping on validation loss or on accuracy?",
    "body": "<p>I am currently training a neural network and I cannot decide which to use to implement my Early Stopping criteria: validation loss or a metrics like accuracy/f1score/auc/whatever calculated on the validation set.</p>\n\n<p>In my research, I came upon articles defending both standpoints. Keras seems to default to the validation loss but I have also come across convincing answers for the opposite approach (e.g. <a href=\"https://github.com/keras-team/keras/issues/5359\" rel=\"noreferrer\">here</a>).</p>\n\n<p>Anyone has directions on when to use preferably the validation loss and when to use a specific metric?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "parameter",
      "hyperparameter",
      "language-model"
    ],
    "owner": {
      "account_id": 2252789,
      "reputation": 2177,
      "user_id": 22012,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://i.sstatic.net/4SJZF.jpg?s=256",
      "display_name": "tastyminerals",
      "link": "https://datascience.stackexchange.com/users/22012/tastyminerals"
    },
    "is_answered": true,
    "view_count": 58360,
    "accepted_answer_id": 14194,
    "answer_count": 7,
    "score": 49,
    "last_activity_date": 1660216812,
    "creation_date": 1474716290,
    "last_edit_date": 1474965390,
    "question_id": 14187,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/14187/what-is-the-difference-between-model-hyperparameters-and-model-parameters",
    "title": "What is the difference between model hyperparameters and model parameters?",
    "body": "<p>I have noticed that such terms as model <strong>hyperparameter</strong> and model <strong>parameter</strong> have been used interchangeably on the web without prior clarification. I think this is incorrect and needs explanation. Consider a machine learning model, an SVM/NN/NB based classificator or image recognizer, just anything that first springs to mind. </p>\n\n<p>What are the <strong>hyperparameters</strong> and <strong>parameters</strong> of the model?<br>\nGive your examples please.</p>\n"
  },
  {
    "tags": [
      "python",
      "bigdata",
      "pandas",
      "anaconda"
    ],
    "owner": {
      "account_id": 12237323,
      "reputation": 501,
      "user_id": 41807,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-uHfPMHVI99g/AAAAAAAAAAI/AAAAAAAAADA/r4BniAcCDzg/s256-rj/photo.jpg",
      "display_name": "Hari Prasad",
      "link": "https://datascience.stackexchange.com/users/41807/hari-prasad"
    },
    "is_answered": true,
    "view_count": 106085,
    "answer_count": 5,
    "score": 49,
    "last_activity_date": 1559892814,
    "creation_date": 1518530619,
    "last_edit_date": 1518533885,
    "question_id": 27767,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/27767/opening-a-20gb-file-for-analysis-with-pandas",
    "title": "Opening a 20GB file for analysis with pandas",
    "body": "<p>I am currently trying to open a file with pandas and python for machine learning purposes it would be ideal for me to have them all in a DataFrame. Now The file is 18GB large and my RAM is 32 GB but I keep getting memory errors.</p>\n\n<p>From your experience is it possible? If not do you know of a better way to go around this? (hive table? increase the size of my RAM to 64? create a database and access it from python)</p>\n"
  },
  {
    "tags": [
      "python",
      "clustering",
      "scikit-learn"
    ],
    "owner": {
      "account_id": 79883,
      "reputation": 793,
      "user_id": 8206,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0970bec632e502b1a50f1ae75ccfaf4a?s=256&d=identicon&r=PG",
      "display_name": "Nanda",
      "link": "https://datascience.stackexchange.com/users/8206/nanda"
    },
    "is_answered": true,
    "view_count": 129019,
    "accepted_answer_id": 9271,
    "answer_count": 6,
    "score": 48,
    "last_activity_date": 1621628332,
    "creation_date": 1449571064,
    "last_edit_date": 1596462097,
    "question_id": 9262,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/9262/calculating-kl-divergence-in-python",
    "title": "Calculating KL Divergence in Python",
    "body": "<p>I am rather new to this and can't say I have a complete understanding of the theoretical concepts behind this. I am trying to calculate the KL Divergence between several lists of points in Python. I am using <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html\" rel=\"noreferrer\">this</a> to try and do this. The problem that I'm running into is that the value returned is the same for any 2 lists of numbers (its 1.3862943611198906). I have a feeling that I'm making some sort of theoretical mistake here but can't spot it.</p>\n<pre><code>values1 = [1.346112,1.337432,1.246655]\nvalues2 = [1.033836,1.082015,1.117323]\nmetrics.mutual_info_score(values1,values2)\n</code></pre>\n<p>That is an example of what I'm running - just that I'm getting the same output for any 2 input. Any advice/help would be appreciated!</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "feature-extraction",
      "feature-engineering",
      "encoding",
      "numerical"
    ],
    "owner": {
      "account_id": 2035987,
      "reputation": 635,
      "user_id": 30238,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/665ac068b26409d8c68e16bdeeb3fd97?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Funkwecker",
      "link": "https://datascience.stackexchange.com/users/30238/funkwecker"
    },
    "is_answered": true,
    "view_count": 42875,
    "accepted_answer_id": 24003,
    "answer_count": 6,
    "score": 48,
    "last_activity_date": 1559085304,
    "creation_date": 1490168637,
    "last_edit_date": 1490251408,
    "question_id": 17759,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/17759/encoding-features-like-month-and-hour-as-categorial-or-numeric",
    "title": "Encoding features like month and hour as categorial or numeric?",
    "body": "<p>Is it better to encode features like month and hour as factor or numeric in a machine learning model?</p>\n\n<p>On the one hand, I feel numeric encoding might be reasonable, because time is a forward progressing process (the fifth month is followed by the sixth month), but on the other hand I think categorial encoding might be more reasonable because of the cyclic nature of years and days ( the 12th month is followed by the first one).</p>\n\n<p>Is there a general solution or convention for this? </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning"
    ],
    "owner": {
      "account_id": 6631713,
      "reputation": 14308,
      "user_id": 28175,
      "user_type": "registered",
      "accept_rate": 95,
      "profile_image": "https://i.sstatic.net/ojtVl.jpg?s=256",
      "display_name": "Green Falcon",
      "link": "https://datascience.stackexchange.com/users/28175/green-falcon"
    },
    "is_answered": true,
    "view_count": 105211,
    "accepted_answer_id": 17840,
    "answer_count": 3,
    "score": 48,
    "last_activity_date": 1600178501,
    "creation_date": 1490357354,
    "last_edit_date": 1592305723,
    "question_id": 17839,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/17839/what-is-ground-truth",
    "title": "What is Ground Truth",
    "body": "<p>In the context of <em>Machine Learning</em>, I have seen the term <em>Ground Truth</em> used a lot. I have searched a lot and found the following definition in <a href=\"https://en.wikipedia.org/wiki/Ground_truth\" rel=\"noreferrer\">Wikipedia</a>:</p>\n<blockquote>\n<p>In machine learning, the term &quot;ground truth&quot; refers to the accuracy of the training set's classification for supervised learning techniques. This is used in statistical models to prove or disprove research hypotheses. The term &quot;ground truthing&quot; refers to the process of gathering the proper objective (provable) data for this test. Compare with gold standard.</p>\n<p>Bayesian spam filtering is a common example of supervised learning. In this system, the algorithm is manually taught the differences between spam and non-spam. This depends on the ground truth of the messages used to train the algorithm  inaccuracies in the ground truth will correlate to inaccuracies in the resulting spam/non-spam verdicts.</p>\n</blockquote>\n<p>The point is that I really can not get what it means. Is that the <em>label</em> used for each <em>data object</em> or the <em>target function</em> which gives a label to each <em>data object</em>, or maybe something else?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "activation-function"
    ],
    "owner": {
      "account_id": 84460,
      "reputation": 603,
      "user_id": 16807,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/46ea107d3eb130cdea38bece3f7d3c26?s=256&d=identicon&r=PG",
      "display_name": "Bunny Rabbit",
      "link": "https://datascience.stackexchange.com/users/16807/bunny-rabbit"
    },
    "is_answered": true,
    "view_count": 35628,
    "accepted_answer_id": 26481,
    "answer_count": 4,
    "score": 48,
    "last_activity_date": 1645597624,
    "creation_date": 1515589667,
    "last_edit_date": 1515594034,
    "question_id": 26475,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/26475/why-is-relu-used-as-an-activation-function",
    "title": "Why is ReLU used as an activation function?",
    "body": "<p>Activation functions are used to introduce non-linearities in the linear output of the type <code>w * x + b</code> in a neural network.</p>\n\n<p>Which I am able to understand intuitively for the activation functions like sigmoid.</p>\n\n<p>I understand the advantages of ReLU, which is avoiding dead neurons during backpropagation. However, I am not able to understand why is ReLU used as an activation function if its output is linear?</p>\n\n<p>Doesn't the whole point of being the activation function get defeated if it won't introduce non-linearity?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user10640"
    },
    "is_answered": true,
    "view_count": 30557,
    "accepted_answer_id": 26945,
    "answer_count": 2,
    "score": 48,
    "last_activity_date": 1723638298,
    "creation_date": 1516663112,
    "last_edit_date": 1529064887,
    "question_id": 26938,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/26938/what-exactly-is-bootstrapping-in-reinforcement-learning",
    "title": "What exactly is bootstrapping in reinforcement learning?",
    "body": "<p>Apparently, in reinforcement learning, temporal-difference (TD) method is a bootstrapping method. On the other hand, Monte Carlo methods are not bootstrapping methods. </p>\n\n<p>What exactly is bootstrapping in RL? What is a bootstrapping method in RL?</p>\n"
  },
  {
    "tags": [
      "keras",
      "data",
      "cross-validation"
    ],
    "owner": {
      "account_id": 4311107,
      "reputation": 1608,
      "user_id": 49700,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2f1d9180433a779d2b10652cd9ebc02f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "rnso",
      "link": "https://datascience.stackexchange.com/users/49700/rnso"
    },
    "is_answered": true,
    "view_count": 134769,
    "accepted_answer_id": 38956,
    "answer_count": 2,
    "score": 48,
    "last_activity_date": 1639168607,
    "creation_date": 1538289057,
    "last_edit_date": 1575652515,
    "question_id": 38955,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/38955/how-does-the-validation-split-parameter-of-keras-fit-function-work",
    "title": "How does the validation_split parameter of Keras&#39; fit function work?",
    "body": "<p>Validation-split in Keras Sequential model fit function is documented as following on <a href=\"https://keras.io/models/sequential/\" rel=\"noreferrer\">https://keras.io/models/sequential/</a> : </p>\n\n<blockquote>\n  <p>validation_split: Float between 0 and 1. Fraction of the training data\n  to be used as validation data. The model will set apart this fraction\n  of the training data, will not train on it, and will evaluate the loss\n  and any model metrics on this data at the end of each epoch. The\n  validation data is selected from the last samples in the x and y data\n  provided, before shuffling.</p>\n</blockquote>\n\n<p>Please note the last line: </p>\n\n<blockquote>\n  <p>The validation data is selected from the last samples in the x and y\n  data provided, before shuffling.</p>\n</blockquote>\n\n<p>Does it means that validation data is always fixed and taken from bottom of main dataset?</p>\n\n<p>Is there any way it can be made to randomly select given fraction of data from main dataset?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "training"
    ],
    "owner": {
      "account_id": 9262288,
      "reputation": 1217,
      "user_id": 74397,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ef6f8746914a09cff205b6fe1306f7ae?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Ashwin Geet D&#39;Sa",
      "link": "https://datascience.stackexchange.com/users/74397/ashwin-geet-dsa"
    },
    "is_answered": true,
    "view_count": 68893,
    "accepted_answer_id": 56005,
    "answer_count": 5,
    "score": 48,
    "last_activity_date": 1635440457,
    "creation_date": 1563531022,
    "last_edit_date": 1635440457,
    "question_id": 55991,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/55991/in-the-context-of-deep-learning-what-is-training-warmup-steps",
    "title": "In the context of Deep Learning, what is training warmup steps",
    "body": "<p>I found the term &quot;training warmup steps&quot; in some of the papers. What exactly does this term mean? Has it got anything to do with &quot;learning rate&quot;? If so, how does it affect it?</p>\n"
  },
  {
    "tags": [
      "python",
      "scikit-learn",
      "linear-regression"
    ],
    "owner": {
      "account_id": 2811843,
      "reputation": 2023,
      "user_id": 13736,
      "user_type": "registered",
      "accept_rate": 45,
      "profile_image": "https://www.gravatar.com/avatar/9fe7def9462f2e9e696d8d5c367af484?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user",
      "link": "https://datascience.stackexchange.com/users/13736/user"
    },
    "is_answered": true,
    "view_count": 52050,
    "answer_count": 5,
    "score": 47,
    "last_activity_date": 1632014775,
    "creation_date": 1491879774,
    "last_edit_date": 1615833091,
    "question_id": 18258,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/18258/how-to-force-weights-to-be-non-negative-in-linear-regression",
    "title": "How to force weights to be non-negative in Linear regression",
    "body": "<p>I am using a standard linear regression using scikit-learn in python.\nHowever, I would like to force the weights to be all non-negative for every feature. is there any way I can accomplish that? I was looking in the documentation but could not find a way to accomplish that.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "optimization",
      "gradient-descent"
    ],
    "owner": {
      "account_id": 3595004,
      "reputation": 573,
      "user_id": 41658,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4a04eb13522013752c3ed009f89bb084?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "wit221",
      "link": "https://datascience.stackexchange.com/users/41658/wit221"
    },
    "is_answered": true,
    "view_count": 72231,
    "accepted_answer_id": 24537,
    "answer_count": 5,
    "score": 47,
    "last_activity_date": 1617098740,
    "creation_date": 1510245680,
    "last_edit_date": 1515603325,
    "question_id": 24534,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/24534/does-gradient-descent-always-converge-to-an-optimum",
    "title": "Does gradient descent always converge to an optimum?",
    "body": "<p>I am wondering whether there is any scenario in which gradient descent does not converge to a minimum.</p>\n\n<p>I am aware that gradient descent is not always guaranteed to converge to a global optimum. I am also aware that it might diverge from an optimum if, say, the step size is too big. However, it seems to me that, if it diverges from some optimum, then it will eventually go to another optimum.</p>\n\n<p>Hence, gradient descent would be guaranteed to converge to a local or global optimum. Is that right? If not, could you please provide a rough counterexample?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "bigdata",
      "statistics",
      "programming",
      "c"
    ],
    "owner": {
      "account_id": 4426354,
      "reputation": 1919,
      "user_id": 2723,
      "user_type": "registered",
      "accept_rate": 53,
      "profile_image": "https://i.sstatic.net/U7HAo.png?s=256",
      "display_name": "Hack-R",
      "link": "https://datascience.stackexchange.com/users/2723/hack-r"
    },
    "is_answered": true,
    "view_count": 54864,
    "accepted_answer_id": 5370,
    "answer_count": 12,
    "score": 46,
    "last_activity_date": 1670256612,
    "creation_date": 1426863383,
    "last_edit_date": 1587772464,
    "question_id": 5357,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/5357/data-science-in-c-or-c",
    "title": "Data Science in C (or C++)",
    "body": "<p>I'm an <code>R</code> language programmer. I'm also in the group of people who are considered Data Scientists but who come from academic disciplines other than CS.</p>\n\n<p>This works out well in my role as a Data Scientist, however, by starting my career in <code>R</code> and only having basic knowledge of other scripting/web languages, I've felt somewhat inadequate in 2 key areas:</p>\n\n<ol>\n<li>Lack of a solid knowledge of programming theory.</li>\n<li>Lack of a competitive level of skill in faster and more widely used languages like <code>C</code>, <code>C++</code> and <code>Java</code>, which could be utilized to increase the speed of the pipeline and Big Data computations as well as to create DS/data products which can be more readily developed into fast back-end scripts or standalone applications.</li>\n</ol>\n\n<p>The solution is simple of course -- go learn about programming, which is what I've been doing by enrolling in some classes (currently C programming). </p>\n\n<p>However, now that I'm starting to address problems #1 and #2 above, I'm left asking myself \"<em>Just how viable are languages like <code>C</code> and <code>C++</code> for Data Science?</em>\".</p>\n\n<p>For instance, I can move data around very quickly and interact with users just fine, but what about advanced regression, Machine Learning, text mining and other more advanced statistical operations? </p>\n\n<p><strong>So. can <code>C</code> do the job -- what tools are available for advanced statistics, ML, AI, and other areas of Data Science?</strong> Or must I lose most of the efficiency gained by programming in <code>C</code> by calling on <code>R</code> scripts or other languages?</p>\n\n<p>The best resource I've found thus far in C is a library called <a href=\"http://image.diku.dk/shark/sphinx_pages/build/html/index.html\" rel=\"nofollow noreferrer\">Shark</a>, which gives <code>C</code>/<code>C++</code> the ability to use Support Vector Machines, linear regression (not non-linear and other advanced regression like multinomial probit, etc) and a shortlist of other (great but) statistical functions.</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "tensorflow",
      "word-embeddings",
      "sampling",
      "loss-function"
    ],
    "owner": {
      "account_id": 2901835,
      "reputation": 4125,
      "user_id": 793,
      "user_type": "registered",
      "accept_rate": 60,
      "profile_image": "https://i.sstatic.net/N5kqP.jpg?s=256",
      "display_name": "tejaskhot",
      "link": "https://datascience.stackexchange.com/users/793/tejaskhot"
    },
    "is_answered": true,
    "view_count": 45067,
    "accepted_answer_id": 17905,
    "answer_count": 5,
    "score": 46,
    "last_activity_date": 1702220244,
    "creation_date": 1470368164,
    "last_edit_date": 1624401565,
    "question_id": 13216,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/13216/intuitive-explanation-of-noise-contrastive-estimation-nce-loss",
    "title": "Intuitive explanation of Noise Contrastive Estimation (NCE) loss?",
    "body": "<p>I read about NCE (a form of candidate sampling) from these two sources:</p>\n<p><a href=\"https://www.tensorflow.org/extras/candidate_sampling.pdf\" rel=\"noreferrer\">Tensorflow writeup</a></p>\n<p><a href=\"http://www.jmlr.org/proceedings/papers/v9/gutmann10a/gutmann10a.pdf\" rel=\"noreferrer\">Original Paper</a></p>\n<p>Can someone help me with the following:</p>\n<ol>\n<li>A simple explanation of how NCE works (I found the above difficult to parse and get an understanding of, so something intuitive that leads to the math presented there would be great)</li>\n<li>After point 1 above, a naturally intuitive description of how this is different from Negative Sampling. I can see that there's a slight change in the formula but could not understand the math. I do have an intuitive understanding of negative sampling in the context of <code>word2vec</code> - we randomly choose some samples from the vocabulary <code>V</code> and update only those because <code>|V|</code> is large and this offers a speedup. Please correct if wrong.</li>\n<li>When to use which one and how is that decided? It would be great if you could include examples(possibly easy to understand applications)</li>\n<li>Is NCE better than Negative Sampling? Better in what manner?</li>\n</ol>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "deep-learning",
      "keras",
      "tensorflow"
    ],
    "owner": {
      "account_id": 353148,
      "reputation": 1033,
      "user_id": 43921,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f8fed02bb0a9ffb6e255b66e2d8310e7?s=256&d=identicon&r=PG",
      "display_name": "Rkz",
      "link": "https://datascience.stackexchange.com/users/43921/rkz"
    },
    "is_answered": true,
    "view_count": 75697,
    "accepted_answer_id": 26119,
    "answer_count": 2,
    "score": 46,
    "last_activity_date": 1655036819,
    "creation_date": 1514535168,
    "last_edit_date": 1548794758,
    "question_id": 26103,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/26103/merging-two-different-models-in-keras",
    "title": "Merging two different models in Keras",
    "body": "<p>I am trying to merge two Keras models into a single model and I am unable to accomplish this.</p>\n\n<p>For example in the attached Figure, I would like to fetch the middle layer $A2$ of dimension 8, and use this as input to the layer $B1$ (of dimension 8 again) in Model $B$ and then combine both Model $A$ and Model $B$ as a single model.</p>\n\n<p>I am using the functional module to create Model $A$ and Model $B$ independently.  How can I accomplish this task?</p>\n\n<p><strong>Note</strong>: $A1$ is the input layer to model $A$ and $B1$ is the input layer to model $B$.</p>\n\n<p><a href=\"https://i.sstatic.net/Chgpo.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/Chgpo.jpg\" alt=\"See Picture\"></a></p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning"
    ],
    "owner": {
      "account_id": 7196137,
      "reputation": 685,
      "user_id": 37063,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/XovDd.jpg?s=256",
      "display_name": "Kishan Kumar",
      "link": "https://datascience.stackexchange.com/users/37063/kishan-kumar"
    },
    "is_answered": true,
    "view_count": 71863,
    "protected_date": 1591277740,
    "accepted_answer_id": 27616,
    "answer_count": 2,
    "score": 46,
    "last_activity_date": 1654613889,
    "creation_date": 1518108804,
    "last_edit_date": 1654613889,
    "question_id": 27615,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/27615/should-we-apply-normalization-to-test-data-as-well",
    "title": "Should we apply normalization to test data as well?",
    "body": "<p>I am doing a project on an author identification problem. I applied the tf-idf normalization to train data and then trained an SVM on that data.</p>\n<p>Now when using the classifier, should I normalize test data as well. I feel that the basic aim of normalization is to make the learning algorithm give more weight to more important features while learning. So once it has been trained, it already knows which features are important and which are not. So is there any need to apply normalization to test data as well?</p>\n<p>I am new to this field. So please ignore if the question appears silly?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "model-evaluations"
    ],
    "owner": {
      "account_id": 11188804,
      "reputation": 1167,
      "user_id": 50247,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/a8c56d24daf403c7208b0a7d7fb3b53b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Rajat",
      "link": "https://datascience.stackexchange.com/users/50247/rajat"
    },
    "is_answered": true,
    "view_count": 101117,
    "accepted_answer_id": 30882,
    "answer_count": 10,
    "score": 46,
    "last_activity_date": 1718244976,
    "creation_date": 1524753061,
    "last_edit_date": 1612750410,
    "question_id": 30881,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/30881/when-is-precision-more-important-over-recall",
    "title": "When is precision more important over recall?",
    "body": "<p>Can anyone give me some examples where precision is important and some examples where recall is important?</p>\n"
  },
  {
    "tags": [
      "data-wrangling"
    ],
    "owner": {
      "account_id": 2851781,
      "reputation": 569,
      "user_id": 53204,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/V62UY.jpg?s=256",
      "display_name": "Victor Valente",
      "link": "https://datascience.stackexchange.com/users/53204/victor-valente"
    },
    "is_answered": true,
    "view_count": 8373,
    "accepted_answer_id": 48607,
    "answer_count": 9,
    "score": 46,
    "last_activity_date": 1576593776,
    "creation_date": 1554304584,
    "last_edit_date": 1576593776,
    "question_id": 48531,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/48531/how-much-of-data-wrangling-is-a-data-scientists-job",
    "title": "How much of data wrangling is a data scientist&#39;s job?",
    "body": "<p>I'm currently working as a data scientist at a large company (my first job as a DS, so this question may be a result of my lack of experience). They have a huge backlog of really important data science projects that would have a great positive impact if implemented. <strong>But.</strong></p>\n\n<p>Data pipelines are non-existent within the company, the standard procedure is for them to hand me gigabytes of <strong>TXT files</strong> whenever I need some information. Think of these files as tabular logs of transactions stored in arcane notation and structure. No whole piece of information is contained in one single data source, and they can't grant me access to their ERP database for \"security reasons\".</p>\n\n<p>Initial data analysis for the simplest project requires brutal, excruciating data wrangling. More than 80% of a project's time spent is me trying to <strong>parse these files</strong> and <strong>cross data sources</strong> in order to build viable datasets. This is not a problem of simply handling missing data or preprocessing it, it's about <strong>the work it takes to build data that can be handled in the first place</strong> (<em>solvable by dba or data engineering, not data science?</em>).</p>\n\n<hr>\n\n<p>1) <em>Feels like most of the work is not related to data science at all. Is this accurate?</em></p>\n\n<p>2) I know this is not a data-driven company with a high-level data engineering department, <em>but it is my opinion that in order to build for a sustainable future of data science projects, <a href=\"https://hackernoon.com/the-ai-hierarchy-of-needs-18f111fcc007\" rel=\"nofollow noreferrer\">minimum levels of data accessibility are required</a>. Am I wrong?</em></p>\n\n<p>3) <em>Is this type of setup common for a company with serious data science needs?</em></p>\n"
  },
  {
    "tags": [
      "machine-learning-model",
      "training",
      "supervised-learning",
      "accuracy",
      "overfitting"
    ],
    "owner": {
      "account_id": 2208562,
      "reputation": 569,
      "user_id": 88251,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f8cf9af204185f8a59357edd4ffaf273?s=256&d=identicon&r=PG",
      "display_name": "EitanT",
      "link": "https://datascience.stackexchange.com/users/88251/eitant"
    },
    "is_answered": true,
    "view_count": 9630,
    "answer_count": 8,
    "score": 46,
    "last_activity_date": 1626370462,
    "creation_date": 1578836905,
    "question_id": 66350,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/66350/what-would-i-prefer-an-over-fitted-model-or-a-less-accurate-model",
    "title": "What would I prefer - an over-fitted model or a less accurate model?",
    "body": "<p>Let's say we have two models trained. And let's say we are looking for good accuracy. \nThe first has an accuracy of 100% on training set and 84% on test set. Clearly over-fitted.\nThe second has an accuracy of 83% on training set and 83% on test set. </p>\n\n<p>On the one hand, model #1 is over-fitted but on the other hand it still yields better performance on an unseen test set than the good general model in #2. </p>\n\n<p>Which model would you choose to use in production? The First or the Second and why?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "keras",
      "tensorflow",
      "loss-function"
    ],
    "owner": {
      "account_id": 18421072,
      "reputation": 573,
      "user_id": 95887,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Ghmj91i_7JZFSBEINOQu1nIYp2XWmm9Wtc4JqES=k-s256",
      "display_name": "Nagendra Prasad",
      "link": "https://datascience.stackexchange.com/users/95887/nagendra-prasad"
    },
    "is_answered": true,
    "view_count": 56718,
    "accepted_answer_id": 124300,
    "answer_count": 3,
    "score": 46,
    "last_activity_date": 1698414980,
    "creation_date": 1588003680,
    "last_edit_date": 1588111345,
    "question_id": 73093,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/73093/what-does-from-logits-true-do-in-sparsecategoricalcrossentropy-loss-function",
    "title": "What does from_logits=True do in SparseCategoricalcrossEntropy loss function?",
    "body": "<p>In the documentation it has been mentioned that <code>y_pred</code> needs to be in the range of [-inf to inf] when <code>from_logits=True</code>. I truly didn't understand what this means, since the probabilities need to be in the range of 0 to 1! Can someone please explain in simple words the effect of using <code>from_logits=True</code>?</p>\n\n<pre><code>model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n</code></pre>\n"
  },
  {
    "tags": [
      "data-cleaning",
      "anonymization"
    ],
    "owner": {
      "account_id": 2736748,
      "reputation": 832,
      "user_id": 322,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/y1Mud.gif?s=256",
      "display_name": "Air",
      "link": "https://datascience.stackexchange.com/users/322/air"
    },
    "is_answered": true,
    "view_count": 6983,
    "accepted_answer_id": 446,
    "answer_count": 6,
    "score": 45,
    "last_activity_date": 1718596072,
    "creation_date": 1402948111,
    "last_edit_date": 1714843138,
    "question_id": 412,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/412/how-can-i-transform-names-in-a-confidential-data-set-to-make-it-anonymous-but-p",
    "title": "How can I transform names in a confidential data set to make it anonymous, but preserve some of the characteristics of the names?",
    "body": "<h1>Motivation</h1>\n<p>I work with datasets that contain personally identifiable information (PII) and sometimes need to share part of a dataset with third parties, in a way that doesn't expose PII and subject my employer to liability. Our usual approach here is to withhold data entirely, or in some cases to reduce its resolution; e.g., replacing an exact street address with the corresponding county or census tract.</p>\n<p>This means that certain types of analysis and processing must be done in-house, even when a third party has resources and expertise more suited to the task. Since the source data is not disclosed, the way we go about this analysis and processing lacks transparency. As a result, any third party's ability to perform QA/QC, adjust parameters or make refinements may be very limited.</p>\n<h1>Anonymizing Confidential Data</h1>\n<p>One task involves identifying individuals by their names, in user-submitted data, while taking into account errors and inconsistencies. A private individual might be recorded in one place as &quot;Dave&quot; and in another as &quot;David,&quot; commercial entities can have many different abbreviations, and there are always some typos. I've developed scripts based on a number of criteria that determine when two records with non-identical names represent the same individual, and assign them a common ID.</p>\n<p>At this point we can make the dataset anonymous by withholding the names and replacing them with this personal ID number. But this means the recipient has almost no information about e.g. the strength of the match. We would prefer to be able to pass along as much information as possible without divulging identity.</p>\n<h1>What Doesn't Work</h1>\n<p>For instance, it would be great to be able to encrypt strings while preserving edit distance. This way, third parties could do some of their own QA/QC, or choose to do further processing on their own, without ever accessing (or being able to potentially reverse-engineer) PII. Perhaps we match strings in-house with edit distance &lt;= 2, and the recipient wants to look at the implications of tightening that tolerance to edit distance &lt;= 1.</p>\n<p>But the only method I am familiar with that does this is <a href=\"https://en.wikipedia.org/wiki/ROT13\" rel=\"nofollow noreferrer\">ROT13</a> (more generally, any <a href=\"https://en.wikipedia.org/wiki/Caesar_cipher\" rel=\"nofollow noreferrer\">shift cipher</a>), which hardly even counts as encryption; it's like writing the names upside down and saying, &quot;Promise you won't flip the paper over?&quot;</p>\n<p>Another <strong>bad</strong> solution would be to abbreviate everything. &quot;Ellen Roberts&quot; becomes &quot;ER&quot; and so forth. This is a poor solution because in some cases the initials, in association with public data, will reveal a person's identity, and in other cases it's too ambiguous; &quot;Benjamin Othello Ames&quot; and &quot;Bank of America&quot; will have the same initials, but their names are otherwise dissimilar. So it doesn't do either of the things we want.</p>\n<p>An inelegant alternative is to introduce additional fields to track certain attributes of the name, e.g.:</p>\n<pre><code>+-----+----+-------------------+-----------+--------+\n| Row | ID | Name              | WordChars | Origin |\n+-----+----+-------------------+-----------+--------+\n| 1   | 17 | &quot;AMELIA BEDELIA&quot;  | (6, 7)    | Eng    |\n+-----+----+-------------------+-----------+--------+\n| 2   | 18 | &quot;CHRISTOPH BAUER&quot; | (9, 5)    | Ger    |\n+-----+----+-------------------+-----------+--------+\n| 3   | 18 | &quot;C J BAUER&quot;       | (1, 1, 5) | Ger    |\n+-----+----+-------------------+-----------+--------+\n| 4   | 19 | &quot;FRANZ HELLER&quot;    | (5, 6)    | Ger    |\n+-----+----+-------------------+-----------+--------+\n</code></pre>\n<p>I call this &quot;inelegant&quot; because it requires anticipating which qualities might be interesting and it's relatively coarse. If the names are removed, there's not much you can reasonably conclude about the strength of the match between rows 2 &amp; 3, or about the distance between rows 2 &amp; 4 (i.e., how close they are to matching).</p>\n<h1>Conclusion</h1>\n<p>The goal is to transform strings in such a way that as many useful qualities of the original string are preserved as possible while obscuring the original string. Decryption should be impossible, or so impractical as to be effectively impossible, no matter the size of the data set. In particular, a method that preserves the edit distance between arbitrary strings would be very useful.</p>\n<p>I've found a couple papers that might be relevant, but they're a bit over my head:</p>\n<ul>\n<li><a href=\"http://www.merl.com/publications/docs/TR2010-109.pdf\" rel=\"nofollow noreferrer\">Privacy Preserving String Comparisons Based on Levenshtein Distance</a></li>\n<li><a href=\"https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=2889272a345601e5bfecd174c341f54af08811aa\" rel=\"nofollow noreferrer\">An Empirical Comparison of Approaches to Approximate String\nMatching in Private Record Linkage</a></li>\n</ul>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "distance"
    ],
    "owner": {
      "account_id": 1009009,
      "reputation": 629,
      "user_id": 34036,
      "user_type": "registered",
      "accept_rate": 80,
      "profile_image": "https://i.sstatic.net/DkJLX.png?s=256",
      "display_name": "Bitcoin Cash - ADA enthusiast",
      "link": "https://datascience.stackexchange.com/users/34036/bitcoin-cash-ada-enthusiast"
    },
    "is_answered": true,
    "view_count": 77663,
    "answer_count": 6,
    "score": 45,
    "last_activity_date": 1648538445,
    "creation_date": 1498804095,
    "last_edit_date": 1586188878,
    "question_id": 20075,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/20075/when-would-one-use-manhattan-distance-as-opposed-to-euclidean-distance",
    "title": "When would one use Manhattan distance as opposed to Euclidean distance?",
    "body": "<p>I am trying to look for a good argument on why one would use the <strong>Manhattan distance</strong> over the <strong>Euclidean distance</strong> in machine learning.</p>\n\n<p>The closest thing I found to a good argument so far is on <a href=\"https://www.youtube.com/watch?v=h0e2HAPTGF4\" rel=\"noreferrer\">this MIT lecture</a>.</p>\n\n<p>At 36:15 you can see on the slides the following statement:</p>\n\n<p><em>\"Typically use Euclidean metric; <strong>Manhattan may be appropriate if different dimensions are not comparable.</strong>\"</em></p>\n\n<p>Shortly after, the professor says that, because the number of legs of a reptile varies from 0 to 4 (whereas the other features are binary, only vary from 0 to 1), the \"number of legs\" feature will end up having a much higher weight if the Euclidean distance is used. Sure enough, that is indeed right. But one would also have that problem if using the Manhattan distance (only that the problem would be slightly mitigated because we don't square the difference like we do on the Euclidean distance).</p>\n\n<p>A better way to solve the above problem would be to normalize the \"number of legs\" feature so its value will always be between 0 and 1.</p>\n\n<p>Therefore, since there is a better way to solve the problem, it felt like the argument of using the Manhattan distance in this case lacked a stronger point, at least in my opinion.    </p>\n\n<p><strong>Does anyone actually know why and when someone would use Manhattan distance over Euclidean? Can anyone give me an example in which using the Manhattan distance would yield better results?</strong></p>\n"
  },
  {
    "tags": [
      "neural-network",
      "deep-learning",
      "keras",
      "tensorflow",
      "metric"
    ],
    "owner": {
      "account_id": 8453205,
      "reputation": 2015,
      "user_id": 51129,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/29013c1e5a2906d8f3d08f27c953095e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "N.IT",
      "link": "https://datascience.stackexchange.com/users/51129/n-it"
    },
    "is_answered": true,
    "view_count": 71071,
    "answer_count": 7,
    "score": 45,
    "last_activity_date": 1728933993,
    "creation_date": 1544778494,
    "last_edit_date": 1544791618,
    "question_id": 42599,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/42599/what-is-the-relationship-between-the-accuracy-and-the-loss-in-deep-learning",
    "title": "What is the relationship between the accuracy and the loss in deep learning?",
    "body": "<p>I have created three different models using deep learning for multi-class classification and each model gave me a different accuracy and loss value. The results of the testing model as the following:</p>\n\n<ul>\n<li><p>First Model: Accuracy: 98.1% Loss: 0.1882</p></li>\n<li><p>Second Model: Accuracy: 98.5% Loss: 0.0997</p></li>\n<li><p>Third Model: Accuracy: 99.1% Loss: 0.2544</p></li>\n</ul>\n\n<p>My questions are:</p>\n\n<ul>\n<li><p>What is the relationship between the loss and accuracy values?</p></li>\n<li><p>Why the loss of the third model is the higher even though the accuracy is higher?</p></li>\n</ul>\n"
  },
  {
    "tags": [
      "neural-network",
      "optimization",
      "backpropagation"
    ],
    "owner": {
      "account_id": 270432,
      "reputation": 541,
      "user_id": 16753,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/683071653c5e153a6ffcb88ee0c6506f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "mplappert",
      "link": "https://datascience.stackexchange.com/users/16753/mplappert"
    },
    "is_answered": true,
    "view_count": 57867,
    "answer_count": 4,
    "score": 44,
    "last_activity_date": 1618575708,
    "creation_date": 1457083937,
    "last_edit_date": 1546050481,
    "question_id": 10523,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/10523/guidelines-for-selecting-an-optimizer-for-training-neural-networks",
    "title": "Guidelines for selecting an optimizer for training neural networks",
    "body": "<p>I have been using neural networks for a while now. However, one thing that I constantly struggle with is the selection of an optimizer for training the network (using backprop). What I usually do is just start with one (e.g. standard SGD) and then try other others pretty much randomly. I was wondering if there's a better (and less random) approach to finding a good optimizer, e.g. from this list:</p>\n\n<ul>\n<li>SGD (with or without momentum)</li>\n<li>AdaDelta</li>\n<li>AdaGrad</li>\n<li>RMSProp</li>\n<li>Adam</li>\n</ul>\n\n<p>In particular, I am interested if there's some theoretical justification for picking one over another given the training data has some property, e.g. it being sparse. I would also imagine that some optimizers work better than others in specific domains, e.g. when training convolutional networks vs. feed-forward networks or classification vs. regression.</p>\n\n<p>If any of you have developed some strategy and/or intuition on how you pick optimizers, I'd be greatly interested in hearing it. Furthermore, if there's some work that provides theoretical justification for picking one over another, that would be even better.</p>\n"
  },
  {
    "tags": [
      "xgboost"
    ],
    "owner": {
      "account_id": 10817276,
      "reputation": 553,
      "user_id": 32148,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/--MQS4GN786c/AAAAAAAAAAI/AAAAAAAAAAA/luHGATrX2l0/s256-rj/photo.jpg",
      "display_name": "Sergey Nizhevyasov",
      "link": "https://datascience.stackexchange.com/users/32148/sergey-nizhevyasov"
    },
    "is_answered": true,
    "view_count": 28157,
    "accepted_answer_id": 19366,
    "answer_count": 2,
    "score": 44,
    "last_activity_date": 1712626156,
    "creation_date": 1494504726,
    "last_edit_date": 1712626156,
    "question_id": 18903,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/18903/lightgbm-vs-xgboost",
    "title": "LightGBM vs XGBoost",
    "body": "<p>I'm trying to understand which is better (more accurate, especially in classification problems)</p>\n<p>I've been searching articles comparing LightGBM and XGBoost but found only two:</p>\n<ol>\n<li><a href=\"https://medium.com/implodinggradients/benchmarking-lightgbm-how-fast-is-lightgbm-vs-xgboost-15d224568031\" rel=\"nofollow noreferrer\">https://medium.com/implodinggradients/benchmarking-lightgbm-how-fast-is-lightgbm-vs-xgboost-15d224568031</a> - which is only about speed but not accuracy.</li>\n<li><a href=\"https://lightgbm.readthedocs.io/en/latest/Experiments.html\" rel=\"nofollow noreferrer\">https://lightgbm.readthedocs.io/en/latest/Experiments.html</a> - which is from the authors of LightGBM and no surprise LightGBM wins there.</li>\n</ol>\n<p>In my tests I get pretty the same AUC for both algorithms, but LightGBM runs form 2 to 5 times faster.</p>\n<p>If LGBM is so cool, why don't I hear so much about it here and on Kaggle :)</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "terminology"
    ],
    "owner": {
      "account_id": 6444670,
      "reputation": 8476,
      "user_id": 11097,
      "user_type": "registered",
      "accept_rate": 75,
      "profile_image": "https://i.sstatic.net/z0jss.png?s=256",
      "display_name": "Dawny33",
      "link": "https://datascience.stackexchange.com/users/11097/dawny33"
    },
    "is_answered": true,
    "view_count": 24259,
    "accepted_answer_id": 22338,
    "answer_count": 9,
    "score": 44,
    "last_activity_date": 1597948224,
    "creation_date": 1502970795,
    "question_id": 22335,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/22335/why-are-machine-learning-models-called-black-boxes",
    "title": "Why are Machine Learning models called black boxes?",
    "body": "<p>I was reading this blog post titled: <a href=\"https://www.technologyreview.com/s/604122/the-financial-world-wants-to-open-ais-black-boxes/\" rel=\"noreferrer\">The Financial World Wants to Open AIs Black Boxes</a>, where the author repeatedly refer to ML models as \"black boxes\". </p>\n\n<p>A similar terminology has been used at several places when referring to ML models. <strong>Why is it so?</strong></p>\n\n<p>It is not like the ML engineers don't know what goes on inside a neural net. Every layer is selected by the ML engineer knowing what activation function to use, what that type of layer does, how the error is back propagated, etc.</p>\n"
  },
  {
    "tags": [
      "python",
      "deep-learning",
      "tensorflow",
      "keras",
      "gpu"
    ],
    "owner": {
      "account_id": 4911659,
      "reputation": 579,
      "user_id": 40665,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/ZKAREwmS.png?s=256",
      "display_name": "Hector Blandin",
      "link": "https://datascience.stackexchange.com/users/40665/hector-blandin"
    },
    "is_answered": true,
    "view_count": 57271,
    "accepted_answer_id": 25737,
    "answer_count": 4,
    "score": 44,
    "last_activity_date": 1609481886,
    "creation_date": 1508358652,
    "last_edit_date": 1609481886,
    "question_id": 23895,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/23895/multi-gpu-in-keras",
    "title": "Multi GPU in Keras",
    "body": "<p>How we can program in the Keras library (or TensorFlow) to partition training on multiple GPUs? Let's say that you are in an Amazon ec2 instance that has 8 GPUs and you would like to use all of them to train faster, but your code is just for a single CPU or GPU.</p>\n"
  },
  {
    "tags": [
      "activation-function",
      "bert",
      "mathematics"
    ],
    "owner": {
      "account_id": 12563799,
      "reputation": 2485,
      "user_id": 50406,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Eq7j1.jpg?s=256",
      "display_name": "thanatoz",
      "link": "https://datascience.stackexchange.com/users/50406/thanatoz"
    },
    "is_answered": true,
    "view_count": 33341,
    "accepted_answer_id": 49535,
    "answer_count": 2,
    "score": 44,
    "last_activity_date": 1618015289,
    "creation_date": 1555574784,
    "last_edit_date": 1578546798,
    "question_id": 49522,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/49522/what-is-gelu-activation",
    "title": "What is GELU activation?",
    "body": "<p>I was going through <a href=\"https://arxiv.org/pdf/1810.04805.pdf\" rel=\"noreferrer\">BERT paper</a> which uses <a href=\"https://arxiv.org/pdf/1606.08415.pdf\" rel=\"noreferrer\">GELU (Gaussian Error Linear Unit)</a> which states equation as \n<span class=\"math-container\">$$ GELU(x) = xP(X  x) = x(x).$$</span> which in turn is approximated to <span class=\"math-container\">$$0.5x(1 + tanh[\\sqrt{\n2/}(x + 0.044715x^3)])$$</span></p>\n\n<p>Could you simplify the equation and explain how it has been approximated.</p>\n"
  },
  {
    "tags": [
      "tools",
      "career",
      "excel"
    ],
    "owner": {
      "account_id": 971357,
      "reputation": 533,
      "user_id": 8944,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/kAf2h.jpg?s=256",
      "display_name": "JHowIX",
      "link": "https://datascience.stackexchange.com/users/8944/jhowix"
    },
    "is_answered": true,
    "view_count": 33151,
    "protected_date": 1561776648,
    "accepted_answer_id": 5450,
    "answer_count": 10,
    "score": 43,
    "last_activity_date": 1561761671,
    "creation_date": 1428092916,
    "last_edit_date": 1450599518,
    "question_id": 5443,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/5443/do-data-scientists-use-excel",
    "title": "Do data scientists use Excel?",
    "body": "<p>I would consider myself a journeyman data scientist.  Like most (I think), I made my first charts and did my first aggregations in high school and college, using Excel.  As I went through college, grad school and ~7 years of work experience, I quickly picked up what I consider to be more advanced tools, like SQL, R, Python, Hadoop, LaTeX, etc.</p>\n\n<p>We are interviewing for a data scientist position and one candidate advertises himself as a \"senior data scientist\" (a very buzzy term these days) with 15+ years experience.  When asked what his preferred toolset was, he responded that it was Excel.</p>\n\n<p>I took this as evidence that he was not as experienced as his resume would claim, but wasn't sure.  After all, just because it's not my preferred tool, doesn't mean it's not other people's. <strong>Do experienced data scientists use Excel?  Can you assume a lack of experience from someone who does primarily use Excel?</strong></p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "random-forest",
      "svm"
    ],
    "owner": {
      "account_id": 1782823,
      "reputation": 585,
      "user_id": 12350,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2ae7ca5b1de8c1c84f39df6770544ad0?s=256&d=identicon&r=PG",
      "display_name": "Rohit",
      "link": "https://datascience.stackexchange.com/users/12350/rohit"
    },
    "is_answered": true,
    "view_count": 110530,
    "accepted_answer_id": 6855,
    "answer_count": 5,
    "score": 43,
    "last_activity_date": 1654909795,
    "creation_date": 1440044203,
    "last_edit_date": 1493223872,
    "question_id": 6838,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/6838/when-to-use-random-forest-over-svm-and-vice-versa",
    "title": "When to use Random Forest over SVM and vice versa?",
    "body": "<p><strong>When would one use <code>Random Forest</code> over <code>SVM</code> and vice versa?</strong> </p>\n\n<p>I understand that <code>cross-validation</code> and model comparison is an important aspect of choosing a model, but here I would like to learn more about rules of thumb and heuristics of the two methods.</p>\n\n<p>Can someone please explain the subtleties, strengths, and weaknesses of the classifiers as well as problems, which are best suited to each of them?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network"
    ],
    "owner": {
      "account_id": 10412378,
      "reputation": 545,
      "user_id": 34037,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/686df1f4367033782c57aa8131fc3ab6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user7677413",
      "link": "https://datascience.stackexchange.com/users/34037/user7677413"
    },
    "is_answered": true,
    "view_count": 42706,
    "accepted_answer_id": 20230,
    "answer_count": 1,
    "score": 43,
    "last_activity_date": 1552782527,
    "creation_date": 1499367944,
    "question_id": 20222,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/20222/how-to-decide-neural-network-architecture",
    "title": "How to decide neural network architecture?",
    "body": "<p>I was wondering how do we have to decide how many nodes in hidden layers, and how many hidden layers to put when we build a neural network architecture.</p>\n\n<p>I understand the input and output layer depends on the training set that we have but how do we decide the hidden layer and the overall architecture in general?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning"
    ],
    "owner": {
      "account_id": 6631713,
      "reputation": 14308,
      "user_id": 28175,
      "user_type": "registered",
      "accept_rate": 95,
      "profile_image": "https://i.sstatic.net/ojtVl.jpg?s=256",
      "display_name": "Green Falcon",
      "link": "https://datascience.stackexchange.com/users/28175/green-falcon"
    },
    "is_answered": true,
    "view_count": 27431,
    "protected_date": 1545226851,
    "closed_date": 1603871702,
    "answer_count": 13,
    "community_owned_date": 1545113877,
    "score": 43,
    "last_activity_date": 1545223074,
    "creation_date": 1544798251,
    "last_edit_date": 1545121407,
    "question_id": 42621,
    "link": "https://datascience.stackexchange.com/questions/42621/data-science-related-funny-quotes",
    "closed_reason": "Not suitable for this site",
    "title": "Data science related funny quotes",
    "body": "<p>It has been customary for the users of different <a href=\"https://stats.stackexchange.com/q/1337/179078\">communities</a> to quote funny things about their fields. It may be fun to share your funny things about Machine Learning, Deep Learning, Data Science and the things that you face every day! </p>\n"
  },
  {
    "tags": [
      "normalization"
    ],
    "owner": {
      "account_id": 14944581,
      "reputation": 819,
      "user_id": 69963,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/2096074213760835/picture?type=large",
      "display_name": "Tauno",
      "link": "https://datascience.stackexchange.com/users/69963/tauno"
    },
    "is_answered": true,
    "view_count": 60296,
    "answer_count": 5,
    "score": 43,
    "last_activity_date": 1686770209,
    "creation_date": 1562070101,
    "last_edit_date": 1685941879,
    "question_id": 54908,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/54908/data-normalization-before-or-after-train-test-split",
    "title": "Data normalization before or after train-test split?",
    "body": "<p>Which one is the right approach to make data normalization - before or after train-test split?</p>\n<p><strong>Normalization before split</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>from sklearn.preprocessing import StandardScaler\n\nnormalized_X_features = pd.DataFrame(\n    StandardScaler().fit_transform(X_features),\n    columns = X_features.columns\n)\n\nx_train, x_test, y_train, y_test = train_test_split(\n    normalized_X_features,\n    Y_feature,\n    test_size=0.20,\n    random_state=4\n)\nLR = LogisticRegression(\n    C=0.01,\n    solver='liblinear'\n).fit(x_train, y_train)\n\ny_test_pred = LR.predict(x_test)\n</code></pre>\n<p><strong>Normalization after split</strong></p>\n<pre class=\"lang-py prettyprint-override\"><code>x_train, x_test, y_train, y_test = train_test_split(\n    X_features,\n    Y_feature,\n    test_size=0.20,\n    random_state=4\n)\nnormalized_x_train = pd.DataFrame(\n    StandardScaler().fit_transform(x_train),\n    columns = x_train.columns\n)\nLR = LogisticRegression(\n    C=0.01,\n    solver='liblinear'\n).fit(normalized_x_train, y_train)\n\nnormalized_x_test = pd.DataFrame(\n    StandardScaler().fit_transform(x_test),\n    columns = x_test.columns\n)\ny_test_pred = LR.predict(normalized_x_test)\n</code></pre>\n<p>So far I have seen both approaches.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "sports"
    ],
    "owner": {
      "account_id": 2317167,
      "reputation": 3208,
      "user_id": 434,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/P75cO.jpg?s=256",
      "display_name": "Steve Kallestad",
      "link": "https://datascience.stackexchange.com/users/434/steve-kallestad"
    },
    "is_answered": true,
    "view_count": 46383,
    "protected_date": 1480417142,
    "accepted_answer_id": 285,
    "answer_count": 10,
    "score": 42,
    "last_activity_date": 1597947942,
    "creation_date": 1402397938,
    "last_edit_date": 1425299591,
    "question_id": 265,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/265/can-machine-learning-algorithms-predict-sports-scores-or-plays",
    "title": "Can machine learning algorithms predict sports scores or plays?",
    "body": "<p>I have a variety of NFL datasets that I think might make a good side-project, but I haven't done anything with them just yet.</p>\n\n<p>Coming to this site made me think of machine learning algorithms and I wondering how good they might be at either predicting the outcome of football games or even the next play.</p>\n\n<p>It seems to me that there would be some trends that could be identified - on 3rd down and 1, a team with a strong running back <em>theoretically should</em> have a tendency to run the ball in that situation.</p>\n\n<p>Scoring might be more difficult to predict, but the winning team might be.</p>\n\n<p>My question is whether these are good questions to throw at a machine learning algorithm.  It could be that a thousand people have tried it before, but the nature of sports makes it an unreliable topic.</p>\n"
  },
  {
    "tags": [
      "classification",
      "xgboost",
      "multiclass-classification",
      "class-imbalance"
    ],
    "owner": {
      "account_id": 968001,
      "reputation": 585,
      "user_id": 14434,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/aba20b487b9ef2bada1aa5c1a83fd5ab?s=256&d=identicon&r=PG",
      "display_name": "shda",
      "link": "https://datascience.stackexchange.com/users/14434/shda"
    },
    "is_answered": true,
    "view_count": 56250,
    "accepted_answer_id": 18823,
    "answer_count": 6,
    "score": 42,
    "last_activity_date": 1659723782,
    "creation_date": 1484571192,
    "last_edit_date": 1546180155,
    "question_id": 16342,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/16342/unbalanced-multiclass-data-with-xgboost",
    "title": "Unbalanced multiclass data with XGBoost",
    "body": "<p>I have 3 classes with this distribution:</p>\n\n<pre><code>Class 0: 0.1169\nClass 1: 0.7668\nClass 2: 0.1163\n</code></pre>\n\n<p>And I am using <code>xgboost</code> for classification. I know that there is a parameter called <code>scale_pos_weight</code>. </p>\n\n<p>But how is it handled for 'multiclass' case, and how can I properly set it?</p>\n"
  },
  {
    "tags": [
      "correlation",
      "pearsons-correlation-coefficient",
      "spearmans-rank-correlation",
      "kendalls-tau-coefficient"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user86099"
    },
    "is_answered": true,
    "view_count": 56695,
    "accepted_answer_id": 64261,
    "answer_count": 1,
    "score": 42,
    "last_activity_date": 1621678161,
    "creation_date": 1575545597,
    "last_edit_date": 1615490515,
    "question_id": 64260,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/64260/pearson-vs-spearman-vs-kendall",
    "title": "Pearson vs Spearman vs Kendall",
    "body": "<p>What are the  characteristics of the three correlation coefficients and what are the comparisons of each of them/assumptions?</p>\n<p>Can somebody kindly take me through the concepts?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "image-classification",
      "convolutional-neural-network",
      "preprocessing"
    ],
    "owner": {
      "account_id": 2048845,
      "reputation": 3152,
      "user_id": 26,
      "user_type": "registered",
      "accept_rate": 60,
      "profile_image": "https://i.sstatic.net/S6fiE.png?s=256",
      "display_name": "Alex I",
      "link": "https://datascience.stackexchange.com/users/26/alex-i"
    },
    "is_answered": true,
    "view_count": 42013,
    "accepted_answer_id": 9227,
    "answer_count": 2,
    "score": 41,
    "last_activity_date": 1655406524,
    "creation_date": 1424779176,
    "last_edit_date": 1655406524,
    "question_id": 5224,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/5224/how-to-prepare-augment-images-for-neural-network",
    "title": "How to prepare/augment images for neural network?",
    "body": "<p>I would like to use a neural network for image classification.  I'll start with pre-trained CaffeNet and train it for my application.  </p>\n\n<h1>How should I prepare the input images?</h1>\n\n<p>In this case, all the images are of the same object but with variations (think: quality control).  They are at somewhat different scales/resolutions/distances/lighting conditions (and in many cases I don't know the scale).  Also, in each image there is an area (known) around the object of interest that should be ignored by the network.</p>\n\n<p>I could (for example) crop the center of each image, which is guaranteed to contain a portion of the object of interest and none of the ignored area; but that seems like it would throw away information, and also the results wouldn't be really the same scale (maybe 1.5x variation).</p>\n\n<h1>Dataset augmentation</h1>\n\n<p>I've heard of creating more training data by random crop/mirror/etc, is there a standard method for this?  Any results on how much improvement it produces to classifier accuracy?</p>\n"
  },
  {
    "tags": [
      "python",
      "clustering",
      "anomaly-detection"
    ],
    "owner": {
      "account_id": 3075710,
      "reputation": 869,
      "user_id": 10512,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/MpAe0.png?s=256",
      "display_name": "makansij",
      "link": "https://datascience.stackexchange.com/users/10512/makansij"
    },
    "is_answered": true,
    "view_count": 68345,
    "accepted_answer_id": 6718,
    "answer_count": 5,
    "score": 41,
    "last_activity_date": 1618637310,
    "creation_date": 1438894737,
    "question_id": 6715,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/6715/is-it-necessary-to-standardize-your-data-before-clustering",
    "title": "Is it necessary to standardize your data before clustering?",
    "body": "<p>Is it necessary to standardize your data before cluster?  In the example from <code>scikit learn</code> about DBSCAN, <a href=\"http://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#example-cluster-plot-dbscan-py\">here</a> they do this in the line:</p>\n\n<pre><code>X = StandardScaler().fit_transform(X)\n</code></pre>\n\n<p>But I do not understand why it is necessary.  After all, clustering does not assume any particular distribution of data - it is an unsupervised learning method so its objective is to explore the data.  </p>\n\n<p>Why would it be necessary to transform the data?  </p>\n"
  },
  {
    "tags": [
      "tensorflow",
      "keras",
      "theano",
      "gpu"
    ],
    "owner": {
      "account_id": 7149567,
      "reputation": 1265,
      "user_id": 27616,
      "user_type": "registered",
      "accept_rate": 25,
      "profile_image": "https://www.gravatar.com/avatar/7ae925e98424ffd55234ebdff0c5b5c3?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "James Bond",
      "link": "https://datascience.stackexchange.com/users/27616/james-bond"
    },
    "is_answered": true,
    "view_count": 79450,
    "accepted_answer_id": 17591,
    "answer_count": 8,
    "score": 41,
    "last_activity_date": 1694566760,
    "creation_date": 1489513338,
    "last_edit_date": 1612844602,
    "question_id": 17578,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/17578/using-tensorflow-with-intel-gpu",
    "title": "Using TensorFlow with Intel GPU",
    "body": "<p>Is there any way now to use TensorFlow with Intel GPUs? If yes, please point me in the right direction.</p>\n<p>If not, please let me know which framework, if any, (Keras, Theano, etc) can I use for my Intel Corporation Xeon E3-1200 v3/4th Gen Core Processor Integrated Graphics Controller.</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "deep-learning",
      "gpu"
    ],
    "owner": {
      "account_id": 6706432,
      "reputation": 2021,
      "user_id": 31434,
      "user_type": "registered",
      "accept_rate": 67,
      "profile_image": "https://www.gravatar.com/avatar/08def0b5891022fb786be8989e620e2b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "StatsSorceress",
      "link": "https://datascience.stackexchange.com/users/31434/statssorceress"
    },
    "is_answered": true,
    "view_count": 50505,
    "accepted_answer_id": 19372,
    "answer_count": 3,
    "score": 41,
    "last_activity_date": 1496795536,
    "creation_date": 1495756106,
    "last_edit_date": 1496261375,
    "question_id": 19220,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/19220/choosing-between-cpu-and-gpu-for-training-a-neural-network",
    "title": "Choosing between CPU and GPU for training a neural network",
    "body": "<p>I've seen discussions about the 'overhead' of a GPU, and that for 'small' networks, it may actually be faster to train on a CPU (or network of CPUs) than a GPU. </p>\n\n<p>What is meant by 'small'? </p>\n\n<p>For example, would a single-layer MLP with 100 hidden units be 'small'? </p>\n\n<p>Does our definition of 'small' change for recurrent architectures?</p>\n\n<p>Are there any other criteria that should be considered when deciding whether to train on CPU or GPU?</p>\n\n<p>EDIT 1:</p>\n\n<p>I just <a href=\"http://timdettmers.com/2014/09/21/how-to-build-and-use-a-multi-gpu-system-for-deep-learning/\" rel=\"noreferrer\">found a blog post</a> (possibly outdated? It's from 2014):</p>\n\n<p>\"...Most network card[s] only work with memory that is registered with the CPU and so the GPU to GPU transfer between two nodes would be like this: GPU 1 to CPU 1 to Network Card 1 to Network Card 2 to CPU 2 to GPU 2. What this means is, if one chooses a slow network card then there might be no speedups over a single computer. Even with fast network cards, if the cluster is large, one does not even get speedups from GPUs when compared to CPUs as the GPUs just work too fast for the network cards to keep up with them.</p>\n\n<p>This is the reason why many big companies like Google and Microsoft are using CPU rather than GPU clusters to train their big neural networks. \"</p>\n\n<p>So at some point, according to this post, it could have been faster to use CPUs. Is this still the case?</p>\n\n<p>EDIT 2: Yes, that blog post may very well be outdated because:</p>\n\n<p>Now it seems that GPUs within a node are connected via PCIe bus, so communication can happen at about 6GiB/s. (For example: <a href=\"https://www.youtube.com/watch?v=el1iSlP1uOs\" rel=\"noreferrer\">https://www.youtube.com/watch?v=el1iSlP1uOs</a>, about 35 minutes in). The speaker implies that this is faster than going from GPU1 to CPU to GPU2. It would mean the network card is no longer the bottleneck.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-mining",
      "random-forest",
      "decision-trees",
      "xgboost"
    ],
    "owner": {
      "account_id": 5324263,
      "reputation": 707,
      "user_id": 39331,
      "user_type": "registered",
      "accept_rate": 14,
      "profile_image": "https://i.sstatic.net/wsXRZ.jpg?s=256",
      "display_name": "John Constantine",
      "link": "https://datascience.stackexchange.com/users/39331/john-constantine"
    },
    "is_answered": true,
    "view_count": 37667,
    "accepted_answer_id": 23913,
    "answer_count": 4,
    "score": 41,
    "last_activity_date": 1679140994,
    "creation_date": 1507984380,
    "question_id": 23789,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/23789/why-do-we-need-xgboost-and-random-forest",
    "title": "Why do we need XGBoost and Random Forest?",
    "body": "<p>I wasn't clear on couple of concepts: </p>\n\n<ol>\n<li><p>XGBoost converts weak learners to strong learners. What's the advantage of doing this ? Combining many weak learners instead of just using a single tree ?</p></li>\n<li><p>Random Forest uses various sample from tree to create a tree. What's the advantage of this method instead of just using a singular tree? </p></li>\n</ol>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "cloud-computing"
    ],
    "owner": {
      "account_id": 6631713,
      "reputation": 14308,
      "user_id": 28175,
      "user_type": "registered",
      "accept_rate": 95,
      "profile_image": "https://i.sstatic.net/ojtVl.jpg?s=256",
      "display_name": "Green Falcon",
      "link": "https://datascience.stackexchange.com/users/28175/green-falcon"
    },
    "is_answered": true,
    "view_count": 56164,
    "protected_date": 1600180873,
    "accepted_answer_id": 24321,
    "answer_count": 6,
    "score": 41,
    "last_activity_date": 1600176799,
    "creation_date": 1509712914,
    "last_edit_date": 1519833973,
    "question_id": 24319,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/24319/are-there-free-cloud-services-to-train-machine-learning-models",
    "title": "Are there free cloud services to train machine learning models?",
    "body": "<p>I want to train a deep model with a large amount of training data, but my desktop does not have that power to train such a deep model with these abundant data. </p>\n\n<p>I'd like to know whether there are any free cloud services that can be used for training machine learning and deep learning models? </p>\n\n<p>I also would like to know if there is a cloud service, where I would be able to track the training results, and the training would continue even if I am not connected to the cloud.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "svm",
      "computer-vision",
      "object-recognition"
    ],
    "owner": {
      "account_id": 1696822,
      "reputation": 649,
      "user_id": 37736,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://www.gravatar.com/avatar/686df1f4367033782c57aa8131fc3ab6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Alex",
      "link": "https://datascience.stackexchange.com/users/37736/alex"
    },
    "is_answered": true,
    "view_count": 53111,
    "accepted_answer_id": 25228,
    "answer_count": 2,
    "score": 41,
    "last_activity_date": 1708245542,
    "creation_date": 1511724777,
    "last_edit_date": 1597948995,
    "question_id": 25119,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/25119/how-to-calculate-map-for-detection-task-for-the-pascal-voc-challenge",
    "title": "How to calculate mAP for detection task for the PASCAL VOC Challenge?",
    "body": "<p>How to calculate the mAP (mean Average Precision) for the detection task for the Pascal VOC leaderboards?</p>\n<p>There said - at <strong><a href=\"http://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf\" rel=\"noreferrer\">page 11</a></strong>:</p>\n<blockquote>\n<p>Average Precision (AP). For the VOC2007 challenge, the interpolated\naverage precision (Salton and Mcgill 1986) was used to evaluate both\nclassification and detection. For a given task and class, the\nprecision/recall curve is computed from a methods ranked output.\nRecall is defined as the proportion of all positive examples ranked\nabove a given rank. Precision is the proportion of all examples above\nthat rank which are from the positive class. The AP summarises the\nshape of the precision/recall curve, and is defined as the mean\nprecision at a set of eleven equally spaced recall levels\n[0,0.1,...,1]:\n<code>AP = 1/11  r{0,0.1,...,1} pinterp(r)</code></p>\n<p>The precision at each recall level r is interpolated by taking the\nmaximum precision measured for a method for which the corresponding\nrecall exceeds r: <code>pinterp(r) = max p(r)</code>, where p(r) is the measured\nprecision at recall r</p>\n</blockquote>\n<p>About <a href=\"http://0agr.ru/wiki/index.php/Precision_and_Recall#Average_Precision\" rel=\"noreferrer\">mAP</a></p>\n<hr />\n<p>So does it mean that:</p>\n<ol>\n<li>We <strong>calculate Precision and Recall</strong>:</li>\n</ol>\n<ul>\n<li><p>A) For <strong>many different <code>IoU</code></strong> <code>&gt; {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1}</code> we calculate True/False Positive/Negative values</p>\n<p>Where <code>True positive = Number_of_detection with IoU &gt; {0, 0.1,..., 1}</code>, as said <a href=\"https://datascience.stackexchange.com/a/16813/98307\">here</a> and then we calculate:</p>\n<p><code>Precision = True positive / (True positive + False positive)</code></p>\n<p><code>Recall = True positive / (True positive + False negative)</code></p>\n</li>\n</ul>\n<hr />\n<ul>\n<li><p>B) Or for <strong>many different thresholds</strong> of detection algorithms we calculate:</p>\n<p><code>Precision = True positive / (True positive + False positive)</code></p>\n<p><code>Recall = True positive / (True positive + False negative)</code></p>\n<p>Where <code>True positive = Number_of_detection with IoU &gt; 0.5</code> as said <a href=\"https://stackoverflow.com/a/43168882/9214357\">here</a></p>\n</li>\n</ul>\n<hr />\n<ul>\n<li><p>C) Or for <strong>many different thresholds</strong> of detection algorithms we calculate:</p>\n<p><code>Precision = Intersect / Detected_box</code></p>\n<p><code>Recall = Intersect / Object</code></p>\n<p>As shown <a href=\"https://en.wikipedia.org/wiki/Precision_and_recall\" rel=\"noreferrer\">here</a>?\n<a href=\"https://i.sstatic.net/JlHnn.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/JlHnn.jpg\" alt=\"enter image description here\" /></a></p>\n</li>\n</ul>\n<hr />\n<ol start=\"2\">\n<li>Then we <strong>build Precision-Recall curve</strong>, as shown here:\n<a href=\"https://i.sstatic.net/Br0IM.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/Br0IM.png\" alt=\"enter image description here\" /></a></li>\n</ol>\n<hr />\n<ol start=\"3\">\n<li>Then we calculate AP (average precision) as <strong>average of 11 values of <code>Precision</code></strong> at the points where <code>Recall = {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1}</code>, i.e. <code>AP = 1/11  recall{0,0.1,...,1} Precision(Recall)</code></li>\n</ol>\n<p>(In general for each point, for example 0.3, we get MAX of Precision for Recall &lt;= 0.3, instead of value of Precision at this point Recall=0.3)</p>\n<hr />\n<ol start=\"4\">\n<li>And when we calculate AP only for 1 something object class on all images - then we get <strong>AP (average precision)</strong> for this class, for example, only for <code>air</code>.</li>\n</ol>\n<p>So AP is a  integral (<a href=\"https://stats.stackexchange.com/a/157019\">area under the curve</a>)</p>\n<p>But when we calculate AP for all object classes on all images - then we get <strong>mAP (mean average precision)</strong> for all images dataset.</p>\n<hr />\n<p><strong>Questions:</strong></p>\n<ol>\n<li>Is it right, and if it isn't, then how to calculate mAP for Pascal VOC Challenge?</li>\n<li>And which of the 3 formulas (A, B or C) is correct for calculating Precision and Recall, in paragraph 1?</li>\n</ol>\n<hr />\n<p><strong>Short answer:</strong></p>\n<ul>\n<li>mAP = AVG(AP for each object class)</li>\n<li>AP = AVG(Precision for each of 11 Recalls {precision = 0, 0.1, ..., 1})</li>\n<li>PR-curve = Precision and Recall (for each Threshold that is in the Predictions bound-boxes)</li>\n<li>Precision = TP / (TP + FP)</li>\n<li>Recall = TP / (TP + FN)</li>\n<li>TP = number of detections with IoU&gt;0.5</li>\n<li>FP = number of detections with IoU&lt;=0.5 or detected more than once</li>\n<li>FN = number of objects that not detected or detected with IoU&lt;=0.5</li>\n</ul>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "hyperparameter",
      "hyperparameter-tuning"
    ],
    "owner": {
      "account_id": 1504399,
      "reputation": 583,
      "user_id": 39854,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4d47e03e58da39c9804e46b1bdbc5ce0?s=256&d=identicon&r=PG",
      "display_name": "stk1234",
      "link": "https://datascience.stackexchange.com/users/39854/stk1234"
    },
    "is_answered": true,
    "view_count": 11634,
    "answer_count": 6,
    "score": 41,
    "last_activity_date": 1597948596,
    "creation_date": 1515857191,
    "last_edit_date": 1516139418,
    "question_id": 26597,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/26597/how-to-set-the-number-of-neurons-and-layers-in-neural-networks",
    "title": "How to set the number of neurons and layers in neural networks",
    "body": "<p>I am a beginner to neural networks and have had trouble grasping two concepts:</p>\n\n<ol>\n<li>How does one decide the number of middle layers a given neural network have? 1 vs. 10 or whatever.</li>\n<li>How does one decide the number of neurons in each middle layer? Is it recommended having an equal number of neurons in each middle layer or does it vary with the application?</li>\n</ol>\n"
  },
  {
    "tags": [
      "nlp",
      "encoding",
      "transformer",
      "attention-mechanism"
    ],
    "owner": {
      "account_id": 2201572,
      "reputation": 523,
      "user_id": 77970,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0abab320b8888891fa3c00256fca89d2?s=256&d=identicon&r=PG",
      "display_name": "FremyCompany",
      "link": "https://datascience.stackexchange.com/users/77970/fremycompany"
    },
    "is_answered": true,
    "view_count": 13084,
    "accepted_answer_id": 76291,
    "answer_count": 8,
    "score": 41,
    "last_activity_date": 1708058975,
    "creation_date": 1563438886,
    "question_id": 55901,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/55901/in-a-transformer-model-why-does-one-sum-positional-encoding-to-the-embedding-ra",
    "title": "In a Transformer model, why does one sum positional encoding to the embedding rather than concatenate it?",
    "body": "<p>While reviewing the Transformer architecture, I realized something I didn't expect, which is that :</p>\n\n<ul>\n<li>the positional encoding is summed to the word embeddings </li>\n<li>rather than concatenated to it.</li>\n</ul>\n\n<blockquote>\n  <p><a href=\"https://i.sstatic.net/bFPI9.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/bFPI9.png\" alt=\"positional encoding summed to word embedding\"></a></p>\n  \n  <p><a href=\"http://jalammar.github.io/images/t/transformer_positional_encoding_example.png\" rel=\"noreferrer\">http://jalammar.github.io/images/t/transformer_positional_encoding_example.png</a></p>\n</blockquote>\n\n<p>Based on the graphs I have seen wrt what the encoding looks like, that means that :</p>\n\n<ul>\n<li>the first few bits of the embedding are completely unusable by the network because the position encoding will distort them a lot, </li>\n<li>while there is also a large amount of positions in the embedding that are only slightly affected by the positional encoding (when you move further towards the end).</li>\n</ul>\n\n<blockquote>\n  <p><a href=\"https://i.sstatic.net/XLT9V.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/XLT9V.png\" alt=\"graph shows positional encoding affects firsts logits a lot, last logits hardly not\"></a></p>\n  \n  <p><a href=\"https://www.tensorflow.org/beta/tutorials/text/transformer_files/output_1kLCla68EloE_1.png\" rel=\"noreferrer\">https://www.tensorflow.org/beta/tutorials/text/transformer_files/output_1kLCla68EloE_1.png</a></p>\n</blockquote>\n\n<p>So, why not instead have smaller word embeddings (reduce memory usage) and a smaller positional encoding retaining only the most important bits of the encoding, and instead of summing the positional encoding of words keep it concatenated to word embeddings?</p>\n"
  },
  {
    "tags": [
      "bigdata",
      "apache-hadoop"
    ],
    "owner": {
      "account_id": 404626,
      "reputation": 591,
      "user_id": 456,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f494bf18ea8fc96190a398ecee6deedc?s=256&d=identicon&r=PG",
      "display_name": "Pensu",
      "link": "https://datascience.stackexchange.com/users/456/pensu"
    },
    "is_answered": true,
    "view_count": 21276,
    "protected_date": 1461773813,
    "accepted_answer_id": 256,
    "answer_count": 10,
    "score": 40,
    "last_activity_date": 1661493069,
    "creation_date": 1402381220,
    "question_id": 253,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/253/do-i-need-to-learn-hadoop-to-be-a-data-scientist",
    "title": "Do I need to learn Hadoop to be a Data Scientist?",
    "body": "<p>An aspiring data scientist here. I don't know anything about Hadoop, but as I have been reading about Data Science and Big Data, I see a lot of talk about Hadoop. Is it absolutely necessary to learn Hadoop to be a Data Scientist? </p>\n"
  },
  {
    "tags": [
      "data-formats",
      "hierarchical-data-format"
    ],
    "owner": {
      "account_id": 241216,
      "reputation": 5474,
      "user_id": 97,
      "user_type": "registered",
      "accept_rate": 77,
      "profile_image": "https://www.gravatar.com/avatar/68acdc33e278ab19debf230662a7ad96?s=256&d=identicon&r=PG",
      "display_name": "IgorS",
      "link": "https://datascience.stackexchange.com/users/97/igors"
    },
    "is_answered": true,
    "view_count": 29455,
    "accepted_answer_id": 293,
    "answer_count": 4,
    "score": 40,
    "last_activity_date": 1625186602,
    "creation_date": 1402392366,
    "last_edit_date": 1586883248,
    "question_id": 262,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/262/what-are-the-advantages-of-hdf-compared-to-alternative-formats",
    "title": "What are the advantages of HDF compared to alternative formats?",
    "body": "<p>What are the advantages of HDF compared to alternative formats? What are the main data science tasks where HDF is really suitable and useful?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-mining",
      "nlp",
      "text-mining",
      "similarity"
    ],
    "owner": {
      "account_id": 4647119,
      "reputation": 821,
      "user_id": 1097,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://i.sstatic.net/JFf6q.jpg?s=256",
      "display_name": "Matt",
      "link": "https://datascience.stackexchange.com/users/1097/matt"
    },
    "is_answered": true,
    "view_count": 11479,
    "accepted_answer_id": 689,
    "answer_count": 5,
    "score": 40,
    "last_activity_date": 1596718867,
    "creation_date": 1404576621,
    "last_edit_date": 1446800424,
    "question_id": 678,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/678/what-are-some-standard-ways-of-computing-the-distance-between-documents",
    "title": "What are some standard ways of computing the distance between documents?",
    "body": "<p>When I say \"document\", I have in mind web pages like Wikipedia articles and news stories.  I prefer answers giving either vanilla lexical distance metrics or state-of-the-art semantic distance metrics, with stronger preference for the latter.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "algorithms"
    ],
    "owner": {
      "account_id": 118734,
      "reputation": 1500,
      "user_id": 5143,
      "user_type": "registered",
      "accept_rate": 40,
      "profile_image": "https://www.gravatar.com/avatar/0b9f57174e3bafdf65e512aca66cdb9b?s=256&d=identicon&r=PG",
      "display_name": "Javierfdr",
      "link": "https://datascience.stackexchange.com/users/5143/javierfdr"
    },
    "is_answered": true,
    "view_count": 2692,
    "closed_date": 1422290590,
    "answer_count": 3,
    "score": 40,
    "last_activity_date": 1422455012,
    "creation_date": 1421767658,
    "last_edit_date": 1422455012,
    "question_id": 4914,
    "link": "https://datascience.stackexchange.com/questions/4914/when-to-use-what-machine-learning",
    "closed_reason": "Needs more focus",
    "title": "When to use what - Machine Learning",
    "body": "<p>Recently in a Machine Learning class from professor Oriol Pujol at UPC/Barcelona he described the most common algorithms, principles and concepts to use for a wide range of machine learning related task. Here I share them with you and ask you: </p>\n\n<ul>\n<li>is there any comprehensive framework matching tasks with approaches or methods related to different types of machine learning related problems?</li>\n</ul>\n\n<hr>\n\n<p><strong>How do I learn a simple Gaussian?</strong> \nProbability, random variables, distributions; estimation, convergence and asymptotics, confidence interval.</p>\n\n<p><strong>How do I learn a mixture of Gaussians (MoG)?</strong> Likelihood, Expectation-Maximization (EM); generalization, model selection, cross-validation; k-means, hidden markov models (HMM)</p>\n\n<p><strong>How do I learn any density?</strong> Parametric vs. non-Parametric estimation, Sobolev and other functional spaces; l   2 error; Kernel density estimation (KDE), optimal kernel, KDE theory</p>\n\n<p><strong>How do I predict a continuous variable (regression)?</strong> Linear regression, regularization, ridge regression, and LASSO; local linear regression; conditional density estimation.</p>\n\n<p><strong>How do I predict a discrete variable (classification)?</strong> Bayes classifier, naive Bayes, generative vs. discriminative; perceptron, weight decay, linear support vector machine; nearest neighbor classifier and theory</p>\n\n<p><strong>Which loss function should I use?</strong> Maximum likelihood estimation theory; l -2 estimation; Bayessian estimation; minimax and decision theory, Bayesianism vs frequentism</p>\n\n<p><strong>Which model should I use?</strong> AIC and BIC; Vapnik-Chervonenskis theory; cross-validation theory; bootstrapping; Probably Approximately Correct (PAC) theory; Hoeffding-derived bounds</p>\n\n<p><strong>How can I learn fancier (combined) models?</strong> Ensemble learning theory; boosting; bagging; stacking</p>\n\n<p><strong>How can I learn fancier (nonlinear) models?</strong> Generalized linear models, logistic regression; Kolmogorov theorem, generalized additive models; kernelization, reproducing kernel Hilbert spaces, non-linear SVM, Gaussian process regression</p>\n\n<p><strong>How can I learn fancier (compositional) models?</strong> Recursive models, decision trees, hierarchical clustering; neural networks, back propagation, deep belief networks; graphical models, mixtures of HMMs, conditional random fields, max-margin Markov networks; log-linear models; grammars</p>\n\n<p><strong>How do I reduce or relate features?</strong> Feature selection vs dimensionality reduction, wrapper methods for feature selection; causality vs correlation, partial correlation, Bayes net structure learning</p>\n\n<p><strong>How do I create new features?</strong> principal component analysis (PCA), independent component analysis (ICA), multidimensional scaling, manifold learning, supervised dimensionality reduction, metric learning</p>\n\n<p><strong>How do I reduce or relate the data?</strong> Clustering, bi-clustering, constrained clustering; association rules and market basket analysis; ranking/ordinal regression; link analysis; relational data</p>\n\n<p><strong>How do I treat time series?</strong> ARMA; Kalman filter and stat-space models, particle filter; functional data analysis; change-point detection; cross-validation for time series</p>\n\n<p><strong>How do I treat non-ideal data?</strong> covariate shift; class imbalance; missing data, irregularly sampled data, measurement errors; anomaly detection, robustness</p>\n\n<p><strong>How do I optimize the parameters?</strong> Unconstrained vs constrained/Convex optimization, derivative-free methods, first- and second-order methods, backfitting; natural gradient; bound optimization and EM</p>\n\n<p><strong>How do I optimize linear functions?</strong> computational linear algebra, matrix inversion for regression, singular value decomposition (SVD) for dimensionality reduction</p>\n\n<p><strong>How do I optimize with constraints?</strong> Convexity, Lagrange multipliers, Karush-Kuhn-Tucker conditions, interior point methods, SMO algorithm for SVM</p>\n\n<p><strong>How do I evaluate deeply-nested sums?</strong> Exact graphical model inference, variational bounds on sums, approximate graphical model inference, expectation propagation</p>\n\n<p><strong>How do I evaluate large sums and searches?</strong> Generalized N-body problems (GNP), hierarchical data structures, nearest neighbor search, fast multiple method; Monte Carlo integration, Markov Chain Monte Carlo, Monte Carlo SVD</p>\n\n<p><strong>How do I treat even larger problems?</strong> Parallel/distributed EM, parallel/distributed GNP; stochastic subgradient methods, online learning</p>\n\n<p><strong>How do I apply all this in the real world?</strong> Overview of the parts of the ML, choosing between the methods to use for each task, prior knowledge and assumptions; exploratory data analysis and information visualization; evaluation and interpretation, using confidence intervals and hypothesis test, ROC curves; where the research problems in ML are</p>\n"
  },
  {
    "tags": [
      "similarity"
    ],
    "owner": {
      "account_id": 357722,
      "reputation": 563,
      "user_id": 5184,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/9623d5a353e1e5b5ec394a124ffd0ca0?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "shihpeng",
      "link": "https://datascience.stackexchange.com/users/5184/shihpeng"
    },
    "is_answered": true,
    "view_count": 59272,
    "accepted_answer_id": 5128,
    "answer_count": 4,
    "score": 40,
    "last_activity_date": 1628613947,
    "creation_date": 1423724896,
    "question_id": 5121,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/5121/applications-and-differences-for-jaccard-similarity-and-cosine-similarity",
    "title": "Applications and differences for Jaccard similarity and Cosine Similarity",
    "body": "<p><strong>Jaccard similarity</strong> and <strong>cosine similarity</strong> are two very common measurements while comparing item similarities. However, I am not very clear in what situation which one should be preferable than another.</p>\n\n<p>Can somebody help clarify the differences of these two measurements (the difference in concept or principle, not the definition or computation) and their preferable applications?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "decision-trees",
      "algorithms",
      "pac-learning"
    ],
    "owner": {
      "account_id": 3551407,
      "reputation": 541,
      "user_id": 11074,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/56704062637b6d1c33436a591d10c259?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user2966197",
      "link": "https://datascience.stackexchange.com/users/11074/user2966197"
    },
    "is_answered": true,
    "view_count": 61060,
    "answer_count": 5,
    "score": 40,
    "last_activity_date": 1568154426,
    "creation_date": 1439474392,
    "last_edit_date": 1551098299,
    "question_id": 6787,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/6787/are-decision-tree-algorithms-linear-or-nonlinear",
    "title": "Are decision tree algorithms linear or nonlinear",
    "body": "<p>Recently a friend of mine was asked whether decision tree algorithms are linear or nonlinear algorithms in an interview. I tried to look for answers to this question but couldn't find any satisfactory explanation. Can anyone answer and explain the solution to this question? Also, what are some other examples of nonlinear machine learning algorithms?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "keras"
    ],
    "owner": {
      "account_id": 4937688,
      "reputation": 503,
      "user_id": 14007,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-qOpr0OlDCkA/AAAAAAAAAAI/AAAAAAAAALE/8bNsDCgdbuk/s256-rj/photo.jpg",
      "display_name": "fluency03",
      "link": "https://datascience.stackexchange.com/users/14007/fluency03"
    },
    "is_answered": true,
    "view_count": 21088,
    "accepted_answer_id": 13396,
    "answer_count": 1,
    "score": 40,
    "last_activity_date": 1645539613,
    "creation_date": 1458677063,
    "question_id": 10836,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10836/the-difference-between-dense-and-timedistributeddense-of-keras",
    "title": "The difference between `Dense` and `TimeDistributedDense` of `Keras`",
    "body": "<p>I am still confused about the difference between <code>Dense</code> and <code>TimeDistributedDense</code> of <code>Keras</code> even though there are already some similar questions asked <a href=\"https://github.com/fchollet/keras/issues/1029\" rel=\"noreferrer\">here</a> and <a href=\"https://github.com/fchollet/keras/issues/278\" rel=\"noreferrer\">here</a>. People are discussing a lot but no common-agreed conclusions. </p>\n\n<p>And even though, <a href=\"https://github.com/fchollet/keras/issues/1029#issuecomment-157786333\" rel=\"noreferrer\">here</a>, @fchollet stated that:</p>\n\n<blockquote>\n  <p><code>TimeDistributedDense</code> applies a same <code>Dense</code> (fully-connected) operation to every timestep of a 3D tensor.</p>\n</blockquote>\n\n<p>I still need detailed illustration about what exactly the difference between them. </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "cross-validation"
    ],
    "owner": {
      "account_id": 2045380,
      "reputation": 1416,
      "user_id": 27916,
      "user_type": "registered",
      "accept_rate": 88,
      "profile_image": "https://www.gravatar.com/avatar/e6537656fff8a1e4ada8d78983063e42?s=256&d=identicon&r=PG",
      "display_name": "user1825567",
      "link": "https://datascience.stackexchange.com/users/27916/user1825567"
    },
    "is_answered": true,
    "view_count": 38684,
    "accepted_answer_id": 18346,
    "answer_count": 3,
    "score": 40,
    "last_activity_date": 1700546692,
    "creation_date": 1492112033,
    "question_id": 18339,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/18339/why-use-both-validation-set-and-test-set",
    "title": "Why use both validation set and test set?",
    "body": "<p>Consider a neural network:</p>\n\n<p>For a given set of data, we divide it into training, validation and test set. Suppose we do it in the classic 60:20:20 ratio, then we prevent overfitting by validating the network by checking it on validation set. Then what is the need to test it on the test set to check its performance? </p>\n\n<p>Won't the error on the test set be somewhat same as the validation set as for the network it is an unseen data just like the validation set and also both of them are same in number?</p>\n\n<p>Instead can't we increase the training set by merging the test set to it so that we have more training data and the network trains better and then use validation set to prevent overfitting?\nWhy don't we do this?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "keras"
    ],
    "owner": {
      "account_id": 4603716,
      "reputation": 569,
      "user_id": 39706,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/c9fb1e3d9176a37526afe8c3b379503e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Vladimir Shebuniayeu",
      "link": "https://datascience.stackexchange.com/users/39706/vladimir-shebuniayeu"
    },
    "is_answered": true,
    "view_count": 111186,
    "answer_count": 2,
    "score": 40,
    "last_activity_date": 1679236606,
    "creation_date": 1512070403,
    "last_edit_date": 1514726133,
    "question_id": 25267,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/25267/keras-difference-beetween-val-loss-and-loss-during-training",
    "title": "Keras difference beetween val_loss and loss during training",
    "body": "<p>What is the difference between <code>val_loss</code> and <code>loss</code> during training in Keras?</p>\n\n<p>E.g.</p>\n\n<pre><code>Epoch 1/20\n1000/1000 [==============================] - 1s - loss: 0.1760, val_loss: 0.2032  \n</code></pre>\n\n<p>On some sites I read that on validation, dropout was not working.</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "deep-learning",
      "keras"
    ],
    "owner": {
      "account_id": 1695359,
      "reputation": 1042,
      "user_id": 14360,
      "user_type": "registered",
      "accept_rate": 56,
      "profile_image": "https://www.gravatar.com/avatar/ae52e7465a397c7740bc8de66af2b9ee?s=256&d=identicon&r=PG",
      "display_name": "pseudomonas",
      "link": "https://datascience.stackexchange.com/users/14360/pseudomonas"
    },
    "is_answered": true,
    "view_count": 56754,
    "accepted_answer_id": 14742,
    "answer_count": 1,
    "score": 39,
    "last_activity_date": 1612749829,
    "creation_date": 1475827851,
    "question_id": 14415,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/14415/how-does-keras-calculate-accuracy",
    "title": "How does Keras calculate accuracy?",
    "body": "<p>How does Keras calculate accuracy from the classwise probabilities? Say, for example we have 100 samples in the test set which can belong to one of two classes. We also have a list of the classwise probabilites. What threshold does Keras use to assign a sample to either of the two classes?</p>\n"
  },
  {
    "tags": [
      "cnn",
      "attention-mechanism"
    ],
    "owner": {
      "account_id": 2390423,
      "reputation": 523,
      "user_id": 71587,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/385179d88397bbf7e2ed65db7aaecc16?s=256&d=identicon&r=PG",
      "display_name": "Pratik.S",
      "link": "https://datascience.stackexchange.com/users/71587/pratik-s"
    },
    "is_answered": true,
    "view_count": 41417,
    "answer_count": 3,
    "score": 39,
    "last_activity_date": 1613867968,
    "creation_date": 1555497574,
    "question_id": 49468,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/49468/whats-the-difference-between-attention-vs-self-attention-what-problems-does-ea",
    "title": "What&#39;s the difference between Attention vs Self-Attention? What problems does each other solve that the other can&#39;t?",
    "body": "<p>As stated in the question above..is there a difference between attention and self attention mechanism ? Also additionally can anybody share with me tips and tricks about how self attention mechanism can be implemented in CNN? </p>\n"
  },
  {
    "tags": [
      "tensorflow",
      "nlp",
      "pytorch",
      "bert"
    ],
    "owner": {
      "account_id": 7939178,
      "reputation": 1511,
      "user_id": 83473,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/feab58f314adf1beb699a3548726a73c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "star",
      "link": "https://datascience.stackexchange.com/users/83473/star"
    },
    "is_answered": true,
    "view_count": 80522,
    "answer_count": 7,
    "score": 39,
    "last_activity_date": 1673022450,
    "creation_date": 1572880952,
    "last_edit_date": 1592305723,
    "question_id": 62658,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/62658/how-to-get-sentence-embedding-using-bert",
    "title": "How to get sentence embedding using BERT?",
    "body": "<p>How to get sentence embedding using BERT?</p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import BertTokenizer\ntokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\nsentence='I really enjoyed this movie a lot.'\n#1.Tokenize the sequence:\ntokens=tokenizer.tokenize(sentence)\nprint(tokens)\nprint(type(tokens))\n</code></pre>\n<h1>2. Add [CLS] and [SEP] tokens:</h1>\n<pre class=\"lang-py prettyprint-override\"><code>tokens = ['[CLS]'] + tokens + ['[SEP]']\nprint(&quot; Tokens are \\n {} &quot;.format(tokens))\n</code></pre>\n<h1>3. Padding the input:</h1>\n<pre class=\"lang-py prettyprint-override\"><code>T=15\npadded_tokens=tokens +['[PAD]' for _ in range(T-len(tokens))]\nprint(&quot;Padded tokens are \\n {} &quot;.format(padded_tokens))\nattn_mask=[ 1 if token != '[PAD]' else 0 for token in padded_tokens  ]\nprint(&quot;Attention Mask are \\n {} &quot;.format(attn_mask))\n</code></pre>\n<h1>4. Maintain a list of segment tokens:</h1>\n<pre class=\"lang-py prettyprint-override\"><code>seg_ids=[0 for _ in range(len(padded_tokens))]\nprint(&quot;Segment Tokens are \\n {}&quot;.format(seg_ids))\n</code></pre>\n<h1>5. Obtaining indices of the tokens in BERTs vocabulary:</h1>\n<pre class=\"lang-py prettyprint-override\"><code>sent_ids=tokenizer.convert_tokens_to_ids(padded_tokens)\nprint(&quot;senetence idexes \\n {} &quot;.format(sent_ids))\ntoken_ids = torch.tensor(sent_ids).unsqueeze(0) \nattn_mask = torch.tensor(attn_mask).unsqueeze(0) \nseg_ids   = torch.tensor(seg_ids).unsqueeze(0)\n</code></pre>\n<h1>Feed them to BERT</h1>\n<pre class=\"lang-py prettyprint-override\"><code>hidden_reps, cls_head = bert_model(token_ids, attention_mask = attn_mask,token_type_ids = seg_ids)\nprint(type(hidden_reps))\nprint(hidden_reps.shape ) #hidden states of each token in inout sequence \nprint(cls_head.shape ) #hidden states of each [cls]\n\noutput:\nhidden_reps size \ntorch.Size([1, 15, 768])\n\ncls_head size\ntorch.Size([1, 768])\n</code></pre>\n<p>Which vector represents the sentence embedding here? Is it <code>hidden_reps</code>  or <code>cls_head</code> ?</p>\n<p>Is there any other way to get sentence embedding from BERT in order to perform similarity check with other sentences?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-mining",
      "recommender-system"
    ],
    "owner": {
      "account_id": 2871380,
      "reputation": 739,
      "user_id": 728,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/37fab8969beac8475edbef85f84367c9?s=256&d=identicon&r=PG",
      "display_name": "Jack Twain",
      "link": "https://datascience.stackexchange.com/users/728/jack-twain"
    },
    "is_answered": true,
    "view_count": 21551,
    "answer_count": 4,
    "score": 38,
    "last_activity_date": 1597572948,
    "creation_date": 1405502691,
    "last_edit_date": 1445171863,
    "question_id": 749,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/749/meaning-of-latent-features",
    "title": "Meaning of latent features?",
    "body": "<p>I am learning about matrix factorization for recommender systems and I am seeing the term <code>latent features</code> occurring too frequently but I am unable to understand what it means. I know what a feature is but I don't understand the idea of latent features. Could please explain it? Or at least point me to a paper/place where I can read about it?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "random-forest"
    ],
    "owner": {
      "account_id": 200365,
      "reputation": 595,
      "user_id": 3054,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7020f8bbc294513290fdf0395ab1c7aa?s=256&d=identicon&r=PG",
      "display_name": "papafe",
      "link": "https://datascience.stackexchange.com/users/3054/papafe"
    },
    "is_answered": true,
    "view_count": 59348,
    "accepted_answer_id": 2315,
    "answer_count": 4,
    "score": 38,
    "last_activity_date": 1612750310,
    "creation_date": 1408812846,
    "question_id": 1028,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/1028/do-random-forest-overfit",
    "title": "Do Random Forest overfit?",
    "body": "<p>I have been reading around about Random Forests but I cannot really find a definitive answer about the problem of overfitting. According to the original paper of Breiman, they should not overfit when increasing the number of trees in the forest, but it seems that there is not consensus about this. This is creating me quite some confusion about the issue.</p>\n\n<p>Maybe someone more expert than me can give me a more concrete answer or point me in the right direction to better understand the problem.</p>\n"
  },
  {
    "tags": [
      "python",
      "statistics",
      "visualization",
      "pandas"
    ],
    "owner": {
      "account_id": 1791093,
      "reputation": 535,
      "user_id": 16096,
      "user_type": "registered",
      "accept_rate": 60,
      "profile_image": "https://www.gravatar.com/avatar/a76b9fce3af051436ba4126aee5adf67?s=256&d=identicon&r=PG",
      "display_name": "gdlm",
      "link": "https://datascience.stackexchange.com/users/16096/gdlm"
    },
    "is_answered": true,
    "view_count": 193462,
    "accepted_answer_id": 10461,
    "answer_count": 3,
    "score": 38,
    "last_activity_date": 1535621103,
    "creation_date": 1456811797,
    "last_edit_date": 1456819401,
    "question_id": 10459,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10459/calculation-and-visualization-of-correlation-matrix-with-pandas",
    "title": "Calculation and Visualization of Correlation Matrix with Pandas",
    "body": "<p>I have a pandas data frame with several entries, and I want to calculate the correlation between the income of some type of stores. There are a number of stores with income data, classification of area of activity (theater, cloth stores, food ...) and other data. </p>\n\n<p>I tried to create a new data frame and insert a column with the income of all kinds of stores that belong to the same category, and the returning data frame has only the first column filled and the rest is full of NaN's.\nThe code that I tired: </p>\n\n<pre><code>corr = pd.DataFrame()\nfor at in activity:\n    stores.loc[stores['Activity']==at]['income']\n</code></pre>\n\n<p>I want to do so, so I can use <code>.corr()</code> to gave the correlation matrix between the category of stores.</p>\n\n<p>After that, I would like to know how I can plot the matrix values (-1 to 1, since I want to use Pearson's correlation) with matplolib.</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "tensorflow",
      "rnn"
    ],
    "owner": {
      "account_id": 2111414,
      "reputation": 849,
      "user_id": 21319,
      "user_type": "registered",
      "accept_rate": 70,
      "profile_image": "https://i.sstatic.net/0jbhF.gif?s=256",
      "display_name": "Brans Ds",
      "link": "https://datascience.stackexchange.com/users/21319/brans-ds"
    },
    "is_answered": true,
    "view_count": 49603,
    "accepted_answer_id": 13162,
    "answer_count": 4,
    "score": 38,
    "last_activity_date": 1586242903,
    "creation_date": 1469355455,
    "last_edit_date": 1586242903,
    "question_id": 12964,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/12964/what-is-the-meaning-of-the-number-of-units-in-the-lstm-cell",
    "title": "What is the meaning of &quot;The number of units in the LSTM cell&quot;?",
    "body": "<p>From Tensorflow code: <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py\" rel=\"noreferrer\">Tensorflow. RnnCell.</a></p>\n\n<pre><code>num_units: int, The number of units in the LSTM cell.\n</code></pre>\n\n<p>I can't understand what this means. What are the units of LSTM cell? Input, Output and Forget gates? Does this mean \"the number of units in the recurrent projection layer for Deep LSTM\"? Then why is it called \"number of units in the LSTM cell\"?   What is an LSTM cell and how is it different from an LSTM block, what is the minimal LSTM unit if not a cell?</p>\n"
  },
  {
    "tags": [
      "python",
      "databases",
      "binary"
    ],
    "owner": {
      "account_id": 6877712,
      "reputation": 481,
      "user_id": 33519,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-g30toIHtm8A/AAAAAAAAAAI/AAAAAAAAF0M/I4ERmSoVEi0/s256-rj/photo.jpg",
      "display_name": "Antoine Duss&#233;aux",
      "link": "https://datascience.stackexchange.com/users/33519/antoine-duss%c3%a9aux"
    },
    "is_answered": true,
    "view_count": 32137,
    "answer_count": 5,
    "score": 38,
    "last_activity_date": 1723039887,
    "creation_date": 1497776637,
    "last_edit_date": 1497793069,
    "question_id": 19802,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/19802/best-practices-to-store-python-machine-learning-models",
    "title": "Best practices to store Python machine learning models",
    "body": "<p>What are the best practices to save, store, and share machine learning models?</p>\n\n<p>In Python, we generally store the binary representation of the model, using pickle or joblib. Models, in my case, can be ~100Mo large. Also, joblib can save one model to multiple files unless you set <code>compress=1</code> (<a href=\"https://stackoverflow.com/questions/33497314/sklearn-dumping-model-using-joblib-dumps-multiple-files-which-one-is-the-corre\">https://stackoverflow.com/questions/33497314/sklearn-dumping-model-using-joblib-dumps-multiple-files-which-one-is-the-corre</a>).</p>\n\n<p>But then, if you want to control access rights to models, and be able to use models from different machines, what's the best way to store them?</p>\n\n<p>I have a few choices:</p>\n\n<ul>\n<li>Store them as files, and then put them in a repository using Git LFS</li>\n<li>Store them in an SQL database as binary files:\n\n<ul>\n<li>For instance in Postgresql <a href=\"https://wiki.postgresql.org/wiki/BinaryFilesInDB\" rel=\"noreferrer\">https://wiki.postgresql.org/wiki/BinaryFilesInDB</a></li>\n<li>This is also the method recommended by the SQL Server team:\n\n<ul>\n<li><a href=\"https://docs.microsoft.com/en-us/sql/advanced-analytics/tutorials/walkthrough-build-and-save-the-model\" rel=\"noreferrer\">https://docs.microsoft.com/en-us/sql/advanced-analytics/tutorials/walkthrough-build-and-save-the-model</a></li>\n<li><a href=\"https://microsoft.github.io/sql-ml-tutorials/python/rentalprediction/step/3.html\" rel=\"noreferrer\">https://microsoft.github.io/sql-ml-tutorials/python/rentalprediction/step/3.html</a></li>\n<li><a href=\"https://blogs.technet.microsoft.com/dataplatforminsider/2016/10/17/sql-server-as-a-machine-learning-model-management-system\" rel=\"noreferrer\">https://blogs.technet.microsoft.com/dataplatforminsider/2016/10/17/sql-server-as-a-machine-learning-model-management-system</a></li>\n</ul></li>\n</ul></li>\n<li>HDFS</li>\n</ul>\n"
  },
  {
    "tags": [
      "deep-learning",
      "predictive-modeling",
      "time-series",
      "forecast",
      "lstm"
    ],
    "owner": {
      "account_id": 6576609,
      "reputation": 815,
      "user_id": 23644,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/F0tsiwRV.png?s=256",
      "display_name": "PixelPioneer",
      "link": "https://datascience.stackexchange.com/users/23644/pixelpioneer"
    },
    "is_answered": true,
    "view_count": 17279,
    "accepted_answer_id": 24838,
    "answer_count": 1,
    "score": 38,
    "last_activity_date": 1576513262,
    "creation_date": 1510819074,
    "last_edit_date": 1537110375,
    "question_id": 24800,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/24800/time-series-prediction-using-lstms-importance-of-making-time-series-stationary",
    "title": "Time Series prediction using LSTMs: Importance of making time series stationary",
    "body": "<p>In this <a href=\"https://people.duke.edu/~rnau/411diff.htm\" rel=\"noreferrer\">link on Stationarity and differencing</a>, it has been mentioned that models like ARIMA require a stationarized time series for forecasting as it's statistical properties like mean, variance, autocorrelation etc are constant over time. Since RNNs have a better capacity to learn non-linear relationships (<a href=\"https://machinelearningmastery.com/promise-recurrent-neural-networks-time-series-forecasting/\" rel=\"noreferrer\">as per given here: The Promise of Recurrent Neural Networks for Time Series Forecasting</a>) and perform better than traditional time series models when the data is large, it is essential to understand how stationarized data would affect its results. The questions I need to know the answer of are as follows:</p>\n\n<ol>\n<li><p>In case of traditional time series forecasting models, stationarity in time series data makes it easier to predict, why and how? </p></li>\n<li><p>While building a time series prediction model using <strong>LSTMs</strong>, is it important to make the time series data stationary? If so, then why?</p></li>\n</ol>\n"
  },
  {
    "tags": [
      "decision-trees",
      "xgboost",
      "normalization"
    ],
    "owner": {
      "account_id": 1891013,
      "reputation": 1455,
      "user_id": 57724,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/dc4597c8aedf386296097d025d0c1c78?s=256&d=identicon&r=PG",
      "display_name": "user781486",
      "link": "https://datascience.stackexchange.com/users/57724/user781486"
    },
    "is_answered": true,
    "view_count": 42353,
    "accepted_answer_id": 60954,
    "answer_count": 3,
    "score": 38,
    "last_activity_date": 1643440259,
    "creation_date": 1569677703,
    "last_edit_date": 1628089301,
    "question_id": 60950,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/60950/is-it-necessary-to-normalize-data-for-xgboost",
    "title": "Is it necessary to normalize data for XGBoost?",
    "body": "<p><code>MinMaxScaler()</code> in <code>scikit-learn</code> is used for data normalization (a.k.a feature scaling). Data normalization is not necessary for decision trees. Since XGBoost is based on decision trees, is it necessary to do data normalization using <code>MinMaxScaler()</code> for data to be fed to XGBoost machine learning models?</p>\n"
  },
  {
    "tags": [
      "r",
      "data-cleaning"
    ],
    "owner": {
      "account_id": 15844,
      "reputation": 471,
      "user_id": 157,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e804c2930b1827a52df3b489af28722e?s=256&d=identicon&r=PG",
      "display_name": "Jay Godse",
      "link": "https://datascience.stackexchange.com/users/157/jay-godse"
    },
    "is_answered": true,
    "view_count": 5321,
    "answer_count": 7,
    "score": 37,
    "last_activity_date": 1485152830,
    "creation_date": 1400081121,
    "last_edit_date": 1400247907,
    "question_id": 52,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/52/organized-processes-to-clean-data",
    "title": "Organized processes to clean data",
    "body": "<p>From my limited dabbling with data science using R, I realized that cleaning bad data is a very important part of preparing data for analysis. </p>\n\n<p>Are there any best practices or processes for cleaning data before processing it? If so, are there any automated or semi-automated tools which implement some of these best practices?</p>\n"
  },
  {
    "tags": [
      "education"
    ],
    "owner": {
      "account_id": 2317167,
      "reputation": 3208,
      "user_id": 434,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/P75cO.jpg?s=256",
      "display_name": "Steve Kallestad",
      "link": "https://datascience.stackexchange.com/users/434/steve-kallestad"
    },
    "is_answered": true,
    "view_count": 20065,
    "protected_date": 1562847522,
    "accepted_answer_id": 338,
    "answer_count": 13,
    "community_owned_date": 1455891354,
    "score": 37,
    "last_activity_date": 1562828562,
    "creation_date": 1402570323,
    "last_edit_date": 1402659351,
    "question_id": 334,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/334/what-do-you-think-of-data-science-certifications",
    "title": "What do you think of Data Science certifications?",
    "body": "<p>I've now seen two data science certification programs - the <a href=\"https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage\">John Hopkins one available at Coursera</a> and the <a href=\"http://cloudera.com/content/cloudera/en/training/certification/ccp-ds.html\">Cloudera one</a>.</p>\n\n<p>I'm sure there are others out there.</p>\n\n<p>The John Hopkins set of classes is focused on R as a toolset, but covers a range of topics:</p>\n\n<ul>\n<li>R Programming</li>\n<li>cleaning and obtaining data</li>\n<li>Data Analysis</li>\n<li>Reproducible Research</li>\n<li>Statistical Inference</li>\n<li>Regression Models</li>\n<li>Machine Learning</li>\n<li>Developing Data Products</li>\n<li>And what looks to be a Project based completion task similar to Cloudera's Data Science Challenge</li>\n</ul>\n\n<p>The Cloudera program looks thin on the surface, but looks to answer the two important questions - \"Do you know the tools\", \"Can you apply the tools in the real world\".  Their program consists of:</p>\n\n<ul>\n<li>Introduction to Data Science</li>\n<li>Data Science Essentials Exam</li>\n<li>Data Science Challenge (a real world data science project scenario)</li>\n</ul>\n\n<p>I am not looking for a recommendation on a program or a quality comparison.</p>\n\n<p>I am curious about other certifications out there, the topics they cover, and how seriously DS certifications are viewed at this point by the community.</p>\n\n<p>EDIT: These are all great answers.  I'm choosing the correct answer by votes.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "keras"
    ],
    "owner": {
      "account_id": 2618092,
      "reputation": 1235,
      "user_id": 26800,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://www.gravatar.com/avatar/e0436fe410b764e66a722f80275eb5e3?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Rjay155",
      "link": "https://datascience.stackexchange.com/users/26800/rjay155"
    },
    "is_answered": true,
    "view_count": 37893,
    "protected_date": 1505510149,
    "accepted_answer_id": 17034,
    "answer_count": 1,
    "score": 37,
    "last_activity_date": 1560100733,
    "creation_date": 1487273730,
    "last_edit_date": 1560100733,
    "question_id": 17024,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/17024/rnns-with-multiple-features",
    "title": "RNN&#39;s with multiple features",
    "body": "<p>I have a bit of self taught knowledge working with Machine Learning algorithms (the basic Random Forest and Linear Regression type stuff). I decided to branch out and begin learning RNN's with Keras. When looking at most of the examples, which usually involve stock predictions, I haven't been able to find any basic examples of multiple features being implemented other than 1 column being the feature date and the other being the output. Is there a key fundamental thing I'm missing or something?</p>\n\n<p>If anyone has an example I would greatly appreciate it.</p>\n\n<p>Thanks!</p>\n"
  },
  {
    "tags": [
      "nlp",
      "gensim"
    ],
    "owner": {
      "account_id": 9281111,
      "reputation": 763,
      "user_id": 33970,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-gJL21XkJ7gM/AAAAAAAAAAI/AAAAAAAAACs/88rlAF7LvB8/s256-rj/photo.jpg",
      "display_name": "Sabbiu Shah",
      "link": "https://datascience.stackexchange.com/users/33970/sabbiu-shah"
    },
    "is_answered": true,
    "view_count": 79048,
    "accepted_answer_id": 20082,
    "answer_count": 6,
    "score": 37,
    "last_activity_date": 1619812064,
    "creation_date": 1498788879,
    "last_edit_date": 1547491231,
    "question_id": 20071,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/20071/how-do-i-load-fasttext-pretrained-model-with-gensim",
    "title": "How do I load FastText pretrained model with Gensim?",
    "body": "<p>I tried to load fastText pretrained model from here <a href=\"https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\" rel=\"noreferrer\">Fasttext model</a>. I am using <a href=\"https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.zip\" rel=\"noreferrer\">wiki.simple.en</a></p>\n\n<pre><code>from gensim.models.keyedvectors import KeyedVectors\n\nword_vectors = KeyedVectors.load_word2vec_format('wiki.simple.bin', binary=True)\n</code></pre>\n\n<p>But, it shows the following errors</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"nltk_check.py\", line 28, in &lt;module&gt;\n    word_vectors = KeyedVectors.load_word2vec_format('wiki.simple.bin', binary=True)\n  File \"P:\\major_project\\venv\\lib\\sitepackages\\gensim\\models\\keyedvectors.py\",line 206, in load_word2vec_format\n     header = utils.to_unicode(fin.readline(), encoding=encoding)\n  File \"P:\\major_project\\venv\\lib\\site-packages\\gensim\\utils.py\", line 235, in any2unicode\n    return unicode(text, encoding, errors=errors)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xba in position 0: invalid start byte\n</code></pre>\n\n<p><strong>Question 1</strong> How do I load fasttext model with Gensim? </p>\n\n<p><strong>Question 2</strong> Also, after loading the model, I want to find the similarity between two words </p>\n\n<pre class=\"lang-py prettyprint-override\"><code> model.find_similarity('teacher', 'teaches')\n # Something like this\n Output : 0.99\n</code></pre>\n\n<p>How do I do this?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "cross-validation"
    ],
    "owner": {
      "account_id": 9480235,
      "reputation": 1752,
      "user_id": 33603,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/pv1Cqofg.jpg?s=256",
      "display_name": "Dee Carter",
      "link": "https://datascience.stackexchange.com/users/33603/dee-carter"
    },
    "is_answered": true,
    "view_count": 87700,
    "accepted_answer_id": 21888,
    "answer_count": 2,
    "score": 37,
    "last_activity_date": 1604484412,
    "creation_date": 1501593651,
    "last_edit_date": 1501595909,
    "question_id": 21877,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/21877/how-to-use-the-output-of-gridsearch",
    "title": "How to use the output of GridSearch?",
    "body": "<p>I'm currently working with Python and Scikit learn for classification purposes, and doing some reading around GridSearch I thought this was a great way for optimising my estimator parameters to get the best results.</p>\n\n<p>My methodology is this:</p>\n\n<ol>\n<li>Split my data into training/test.</li>\n<li>Use GridSearch with 5Fold Cross validation to train and test my estimators(Random Forest, Gradient Boost, SVC amongst others) to get the best estimators with the optimal combination of hyper parameters.</li>\n<li>I then calculate metrics on each of my estimators such as Precision, Recall, FMeasure and Matthews Correlation Coefficient, using my test set to predict the classifications and compare them to actual class labels.</li>\n</ol>\n\n<p>It is at this stage that I see strange behaviour and I'm unsure how to proceed. <strong>Do I take the .best_estimator_ from the GridSearch and use this as the 'optimal' output from the grid search</strong>, and perform prediction using this estimator? If I do this I find that the stage 3 metrics are usually much lower than if I simply train on all training data and test on the test set. <strong>Or, do I simply take the output GridSearchCV object as the new estimator</strong>? If I do this I get better scores for my stage 3 metrics, but it seems odd using a GridSearchCV object instead of the intended classifier (E.g. a random Forest) ...</p>\n\n<p><strong>EDIT:</strong>\nSo my question is what is the difference between the returned GridSearchCV object and the .best_estimator_ attribute? Which one of these should I use for calculating further metrics? Can I use this output like a regular classifier (e.g. using predict), or else how should I use it?</p>\n"
  },
  {
    "tags": [
      "python",
      "nlp",
      "scikit-learn",
      "similarity",
      "text"
    ],
    "owner": {
      "account_id": 3460817,
      "reputation": 1379,
      "user_id": 21254,
      "user_type": "registered",
      "accept_rate": 57,
      "profile_image": "https://www.gravatar.com/avatar/9e2014846b008a5b75a2fc3f76b9f72d?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "lte__",
      "link": "https://datascience.stackexchange.com/users/21254/lte"
    },
    "is_answered": true,
    "view_count": 85088,
    "protected_date": 1578928404,
    "accepted_answer_id": 23998,
    "answer_count": 6,
    "score": 37,
    "last_activity_date": 1681364923,
    "creation_date": 1508657775,
    "question_id": 23969,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/23969/sentence-similarity-prediction",
    "title": "Sentence similarity prediction",
    "body": "<p>I'm looking to solve the following problem: I have a set of sentences as my dataset, and I want to be able to type a new sentence, and find the sentence that the new one is the most similar to in the dataset. An example would look like:</p>\n\n<p>New sentence: \"<code>I opened a new mailbox</code>\"</p>\n\n<p>Prediction based on dataset:</p>\n\n<pre><code>Sentence                       | Similarity\nA dog ate poop                   0%\nA mailbox is good                50%\nA mailbox was opened by me       80%\n</code></pre>\n\n<p>I've read that <a href=\"https://datascience.stackexchange.com/questions/16488/text-similarity-using-rnn\">cosine similarity</a> can be used to solve these kinds of issues paired with tf-idf (and RNNs should not bring significant improvements to the basic methods), or also <a href=\"https://datascience.stackexchange.com/questions/16713/text-processing\">word2vec</a> is used for similar problems. Are those actually viable for use in this specific case, too? Are there any other techniques/algorithms to solve this (preferably with Python and SKLearn, but I'm open to learn about TensorFlow, too)?</p>\n"
  },
  {
    "tags": [
      "keras",
      "epochs"
    ],
    "owner": {
      "account_id": 12807198,
      "reputation": 761,
      "user_id": 65133,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/9eaf0efe385f5ca2e7f49887986e20b4?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "yamini goel",
      "link": "https://datascience.stackexchange.com/users/65133/yamini-goel"
    },
    "is_answered": true,
    "view_count": 67680,
    "answer_count": 5,
    "score": 37,
    "last_activity_date": 1618696895,
    "creation_date": 1552731911,
    "last_edit_date": 1559888128,
    "question_id": 47405,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/47405/what-to-set-in-steps-per-epoch-in-keras-fit-generator",
    "title": "What to set in steps_per_epoch in Keras&#39; fit_generator?",
    "body": "<p>I am replicating, in Keras, the work of a paper where I know the values of <code>epoch</code> and <code>batch_size</code>. Since the dataset is quite large, I am using <code>fit_generator</code>. I would like to know what to set in <code>steps_per_epoch</code> given <code>epoch</code> value and <code>batch_size</code>. Is there a standard way?</p>\n"
  },
  {
    "tags": [
      "tensorflow",
      "gpu"
    ],
    "owner": {
      "account_id": 20580,
      "reputation": 1130,
      "user_id": 23556,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/GBCLu.jpg?s=256",
      "display_name": "Florin Andrei",
      "link": "https://datascience.stackexchange.com/users/23556/florin-andrei"
    },
    "is_answered": true,
    "view_count": 90596,
    "accepted_answer_id": 58846,
    "answer_count": 4,
    "score": 37,
    "last_activity_date": 1740157224,
    "creation_date": 1567890884,
    "question_id": 58845,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/58845/how-to-disable-gpu-with-tensorflow",
    "title": "How to disable GPU with TensorFlow?",
    "body": "<p>Using tensorflow-gpu 2.0.0rc0. I want to choose whether it uses the GPU or the CPU.</p>\n"
  },
  {
    "tags": [
      "bigdata",
      "data-mining",
      "dimensionality-reduction"
    ],
    "owner": {
      "account_id": 118652,
      "reputation": 547,
      "user_id": 3167,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e77037ae0695146ca8a3ad227d55b98f?s=256&d=identicon&r=PG",
      "display_name": "David S.",
      "link": "https://datascience.stackexchange.com/users/3167/david-s"
    },
    "is_answered": true,
    "view_count": 33555,
    "accepted_answer_id": 1169,
    "answer_count": 6,
    "score": 36,
    "last_activity_date": 1560100472,
    "creation_date": 1411634459,
    "last_edit_date": 1560100472,
    "question_id": 1159,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/1159/how-to-do-svd-and-pca-with-big-data",
    "title": "How to do SVD and PCA with big data?",
    "body": "<p>I have a large set of data (about 8GB). I would like to use machine learning to analyze it. So, I think that I should use SVD then PCA to reduce the data dimensionality for efficiency. However, MATLAB and Octave cannot load such a large dataset.</p>\n\n<p>What tools I can use to do SVD with such a large amount of data? </p>\n"
  },
  {
    "tags": [
      "feature-extraction",
      "feature-scaling",
      "featurization"
    ],
    "owner": {
      "account_id": 1879420,
      "reputation": 579,
      "user_id": 8338,
      "user_type": "registered",
      "accept_rate": 80,
      "profile_image": "https://i.sstatic.net/oBr6e.jpg?s=256",
      "display_name": "Mangat Rai Modi",
      "link": "https://datascience.stackexchange.com/users/8338/mangat-rai-modi"
    },
    "is_answered": true,
    "view_count": 21036,
    "accepted_answer_id": 6335,
    "answer_count": 4,
    "score": 36,
    "last_activity_date": 1707231582,
    "creation_date": 1433311010,
    "last_edit_date": 1433314753,
    "question_id": 5990,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/5990/what-is-a-good-way-to-transform-cyclic-ordinal-attributes",
    "title": "What is a good way to transform Cyclic Ordinal attributes?",
    "body": "<p>I am having 'hour' field as my attribute, but it takes a cyclic values. How could I transform the feature to preserve the information like '23' and '0' hour are close not far. </p>\n\n<p>One way I could think is to do transformation: <code>min(h, 23-h)</code></p>\n\n<pre><code>Input: [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n\nOutput: [0 1 2 3 4 5 6 7 8 9 10 11 11 10 9 8 7 6 5 4 3 2 1]\n</code></pre>\n\n<p>Is there any standard to handle such attributes?</p>\n\n<p>Update: I will be using superviseed learning, to train random forest classifier!</p>\n"
  },
  {
    "tags": [
      "xgboost",
      "weighted-data"
    ],
    "owner": {
      "account_id": 4246433,
      "reputation": 463,
      "user_id": 14895,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/14f7bd035afecfefb4000c5542937fc7?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "kilojoules",
      "link": "https://datascience.stackexchange.com/users/14895/kilojoules"
    },
    "is_answered": true,
    "view_count": 47534,
    "accepted_answer_id": 9489,
    "answer_count": 3,
    "score": 36,
    "last_activity_date": 1500380512,
    "creation_date": 1450804777,
    "question_id": 9488,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9488/xgboost-give-more-importance-to-recent-samples",
    "title": "xgboost: give more importance to recent samples",
    "body": "<p>Is there a way to add more importance to points which are more recent when analyzing data with xgboost? </p>\n"
  },
  {
    "tags": [
      "scikit-learn",
      "xgboost",
      "gbm"
    ],
    "owner": {
      "account_id": 279172,
      "reputation": 1357,
      "user_id": 16050,
      "user_type": "registered",
      "accept_rate": 12,
      "profile_image": "https://www.gravatar.com/avatar/0afae9fd8525ee0e8c7016769da5b472?s=256&d=identicon&r=PG",
      "display_name": "ihadanny",
      "link": "https://datascience.stackexchange.com/users/16050/ihadanny"
    },
    "is_answered": true,
    "view_count": 27750,
    "accepted_answer_id": 10945,
    "answer_count": 1,
    "score": 36,
    "last_activity_date": 1459675168,
    "creation_date": 1459260886,
    "question_id": 10943,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10943/why-is-xgboost-so-much-faster-than-sklearn-gradientboostingclassifier",
    "title": "Why is xgboost so much faster than sklearn GradientBoostingClassifier?",
    "body": "<p>I'm trying to train a gradient boosting model over 50k examples with 100 numeric features. <code>XGBClassifier</code> handles 500 trees within 43 seconds on my machine, while <code>GradientBoostingClassifier</code> handles only 10 trees(!) in 1 minutes and 2 seconds :( I didn't bother trying to grow 500 trees as it will take hours. I'm using the same <code>learning_rate</code> and <code>max_depth</code> settings, see below.</p>\n\n<p>What makes XGBoost so much faster? Does it use some novel implementation for gradient boosting that sklearn guys do not know? Or is it \"cutting corners\" and growing shallower trees? </p>\n\n<p>p.s. I'm aware of this discussion: <a href=\"https://www.kaggle.com/c/higgs-boson/forums/t/10335/xgboost-post-competition-survey\">https://www.kaggle.com/c/higgs-boson/forums/t/10335/xgboost-post-competition-survey</a> but couldn't get the answer there... </p>\n\n<pre><code>XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\ngamma=0, learning_rate=0.05, max_delta_step=0, max_depth=10,\nmin_child_weight=1, missing=None, n_estimators=500, nthread=-1,\nobjective='binary:logistic', reg_alpha=0, reg_lambda=1,\nscale_pos_weight=1, seed=0, silent=True, subsample=1)\n\nGradientBoostingClassifier(init=None, learning_rate=0.05, loss='deviance',\nmax_depth=10, max_features=None, max_leaf_nodes=None,\nmin_samples_leaf=1, min_samples_split=2,\nmin_weight_fraction_leaf=0.0, n_estimators=10,\npresort='auto', random_state=None, subsample=1.0, verbose=0,\nwarm_start=False)\n</code></pre>\n"
  },
  {
    "tags": [
      "deep-learning",
      "rnn",
      "normalization",
      "batch-normalization"
    ],
    "owner": {
      "account_id": 1641798,
      "reputation": 2236,
      "user_id": 9465,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b9700c79a6982daed72c137ed7091d7f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Rizky Luthfianto",
      "link": "https://datascience.stackexchange.com/users/9465/rizky-luthfianto"
    },
    "is_answered": true,
    "view_count": 13401,
    "accepted_answer_id": 19251,
    "answer_count": 1,
    "score": 36,
    "last_activity_date": 1520816762,
    "creation_date": 1469267202,
    "last_edit_date": 1503425141,
    "question_id": 12956,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/12956/paper-whats-the-difference-between-layer-normalization-recurrent-batch-normal",
    "title": "Paper: What&#39;s the difference between Layer Normalization, Recurrent Batch Normalization (2016), and Batch Normalized RNN (2015)?",
    "body": "<p>So, recently there's a <a href=\"https://arxiv.org/abs/1607.06450\">Layer Normalization</a> paper. There's also <a href=\"https://gist.github.com/udibr/7f46e790c9e342d75dcbd9b1deb9d940\">an implementation of it</a> on Keras.</p>\n\n<p>But I remember there are papers titled <a href=\"https://arxiv.org/abs/1603.09025\">Recurrent Batch Normalization</a> (Cooijmans, 2016) and <a href=\"https://arxiv.org/abs/1510.01378\">Batch Normalized Recurrent Neural Networks</a> (Laurent, 2015). What's the difference between those three?</p>\n\n<p>There is this Related Work section I don't understand:</p>\n\n<blockquote>\n  <p>Batch normalization has been previously extended to recurrent neural networks [Laurent et al., 2015, Amodei et al., 2015, Cooijmans et al., 2016]. The previous work [Cooijmans et al., 2016] suggests the best performance of recurrent batch normalization is obtained by keeping independent normalization statistics for each time-step. The authors show that initializing the gain parameter in the recurrent batch normalization layer to 0.1 makes significant difference in the final performance of the model. Our work is also related to weight normalization [Salimans and Kingma, 2016]. In weight normalization, <strong>instead of the variance, the L2 norm of the incoming weights is used to normalize the summed inputs to a neuron</strong>. Applying either weight normalization or batch normalization using expected statistics is equivalent to have a different parameterization of the original feed-forward neural network. Re-parameterization in the ReLU network was studied in the Pathnormalized SGD [Neyshabur et al., 2015]. Our proposed layer normalization method, however, is <strong>not a re-parameterization of the original neural network. The layer normalized model, thus, has different invariance properties than the other methods</strong>, that we will study in the following section</p>\n</blockquote>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "neural-network",
      "cnn",
      "convolutional-neural-network"
    ],
    "owner": {
      "account_id": 8401077,
      "reputation": 461,
      "user_id": 27330,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4425d7101f53c1ac70d0775a552cdc17?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Praise the lord",
      "link": "https://datascience.stackexchange.com/users/27330/praise-the-lord"
    },
    "is_answered": true,
    "view_count": 17287,
    "answer_count": 6,
    "score": 36,
    "last_activity_date": 1613489261,
    "creation_date": 1482497027,
    "last_edit_date": 1613489261,
    "question_id": 15903,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/15903/why-do-convolutional-neural-networks-work",
    "title": "Why do convolutional neural networks work?",
    "body": "<p>I have often heard people saying that why convolutional neural networks are still poorly understood. Is it known why convolutional neural networks always end up learning increasingly sophisticated features as we go up the layers?\nWhat caused them create such a stack of features and would this also be true for other types of deep neural networks?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "nlp",
      "clustering",
      "similarity"
    ],
    "owner": {
      "account_id": 8924195,
      "reputation": 483,
      "user_id": 41487,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/96d838597f868c038ab105b09d17a5d4?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Logan",
      "link": "https://datascience.stackexchange.com/users/41487/logan"
    },
    "is_answered": true,
    "view_count": 45734,
    "answer_count": 4,
    "score": 36,
    "last_activity_date": 1724863297,
    "creation_date": 1518442306,
    "last_edit_date": 1619564558,
    "question_id": 27726,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/27726/when-to-use-cosine-simlarity-over-euclidean-similarity",
    "title": "When to use cosine simlarity over Euclidean similarity",
    "body": "<p>In NLP, people tend to use cosine similarity to measure document/text distances. I want to hear what do people think of the following two scenarios, which to pick, cosine similarity or Euclidean?</p>\n<p>Overview of the task set: The task is to compute context similarities of multi-word expressions. For example, suppose we were given an MWE of <code>put up</code>, context refers to the words on the left side of <code>put up</code> and as well as the words on the right side of it in one text. Mathematically speaking, similarity in this task is about calculating</p>\n<pre><code>sim(context_of_using_&quot;put_up&quot;, context_of_using_&quot;in_short&quot;)\n</code></pre>\n<p>Note that context is the feature that built on top of word embeddings, let's assume each word has an embedding dimension of <code>200</code>:</p>\n<p>Two scenarios of representing <code>context_of_an_expression</code>.</p>\n<ol>\n<li><p>concatenate the left and right context words, producing an embedding vector of dimension <code>200*4=800</code> if picking two words on each side. In other words, a feature vector of [lc1, lc2, rc1, rc2] is build for context, where <code>lc=left_context</code> and <code>rc=right_context</code>.</p>\n</li>\n<li><p>get the mean of the sum of left and right context words, producing a vector of <code>200</code> dimensions. In other words, a feature vector of [mean(lc1+lc2+rc1+rc2)] is built for context.</p>\n</li>\n</ol>\n<p>[Edited] For both scenarios, I think Euclidean distance is a better fit. Cosine similarity is known for handling scale/length effects because of normalization. But I don't think there's much to be normalized.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "deep-learning",
      "keras",
      "activation-function"
    ],
    "owner": {
      "account_id": 14253681,
      "reputation": 1896,
      "user_id": 58433,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d3ab9372a78ad4ad2cea517958df4638?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user10296606",
      "link": "https://datascience.stackexchange.com/users/58433/user10296606"
    },
    "is_answered": true,
    "view_count": 84959,
    "accepted_answer_id": 39043,
    "answer_count": 4,
    "score": 36,
    "last_activity_date": 1685180909,
    "creation_date": 1538453207,
    "last_edit_date": 1685180909,
    "question_id": 39042,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/39042/how-to-use-leakyrelu-as-activation-function-in-sequence-dnn-in-keraswhen-it-per",
    "title": "How to use LeakyRelu as activation function in sequence DNN in keras?When it perfoms better than Relu?",
    "body": "<p>How do you use LeakyRelu as an activation function in sequence DNN in keras?\nIf I want to write something similar to:</p>\n\n<pre><code> model = Sequential()\n    model.add(Dense(90, activation='LeakyRelu'))\n</code></pre>\n\n<p>What is the solution? Put LeakyRelu similar to Relu?</p>\n\n<p>Second question is: what are the best general setting for tuning the parameters of LeakyRelu? When is its performance significantly better than Relu?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "deep-learning",
      "attention-mechanism",
      "softmax"
    ],
    "owner": {
      "account_id": 7728286,
      "reputation": 461,
      "user_id": 80320,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/c33d1a1f19e4d3d24b411bd4a6ff3eb9?s=256&d=identicon&r=PG",
      "display_name": "4-bit",
      "link": "https://datascience.stackexchange.com/users/80320/4-bit"
    },
    "is_answered": true,
    "view_count": 20871,
    "answer_count": 4,
    "score": 36,
    "last_activity_date": 1618117059,
    "creation_date": 1567074650,
    "question_id": 58376,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/58376/gumbel-softmax-trick-vs-softmax-with-temperature",
    "title": "Gumbel-Softmax trick vs Softmax with temperature",
    "body": "<p>From what I understand, the <em>Gumbel-Softmax</em> trick is a technique that enables us to sample discrete random variables, in a way that is differentiable (and therefore suited for end-to-end deep learning).</p>\n\n<p>Many papers and articles describe it as a way of selecting instances in the input (i.e. 'pointers') without using the non-differentiable argmax-function. The thing that confuses me is that this effect can be achieved without randomness by just using <em>Softmax with temperature</em>:</p>\n\n<p><strong>Softmax with temperature</strong>\n<span class=\"math-container\">$$y_=\\frac{exp(\\frac{_}{\\tau})}{\\sum_{}exp(\\frac{_}{\\tau})}$$</span></p>\n\n<p><strong>Gumbel-Softmax</strong>\n<span class=\"math-container\">$$y_=\\frac{exp(\\frac{log(\\pi_)+g_i}{\\tau})}{\\sum_{}exp(\\frac{log(\\pi_j)+g_j}{\\tau})}$$</span></p>\n\n<p><strong>My question</strong> </p>\n\n<p><em>From a practical and theoretical perspective, when is it beneficial to incorporate Gumbel noise into a neural network, as opposed to just using Softmax with temperature?</em></p>\n\n<p>A couple of observations:</p>\n\n<ol>\n<li>When the temperature is low, both Softmax with temperature and the Gumbel-Softmax functions will approximate a one-hot vector. However, before convergence, the Gumbel-Softmax may more suddenly 'change' its decision because of the noise.</li>\n<li>When the temperature is higher, the Gumbel noise will get a larger significance and the distribution will become more uniform. Why is this desired?</li>\n</ol>\n\n<p>My best guess is that the introduction of the Gumbel noise enforces stronger exploration before convergence, but I can't recall reading any papers that use this as a motivation to bring in the extra randomness.</p>\n\n<p>Does anyone have any experience or insights on this? Maybe I've completely missed the key point of Gumbel-Softmax :)</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "dataset",
      "class-imbalance"
    ],
    "owner": {
      "account_id": 241216,
      "reputation": 5474,
      "user_id": 97,
      "user_type": "registered",
      "accept_rate": 77,
      "profile_image": "https://www.gravatar.com/avatar/68acdc33e278ab19debf230662a7ad96?s=256&d=identicon&r=PG",
      "display_name": "IgorS",
      "link": "https://datascience.stackexchange.com/users/97/igors"
    },
    "is_answered": true,
    "view_count": 16181,
    "accepted_answer_id": 1112,
    "answer_count": 4,
    "score": 35,
    "last_activity_date": 1468620608,
    "creation_date": 1410535251,
    "last_edit_date": 1492087841,
    "question_id": 1107,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/1107/quick-guide-into-training-highly-imbalanced-data-sets",
    "title": "Quick guide into training highly imbalanced data sets",
    "body": "<p>I have a classification problem with approximately 1000 positive and 10000 negative samples in training set. So this data set is quite unbalanced. Plain random forest is just trying to mark all test samples as a majority class.</p>\n\n<p>Some good answers about sub-sampling and weighted random forest are given here: <a href=\"https://datascience.stackexchange.com/questions/454/what-are-the-implications-for-training-a-tree-ensemble-with-highly-biased-datase\">What are the implications for training a Tree Ensemble with highly biased datasets?</a></p>\n\n<p>Which classification methods besides RF can handle the problem in the best way?</p>\n"
  },
  {
    "tags": [
      "random-forest",
      "cross-validation"
    ],
    "owner": {
      "account_id": 1414923,
      "reputation": 664,
      "user_id": 1127,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8fc5b99f9f6ffda1d4ee42964b1ff525?s=256&d=identicon&r=PG",
      "display_name": "neuron",
      "link": "https://datascience.stackexchange.com/users/1127/neuron"
    },
    "is_answered": true,
    "view_count": 62967,
    "answer_count": 3,
    "score": 35,
    "last_activity_date": 1727697687,
    "creation_date": 1437399741,
    "last_edit_date": 1535049593,
    "question_id": 6510,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/6510/does-modeling-with-random-forests-require-cross-validation",
    "title": "Does modeling with Random Forests require cross-validation?",
    "body": "<p>As far as I've seen, opinions tend to differ about this. Best practice would certainly dictate using cross-validation (especially if comparing RFs with other algorithms on the same dataset). On the other hand, the original source states that the fact OOB error is calculated during model training is enough of an indicator of test set performance. Even Trevor Hastie, in a relatively recent talks says that \"Random Forests provide free cross-validation\". Intuitively, this makes sense to me, if training and trying to improve a RF-based model on one dataset.</p>\n\n<p>Can someone please lay out the arguments for and against the need for cross-validation with random forests?</p>\n"
  },
  {
    "tags": [
      "python",
      "neural-network",
      "classification",
      "clustering",
      "keras"
    ],
    "owner": {
      "account_id": 5602015,
      "reputation": 557,
      "user_id": 14684,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://www.gravatar.com/avatar/b725e706648f6d77d70c2fe193df8dd2?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "SpanishBoy",
      "link": "https://datascience.stackexchange.com/users/14684/spanishboy"
    },
    "is_answered": true,
    "view_count": 71655,
    "accepted_answer_id": 10052,
    "answer_count": 1,
    "score": 35,
    "last_activity_date": 1578297730,
    "creation_date": 1454339913,
    "last_edit_date": 1493824781,
    "question_id": 10048,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10048/what-is-the-best-keras-model-for-multi-class-classification",
    "title": "What is the best Keras model for multi-class classification?",
    "body": "<p>I am working on research, where need to classify one of three event WINNER=(<code>win</code>, <code>draw</code>, <code>lose</code>)</p>\n\n<pre><code>WINNER  LEAGUE  HOME    AWAY    MATCH_HOME  MATCH_DRAW  MATCH_AWAY  MATCH_U2_50 MATCH_O2_50\n3         13    550      571          1.86        3.34        4.23       1.66     2.11\n3         7     322     334           7.55         4.1         1.4       2.17     1.61\n</code></pre>\n\n<p>My current model is:</p>\n\n<pre><code>def build_model(input_dim, output_classes):\n    model = Sequential()\n    model.add(Dense(input_dim=input_dim, output_dim=12, activation=relu))\n    model.add(Dropout(0.5))\n    model.add(Dense(output_dim=output_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n    return model\n</code></pre>\n\n<ol>\n<li>I am not sure that is the correct one for multi-class classification</li>\n<li>What is the best setup for binary classification?</li>\n</ol>\n\n<p>EDIT: #2 - Like that?</p>\n\n<pre><code>model.add(Dense(input_dim=input_dim, output_dim=12, activation='sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(output_dim=output_classes, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='adadelta')\n</code></pre>\n"
  },
  {
    "tags": [
      "python",
      "apache-spark",
      "cross-validation",
      "pyspark"
    ],
    "owner": {
      "account_id": 3837093,
      "reputation": 1147,
      "user_id": 17116,
      "user_type": "registered",
      "accept_rate": 80,
      "profile_image": "https://www.gravatar.com/avatar/630e126dc5c789a0c830830c27f9140c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "krishna Prasad",
      "link": "https://datascience.stackexchange.com/users/17116/krishna-prasad"
    },
    "is_answered": true,
    "view_count": 174462,
    "protected_date": 1598457073,
    "accepted_answer_id": 11361,
    "answer_count": 6,
    "score": 35,
    "last_activity_date": 1612844805,
    "creation_date": 1461299265,
    "last_edit_date": 1462888597,
    "question_id": 11356,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/11356/merging-multiple-data-frames-row-wise-in-pyspark",
    "title": "Merging multiple data frames row-wise in PySpark",
    "body": "<p>I have 10 data frames <code>pyspark.sql.dataframe.DataFrame</code>, obtained from <code>randomSplit</code> as <code>(td1, td2, td3, td4, td5, td6, td7, td8, td9, td10) = td.randomSplit([.1, .1, .1, .1, .1, .1, .1, .1, .1, .1], seed = 100)</code> Now I want to join 9 <code>td</code>'s into a single data frame, how should I do that?</p>\n\n<p>I have already tried with <code>unionAll</code>, but this function accepts only two arguments. </p>\n\n<pre><code>td1_2 = td1.unionAll(td2) \n# this is working fine\n\ntd1_2_3 = td1.unionAll(td2, td3) \n# error TypeError: unionAll() takes exactly 2 arguments (3 given)\n</code></pre>\n\n<p>Is there any way to combine more than two data frames row-wise? </p>\n\n<p>The purpose of doing this is that I am doing 10-fold Cross Validation manually without using PySpark <code>CrossValidator</code> method, So taking 9 into training and 1 into test data and then I will repeat it for other combinations. </p>\n"
  },
  {
    "tags": [
      "keras",
      "convolutional-neural-network"
    ],
    "owner": {
      "account_id": 261362,
      "reputation": 453,
      "user_id": 28213,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7102e1a91e62a1a3943bbefd8c4d0751?s=256&d=identicon&r=PG",
      "display_name": "ChrisFal",
      "link": "https://datascience.stackexchange.com/users/28213/chrisfal"
    },
    "is_answered": true,
    "view_count": 22299,
    "accepted_answer_id": 16466,
    "answer_count": 2,
    "score": 35,
    "last_activity_date": 1613489021,
    "creation_date": 1485158858,
    "last_edit_date": 1613489021,
    "question_id": 16463,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/16463/what-is-are-the-default-filters-used-by-keras-convolution2d",
    "title": "What is/are the default filters used by Keras Convolution2d()?",
    "body": "<p>I am pretty new to neural networks, but I understand linear algebra and the mathematics of convolution pretty decently. </p>\n\n<p>I am trying to understand the example code I find in various places on the net for training a Keras convolutional NN with MNIST data to recognize digits. My expectation would be that when I create a convolutional layer, I would have to specify a filter or set of filters to apply to the input. But the three samples I have found all create a convolutional layer like this:</p>\n\n<pre><code>model.add(Convolution2D(nb_filter = 32, nb_row = 3, nb_col = 3,\n                        border_mode='valid',\n                        input_shape=input_shape))\n</code></pre>\n\n<p>This seems to be applying a total of 32 3x3 filters to the images processed by the CNN. But what are those filters? How would I describe them mathematically? The keras documentation is no help. </p>\n\n<p>Thanks in advance,</p>\n"
  },
  {
    "tags": [
      "nlp",
      "clustering",
      "word2vec",
      "similarity"
    ],
    "owner": {
      "account_id": 3953311,
      "reputation": 533,
      "user_id": 31810,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e4fea31c3a9c8656ab25d68120a2b971?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "DaveTheAl",
      "link": "https://datascience.stackexchange.com/users/31810/davetheal"
    },
    "is_answered": true,
    "view_count": 63176,
    "answer_count": 8,
    "score": 35,
    "last_activity_date": 1628192658,
    "creation_date": 1511448025,
    "question_id": 25053,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/25053/best-practical-algorithm-for-sentence-similarity",
    "title": "Best practical algorithm for sentence similarity",
    "body": "<p>I have two sentences, S1 and S2, both which have a word count (usually) below 15.</p>\n\n<p>What are the most practically useful and successful (machine learning) algorithms, which are possibly easy to implement (neural network is ok, unless the architecture is as complicated as Google Inception etc.).</p>\n\n<p>I am looking for an algorithm that will work fine without putting too much time into it. Are there any algorithms you've found successful and easy to use? </p>\n\n<p>This can, but does not have to fall into the category of clustering. My background is from machine learning, so any suggestions are welcome :)</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "regression",
      "lstm",
      "rnn"
    ],
    "owner": {
      "account_id": 1298864,
      "reputation": 601,
      "user_id": 44477,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/UkPsU.jpg?s=256",
      "display_name": "DukeLover",
      "link": "https://datascience.stackexchange.com/users/44477/dukelover"
    },
    "is_answered": true,
    "view_count": 132801,
    "answer_count": 6,
    "score": 35,
    "last_activity_date": 1649782753,
    "creation_date": 1545898986,
    "last_edit_date": 1560095022,
    "question_id": 43191,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/43191/validation-loss-is-not-decreasing",
    "title": "Validation loss is not decreasing",
    "body": "<p>I am trying to train a LSTM model. Is this model suffering from overfitting? </p>\n\n<p>Here is train and validation loss graph: </p>\n\n<p><a href=\"https://i.sstatic.net/l3fw7.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/l3fw7.png\" alt=\"loss\"></a></p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "dataset",
      "data",
      "training"
    ],
    "owner": {
      "account_id": 13508670,
      "reputation": 461,
      "user_id": 108739,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0ec0ec9b03e8a02d0c1f03f7286d3004?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "karalis1",
      "link": "https://datascience.stackexchange.com/users/108739/karalis1"
    },
    "is_answered": true,
    "view_count": 17844,
    "answer_count": 10,
    "score": 35,
    "last_activity_date": 1729990516,
    "creation_date": 1607868718,
    "last_edit_date": 1675082304,
    "question_id": 86632,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/86632/why-is-it-wrong-to-train-and-test-a-model-on-the-same-dataset",
    "title": "Why is it wrong to train and test a model on the same dataset?",
    "body": "<p>What are the pitfalls of doing so and why is it a bad practice? Is it possible that the model starts to learn the images &quot;by heart&quot; instead of understanding the underlying logic?</p>\n"
  },
  {
    "tags": [
      "apache-spark",
      "apache-hadoop",
      "distributed",
      "knowledge-base",
      "cloud-computing"
    ],
    "owner": {
      "account_id": 252602,
      "reputation": 521,
      "user_id": 426,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/a9d6c0db2d8698937a13e7ae44bd339a?s=256&d=identicon&r=PG",
      "display_name": "idclark",
      "link": "https://datascience.stackexchange.com/users/426/idclark"
    },
    "is_answered": true,
    "view_count": 13090,
    "protected_date": 1587652397,
    "accepted_answer_id": 449,
    "answer_count": 5,
    "score": 34,
    "last_activity_date": 1587657654,
    "creation_date": 1403038115,
    "last_edit_date": 1587657654,
    "question_id": 441,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/441/what-are-the-use-cases-for-apache-spark-vs-hadoop",
    "title": "What are the use cases for Apache Spark vs Hadoop",
    "body": "<p>With Hadoop 2.0 and YARN Hadoop is supposedly no longer tied only map-reduce solutions. With that advancement, what are the use cases for Apache Spark vs Hadoop considering both sit atop of HDFS? I've read through the introduction documentation for Spark, but I'm curious if anyone has encountered a problem that was more efficient and easier to solve with Spark compared to Hadoop.</p>\n"
  },
  {
    "tags": [
      "data-mining"
    ],
    "owner": {
      "account_id": 5005004,
      "reputation": 341,
      "user_id": 3250,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/3ab401087a2f4dbebe16c63652adf052?s=256&d=identicon&r=PG",
      "display_name": "Damien",
      "link": "https://datascience.stackexchange.com/users/3250/damien"
    },
    "is_answered": true,
    "view_count": 69646,
    "answer_count": 6,
    "score": 34,
    "last_activity_date": 1678730632,
    "creation_date": 1410266656,
    "question_id": 1095,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/1095/gini-coefficient-vs-gini-impurity-decision-trees",
    "title": "Gini coefficient vs Gini impurity - decision trees",
    "body": "<p>The problem refers to decision trees building. According to Wikipedia '<a href=\"http://en.wikipedia.org/wiki/Gini_coefficient\">Gini coefficient</a>' should not be confused with '<a href=\"http://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity\">Gini impurity</a>'. However both measures can be used when building a decision tree - these can support our choices when splitting the set of items.</p>\n\n<p>1) 'Gini impurity' - it is a standard decision-tree splitting metric (see in the link above);</p>\n\n<p>2) 'Gini coefficient' - each splitting can be assessed based on the AUC criterion. For each splitting scenario we can build a ROC curve and compute AUC metric. According to Wikipedia AUC=(GiniCoeff+1)/2;</p>\n\n<p>Question is: are both these measures equivalent? On the one hand, I am informed that Gini coefficient should not be confused with Gini impurity. On the other hand, both these measures can be used in doing the same thing - assessing the quality of a decision tree split.</p>\n"
  },
  {
    "tags": [
      "feature-selection",
      "feature-extraction",
      "feature-construction"
    ],
    "owner": {
      "account_id": 126424,
      "reputation": 441,
      "user_id": 13154,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/feoYE.png?s=256",
      "display_name": "John",
      "link": "https://datascience.stackexchange.com/users/13154/john"
    },
    "is_answered": true,
    "view_count": 10445,
    "protected_date": 1651325047,
    "answer_count": 6,
    "score": 34,
    "last_activity_date": 1702219800,
    "creation_date": 1443845396,
    "last_edit_date": 1443853130,
    "question_id": 8286,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/8286/are-there-any-tools-for-feature-engineering",
    "title": "Are there any tools for feature engineering?",
    "body": "<p>Specifically what I am looking for are tools with some functionality, which is specific to feature engineering. I would like to be able to easily smooth, visualize, fill gaps, etc. Something similar to MS Excel, but that has R as the underlying language instead of VB.</p>\n"
  },
  {
    "tags": [
      "nlp",
      "word-embeddings",
      "word2vec",
      "nltk"
    ],
    "owner": {
      "account_id": 903892,
      "reputation": 695,
      "user_id": 6403,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://www.gravatar.com/avatar/4ee12a004bdd48dab9a040fae4aa51f6?s=256&d=identicon&r=PG",
      "display_name": "Thomas Johnson",
      "link": "https://datascience.stackexchange.com/users/6403/thomas-johnson"
    },
    "is_answered": true,
    "view_count": 40876,
    "answer_count": 5,
    "score": 34,
    "last_activity_date": 1596721016,
    "creation_date": 1468965274,
    "question_id": 12872,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/12872/how-can-i-get-a-measure-of-the-semantic-similarity-of-words",
    "title": "How can I get a measure of the semantic similarity of words?",
    "body": "<p>What is the best way to figure out the semantic similarity of words? Word2Vec is okay, but not ideal:</p>\n\n<pre><code># Using the 840B word Common Crawl GloVe vectors with gensim:\n\n# 'hot' is closer to 'cold' than 'warm'\nIn [7]: model.similarity('hot', 'cold')\nOut[7]: 0.59720456121072973\n\nIn [8]: model.similarity('hot', 'warm')\nOut[8]: 0.56784095376659627\n\n# Cold is much closer to 'hot' than 'popular'\nIn [9]: model.similarity('hot', 'popular')\nOut[9]: 0.33708479049537632\n</code></pre>\n\n<p>NLTK's Wordnet methods appear to just give up:</p>\n\n<pre><code>In [25]: print wn.synset('hot.a.01').path_similarity(wn.synset('warm.a.01'))\nNone\n</code></pre>\n\n<p>What are other options?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "neural-network",
      "convolutional-neural-network",
      "optimization"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 42654,
    "accepted_answer_id": 18415,
    "answer_count": 2,
    "score": 34,
    "last_activity_date": 1617130111,
    "creation_date": 1492445902,
    "last_edit_date": 1617130111,
    "question_id": 18414,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/18414/are-there-any-rules-for-choosing-the-size-of-a-mini-batch",
    "title": "Are there any rules for choosing the size of a mini-batch?",
    "body": "<p>When training neural networks, one hyperparameter is the size of a minibatch. Common choices are 32, 64, and 128 elements per mini batch.</p>\n<p>Are there any rules/guidelines on how big a mini-batch should be? Or any publications which investigate the effect on the training?</p>\n"
  },
  {
    "tags": [
      "neural-network"
    ],
    "owner": {
      "account_id": 48698,
      "reputation": 433,
      "user_id": 2790,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/77878e92ed28784e85b8e019c7e043f1?s=256&d=identicon&r=PG",
      "display_name": "MidnightLightning",
      "link": "https://datascience.stackexchange.com/users/2790/midnightlightning"
    },
    "is_answered": true,
    "view_count": 26107,
    "accepted_answer_id": 871,
    "answer_count": 4,
    "score": 33,
    "last_activity_date": 1503763069,
    "creation_date": 1406737665,
    "question_id": 869,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/869/neural-network-parse-string-data",
    "title": "Neural Network parse string data?",
    "body": "<p>So, I'm just starting to learn how a neural network can operate to recognize patterns and categorize inputs, and I've seen how an artificial neural network can parse image data and categorize the images (<a href=\"http://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html\">demo with convnetjs</a>), and the key there is to downsample the image and each pixel stimulates one input neuron into the network.</p>\n\n<p>However, I'm trying to wrap my head around if this is possible to be done with string inputs? The use-case I've got is a \"recommendation engine\" for movies a user has watched. Movies have lots of string data (title, plot, tags), and I could imagine \"downsampling\" the text down to a few key words that describe that movie, but even if I parse out the top five words that describe this movie, I think I'd need input neurons for every english word in order to compare a set of movies? I could limit the input neurons just to the words used in the set, but then could it grow/learn by adding new movies (user watches a new movie, with new words)? Most of the libraries I've seen don't allow adding new neurons after the system has been trained?</p>\n\n<p>Is there a standard way to map string/word/character data to inputs into a neural network? Or is a neural network really not the right tool for the job of parsing string data like this (what's a better tool for pattern-matching in string data)?</p>\n"
  },
  {
    "tags": [
      "r",
      "python",
      "xgboost"
    ],
    "owner": {
      "account_id": 6444670,
      "reputation": 8476,
      "user_id": 11097,
      "user_type": "registered",
      "accept_rate": 75,
      "profile_image": "https://i.sstatic.net/z0jss.png?s=256",
      "display_name": "Dawny33",
      "link": "https://datascience.stackexchange.com/users/11097/dawny33"
    },
    "is_answered": true,
    "view_count": 45127,
    "accepted_answer_id": 9368,
    "answer_count": 3,
    "score": 33,
    "last_activity_date": 1557601144,
    "creation_date": 1450016394,
    "question_id": 9364,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9364/hypertuning-xgboost-parameters",
    "title": "Hypertuning XGBoost parameters",
    "body": "<p>XGBoost have been doing a great job, when it comes to dealing with both categorical and continuous dependant variables. But, how do I select the optimized parameters for an XGBoost problem?</p>\n\n<p>This is how I applied the parameters for a recent Kaggle problem:</p>\n\n<pre><code>param &lt;- list(  objective           = \"reg:linear\", \n                booster = \"gbtree\",\n                eta                 = 0.02, # 0.06, #0.01,\n                max_depth           = 10, #changed from default of 8\n                subsample           = 0.5, # 0.7\n                colsample_bytree    = 0.7, # 0.7\n                num_parallel_tree   = 5\n                # alpha = 0.0001, \n                # lambda = 1\n)\n\n\nclf &lt;- xgb.train(   params              = param, \n                    data                = dtrain, \n                    nrounds             = 3000, #300, #280, #125, #250, # changed from 300\n                    verbose             = 0,\n                    early.stop.round    = 100,\n                    watchlist           = watchlist,\n                    maximize            = FALSE,\n                    feval=RMPSE\n)\n</code></pre>\n\n<p>All I do to experiment is randomly select (with intuition) another set of parameters for improving on the result.</p>\n\n<p>Is there anyway I automate the selection of optimized(best) set of parameters?</p>\n\n<p>(Answers can be in any language. I'm just looking for the technique)</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "feature-engineering",
      "feature-scaling",
      "normalization"
    ],
    "owner": {
      "account_id": 7744138,
      "reputation": 443,
      "user_id": 23600,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/957eec2b372eb7c8bb8fe7524f8b5e4e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "AllThingsScience",
      "link": "https://datascience.stackexchange.com/users/23600/allthingsscience"
    },
    "is_answered": true,
    "view_count": 29004,
    "closed_date": 1471713630,
    "accepted_answer_id": 13575,
    "answer_count": 1,
    "score": 33,
    "last_activity_date": 1488481705,
    "creation_date": 1471675886,
    "last_edit_date": 1471719419,
    "question_id": 13567,
    "link": "https://datascience.stackexchange.com/questions/13567/ways-to-deal-with-longitude-latitude-feature",
    "closed_reason": "Needs details or clarity",
    "title": "Ways to deal with longitude/latitude feature",
    "body": "<p>I am working on a fictional dataset with 25 features. Two of the features are latitude and longitude of a place and others are pH values, elevation, windSpeed etc with varying ranges. I can perform normalization on the other features but how do I approach latitude/longitude features?</p>\n\n<p>Edit: This is a problem to predict agriculture yield. I would think lat/long is very important since locations can be vital in prediction and hence the dilemma.</p>\n"
  },
  {
    "tags": [
      "classification",
      "data",
      "decision-trees"
    ],
    "owner": {
      "account_id": 6322704,
      "reputation": 441,
      "user_id": 41397,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-QdookrlbU_g/AAAAAAAAAAI/AAAAAAAAABM/lOy93Mm4Vrc/s256-rj/photo.jpg",
      "display_name": "WALID BELRHALMIA",
      "link": "https://datascience.stackexchange.com/users/41397/walid-belrhalmia"
    },
    "is_answered": true,
    "view_count": 34293,
    "accepted_answer_id": 24342,
    "answer_count": 1,
    "score": 33,
    "last_activity_date": 1640360950,
    "creation_date": 1509745509,
    "last_edit_date": 1509822443,
    "question_id": 24339,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/24339/how-is-a-splitting-point-chosen-for-continuous-variables-in-decision-trees",
    "title": "How is a splitting point chosen for continuous variables in decision trees?",
    "body": "<p>I have two questions related to decision trees:</p>\n\n<ol>\n<li><p>If we have a continuous attribute, how do we choose the splitting value?</p>\n\n<p>Example: Age=(20,29,50,40....)</p></li>\n<li><p>Imagine that we have a continuous attribute $f$ that have values in $R$. How can I  write an algorithm that finds the split point $v$, in order that when we split $f$ by $v$, we have a minimum gain for $f&gt;v$?</p></li>\n</ol>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "scikit-learn",
      "cross-validation"
    ],
    "owner": {
      "account_id": 9714239,
      "reputation": 951,
      "user_id": 46433,
      "user_type": "registered",
      "accept_rate": 71,
      "profile_image": "https://i.sstatic.net/Ge2WM.jpg?s=256",
      "display_name": "Taimur Islam",
      "link": "https://datascience.stackexchange.com/users/46433/taimur-islam"
    },
    "is_answered": true,
    "view_count": 62360,
    "accepted_answer_id": 28159,
    "answer_count": 2,
    "score": 33,
    "last_activity_date": 1557981198,
    "creation_date": 1519277023,
    "last_edit_date": 1557981198,
    "question_id": 28158,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/28158/how-to-calculate-the-fold-number-k-fold-in-cross-validation",
    "title": "How to calculate the fold number (k-fold) in cross validation?",
    "body": "<p>I am confused about how I choose the number of folds (in k-fold CV) when I apply cross validation to check the model. Is it dependent on data size or other parameters?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network"
    ],
    "owner": {
      "account_id": 2000678,
      "reputation": 487,
      "user_id": 49067,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6927fc09abe845246c8afc183525d381?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "lukassz",
      "link": "https://datascience.stackexchange.com/users/49067/lukassz"
    },
    "is_answered": true,
    "view_count": 13793,
    "accepted_answer_id": 30679,
    "answer_count": 4,
    "score": 33,
    "last_activity_date": 1597954012,
    "creation_date": 1524476328,
    "last_edit_date": 1524476418,
    "question_id": 30676,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/30676/role-derivative-of-sigmoid-function-in-neural-networks",
    "title": "Role derivative of sigmoid function in neural networks",
    "body": "<p>I try to understand role of derivative of sigmoid function in neural networks. \n<a href=\"https://i.sstatic.net/N8vbm.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/N8vbm.png\" alt=\"enter image description here\"></a></p>\n\n<p>First I plot sigmoid function, and derivative of all points from definition using python. What is the role of this derivative exactly? \n<a href=\"https://i.sstatic.net/inMoa.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/inMoa.png\" alt=\"enter image description here\"></a></p>\n\n<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef derivative(x, step):\n    return (sigmoid(x+step) - sigmoid(x)) / step\n\nx = np.linspace(-10, 10, 1000)\n\ny1 = sigmoid(x)\ny2 = derivative(x, 0.0000000000001)\n\nplt.plot(x, y1, label='sigmoid')\nplt.plot(x, y2, label='derivative')\nplt.legend(loc='upper left')\nplt.show()\n</code></pre>\n"
  },
  {
    "tags": [
      "machine-learning",
      "nlp",
      "text-mining",
      "data-cleaning"
    ],
    "owner": {
      "account_id": 1690033,
      "reputation": 431,
      "user_id": 8643,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/3365d984caa848b40fdb0c241e8e983c?s=256&d=identicon&r=PG",
      "display_name": "William Falcon",
      "link": "https://datascience.stackexchange.com/users/8643/william-falcon"
    },
    "is_answered": true,
    "view_count": 33630,
    "answer_count": 3,
    "score": 32,
    "last_activity_date": 1597948794,
    "creation_date": 1426264889,
    "question_id": 5316,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/5316/general-approach-to-extract-key-text-from-sentence-nlp",
    "title": "General approach to extract key text from sentence (nlp)",
    "body": "<p>Given a sentence like:</p>\n\n<pre><code>Complimentary gym access for two for the length of stay ($12 value per person per day)\n</code></pre>\n\n<p>What general approach can I take to identify the word gym or gym access?</p>\n"
  },
  {
    "tags": [
      "scikit-learn",
      "logistic-regression"
    ],
    "owner": {
      "account_id": 8170907,
      "reputation": 421,
      "user_id": 26579,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7edda03510d2cb222e3845f93668a7a6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "hminle",
      "link": "https://datascience.stackexchange.com/users/26579/hminle"
    },
    "is_answered": true,
    "view_count": 144670,
    "answer_count": 3,
    "score": 32,
    "last_activity_date": 1656611937,
    "creation_date": 1480353045,
    "last_edit_date": 1480416705,
    "question_id": 15398,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/15398/how-to-get-p-value-and-confident-interval-in-logisticregression-with-sklearn",
    "title": "How to get p-value and confident interval in LogisticRegression with sklearn?",
    "body": "<p>I am building a multinomial logistic regression with sklearn (LogisticRegression). But after it finishes, how can I get a p-value and confident interval of my model? It only appears that sklearn only provides coefficient and intercept.</p>\n\n<p>Thank you a lot.</p>\n"
  },
  {
    "tags": [
      "python",
      "r",
      "pandas",
      "data",
      "data-table"
    ],
    "owner": {
      "account_id": 86224,
      "reputation": 640,
      "user_id": 40951,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/f4ZA3.jpg?s=256",
      "display_name": "xiaodai",
      "link": "https://datascience.stackexchange.com/users/40951/xiaodai"
    },
    "is_answered": true,
    "view_count": 27356,
    "answer_count": 4,
    "score": 32,
    "last_activity_date": 1615745784,
    "creation_date": 1508899429,
    "last_edit_date": 1612210778,
    "question_id": 24052,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/24052/is-pandas-now-faster-than-data-table",
    "title": "Is pandas now faster than data.table?",
    "body": "<p><a href=\"https://github.com/Rdatatable/data.table/wiki/Benchmarks-%3A-Grouping\" rel=\"noreferrer\">Here</a> is the GitHub link to the most recent <code>data.table</code> benchmark.</p>\n<p>The <code>data.table</code> benchmarks has not been updated since 2014. I heard somewhere that <code>Pandas</code> is now faster than <code>data.table</code>. Is this true? Has anyone done any benchmarks? I have never used Python before but would consider switching if <code>pandas</code> can beat <code>data.table</code>?</p>\n"
  },
  {
    "tags": [
      "loss-function"
    ],
    "owner": {
      "account_id": 4962749,
      "reputation": 2785,
      "user_id": 17310,
      "user_type": "registered",
      "accept_rate": 25,
      "profile_image": "https://www.gravatar.com/avatar/6b2ddcbdf322159e5ff9fa987d2af7c9?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Edamame",
      "link": "https://datascience.stackexchange.com/users/17310/edamame"
    },
    "is_answered": true,
    "view_count": 88677,
    "answer_count": 7,
    "score": 32,
    "last_activity_date": 1717767319,
    "creation_date": 1514771936,
    "last_edit_date": 1717767319,
    "question_id": 26180,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/26180/l2-loss-vs-mean-squared-loss",
    "title": "L2 loss vs. mean squared loss",
    "body": "<p>I see literature that suggests L2 loss and mean squared error loss are two different kinds of loss functions.</p>\n<p>However, it seems to me these two loss functions essentially compute the same thing (with a 1/n factor difference).</p>\n<p>So I am wondering if I have missed anything? Is there any scenario that one should use one of the two loss functions?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "keras",
      "overfitting",
      "regularization",
      "dropout"
    ],
    "owner": {
      "account_id": 1891013,
      "reputation": 1455,
      "user_id": 57724,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/dc4597c8aedf386296097d025d0c1c78?s=256&d=identicon&r=PG",
      "display_name": "user781486",
      "link": "https://datascience.stackexchange.com/users/57724/user781486"
    },
    "is_answered": true,
    "view_count": 27366,
    "accepted_answer_id": 37370,
    "answer_count": 2,
    "score": 32,
    "last_activity_date": 1685180977,
    "creation_date": 1535039214,
    "last_edit_date": 1685180977,
    "question_id": 37362,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/37362/when-should-one-use-l1-l2-regularization-instead-of-dropout-layer-given-that-b",
    "title": "When should one use L1, L2 regularization instead of dropout layer, given that both serve same purpose of reducing overfitting?",
    "body": "<p>In Keras, there are 2 methods to reduce over-fitting. <a href=\"https://keras.io/regularizers/\" rel=\"noreferrer\">L1,L2 regularization</a> or <a href=\"https://keras.io/layers/core/#dropout\" rel=\"noreferrer\">dropout layer</a>.</p>\n<p>What are some situations to use L1,L2 regularization instead of dropout layer? What are some situations when dropout layer is better?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "scikit-learn",
      "regression",
      "linear-regression"
    ],
    "owner": {
      "account_id": 13764581,
      "reputation": 961,
      "user_id": 60105,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/87c1a95a24bb3ccd7bd6e5d274c94089?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Anjith",
      "link": "https://datascience.stackexchange.com/users/60105/anjith"
    },
    "is_answered": true,
    "view_count": 146979,
    "accepted_answer_id": 39217,
    "answer_count": 3,
    "score": 32,
    "last_activity_date": 1567371404,
    "creation_date": 1538592207,
    "last_edit_date": 1538592937,
    "question_id": 39137,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/39137/how-can-i-check-the-correlation-between-features-and-target-variable",
    "title": "How can I check the correlation between features and target variable?",
    "body": "<p>I am trying to build a <code>Regression</code> model and I am looking for a way to check whether there's any correlation between features and target variables?</p>\n\n<p>This is my sample <code>dataset</code></p>\n\n<pre><code>     Loan_ID    Gender  Married Dependents  Education Self_Employed ApplicantIncome\\    \n\n0   LP001002    Male    No         0        Graduate      No            5849    \n1   LP001003    Male    Yes        1        Graduate      No            4583    \n2   LP001005    Male    Yes        0        Graduate     Yes            3000    \n3   LP001006    Male    Yes        0        Not Graduate  No            2583    \n4   LP001008    Male    No         0        Graduate      No            6000    \n\nCoapplicantIncome  LoanAmount   Loan_Amount_Term  Credit_History Area Loan_Status\n      0.0               123          360.0            1.0        Urban     Y\n      1508.0          128.0          360.0            1.0        Rural     N\n      0.0              66.0          360.0            1.0        Urban     Y\n      2358.0          120.0          360.0            1.0        Urban     Y\n      0.0             141.0          360.0            1.0        Urban     Y\n</code></pre>\n\n<p>I am trying to predict <code>LoanAmount</code> column based on the features available above.</p>\n\n<p>I just want to see if there's a correlation between the features and target variable. I tried <code>LinearRegression</code>, <code>GradientBoostingRegressor</code> and I'm hardly getting a accuracy of around <code>0.30 - 0.40%</code>. </p>\n\n<p>Any suggestions on algorithms, params etc that I should use for better prediction? </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "classification",
      "scikit-learn"
    ],
    "owner": {
      "account_id": 14001007,
      "reputation": 631,
      "user_id": 61261,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/5cf5a8fb827e2e4d5e38fe1a99be1ead?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Sai Kumar",
      "link": "https://datascience.stackexchange.com/users/61261/sai-kumar"
    },
    "is_answered": true,
    "view_count": 28664,
    "accepted_answer_id": 40093,
    "answer_count": 6,
    "score": 32,
    "last_activity_date": 1540525790,
    "creation_date": 1540300082,
    "last_edit_date": 1540525790,
    "question_id": 40089,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/40089/what-is-the-reason-behind-taking-log-transformation-of-few-continuous-variables",
    "title": "What is the reason behind taking log transformation of few continuous variables?",
    "body": "<p>I have been doing a classification problem and I have read many people's code and tutorials. One thing I've noticed is that many people take <code>np.log</code> or <code>log</code> of continuous variable like <code>loan_amount</code> or <code>applicant_income</code> etc. </p>\n\n<p>I just want to understand the reason behind it. Does it help improve our model prediction accuracy. Is it mandatory? or Is there any logic behind it?</p>\n\n<p>Please provide some explanation if possible. Thank you.</p>\n"
  },
  {
    "tags": [
      "variance",
      "bias"
    ],
    "owner": {
      "account_id": 12899103,
      "reputation": 2403,
      "user_id": 46740,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/2210822062268656/picture?type=large",
      "display_name": "Vaibhav Thakur",
      "link": "https://datascience.stackexchange.com/users/46740/vaibhav-thakur"
    },
    "is_answered": true,
    "view_count": 27942,
    "answer_count": 5,
    "score": 32,
    "last_activity_date": 1606811642,
    "creation_date": 1550154834,
    "last_edit_date": 1550155207,
    "question_id": 45578,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/45578/why-underfitting-is-called-high-bias-and-overfitting-is-called-high-variance",
    "title": "Why underfitting is called high bias and overfitting is called high variance?",
    "body": "<p>I have been using terms like underfitting/overfitting and bias-variance tradeoff for quite some while in data science discussions and I understand that underfitting is associated with high bias and over fitting is associated with high variance. But what is the reason of such association or in terms of a model what is high bias and high variance, How can one understand it intuitively?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "nlp",
      "text-mining"
    ],
    "owner": {
      "account_id": 1969960,
      "reputation": 431,
      "user_id": 1315,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/81b045f730574a8bb7b9678b08bef510?s=256&d=identicon&r=PG",
      "display_name": "user1769197",
      "link": "https://datascience.stackexchange.com/users/1315/user1769197"
    },
    "is_answered": true,
    "view_count": 30416,
    "answer_count": 4,
    "score": 31,
    "last_activity_date": 1554173237,
    "creation_date": 1404403882,
    "last_edit_date": 1404865141,
    "question_id": 662,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/662/what-algorithms-should-i-use-to-perform-job-classification-based-on-resume-data",
    "title": "What algorithms should I use to perform job classification based on resume data?",
    "body": "<p>Note that I am doing everything in R. </p>\n\n<p>The problem goes as follow: </p>\n\n<p>Basically, I have a list of resumes (CVs). Some candidates will have work experience before and some don't. The goal here is to: based on the text on their CVs, I want to classify them into different job sectors. I am particular in those cases, in which the candidates do not have any experience / is a student, and I want to make a prediction to classify which job sectors this candidate will most likely belongs to after graduation . </p>\n\n<p>Question 1: I know machine learning algorithms. However, I have never done NLP before. I came across Latent Dirichlet allocation on the internet. However, I am not sure if this is the best approach to tackle my problem. </p>\n\n<p>My original idea:  <em>make this a supervised learning problem</em>. \nSuppose we already have large amount of labelled data, meaning that we have correctly labelled the job sectors for a list of candidates. We train the model up using ML algorithms (i.e. nearest neighbor... )and feed in those <em>unlabelled data</em>, which are candidates that have no work experience / are students, and try to predict which job sector they will belong to. </p>\n\n<p><strong>Update</strong>\nQuestion 2: Would it be a good idea to create an text file by extracting everything in a resume and print these data out in the text file, so that each resume is associated with a text file,which contains unstructured strings, and then we applied text mining techniques to the text files and make the data become structured or even to create a  frequency matrix of terms used out of the text files ? For example, the text file may look something like this:</p>\n\n<p><code>I deployed ML algorithm in this project and... Skills: Java, Python, c++ ...</code></p>\n\n<p>This is what I meant by 'unstructured', i.e. collapsing everything into a single line string.</p>\n\n<p>Is this approach wrong ? Please correct me if you think my approach is wrong. </p>\n\n<p>Question 3: The tricky part is: how to <strong>identify and extract the keywords</strong> ? Using the <code>tm</code> package in R ? what algorithm is the <code>tm</code>   package based on ?  Should I use NLP algorithms ? If yes, what algorithms should I look at ? Please point me to some good resources to look at as well. </p>\n\n<p>Any ideas would be great.</p>\n"
  },
  {
    "tags": [
      "nlp",
      "word-embeddings"
    ],
    "owner": {
      "account_id": 2460255,
      "reputation": 3480,
      "user_id": 13023,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://www.gravatar.com/avatar/a15c4546ec5b537e7340c95be39aa52e?s=256&d=identicon&r=PG",
      "display_name": "wacax",
      "link": "https://datascience.stackexchange.com/users/13023/wacax"
    },
    "is_answered": true,
    "view_count": 20124,
    "accepted_answer_id": 9278,
    "answer_count": 3,
    "score": 31,
    "last_activity_date": 1596718889,
    "creation_date": 1446956276,
    "last_edit_date": 1521930643,
    "question_id": 8753,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/8753/what-is-a-better-input-for-word2vec",
    "title": "What is a better input for Word2Vec?",
    "body": "<p>This is more like a general NLP question. \nWhat is the appropriate input to train a word embedding namely Word2Vec? Should all sentences belonging to an article be a separate document in a corpus? Or should each article be a document in said corpus?\nThis is just an example using python and gensim.</p>\n\n<p>Corpus split by sentence:</p>\n\n<pre><code>SentenceCorpus = [[\"first\", \"sentence\", \"of\", \"the\", \"first\", \"article.\"],\n                  [\"second\", \"sentence\", \"of\", \"the\", \"first\", \"article.\"],\n                  [\"first\", \"sentence\", \"of\", \"the\", \"second\", \"article.\"],\n                  [\"second\", \"sentence\", \"of\", \"the\", \"second\", \"article.\"]]\n</code></pre>\n\n<p>Corpus split by article:</p>\n\n<pre><code>ArticleCorpus = [[\"first\", \"sentence\", \"of\", \"the\", \"first\", \"article.\",\n                  \"second\", \"sentence\", \"of\", \"the\", \"first\", \"article.\"],\n                 [\"first\", \"sentence\", \"of\", \"the\", \"second\", \"article.\",\n                  \"second\", \"sentence\", \"of\", \"the\", \"second\", \"article.\"]]\n</code></pre>\n\n<p>Training Word2Vec in Python:</p>\n\n<pre><code>from gensim.models import Word2Vec\n\nwikiWord2Vec = Word2Vec(ArticleCorpus)\n</code></pre>\n"
  },
  {
    "tags": [
      "python",
      "pandas"
    ],
    "owner": {
      "account_id": 7545308,
      "reputation": 2967,
      "user_id": 15064,
      "user_type": "registered",
      "accept_rate": 75,
      "profile_image": "https://www.gravatar.com/avatar/9cdd4d966d919fd8105714d75ce1f27f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Kaggle",
      "link": "https://datascience.stackexchange.com/users/15064/kaggle"
    },
    "is_answered": true,
    "view_count": 157481,
    "protected_date": 1577728099,
    "accepted_answer_id": 12646,
    "answer_count": 8,
    "score": 31,
    "last_activity_date": 1592140503,
    "creation_date": 1467887183,
    "question_id": 12645,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/12645/how-to-count-the-number-of-missing-values-in-each-row-in-pandas-dataframe",
    "title": "How to count the number of missing values in each row in Pandas dataframe?",
    "body": "<p>How can I get the number of missing value in each row in Pandas dataframe.\nI would like to split dataframe to different dataframes which have same number of missing values in each row.</p>\n\n<p>Any suggestion?</p>\n"
  },
  {
    "tags": [
      "nlp",
      "topic-model",
      "sentiment-analysis"
    ],
    "owner": {
      "account_id": 3764473,
      "reputation": 435,
      "user_id": 27129,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/aab40558451ab4f10eef68367fb8b32e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "E.K.",
      "link": "https://datascience.stackexchange.com/users/27129/e-k"
    },
    "is_answered": true,
    "view_count": 10675,
    "accepted_answer_id": 15766,
    "answer_count": 1,
    "score": 31,
    "last_activity_date": 1482360505,
    "creation_date": 1481840416,
    "last_edit_date": 1482360505,
    "question_id": 15765,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/15765/nlp-why-is-not-a-stop-word",
    "title": "NLP - why is &quot;not&quot; a stop word?",
    "body": "<p>I am trying to remove stop words before performing topic modeling. I noticed that some negation words (not, nor, never, none etc..) are usually considered to be stop words. For example, NLTK, spacy and sklearn include \"not\" on their stop word lists. However, if we remove \"not\" from these sentences below they lose the significant meaning and that would not be accurate for topic modeling or sentiment analysis.</p>\n\n<pre><code>1). StackOverflow is helpful      =&gt; StackOverflow helpful\n2). StackOverflow is not helpful  =&gt; StackOverflow helpful\n</code></pre>\n\n<p>Can anyone please explain why these negation words are typically considered to be stop words? </p>\n"
  },
  {
    "tags": [
      "neural-network",
      "regression",
      "tensorflow"
    ],
    "owner": {
      "account_id": 8430986,
      "reputation": 411,
      "user_id": 28868,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/56103e489c137d21bfaff80700c5e560?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "sjishan",
      "link": "https://datascience.stackexchange.com/users/28868/sjishan"
    },
    "is_answered": true,
    "view_count": 55091,
    "answer_count": 3,
    "score": 31,
    "last_activity_date": 1612844718,
    "creation_date": 1486768661,
    "last_edit_date": 1486845878,
    "question_id": 16890,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/16890/neural-network-for-multiple-output-regression",
    "title": "Neural Network for Multiple Output Regression",
    "body": "<p>I have a dataset containing 34 input columns and 8 output columns.</p>\n\n<p>One way to solve the problem is to take the 34 inputs and build individual regression model for each output column.</p>\n\n<p>I am wondering if this problem can be solved using just one model particularly using Neural Network.</p>\n\n<p>I have used Multilayer Perceptron but that needs multiple models just like linear regression. Can Sequence to Sequence be a viable option? </p>\n\n<p>I am using TensorFlow. I have code but I think it is more important to understand what I am missing out in terms of the multilayer perceptron theory. </p>\n\n<p>I understand that in MLP if you have one output node it will provide one output. If you have 10 output nodes then it is a multi class problem. You pick the class with the highest probability out of the 10 outputs. But in my case it is certain there will be 8 outputs for same input. </p>\n\n<p>Lets say, for a set of inputs you will get the 3D coordinate of something (X,Y,Z). Like, Inputs = {1,10,5,7} Output = {1,2,1}. So for the same input {1,10,5,7} I need to make models for X value Y value and Z. One solution is to have 3 different models using MLP. But I would like to see if I can have one model. So I thought about using seq2seq. Because the encoder takes a series of input and the decoder provides series of output. But it seems seq2seq in tensorflow cannot handle float values. I can be wrong about this though.</p>\n"
  },
  {
    "tags": [
      "pandas"
    ],
    "owner": {
      "account_id": 2984801,
      "reputation": 429,
      "user_id": 29911,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/dd3441d854cd126ca1fc6180a41ec93b?s=256&d=identicon&r=PG",
      "display_name": "KyL",
      "link": "https://datascience.stackexchange.com/users/29911/kyl"
    },
    "is_answered": true,
    "view_count": 184481,
    "accepted_answer_id": 17770,
    "answer_count": 6,
    "score": 31,
    "last_activity_date": 1628107445,
    "creation_date": 1490187472,
    "last_edit_date": 1490188244,
    "question_id": 17769,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/17769/how-to-fill-missing-value-based-on-other-columns-in-pandas-dataframe",
    "title": "How to fill missing value based on other columns in Pandas dataframe?",
    "body": "<p>Suppose I have a 5*3 data frame in which third column contains missing value</p>\n\n<pre><code>1 2 3\n4 5 NaN\n7 8 9\n3 2 NaN\n5 6 NaN\n</code></pre>\n\n<p>I hope to generate value for missing value based rule that first product second column </p>\n\n<pre><code>1 2 3\n4 5 20 &lt;--4*5\n7 8 9\n3 2 6 &lt;-- 3*2\n5 6 30 &lt;-- 5*6\n</code></pre>\n\n<p>How can I do it use data frame? Thanks.</p>\n\n<p>How to add condition to calculate missing value like this?</p>\n\n<p><code>if 1st % 2 == 0 then 3rd = 1st * 2nd\nelse 3rd = 1st + 2nd</code></p>\n\n<pre><code>1 2 3\n4 5 20 &lt;-- 4*5 because 4%2==0\n7 8 9\n3 2 5 &lt;-- 3+2 because 3%2==1\n5 6 11 &lt;-- 5+6 because 5%2==1\n</code></pre>\n"
  },
  {
    "tags": [
      "machine-learning",
      "accuracy"
    ],
    "owner": {
      "account_id": 1224386,
      "reputation": 505,
      "user_id": 31446,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/fdf7decdb6b660d658d766ce64691a2a?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user345394",
      "link": "https://datascience.stackexchange.com/users/31446/user345394"
    },
    "is_answered": true,
    "view_count": 26733,
    "accepted_answer_id": 18819,
    "answer_count": 1,
    "score": 31,
    "last_activity_date": 1541478723,
    "creation_date": 1494220404,
    "question_id": 18818,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/18818/what-is-a-lb-score-in-machine-learning",
    "title": "What is a LB score in machine learning?",
    "body": "<p>I was going through an <a href=\"http://blog.kaggle.com/2015/11/09/profiling-top-kagglers-gilberto-titericz-new-1-in-the-world/\" rel=\"noreferrer\">article</a> on kaggle blogs. Repeatedly, the author mentions 'LB score' and 'LB fit') as a metric for effectiveness of machine learning (along with cross validation (CV) score).</p>\n\n<p>With a research for the meaning of 'LB' I spent quite a bit of time, I realised that generally people directly refer it as LB without much background. </p>\n\n<p>So my question is - What is a 'LB'?</p>\n"
  },
  {
    "tags": [
      "python",
      "keras"
    ],
    "owner": {
      "account_id": 72038,
      "reputation": 2119,
      "user_id": 23240,
      "user_type": "registered",
      "accept_rate": 20,
      "profile_image": "https://i.sstatic.net/wqY8n.jpg?s=256",
      "display_name": "I_Play_With_Data",
      "link": "https://datascience.stackexchange.com/users/23240/i-play-with-data"
    },
    "is_answered": true,
    "view_count": 53247,
    "protected_date": 1674563290,
    "accepted_answer_id": 28211,
    "answer_count": 3,
    "score": 31,
    "last_activity_date": 1674563257,
    "creation_date": 1519335157,
    "question_id": 28210,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/28210/keras-callback-example-for-saving-a-model-after-every-epoch",
    "title": "Keras Callback example for saving a model after every epoch?",
    "body": "<p>Can someone please post a straightforward example of Keras using a callback to save a model after every epoch? I can find examples of saving weights, but I want to be able to save a completely functioning model after every training epoch. </p>\n"
  },
  {
    "tags": [
      "feature-engineering",
      "feature-scaling",
      "data-science-model"
    ],
    "owner": {
      "account_id": 10784747,
      "reputation": 588,
      "user_id": 41312,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/10154710300021379/picture?type=large",
      "display_name": "Suresh Kasipandy",
      "link": "https://datascience.stackexchange.com/users/41312/suresh-kasipandy"
    },
    "is_answered": true,
    "view_count": 19700,
    "accepted_answer_id": 31653,
    "answer_count": 1,
    "score": 31,
    "last_activity_date": 1526321489,
    "creation_date": 1526320498,
    "question_id": 31652,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/31652/should-one-hot-vectors-be-scaled-with-numerical-attributes",
    "title": "Should one hot vectors be scaled with numerical attributes",
    "body": "<p>In the case of having a combination of categorical and numerical Attributes, I usually convert the categorical attributes to one hot vectors. My question is do I leave those vectors as is and scale the numerical attributes through standardization/normalization, or should I scale the one hot vectors along with the numerical attributes?</p>\n"
  },
  {
    "tags": [
      "keras",
      "lstm"
    ],
    "owner": {
      "account_id": 4262249,
      "reputation": 1310,
      "user_id": 67743,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ecc8ba04bbf35d83c866bf36496084d5?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user3486308",
      "link": "https://datascience.stackexchange.com/users/67743/user3486308"
    },
    "is_answered": true,
    "view_count": 31288,
    "accepted_answer_id": 48814,
    "answer_count": 2,
    "score": 31,
    "last_activity_date": 1645852280,
    "creation_date": 1554624294,
    "question_id": 48796,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/48796/how-to-feed-lstm-with-different-input-array-sizes",
    "title": "How to feed LSTM with different input array sizes?",
    "body": "<p>If I like to write a <code>LSTM</code> network and feed it by different input array sizes, how is it possible?</p>\n\n<p>For example I want to get voice messages or text messages in a different language and translate them. So the first input maybe is \"hello\" but the second is \"how are you doing\". How can I design a <code>LSTM</code> that can handle different input array sizes?</p>\n\n<p>I am using <code>Keras</code> implementation of <code>LSTM</code>.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning"
    ],
    "owner": {
      "account_id": 16422642,
      "reputation": 319,
      "user_id": 78739,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/1642b647158c36b6609e092d650f00e5?s=256&d=identicon&r=PG",
      "display_name": "user78739",
      "link": "https://datascience.stackexchange.com/users/78739/user78739"
    },
    "is_answered": true,
    "view_count": 20369,
    "answer_count": 7,
    "score": 31,
    "last_activity_date": 1655644277,
    "creation_date": 1564571176,
    "last_edit_date": 1564572509,
    "question_id": 56676,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/56676/can-machine-learning-learn-a-function-like-finding-maximum-from-a-list",
    "title": "Can machine learning learn a function like finding maximum from a list?",
    "body": "<p>I have an input which is a list and the output is the maximum of the elements of the input-list.</p>\n\n<p>Can machine learning learn such a function which always selects the maximum of the input-elements present in the input?</p>\n\n<p>This might seem as a pretty basic question but it might give me an understanding of what machine learning can do in general. Thanks!</p>\n"
  },
  {
    "tags": [
      "classification",
      "accuracy",
      "class-imbalance"
    ],
    "owner": {
      "account_id": 14253681,
      "reputation": 1896,
      "user_id": 58433,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d3ab9372a78ad4ad2cea517958df4638?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user10296606",
      "link": "https://datascience.stackexchange.com/users/58433/user10296606"
    },
    "is_answered": true,
    "view_count": 67771,
    "accepted_answer_id": 65846,
    "answer_count": 4,
    "score": 31,
    "last_activity_date": 1724360247,
    "creation_date": 1578134314,
    "last_edit_date": 1578134716,
    "question_id": 65839,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/65839/macro-average-and-weighted-average-meaning-in-classification-report",
    "title": "macro average and weighted average meaning in classification_report",
    "body": "<p>I use the \"<strong>classification_report</strong>\" from <code>from sklearn.metrics import classification_report</code> in order to evaluate the imbalanced binary classification</p>\n\n<pre><code>Classification Report :\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     28432\n           1       0.02      0.02      0.02        49\n\n    accuracy                           1.00     28481\n   macro avg       0.51      0.51      0.51     28481\nweighted avg       1.00      1.00      1.00     28481\n</code></pre>\n\n<p>I do not understand clearly what is the meaning of macro avg and weighted average? and how we can clarify the best solution based on how close their amount to one!</p>\n\n<p>I have read it:\nmacro average (averaging the unweighted mean per label), weighted average (averaging the support-weighted mean per label)</p>\n\n<p>but I still have a problem in understanding how good is result based on how close these amount to 1? How I can explain it?</p>\n"
  },
  {
    "tags": [
      "classification",
      "text-mining",
      "topic-model"
    ],
    "owner": {
      "account_id": 4877016,
      "reputation": 361,
      "user_id": 2916,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/fd6693c2154fc4d39e94a471e4ec7447?s=256&d=identicon&r=PG",
      "display_name": "Ali",
      "link": "https://datascience.stackexchange.com/users/2916/ali"
    },
    "is_answered": true,
    "view_count": 24768,
    "accepted_answer_id": 966,
    "answer_count": 3,
    "score": 30,
    "last_activity_date": 1612834431,
    "creation_date": 1407815452,
    "last_edit_date": 1612834431,
    "question_id": 962,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/962/what-is-difference-between-text-classification-and-topic-models",
    "title": "What is difference between text classification and topic models?",
    "body": "<p>I know the difference between clustering and classification in machine learning, but I don't understand the difference between text classification and topic modeling for documents. Can I use topic modeling over documents to identify a topic? Can I use classification methods to classify the text inside these documents?  </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "dimensionality-reduction",
      "visualization"
    ],
    "owner": {
      "account_id": 2292094,
      "reputation": 685,
      "user_id": 8774,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7d6cff955bb3f1865a419650bfe1372c?s=256&d=identicon&r=PG",
      "display_name": "hlin117",
      "link": "https://datascience.stackexchange.com/users/8774/hlin117"
    },
    "is_answered": true,
    "view_count": 3818,
    "accepted_answer_id": 9157,
    "answer_count": 8,
    "score": 30,
    "last_activity_date": 1597949345,
    "creation_date": 1448512097,
    "question_id": 9038,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9038/purpose-of-visualizing-high-dimensional-data",
    "title": "Purpose of visualizing high dimensional data?",
    "body": "<p>There are many techniques for visualizing high dimension datasets, such as T-SNE, isomap, PCA, supervised PCA, etc. And we go through the motions of projecting the data down to a 2D or 3D space, so we have a \"pretty pictures\". Some of these embedding (manifold learning) methods are described <a href=\"http://scikit-learn.org/stable/modules/manifold.html\" rel=\"noreferrer\">here</a>.</p>\n\n<p><a href=\"https://i.sstatic.net/H3FBv.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/H3FBv.png\" alt=\"enter image description here\"></a></p>\n\n<p><strong>But is this \"pretty picture\" actually meaningful? What possible insights can someone grab by trying to visualize this embedded space?</strong></p>\n\n<p>I ask because the projection down to this embedded space is usually meaningless. For example, if you project your data down to principal components generated by PCA, those principal components (eiganvectors) don't correspond to features in the dataset; they're their own feature space. </p>\n\n<p>Similarly, t-SNE projects your data down to a space, where items are near each other if they minimize some KL divergence. This isn't the original feature space anymore. (Correct me if I'm wrong, but I don't even think there is a large effort by the ML community to use t-SNE to aid classification; that's a different problem than data visualization though.)</p>\n\n<p>I'm just very largely confused why people make such a big deal about some of these visualizations. </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "reinforcement-learning"
    ],
    "owner": {
      "account_id": 266740,
      "reputation": 4199,
      "user_id": 12515,
      "user_type": "registered",
      "accept_rate": 37,
      "profile_image": "https://i.sstatic.net/G0s1w.jpg?s=256",
      "display_name": "Ryan Zotti",
      "link": "https://datascience.stackexchange.com/users/12515/ryan-zotti"
    },
    "is_answered": true,
    "view_count": 21887,
    "accepted_answer_id": 11696,
    "answer_count": 7,
    "score": 30,
    "last_activity_date": 1658841128,
    "creation_date": 1459183225,
    "last_edit_date": 1597949872,
    "question_id": 10932,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/10932/difference-between-alphagos-policy-network-and-value-network",
    "title": "Difference between AlphaGo&#39;s policy network and value network",
    "body": "<p>I was reading a high level summary about Google's <a href=\"http://googleresearch.blogspot.co.uk/2016/01/alphago-mastering-ancient-game-of-go.html\" rel=\"noreferrer\">AlphaGo</a>, and I came across the terms &quot;policy network&quot; and &quot;value network&quot;. At a high level, I understand that the policy network is used to suggest moves and the value network is used to, &quot;Reduce the depth of the search tree [and estimate] the winner in each position in place of searching all the way to the end of the game.&quot;</p>\n<p>These two networks seem redundant to me. What is the policy network doing if it's not using the value network to prune its policies? It seems pretty clear that the value network is a deep learning neural network; is the policy network just a theoretical abstraction and not an actual neural network? The target variable for the value network seems to be win/loss. Is there a target variable for the policy network; if so, what is it? What is the policy network trying to optimize?</p>\n<p>The full pdf of Google's paper, published in Nature, can be found <a href=\"https://vk.com/doc-44016343_437229031?dl=56ce06e325d42fbc72\" rel=\"noreferrer\">here</a>.</p>\n"
  },
  {
    "tags": [
      "regression",
      "feature-extraction",
      "feature-engineering",
      "kaggle",
      "feature-scaling"
    ],
    "owner": {
      "account_id": 6576609,
      "reputation": 815,
      "user_id": 23644,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/F0tsiwRV.png?s=256",
      "display_name": "PixelPioneer",
      "link": "https://datascience.stackexchange.com/users/23644/pixelpioneer"
    },
    "is_answered": true,
    "view_count": 22060,
    "accepted_answer_id": 20428,
    "answer_count": 3,
    "score": 30,
    "last_activity_date": 1572344481,
    "creation_date": 1499427305,
    "last_edit_date": 1499428325,
    "question_id": 20237,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/20237/why-do-we-convert-skewed-data-into-a-normal-distribution",
    "title": "Why do we convert skewed data into a normal distribution",
    "body": "<p>I was going through a solution of the Housing prices competition on Kaggle (<a href=\"https://www.kaggle.com/humananalog/xgboost-lasso/code\" rel=\"noreferrer\">Human Analog's Kernel on House Prices: Advance Regression Techniques</a>) and came across this part:</p>\n\n<pre><code># Transform the skewed numeric features by taking log(feature + 1).\n# This will make the features more normal.\nfrom scipy.stats import skew\n\nskewed = train_df_munged[numeric_features].apply(lambda x: skew(x.dropna().astype(float)))\nskewed = skewed[skewed &gt; 0.75]\nskewed = skewed.index\n\ntrain_df_munged[skewed] = np.log1p(train_df_munged[skewed])\ntest_df_munged[skewed] = np.log1p(test_df_munged[skewed])\n</code></pre>\n\n<p>I am not sure of what is the need for converting a skewed distribution into a normal distribution. Please, can someone explain in detail: </p>\n\n<ol>\n<li>Why is this being done here? or How is this helpful?</li>\n<li>How is this different from feature-scaling?</li>\n<li>Is this a necessary step for feature-engineering? What is likely to happen if I skip this step?</li>\n</ol>\n"
  },
  {
    "tags": [
      "deep-learning",
      "cnn",
      "convolution",
      "convolutional-neural-network"
    ],
    "owner": {
      "account_id": 11265168,
      "reputation": 523,
      "user_id": 38047,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/85e7d296641b81b9c75afa7fe4e2ba84?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "JGG",
      "link": "https://datascience.stackexchange.com/users/38047/jgg"
    },
    "is_answered": true,
    "view_count": 29488,
    "accepted_answer_id": 38124,
    "answer_count": 1,
    "score": 30,
    "last_activity_date": 1613177756,
    "creation_date": 1536698990,
    "last_edit_date": 1613177756,
    "question_id": 38118,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/38118/what-is-the-difference-between-upsampling-and-bi-linear-upsampling-in-a-cnn",
    "title": "What is the difference between upsampling and bi-linear upsampling in a CNN?",
    "body": "<p>I am trying to understand <a href=\"https://arxiv.org/abs/1606.00915\" rel=\"noreferrer\">this paper</a> and am unsure of what bi-linear upsampling is. Can anyone explain this at a high-level?</p>\n"
  },
  {
    "tags": [
      "algorithms",
      "similarity",
      "ensemble-modeling",
      "boosting"
    ],
    "owner": {
      "account_id": 14303849,
      "reputation": 808,
      "user_id": 58789,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/08a959eac63c4b4bf6c038c3a706e841?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "CodeMaster GoGo",
      "link": "https://datascience.stackexchange.com/users/58789/codemaster-gogo"
    },
    "is_answered": true,
    "view_count": 39802,
    "accepted_answer_id": 39201,
    "answer_count": 1,
    "score": 30,
    "last_activity_date": 1704815495,
    "creation_date": 1538663148,
    "last_edit_date": 1615490286,
    "question_id": 39193,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/39193/adaboost-vs-gradient-boosting",
    "title": "Adaboost vs Gradient Boosting",
    "body": "<p>How is AdaBoost different from a Gradient Boosting algorithm since both of them use a Boosting technique?</p>\n<p>I could not figure out actual difference between these both algorithms from a theory point of view.</p>\n"
  },
  {
    "tags": [
      "classification",
      "metric",
      "binary"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user77241"
    },
    "is_answered": true,
    "view_count": 69390,
    "accepted_answer_id": 64443,
    "answer_count": 2,
    "score": 30,
    "last_activity_date": 1615493138,
    "creation_date": 1575847039,
    "last_edit_date": 1615493138,
    "question_id": 64441,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/64441/how-to-interpret-classification-report-of-scikit-learn",
    "title": "How to interpret classification report of scikit-learn?",
    "body": "<p><a href=\"https://i.sstatic.net/LIWH1.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/LIWH1.jpg\" alt=\"enter image description here\" /></a></p>\n<p>As you can see, it is about a binary classification with linearSVC. The class 1 has a higher precision than class 0 (+7%), but class 0 has a higher recall than class 1 (+11%). How would you interpret this?</p>\n<p>And two other questions: what does &quot;support&quot; stand for? The precision and recall scores in the classification report are different compared to the results of <code>sklearn.metrics.precision_score</code> or <code>recall_score</code>. Why is that so?</p>\n"
  },
  {
    "tags": [
      "statistics",
      "reference-request"
    ],
    "owner": {
      "account_id": 3511862,
      "reputation": 399,
      "user_id": 663,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/14mj8.jpg?s=256",
      "display_name": "Anton",
      "link": "https://datascience.stackexchange.com/users/663/anton"
    },
    "is_answered": true,
    "view_count": 3554,
    "closed_date": 1403748060,
    "answer_count": 4,
    "score": 29,
    "last_activity_date": 1456027358,
    "creation_date": 1402493315,
    "last_edit_date": 1456027358,
    "question_id": 313,
    "link": "https://datascience.stackexchange.com/questions/313/books-about-the-science-in-data-science",
    "closed_reason": "Needs more focus",
    "title": "Books about the &quot;Science&quot; in Data Science?",
    "body": "<p>What are the books about the science and mathematics behind data science? It feels like so many \"data science\" books are programming tutorials and don't touch things like data generating processes and statistical inference. I can already code, what I am weak on is the math/stats/theory behind what I am doing.</p>\n\n<p>If I am ready to burn $1000 on books (so around 10 books... sigh), what could I buy?</p>\n\n<p>Examples: Agresti's <a href=\"http://rads.stackoverflow.com/amzn/click/0470463635\">Categorical Data Analysis</a>, <a href=\"http://rads.stackoverflow.com/amzn/click/1441902996\">Linear Mixed Models for Longitudinal Data</a>, etc... etc...</p>\n"
  },
  {
    "tags": [
      "r",
      "statistics"
    ],
    "owner": {
      "account_id": 5170437,
      "reputation": 291,
      "user_id": 4637,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/962f91b3f854177663f211c9321a1de4?s=256&d=identicon&r=PG",
      "display_name": "Gotham",
      "link": "https://datascience.stackexchange.com/users/4637/gotham"
    },
    "is_answered": true,
    "view_count": 28427,
    "protected_date": 1582645590,
    "answer_count": 10,
    "score": 29,
    "last_activity_date": 1612834402,
    "creation_date": 1413234828,
    "question_id": 2269,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/2269/any-online-r-console",
    "title": "Any Online R console?",
    "body": "<p>I am looking for an online console for the language R. Like I write the code and the server should execute and provide me with the output.</p>\n\n<p>Similar to the website Datacamp.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning"
    ],
    "owner": {
      "account_id": 2669509,
      "reputation": 433,
      "user_id": 5342,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7ef0b4c0bf0742666ceb70957e20bc8e?s=256&d=identicon&r=PG",
      "display_name": "Maxi",
      "link": "https://datascience.stackexchange.com/users/5342/maxi"
    },
    "is_answered": true,
    "view_count": 10384,
    "protected_date": 1496225562,
    "answer_count": 6,
    "score": 29,
    "last_activity_date": 1597949966,
    "creation_date": 1418078252,
    "question_id": 2651,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/2651/deep-learning-basics",
    "title": "Deep learning basics",
    "body": "<p>I am looking for a paper detailing the very basics of deep learning. Ideally like the Andrew Ng course for deep learning. Do you know where I can find this ?</p>\n"
  },
  {
    "tags": [
      "python",
      "bigdata",
      "nlp",
      "scikit-learn",
      "dimensionality-reduction"
    ],
    "owner": {
      "account_id": 4817698,
      "reputation": 1974,
      "user_id": 16024,
      "user_type": "registered",
      "accept_rate": 60,
      "profile_image": "https://www.gravatar.com/avatar/eb2998bb8ee8b3572618c558e57e3d1f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "chmodsss",
      "link": "https://datascience.stackexchange.com/users/16024/chmodsss"
    },
    "is_answered": true,
    "view_count": 23774,
    "accepted_answer_id": 28772,
    "answer_count": 5,
    "score": 29,
    "last_activity_date": 1596718854,
    "creation_date": 1454768350,
    "last_edit_date": 1454768603,
    "question_id": 10103,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10103/improve-the-speed-of-t-sne-implementation-in-python-for-huge-data",
    "title": "Improve the speed of t-sne implementation in python for huge data",
    "body": "<p>I would like to do dimensionality reduction on nearly 1 million vectors each with 200 dimensions(<code>doc2vec</code>). \nI am using <code>TSNE</code> implementation from <code>sklearn.manifold</code> module for it and the major problem is time complexity. Even with <code>method = barnes_hut</code>, the speed of computation is still low. Some time even it runs out of Memory.</p>\n\n<p>I am running it on a 48 core processor with 130G RAM. Is there a method to run it parallely or make use of the plentiful resource to speed up the process. </p>\n"
  },
  {
    "tags": [
      "feature-selection",
      "model-selection"
    ],
    "owner": {
      "account_id": 6860839,
      "reputation": 513,
      "user_id": 14070,
      "user_type": "registered",
      "accept_rate": 80,
      "profile_image": "https://www.gravatar.com/avatar/f9ebbbcd8a45524bd6b31f961cb3b516?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Arnold Klein",
      "link": "https://datascience.stackexchange.com/users/14070/arnold-klein"
    },
    "is_answered": true,
    "view_count": 56343,
    "accepted_answer_id": 11480,
    "answer_count": 4,
    "score": 29,
    "last_activity_date": 1615490606,
    "creation_date": 1461480937,
    "last_edit_date": 1498393741,
    "question_id": 11390,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/11390/any-rules-of-thumb-on-number-of-features-versus-number-of-instances-small-da",
    "title": "Any &quot;rules of thumb&quot; on number of features versus number of instances? (small data sets)",
    "body": "<p>I am wondering, if there are any heuristics on number of features versus number of observations. Obviously, if a number of features is equal to the number of observations, the model will overfit. By using sparse methods (LASSO, elastic net) we can remove several features to reduce the model. </p>\n\n<p>My question is (theoretically): <strong>before</strong> we use metrics to assess the model selection are there any empirical observations which relate the <strong>optimal</strong> number of features to the number of observations? </p>\n\n<p>For example: for a binary classification problem with 20 instances in each class, is there any upper limit on the number of features to use?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-mining",
      "clustering",
      "nlp",
      "unsupervised-learning"
    ],
    "owner": {
      "account_id": 11161827,
      "reputation": 529,
      "user_id": 33838,
      "user_type": "registered",
      "accept_rate": 78,
      "profile_image": "https://www.gravatar.com/avatar/73494c73d50aad8724b75f1a3af7f31d?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Smith",
      "link": "https://datascience.stackexchange.com/users/33838/smith"
    },
    "is_answered": true,
    "view_count": 37957,
    "accepted_answer_id": 20078,
    "answer_count": 1,
    "score": 29,
    "last_activity_date": 1626751535,
    "creation_date": 1498806333,
    "question_id": 20076,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/20076/word2vec-vs-sentence2vec-vs-doc2vec",
    "title": "Word2Vec vs. Sentence2Vec vs. Doc2Vec",
    "body": "<p>I recently came across the terms <strong>Word2Vec</strong>, <strong>Sentence2Vec</strong> and <strong>Doc2Vec</strong> and kind of confused as I am new to vector semantics. Can someone please elaborate the differences in these methods in simple words. What are the most suitable tasks for each method?</p>\n"
  },
  {
    "tags": [
      "python",
      "neural-network",
      "backpropagation"
    ],
    "owner": {
      "account_id": 10380185,
      "reputation": 425,
      "user_id": 34042,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/015d516ef804cdd3ca93ef1a68cdcd63?s=256&d=identicon&r=PG",
      "display_name": "user34042",
      "link": "https://datascience.stackexchange.com/users/34042/user34042"
    },
    "is_answered": true,
    "view_count": 46681,
    "accepted_answer_id": 20142,
    "answer_count": 4,
    "score": 29,
    "last_activity_date": 1646242996,
    "creation_date": 1499101404,
    "last_edit_date": 1499109695,
    "question_id": 20139,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/20139/gradients-for-bias-terms-in-backpropagation",
    "title": "Gradients for bias terms in backpropagation",
    "body": "<p>I was trying to implement neural network from scratch to understand the maths behind it. My problem is completely related to backpropagation  when we take derivative with respect to bias) and I derived all the equations used in backpropagation. Now every equation is matching with the code for neural network except for that the derivative with respect to biases.</p>\n\n<pre><code>z1=x.dot(theta1)+b1\n\nh1=1/(1+np.exp(-z1))\nz2=h1.dot(theta2)+b2\nh2=1/(1+np.exp(-z2))\n\ndh2=h2-y\n#back prop\n\ndz2=dh2*(1-dh2)\nH1=np.transpose(h1)\ndw2=np.dot(H1,dz2)\ndb2=np.sum(dz2,axis=0,keepdims=True)\n</code></pre>\n\n<p>I looked up online for the code, and i want to know \nwhy do we add up the matrix and then the scalar <code>db2=np.sum(dz2,axis=0,keepdims=True)</code> is subtracted from the original bias, why not the matrix as a whole is subtracted. Can anyone help me to give some intuion behind it. If i take partial derivative of loss with respect to bias it will give me upper gradient only which is dz2 because  <code>z2=h1.dot(theta2)+b2</code> h1 and theta will be 0 and b2 will be 1. So the upper term will be left.</p>\n\n<pre><code>b2+=-alpha*db2\n</code></pre>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-mining",
      "text-mining",
      "distance"
    ],
    "owner": {
      "account_id": 11616669,
      "reputation": 685,
      "user_id": 38395,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/ac277f6f12e623b0a942cf11d31235ff?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Smith Volka",
      "link": "https://datascience.stackexchange.com/users/38395/smith-volka"
    },
    "is_answered": true,
    "view_count": 21108,
    "answer_count": 1,
    "score": 29,
    "last_activity_date": 1523848842,
    "creation_date": 1504145498,
    "last_edit_date": 1504148800,
    "question_id": 22725,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/22725/what-is-hellinger-distance-and-when-to-use-it",
    "title": "What is Hellinger Distance and when to use it?",
    "body": "<p>I am interested in knowing what really happens in  Hellinger Distance (in simple terms). Furthermore, I am also interested in knowing what are types of problems that we can use  Hellinger Distance? What are the benefits of using  Hellinger Distance?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "computer-vision",
      "convolutional-neural-network",
      "convolution"
    ],
    "owner": {
      "account_id": 8613515,
      "reputation": 610,
      "user_id": 39198,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-YX-fKMoU-ps/AAAAAAAAAAI/AAAAAAAAACE/qDO4Xj_LSFk/s256-rj/photo.jpg",
      "display_name": "Jonathan DEKHTIAR",
      "link": "https://datascience.stackexchange.com/users/39198/jonathan-dekhtiar"
    },
    "is_answered": true,
    "view_count": 33659,
    "accepted_answer_id": 23186,
    "answer_count": 3,
    "score": 29,
    "last_activity_date": 1644699943,
    "creation_date": 1505930038,
    "last_edit_date": 1644699943,
    "question_id": 23183,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/23183/why-convolutions-always-use-odd-numbers-as-filter-size",
    "title": "Why convolutions always use odd-numbers as filter size",
    "body": "<p>If we have a look to 90-99% of the papers published using a CNN (ConvNet).\nThe vast majority of them use filter size of <strong>odd numbers</strong>:{1, 3, 5, 7} for the most used.</p>\n\n<p>This situation can lead to some problem: With these filter sizes, usually the convolution operation is not perfect with a padding of 2 (common padding) and some edges of the input_field get lost in the process...</p>\n\n<p><strong>Question1:</strong> Why using only odd_numbers for convolutions filter sizes ?</p>\n\n<p><strong>Question2:</strong> Is it actually a problem to omit a small part of the input_field during the convolution ? Why so/not ?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "keras"
    ],
    "owner": {
      "account_id": 12444982,
      "reputation": 403,
      "user_id": 42945,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d9893bdf312ee773e00fba1995def22b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "P.Joseph",
      "link": "https://datascience.stackexchange.com/users/42945/p-joseph"
    },
    "is_answered": true,
    "view_count": 18582,
    "answer_count": 2,
    "score": 29,
    "last_activity_date": 1615490350,
    "creation_date": 1516377228,
    "last_edit_date": 1615490350,
    "question_id": 26833,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/26833/is-there-away-to-change-the-metric-used-by-the-early-stopping-callback-in-keras",
    "title": "Is there away to change the metric used by the Early Stopping callback in Keras?",
    "body": "<p>When using the early stopping callback in Keras, training stops when some metric (usually validation loss) is not increasing. Is there a way to use another metric (like precision, recall, or f-measure) instead of validation loss?</p>\n<p>All the examples I have seen so far are similar to this one:</p>\n<pre><code>callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n</code></pre>\n"
  },
  {
    "tags": [
      "neural-network",
      "feature-selection",
      "categorical-data",
      "feature-construction"
    ],
    "owner": {
      "account_id": 11752789,
      "reputation": 525,
      "user_id": 49119,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/76296d3ef48e57bfae96a883ecf4bca0?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "JunjieChen",
      "link": "https://datascience.stackexchange.com/users/49119/junjiechen"
    },
    "is_answered": true,
    "view_count": 15178,
    "answer_count": 3,
    "score": 29,
    "last_activity_date": 1704884169,
    "creation_date": 1522226944,
    "question_id": 29634,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/29634/how-to-combine-categorical-and-continuous-input-features-for-neural-network-trai",
    "title": "How to combine categorical and continuous input features for neural network training",
    "body": "<p>Suppose we have two kinds of input features, categorical and continuous. The categorical data may be represented as one-hot code A, while the continuous data is just a vector B in N-dimension space. It seems that simply using concat(A, B) is not a good choice because A, B are totally different kinds of data. For example, unlike B, there is no numerical order in A. So my question is how to combine such two kinds of data or is there any conventional method to handle them.</p>\n\n<p>In fact, I propose a naive structure as presented in the picture</p>\n\n<p><a href=\"https://i.sstatic.net/QgQFq.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/QgQFq.png\" alt=\"enter image description here\"></a></p>\n\n<p>As you see, the first few layers are used to change(or map) data A to some middle output in continuous space and it is then being concated with data B which forms a new input feature in continuous space for later layers. I wonder whether it is reasonable or it is just a \"trial and error\" game. Thank you.</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "keras"
    ],
    "owner": {
      "account_id": 8453205,
      "reputation": 2015,
      "user_id": 51129,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/29013c1e5a2906d8f3d08f27c953095e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "N.IT",
      "link": "https://datascience.stackexchange.com/users/51129/n-it"
    },
    "is_answered": true,
    "view_count": 30194,
    "accepted_answer_id": 34452,
    "answer_count": 2,
    "score": 29,
    "last_activity_date": 1561824641,
    "creation_date": 1531513301,
    "last_edit_date": 1531574841,
    "question_id": 34444,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/34444/what-is-the-difference-between-fit-and-fit-generator-in-keras",
    "title": "What is the difference between fit() and fit_generator() in Keras?",
    "body": "<p>What is the difference between <a href=\"https://keras.io/models/sequential/#fit\" rel=\"noreferrer\"><code>fit()</code></a> and <a href=\"https://keras.io/models/sequential/#fit_generator\" rel=\"noreferrer\"><code>fit_generator()</code></a> in Keras?</p>\n\n<p>When should I use <code>fit()</code> vs <code>fit_generator()</code>?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "loss-function"
    ],
    "owner": {
      "account_id": 12190501,
      "reputation": 871,
      "user_id": 41591,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/82CEl.jpg?s=256",
      "display_name": "rawwar",
      "link": "https://datascience.stackexchange.com/users/41591/rawwar"
    },
    "is_answered": true,
    "view_count": 38869,
    "accepted_answer_id": 40719,
    "answer_count": 2,
    "score": 29,
    "last_activity_date": 1647367138,
    "creation_date": 1541356168,
    "last_edit_date": 1647367138,
    "question_id": 40714,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/40714/what-is-the-advantage-of-using-log-softmax-instead-of-softmax",
    "title": "What is the advantage of using log softmax instead of softmax?",
    "body": "<p>Are there any advantages to using log softmax over softmax? What are the reasons to choose one over the other?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "cnn"
    ],
    "owner": {
      "account_id": 8523933,
      "reputation": 941,
      "user_id": 84229,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/51532a2365b31bc2784ff08a35b941de?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "J.D.",
      "link": "https://datascience.stackexchange.com/users/84229/j-d"
    },
    "is_answered": true,
    "view_count": 84814,
    "accepted_answer_id": 64281,
    "answer_count": 4,
    "score": 29,
    "last_activity_date": 1660740805,
    "creation_date": 1575558855,
    "last_edit_date": 1637158337,
    "question_id": 64278,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/64278/what-is-a-channel-in-a-cnn",
    "title": "What is a channel in a CNN?",
    "body": "<p>I was reading an article about convolutional neural networks, and I found something that I don't understand, which is:</p>\n<blockquote>\n<p>The filter must have the same number of channels as the input image so that the element-wise multiplication can take place.</p>\n</blockquote>\n<p>Now, what I don't understand is: What is a channel in a convolutional neural network? I have tried looking for the answer, but can't understand what is it yet.</p>\n<p>Can someone explain it to me?</p>\n<p>Thanks in advance.</p>\n"
  },
  {
    "tags": [
      "nlp",
      "bert",
      "machine-translation",
      "attention-mechanism"
    ],
    "owner": {
      "account_id": 14567810,
      "reputation": 445,
      "user_id": 74045,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-2alZanoSFtE/AAAAAAAAAAI/AAAAAAAAAAA/AAN31DWsXdIQjoaAiaXshKgm-3p5cGDZdA/mo/s256-rj/photo.jpg",
      "display_name": "hathalye7",
      "link": "https://datascience.stackexchange.com/users/74045/hathalye7"
    },
    "is_answered": true,
    "view_count": 27808,
    "accepted_answer_id": 65242,
    "answer_count": 7,
    "score": 29,
    "last_activity_date": 1683700748,
    "creation_date": 1576948147,
    "last_edit_date": 1675704202,
    "question_id": 65241,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/65241/why-is-the-decoder-not-a-part-of-bert-architecture",
    "title": "Why is the decoder not a part of BERT architecture?",
    "body": "<p>I can't see how BERT makes predictions without using a decoder unit, which was a part of all models before it including transformers and standard RNNs. How are output predictions made in the BERT architecture without using a decoder? How does it do away with decoders completely?</p>\n<p>To put the question another way: what decoder can I use, along with BERT, to generate output text? If BERT only encodes, what library/tool can I use to decode from the embeddings?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "dimensionality-reduction",
      "python"
    ],
    "owner": {
      "account_id": 1503498,
      "reputation": 916,
      "user_id": 173,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Z4uAf.jpg?s=256",
      "display_name": "Wojciech Walczak",
      "link": "https://datascience.stackexchange.com/users/173/wojciech-walczak"
    },
    "is_answered": true,
    "view_count": 3364,
    "accepted_answer_id": 121,
    "answer_count": 6,
    "score": 28,
    "last_activity_date": 1612845068,
    "creation_date": 1400318178,
    "last_edit_date": 1400354813,
    "question_id": 116,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/116/machine-learning-techniques-for-estimating-users-age-based-on-facebook-sites-th",
    "title": "Machine learning techniques for estimating users&#39; age based on Facebook sites they like",
    "body": "<p>I have a database from my Facebook application and I am trying to use machine learning to estimate users' age based on what Facebook sites they like.</p>\n\n<p>There are three crucial characteristics of my database:</p>\n\n<ul>\n<li><p>the age distribution in my training set (12k of users in sum) is skewed towards younger users (i.e. I have 1157 users aged 27, and 23 users aged 65);</p></li>\n<li><p>many sites have no more than 5 likers (I filtered out the FB sites with less than 5 likers).</p></li>\n<li><p>there's many more features than samples.</p></li>\n</ul>\n\n<p>So, my questions are: what strategy would you suggest to prepare the data for further analysis? Should I perform some sort of dimensionality reduction? Which ML method would be most appropriate to use in this case?</p>\n\n<p>I mainly use Python, so Python-specific hints would be greatly appreciated.</p>\n"
  },
  {
    "tags": [
      "open-source",
      "dataset",
      "crawling"
    ],
    "owner": {
      "account_id": 1822136,
      "reputation": 4117,
      "user_id": 84,
      "user_type": "registered",
      "accept_rate": 82,
      "profile_image": "https://www.gravatar.com/avatar/5394d14f632e89b2dfc937e3660f0079?s=256&d=identicon&r=PG",
      "display_name": "Rubens",
      "link": "https://datascience.stackexchange.com/users/84/rubens"
    },
    "is_answered": true,
    "view_count": 27641,
    "accepted_answer_id": 510,
    "answer_count": 7,
    "score": 28,
    "last_activity_date": 1612844840,
    "creation_date": 1402982951,
    "last_edit_date": 1492087841,
    "question_id": 422,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/422/publicly-available-social-network-datasets-apis",
    "title": "Publicly available social network datasets/APIs",
    "body": "<p>As an extension to our great list of <a href=\"https://datascience.stackexchange.com/questions/155/publicly-available-datasets\">publicly available datasets</a>, I'd like to know if there is any list of publicly available social network datasets/crawling APIs. It would be very nice if alongside with a link to the dataset/API, characteristics of the data available were added. Such information should be, and is not limited to:</p>\n\n<ul>\n<li>the name of the social network;</li>\n<li>what kind of user information it provides (posts, profile, friendship network, ...);</li>\n<li>whether it allows for crawling its contents via an API (and rate: 10/min, 1k/month, ...);</li>\n<li>whether it simply provides a snapshot of the whole dataset.</li>\n</ul>\n\n<p>Any suggestions and further characteristics to be added are very welcome.</p>\n"
  },
  {
    "tags": [
      "visualization",
      "graphs"
    ],
    "owner": {
      "account_id": 2478768,
      "reputation": 453,
      "user_id": 192,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/14fe6116fcafe955ec373767be9b0921?s=256&d=identicon&r=PG",
      "display_name": "Cici",
      "link": "https://datascience.stackexchange.com/users/192/cici"
    },
    "is_answered": true,
    "view_count": 28299,
    "accepted_answer_id": 814,
    "answer_count": 8,
    "score": 28,
    "last_activity_date": 1633612591,
    "creation_date": 1406042224,
    "last_edit_date": 1406054084,
    "question_id": 812,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/812/visualizing-a-graph-with-a-million-vertices",
    "title": "Visualizing a graph with a million vertices",
    "body": "<p>What is the best tool to use to visualize (draw the vertices and edges) a graph with 1000000 vertices? There are about 50000 edges in the graph. And I can compute the location of individual vertices and edges.</p>\n\n<p>I am thinking about writing a program to generate a svg. Any other suggestions?  </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "bigdata",
      "dataset"
    ],
    "owner": {
      "account_id": 3527213,
      "reputation": 383,
      "user_id": 2725,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/634383074/picture?type=large",
      "display_name": "Kevin Desai",
      "link": "https://datascience.stackexchange.com/users/2725/kevin-desai"
    },
    "is_answered": true,
    "view_count": 42951,
    "closed_date": 1406387396,
    "accepted_answer_id": 843,
    "answer_count": 3,
    "score": 28,
    "last_activity_date": 1597949832,
    "creation_date": 1406313391,
    "last_edit_date": 1406432106,
    "question_id": 842,
    "link": "https://datascience.stackexchange.com/questions/842/data-science-project-ideas",
    "closed_reason": "Opinion-based",
    "title": "Data Science Project Ideas",
    "body": "<p>I don't know if this is a right place to ask this question, but a community dedicated to Data Science should be the most appropriate place in my opinion.</p>\n\n<p>I have just started with Data Science and Machine learning. I am looking for long term project ideas which I can work on for like 8 months.</p>\n\n<p>A mix of Data Science and Machine learning would be great.</p>\n\n<p>A project big enough to help me understand the core concepts and also implement them at the same time would be very beneficial.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-mining",
      "neural-network",
      "nlp",
      "deep-learning"
    ],
    "owner": {
      "account_id": 4082491,
      "reputation": 449,
      "user_id": 3615,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4480675ad0ede0043e7caba04750d974?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user3352632",
      "link": "https://datascience.stackexchange.com/users/3615/user3352632"
    },
    "is_answered": true,
    "view_count": 2269,
    "answer_count": 3,
    "score": 28,
    "last_activity_date": 1596617298,
    "creation_date": 1413023041,
    "last_edit_date": 1447256757,
    "question_id": 1253,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/1253/why-are-nlp-and-machine-learning-communities-interested-in-deep-learning",
    "title": "Why are NLP and Machine Learning communities interested in deep learning?",
    "body": "<p>I hope you can help me, as I have some questions on this topic. I'm new in the field of deep learning, and while I did some tutorials, I can't relate or distinguish concepts from one another.</p>\n"
  },
  {
    "tags": [
      "python",
      "r",
      "tools"
    ],
    "owner": {
      "account_id": 5006988,
      "reputation": 431,
      "user_id": 7923,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s256-rj/photo.jpg",
      "display_name": "JeanVuda",
      "link": "https://datascience.stackexchange.com/users/7923/jeanvuda"
    },
    "is_answered": true,
    "view_count": 3316,
    "answer_count": 5,
    "score": 28,
    "last_activity_date": 1612844920,
    "creation_date": 1421962497,
    "last_edit_date": 1493055467,
    "question_id": 4925,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/4925/vm-image-for-data-science-projects",
    "title": "VM image for data science projects",
    "body": "<p>As there are numerous tools available for data science tasks, and it's cumbersome to install everything and build up a perfect system.</p>\n\n<p>Is there a Linux/Mac OS image with Python, R and other open-source data science tools installed and available for people to use right away? An Ubuntu or a light weight OS with latest version of Python, R (including IDEs), and other open source data visualization tools installed will be ideal. I haven't come across one in my quick search on Google.</p>\n\n<p>Please let me know if there are any or if someone of you have created one for yourself? I assume some universities might have their own VM images. Please share such links.</p>\n"
  },
  {
    "tags": [
      "python",
      "logistic-regression",
      "scikit-learn",
      "gradient-descent"
    ],
    "owner": {
      "account_id": 2292094,
      "reputation": 685,
      "user_id": 8774,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7d6cff955bb3f1865a419650bfe1372c?s=256&d=identicon&r=PG",
      "display_name": "hlin117",
      "link": "https://datascience.stackexchange.com/users/8774/hlin117"
    },
    "is_answered": true,
    "view_count": 46523,
    "accepted_answer_id": 9794,
    "answer_count": 4,
    "score": 28,
    "last_activity_date": 1518764519,
    "creation_date": 1438675890,
    "question_id": 6676,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/6676/scikit-learn-getting-sgdclassifier-to-predict-as-well-as-a-logistic-regression",
    "title": "Scikit-learn: Getting SGDClassifier to predict as well as a Logistic Regression",
    "body": "<p>A way to train a Logistic Regression is by using stochastic gradient descent, which scikit-learn offers an interface to.</p>\n\n<p>What I would like to do is take a scikit-learn's <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\" rel=\"noreferrer\">SGDClassifier</a> and have it score the same as a Logistic Regression <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\" rel=\"noreferrer\">here</a>. However, I must be missing some machine learning enhancements, since my scores are not equivalent.</p>\n\n<p>This is my current code. What am I missing on the SGDClassifier which would have it produce the same results as a Logistic Regression?</p>\n\n<pre><code>from sklearn import datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import KFold\nfrom sklearn.metrics import accuracy_score\n\n# Note that the iris dataset is available in sklearn by default.\n# This data is also conveniently preprocessed.\niris = datasets.load_iris()\nX = iris[\"data\"]\nY = iris[\"target\"]\n\nnumFolds = 10\nkf = KFold(len(X), numFolds, shuffle=True)\n\n# These are \"Class objects\". For each Class, find the AUC through\n# 10 fold cross validation.\nModels = [LogisticRegression, SGDClassifier]\nparams = [{}, {\"loss\": \"log\", \"penalty\": \"l2\"}]\nfor param, Model in zip(params, Models):\n    total = 0\n    for train_indices, test_indices in kf:\n\n        train_X = X[train_indices, :]; train_Y = Y[train_indices]\n        test_X = X[test_indices, :]; test_Y = Y[test_indices]\n\n        reg = Model(**param)\n        reg.fit(train_X, train_Y)\n        predictions = reg.predict(test_X)\n        total += accuracy_score(test_Y, predictions)\n    accuracy = total / numFolds\n    print \"Accuracy score of {0}: {1}\".format(Model.__name__, accuracy)\n</code></pre>\n\n<p>My output:</p>\n\n<pre><code>Accuracy score of LogisticRegression: 0.946666666667\nAccuracy score of SGDClassifier: 0.76\n</code></pre>\n"
  },
  {
    "tags": [
      "databases",
      "tools"
    ],
    "owner": {
      "account_id": 6444670,
      "reputation": 8476,
      "user_id": 11097,
      "user_type": "registered",
      "accept_rate": 75,
      "profile_image": "https://i.sstatic.net/z0jss.png?s=256",
      "display_name": "Dawny33",
      "link": "https://datascience.stackexchange.com/users/11097/dawny33"
    },
    "is_answered": true,
    "view_count": 10927,
    "accepted_answer_id": 8252,
    "answer_count": 4,
    "score": 28,
    "last_activity_date": 1492024516,
    "creation_date": 1443609795,
    "last_edit_date": 1492024516,
    "question_id": 8244,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/8244/what-makes-columnar-databases-suitable-for-data-science",
    "title": "What makes columnar databases suitable for data science?",
    "body": "<p>What are some of the advantages of columnar data-stores which make them more suitable for data science and analytics?</p>\n"
  },
  {
    "tags": [
      "r",
      "data-cleaning"
    ],
    "owner": {
      "account_id": 4168802,
      "reputation": 942,
      "user_id": 3151,
      "user_type": "registered",
      "accept_rate": 56,
      "profile_image": "https://i.sstatic.net/wie3Z.jpg?s=256",
      "display_name": "Hamideh",
      "link": "https://datascience.stackexchange.com/users/3151/hamideh"
    },
    "is_answered": true,
    "view_count": 178921,
    "accepted_answer_id": 8924,
    "answer_count": 2,
    "score": 28,
    "last_activity_date": 1615489676,
    "creation_date": 1447937980,
    "last_edit_date": 1615489676,
    "question_id": 8922,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/8922/removing-strings-after-a-certain-character-in-a-given-text",
    "title": "Removing strings after a certain character in a given text",
    "body": "<p>I have a dataset like the one below. I would like to remove all characters after the character . How can I do that in R?</p>\n<pre><code>data_clean_phrase &lt;- c(&quot;Copyright  The Society of Geomagnetism and Earth&quot;, \n&quot; 2013 Chinese National Committee &quot;)\n\ndata_clean_df &lt;- as.data.frame(data_clean_phrase)\n</code></pre>\n"
  },
  {
    "tags": [
      "nlp",
      "predictive-modeling",
      "word-embeddings"
    ],
    "owner": {
      "account_id": 1317171,
      "reputation": 345,
      "user_id": 15402,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/P7uOS.jpg?s=256",
      "display_name": "DED",
      "link": "https://datascience.stackexchange.com/users/15402/ded"
    },
    "is_answered": true,
    "view_count": 24683,
    "accepted_answer_id": 10417,
    "answer_count": 2,
    "score": 28,
    "last_activity_date": 1612901859,
    "creation_date": 1452755625,
    "last_edit_date": 1612901859,
    "question_id": 9785,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/9785/predicting-a-word-using-word2vec-model",
    "title": "Predicting a word using Word2vec model",
    "body": "<p>Given a sentence:\n&quot;When I open the <strong>??</strong> door it starts heating automatically&quot;</p>\n<p>I would like to get the list of possible words in ?? with a probability.</p>\n<p>The basic concept used in word2vec model is to &quot;predict&quot; a word given surrounding context.</p>\n<p>Once the model is build, what is the right context vectors operation to perform my prediction task on new sentences?</p>\n<p>Is it simply a linear sum?</p>\n<pre><code>model.most_similar(positive=['When','I','open','the','door','it','starts' ,\n                   'heating','automatically'])\n</code></pre>\n"
  },
  {
    "tags": [
      "python",
      "tensorflow"
    ],
    "owner": {
      "account_id": 10031831,
      "reputation": 281,
      "user_id": 27938,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/eaca21aa34f8343362a4b21cabe05565?s=256&d=identicon&r=PG",
      "display_name": "striki70",
      "link": "https://datascience.stackexchange.com/users/27938/striki70"
    },
    "is_answered": true,
    "view_count": 19235,
    "answer_count": 5,
    "score": 28,
    "last_activity_date": 1603279955,
    "creation_date": 1484384376,
    "question_id": 16318,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/16318/what-is-the-benefit-of-splitting-tfrecord-file-into-shards",
    "title": "What is the benefit of splitting tfrecord file into shards?",
    "body": "<p>I'm working on speech recognition with Tensorflow and plan to train LSTM NN with massive waves dataset. Because of the performance gains, I plan to use tfrecords. There are several examples on internet (Inception for ex.) where tfrecords files are split into shards.\nMy question is: what is the benefit of having tfrecords file into shards? Is there any additional performance gain of this split?</p>\n"
  },
  {
    "tags": [
      "python",
      "deep-learning",
      "tensorflow",
      "pytorch"
    ],
    "owner": {
      "account_id": 343021,
      "reputation": 28133,
      "user_id": 14675,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/L5oHj.jpg?s=256",
      "display_name": "noe",
      "link": "https://datascience.stackexchange.com/users/14675/noe"
    },
    "is_answered": true,
    "view_count": 9874,
    "accepted_answer_id": 18073,
    "answer_count": 1,
    "score": 28,
    "last_activity_date": 1493815428,
    "creation_date": 1486549576,
    "last_edit_date": 1487849516,
    "question_id": 16835,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/16835/pytorch-vs-tensorflow-fold",
    "title": "PyTorch vs. Tensorflow Fold",
    "body": "<p>Both <a href=\"https://github.com/hughperkins/pytorch\" rel=\"noreferrer\">PyTorch</a> and <a href=\"https://github.com/tensorflow/fold\" rel=\"noreferrer\">Tensorflow Fold</a> are deep learning frameworks meant to deal with situations where the input data has non-uniform length or dimensions (that is, situations where dynamic graphs are useful or needed).</p>\n\n<p>I would like to know how they compare, in the sense of paradigms they rely on (e.g. dynamic batching) and their implications, things that can/cannot be implemented in each one, weaknesses/strengths, etc.</p>\n\n<p>I intend to use this info to choose one of them to start exploring dynamic computation graphs, but I have no specific task in mind.</p>\n\n<p>Note 1: other dynamic computation graph frameworks like <a href=\"https://github.com/clab/dynet\" rel=\"noreferrer\">DyNet</a> or <a href=\"https://github.com/pfnet/chainer\" rel=\"noreferrer\">Chainer</a> are also welcome in the comparison, but I'd like to focus on PyTorch and Tensorflow Fold because I think they are/will be the most used ones.</p>\n\n<p>Note 2: I have found <a href=\"https://news.ycombinator.com/item?id=13428098\" rel=\"noreferrer\">this hackernews thread on PyTorch</a> with some sparse info, but  not much.</p>\n\n<p>Note 3: Another relevant <a href=\"https://news.ycombinator.com/item?id=13591578\" rel=\"noreferrer\">hackernews thread</a>, about Tensorflow Fold, that contains some info about how they compare.</p>\n\n<p>Note 4: relevant <a href=\"https://www.reddit.com/r/MachineLearning/comments/5sn987/n_announcing_tensorflow_fold_deep_learning_with/\" rel=\"noreferrer\">Reddit thread</a>.</p>\n\n<p>Note 5: <a href=\"https://github.com/tensorflow/fold/issues/8\" rel=\"noreferrer\">relevant bug in Tensorflow Fold's github</a> that identifies an important limitation: impossibility to do conditional branching during evaluation.</p>\n\n<p>Note 6: <a href=\"https://discuss.pytorch.org/t/about-the-variable-length-input-in-rnn-scenario/345\" rel=\"noreferrer\">discussion on pytorch forum</a> about variable length inputs in relation to the algorithms used (e.g. dynamic batching).</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "tensorflow"
    ],
    "owner": {
      "account_id": 1299975,
      "reputation": 389,
      "user_id": 32459,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/MPM6i.jpg?s=256",
      "display_name": "Umer Farooq",
      "link": "https://datascience.stackexchange.com/users/32459/umer-farooq"
    },
    "is_answered": true,
    "view_count": 56267,
    "answer_count": 3,
    "score": 28,
    "last_activity_date": 1615042236,
    "creation_date": 1495316408,
    "question_id": 19099,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/19099/what-is-weight-and-bias-in-deep-learning",
    "title": "What is weight and bias in deep learning?",
    "body": "<p>I'm starting to learn Machine learning from Tensorflow website. I have developed a very very rudimentary understanding of the flow a deep learning program follows (this method makes me learn fast instead of reading books and big articles). </p>\n\n<p>There are a few confusing things that I have come across, 2 of them are:</p>\n\n<ol>\n<li>Bias</li>\n<li>Weight</li>\n</ol>\n\n<p>In the MNIST tutorial on tensorflow website, they have mentioned that we need bias and weight to find the evidence of the existence of a particular pattern in an image. What I don't understand is, where and how the values for Bias and Weight are determined? </p>\n\n<p>Do we have to provide these values or does the TensorFlow library calculates these values automatically based on the training data set?</p>\n\n<p>Also if you could provide some suggestions on how to accelerate my pace in deep learning, that would be great!</p>\n\n<p><a href=\"https://www.tensorflow.org/get_started/mnist/beginners\" rel=\"noreferrer\">Tensorflow Beginners Tutorial</a></p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "optimization",
      "convergence"
    ],
    "owner": {
      "account_id": 7977652,
      "reputation": 6482,
      "user_id": 23305,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/9fae98b0af48edfaf4d2c649faa62beb?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "oW_",
      "link": "https://datascience.stackexchange.com/users/23305/ow"
    },
    "is_answered": true,
    "view_count": 13216,
    "accepted_answer_id": 24821,
    "answer_count": 3,
    "score": 28,
    "last_activity_date": 1718818180,
    "creation_date": 1504638870,
    "last_edit_date": 1526978169,
    "question_id": 22853,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/22853/local-minima-vs-saddle-points-in-deep-learning",
    "title": "local minima vs saddle points in deep learning",
    "body": "<p>I heard Andrew Ng (in a video I unfortunately can't find anymore) talk about how the understanding of local minima in deep learning problems has changed in the sense that they are now regarded as less problematic because in high-dimensional spaces (encountered in deep learning) critical points are more likely to be saddle points or plateaus rather than local minima. </p>\n\n<p>I've seen papers (e.g. <a href=\"http://papers.nips.cc/paper/6112-deep-learning-without-poor-local-minima.pdf\" rel=\"noreferrer\">this one</a>) that discuss assumptions under which \"every local minimum is a global minimum\". These assumptions are all rather technical, but from what I understand they tend to impose a structure on the neural network that make it somewhat linear.</p>\n\n<p>Is it a valid claim that, in deep learning (incl. nonlinear architectures), plateaus are more likely than local minima? And if so, is there a (possibly mathematical) intuition behind it? </p>\n\n<p><em>Is there anything particular about deep learning and saddle points?</em></p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "scikit-learn"
    ],
    "owner": {
      "account_id": 9714239,
      "reputation": 951,
      "user_id": 46433,
      "user_type": "registered",
      "accept_rate": 71,
      "profile_image": "https://i.sstatic.net/Ge2WM.jpg?s=256",
      "display_name": "Taimur Islam",
      "link": "https://datascience.stackexchange.com/users/46433/taimur-islam"
    },
    "is_answered": true,
    "view_count": 25378,
    "accepted_answer_id": 28442,
    "answer_count": 2,
    "score": 28,
    "last_activity_date": 1615490754,
    "creation_date": 1519884812,
    "last_edit_date": 1615490717,
    "question_id": 28441,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/28441/what-is-the-difference-between-cross-validate-and-cross-val-score",
    "title": "What is the difference between cross_validate and cross_val_score?",
    "body": "<p>I understand <code>cross_validate</code> and how it works, but now I am confused about what  <code>cross_val_score</code> actually does. Can anyone give me some example?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "regression",
      "predictive-modeling",
      "terminology"
    ],
    "owner": {
      "account_id": 11939075,
      "reputation": 411,
      "user_id": 41968,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d498a2290afd554bd40485e89e41637f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Meiiso",
      "link": "https://datascience.stackexchange.com/users/41968/meiiso"
    },
    "is_answered": true,
    "view_count": 73529,
    "accepted_answer_id": 30914,
    "answer_count": 3,
    "score": 28,
    "last_activity_date": 1565187572,
    "creation_date": 1524784636,
    "last_edit_date": 1551529450,
    "question_id": 30912,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/30912/what-does-baseline-mean-in-the-context-of-machine-learning",
    "title": "What does &quot;baseline&quot; mean in the context of machine learning?",
    "body": "<p>What does \"baseline\" mean in the context of machine learning and data science?</p>\n\n<p>Someone wrote me:</p>\n\n<blockquote>\n  <p>Hint: An appropriate baseline will give an RMSE of approximately 200.</p>\n</blockquote>\n\n<p>I don't get this. Does he mean that if my predictive model on the training data has a RMSE below 500, it's good? </p>\n\n<blockquote>\n  <p>And what could be a \"baseline approach\"?</p>\n</blockquote>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "vc-theory"
    ],
    "owner": {
      "account_id": 6979720,
      "reputation": 383,
      "user_id": 53083,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/ACdqv.png?s=256",
      "display_name": "Kaushal28",
      "link": "https://datascience.stackexchange.com/users/53083/kaushal28"
    },
    "is_answered": true,
    "view_count": 38404,
    "accepted_answer_id": 32563,
    "answer_count": 3,
    "score": 28,
    "last_activity_date": 1613848019,
    "creation_date": 1528002789,
    "question_id": 32557,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/32557/what-is-the-exact-definition-of-vc-dimension",
    "title": "What is the exact definition of VC dimension?",
    "body": "<p>I'm studying machine learning from Andrew Ng Stanford lectures and just came across the theory of VC dimensions. According to the lectures and what I understood, the definition of VC dimension can be given as, </p>\n\n<blockquote>\n  <p>If you can find a set of $n$ points, so that it can be shattered by the classifier (i.e. classify all possible $2^n$ labeling correctly) and you cannot find any set of $n+1$ points that can be shattered (i.e. for any set of $n+1$ points there is at least one labeling order so that the classifier can not separate all points correctly), then the VC dimension is $n$. </p>\n</blockquote>\n\n<p>Also Professor took an example and explained this nicely. Which is: </p>\n\n<p>Let,</p>\n\n<p>$H=\\{{set\\ of\\ linear\\ classifiers\\ in\\ 2\\ Dimensions \\}}$</p>\n\n<p>Then any 3 points can be classified by $H$ correctly with separating hyper plane as shown in the following figure. </p>\n\n<p><a href=\"https://i.sstatic.net/iPJBN.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/iPJBN.png\" alt=\"enter image description here\"></a></p>\n\n<p>And that's why the VC dimension of $H$ is 3. Because for any 4 points in 2D plane, a linear classifier can not shatter all the combinations of the points. For example, </p>\n\n<p><a href=\"https://i.sstatic.net/U7xVB.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/U7xVB.png\" alt=\"enter image description here\"></a> </p>\n\n<p>For this set of points, <strong>there is no separating hyper plane can be drawn to classify this set. So the VC dimension is 3.</strong></p>\n\n<p>I get the idea till here. But what if we've following type of pattern?</p>\n\n<p><a href=\"https://i.sstatic.net/4nLut.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/4nLut.png\" alt=\"enter image description here\"></a></p>\n\n<p>Or the pattern where a three points coincides on each other, Here also we can not draw separating hyper plane between 3 points. But still this pattern is not considered in the definition of the VC dimension. Why? The same point is also discussed the lectures I'm watching <a href=\"https://www.youtube.com/watch?v=0kWZoyNRxTY&amp;index=10&amp;list=PLA89DCFA6ADACE599\" rel=\"noreferrer\">Here at 16:24</a> but professor does not mention the exact reason behind this.</p>\n\n<p>Any intuitive example of explanation will be appreciated. Thanks</p>\n"
  },
  {
    "tags": [
      "python",
      "deep-learning",
      "keras",
      "tensorflow"
    ],
    "owner": {
      "account_id": 6764774,
      "reputation": 1255,
      "user_id": 38657,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/yeOC5.png?s=256",
      "display_name": "Mo-",
      "link": "https://datascience.stackexchange.com/users/38657/mo"
    },
    "is_answered": true,
    "view_count": 12352,
    "answer_count": 2,
    "score": 28,
    "last_activity_date": 1568843231,
    "creation_date": 1553199604,
    "last_edit_date": 1553199908,
    "question_id": 47759,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/47759/keras-vs-tf-keras",
    "title": "Keras vs. tf.keras",
    "body": "<p>I'm a bit confused in choosing between <a href=\"https://github.com/keras-team/keras/tree/master/keras\" rel=\"noreferrer\">Keras</a> (keras-team/keras) and <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras\" rel=\"noreferrer\">tf.keras</a> (tensorflow/tensorflow/python/keras/) for my new research project.</p>\n\n<p>There is a debate that <strong>Keras</strong> isn't owned by anyone, so people are happier to contribute in and it'll be much easier to manage the project in the future. </p>\n\n<p>On the other side, <strong>tf.keras</strong> is owned by Google, so more rigorous test and maintenance. Moreover, it seems this is a better option for taking advantage of new features which are presenting in Tensorflow v.2.</p>\n\n<p>So, to start a data science (machine learning) project (in the research phase), that both are okay at the beginning, which one do you choose?!</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "cross-validation"
    ],
    "owner": {
      "account_id": 14931398,
      "reputation": 455,
      "user_id": 73650,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/vhNQb.jpg?s=256",
      "display_name": "NaveganTeX",
      "link": "https://datascience.stackexchange.com/users/73650/navegantex"
    },
    "is_answered": true,
    "view_count": 28397,
    "accepted_answer_id": 52643,
    "answer_count": 4,
    "score": 28,
    "last_activity_date": 1673045249,
    "creation_date": 1558851352,
    "last_edit_date": 1558865196,
    "question_id": 52632,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/52632/cross-validation-vs-train-validate-test",
    "title": "Cross validation Vs. Train Validate Test",
    "body": "<p>I have a doubt regarding the cross validation approach and train-validation-test approach.</p>\n\n<p>I was told that I can split a dataset into 3 parts: </p>\n\n<ol>\n<li>Train: we train the model.</li>\n<li>Validation: we validate and adjust model parameters.</li>\n<li>Test: never seen before data. We get an unbiased final estimate.</li>\n</ol>\n\n<p>So far, we have split into three subsets.  Until here everything is okay. Attached is a picture:</p>\n\n<p><a href=\"https://i.sstatic.net/osBuF.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/osBuF.png\" alt=\"enter image description here\"></a></p>\n\n<p>Then I came across the K-fold cross validation approach and what I dont understand is how I can relate the <strong>Test</strong> subset from the above approach. Meaning, in 5-fold cross validation we split the data into 5 and in each iteration the non-validation subset is used as the train subset and the validation is used as test set. But, in terms of the above mentioned example, where is the validation part in k-fold cross validation? We either have validation or test subset. </p>\n\n<p>When I refer myself to train/validation/test, that test is the scoring: </p>\n\n<p>Model development is generally a two-stage process. The first stage is training and validation, during which you apply algorithms to data for which you know the outcomes to uncover patterns between its features and thetarget variable. The second stage is scoring, in which you apply the trained model to a new dataset. Then, it returns outcomes in the form of probability scores forclassification problems and estimated averages forregression problems. Finally, youdeploythe trained model into a production application or use the insights it uncovers to improve business processes.</p>\n\n<p>As an example, I found the Sci-Kit learn cross validation version as you can see in the following picture:</p>\n\n<p><a href=\"https://i.sstatic.net/8mtoy.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/8mtoy.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>When doing the splitting, you can see that the algorithm that they give you, only takes care of the training part of the original dataset. So, in the end, we are not able to perform the <strong>Final evaluation</strong> process as you can see in the attached picture. </p>\n\n<p>Thank you!</p>\n\n<p><a href=\"https://scikit-learn.org/stable/modules/cross_validation.html\" rel=\"noreferrer\">scikitpage</a></p>\n"
  },
  {
    "tags": [
      "jupyter"
    ],
    "owner": {
      "account_id": 48063,
      "reputation": 687,
      "user_id": 60710,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ea0662af98f5642193079ebbb7e78fd5?s=256&d=identicon&r=PG",
      "display_name": "dzieciou",
      "link": "https://datascience.stackexchange.com/users/60710/dzieciou"
    },
    "is_answered": true,
    "view_count": 65868,
    "accepted_answer_id": 63103,
    "answer_count": 10,
    "score": 28,
    "last_activity_date": 1721343667,
    "creation_date": 1573661663,
    "question_id": 63101,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/63101/collaborating-on-jupyter-notebooks",
    "title": "Collaborating on Jupyter Notebooks",
    "body": "<p>I have prepared Jupyter Notebook with some findings and I shared it with other team members through GitHub to get their feedback in a written form. It used to work like this when working together on a piece of code but does not work for Jupyter Notebook. In GitHub that would mean commenting on HTML or JSON level (internal markup for .ipynb files), not on the document level. An alternative would be for team members to clone the repo and puts inline comments in the document. That's an additional effort for other team member I would like to avoid.</p>\n\n<p>What is the way you collaborate, peer review and provide feedback when working on Jupyter Notebooks?</p>\n"
  },
  {
    "tags": [
      "performance",
      "python",
      "pandas",
      "parallel"
    ],
    "owner": {
      "account_id": 4494374,
      "reputation": 871,
      "user_id": 250,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/cDteV.jpg?s=256",
      "display_name": "Therriault",
      "link": "https://datascience.stackexchange.com/users/250/therriault"
    },
    "is_answered": true,
    "view_count": 21127,
    "answer_count": 4,
    "score": 27,
    "last_activity_date": 1596372019,
    "creation_date": 1400543998,
    "last_edit_date": 1400561245,
    "question_id": 172,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/172/is-there-a-straightforward-way-to-run-pandas-dataframe-isin-in-parallel",
    "title": "Is there a straightforward way to run pandas.DataFrame.isin in parallel?",
    "body": "<p>I have a modeling and scoring program that makes heavy use of the <code>DataFrame.isin</code> function of pandas, searching through lists of facebook \"like\" records of individual users for each of a few thousand specific pages. This is the most time-consuming part of the program, more so than the modeling or scoring pieces, simply because it only runs on one core while the rest runs on a few dozen simultaneously.</p>\n\n<p>Though I know I could manually break up the dataframe into chunks and run the operation in parallel, is there any straightforward way to do that automatically? In other words, is there any kind of package out there that will recognize I'm running an easily-delegated operation and automatically distribute it? Perhaps that's asking for too much, but I've been surprised enough in the past by what's already available in Python, so I figure it's worth asking.</p>\n\n<p>Any other suggestions about how this might be accomplished (even if not by some magic unicorn package!) would also be appreciated. Mainly, just trying to find a way to shave off 15-20 minutes per run without spending an equal amount of time coding the solution.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "neural-network",
      "nlp"
    ],
    "owner": {
      "account_id": 1572823,
      "reputation": 2039,
      "user_id": 684,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://www.gravatar.com/avatar/3e1a7d9ccfa13aed6bc6cb7ec6892518?s=256&d=identicon&r=PG",
      "display_name": "Madison May",
      "link": "https://datascience.stackexchange.com/users/684/madison-may"
    },
    "is_answered": true,
    "view_count": 14377,
    "accepted_answer_id": 2367,
    "answer_count": 4,
    "score": 27,
    "last_activity_date": 1596616862,
    "creation_date": 1403206197,
    "last_edit_date": 1495210318,
    "question_id": 492,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/492/word2vec-for-named-entity-recognition",
    "title": "Word2Vec for Named Entity Recognition",
    "body": "<p>I'm looking to use google's word2vec implementation to build a named entity recognition system.  I've heard that recursive neural nets with back propagation through structure are well suited for named entity recognition tasks, but I've been unable to find a decent implementation or a decent tutorial for that type of model. Because I'm working with an atypical corpus, standard NER tools in NLTK and similar have performed very poorly, and it looks like I'll have to train my own system.   </p>\n\n<p>In short, what resources are available for this kind of problem?  Is there a standard recursive neural net implementation available?</p>\n"
  },
  {
    "tags": [
      "data-mining",
      "clustering",
      "time-series",
      "beginner"
    ],
    "owner": {
      "account_id": 4922735,
      "reputation": 401,
      "user_id": 5246,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7d98d6049274ca387770090680e7c79c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Jo Douglass",
      "link": "https://datascience.stackexchange.com/users/5246/jo-douglass"
    },
    "is_answered": true,
    "view_count": 12910,
    "answer_count": 2,
    "score": 27,
    "last_activity_date": 1599138500,
    "creation_date": 1419219045,
    "last_edit_date": 1592305723,
    "question_id": 3738,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/3738/how-to-deal-with-time-series-which-change-in-seasonality-or-other-patterns",
    "title": "How to deal with time series which change in seasonality or other patterns?",
    "body": "<h3>Background</h3>\n<p>I'm working on a time series data set of energy meter readings. The length of the series varies by meter - for some I have several years, others only a few months, etc. Many display significant seasonality, and often multiple layers - within the day, week, or year.</p>\n<p>One of the things I've been working on is clustering of these time series. My work is academic for the moment, and while I'm doing other analysis of the data as well, I have a specific goal to carry out some clustering.</p>\n<p>I did some initial work where I calculated various features (percentage used on weekends vs. weekday, percentage used in different time blocks, etc.). I then moved on to looking at using Dynamic Time Warping (DTW) to obtain the distance between different series, and clustering based on the difference values, and I've found several papers related to this.</p>\n<h3>Question</h3>\n<p>Will the seasonality in a specific series changing cause my clustering to be incorrect? And if so, how do I deal with it?</p>\n<p>My concern is that the distances obtained by DTW could be misleading in the cases where the pattern in a time series has changed. This could lead to incorrect clustering.</p>\n<p>In case the above is unclear, consider these examples:</p>\n<h3>Example 1</h3>\n<p>A meter has low readings from midnight until 8AM, the readings then increase sharply for the next hour and stay high from 9AM until 5PM, then decrease sharply over the next hour and then stay low from 6PM until midnight. The meter continues this pattern consistently every day for several months, but then changes to a pattern where readings simply stay at a consistent level throughout the day.</p>\n<h3>Example 2</h3>\n<p>A meter shows approximately the same amount of energy being consumed each month. After several years, it changes to a pattern where energy usage is higher during the summer months before returning to the usual amount.</p>\n<h3>Possible Directions</h3>\n<ul>\n<li>I've wondered whether I can continue to compare whole time series, but split them and consider them as a separate series if the pattern changes considerably. However, to do this I'd need to be able to detect such changes. Also, I just don't know if this is a suitable way or working with the data.</li>\n<li>I've also considered splitting the data and considering it as many separate time series. For instance, I could consider every day/meter combination as a separate series. However, I'd then need to do similarly if I wanted to consider the weekly/monthly/yearly patterns. I <em>think</em> this would work, but it's potentially quite onerous and I'd hate to go down this path if there's a better way that I'm missing.</li>\n</ul>\n<h3>Further Notes</h3>\n<p>These are things that have come up in comments, or things I've thought of due to comments, which might be relevant. I'm putting them here so people don't have to read through everything to get relevant information.</p>\n<ul>\n<li>I'm working in Python, but have rpy for those places where R is more suitable. I'm not necessarily looking for a Python answer though - if someone has a practical answer of what should be done I'm happy to figure out implementation details myself.</li>\n<li>I have a lot of working &quot;rough draft&quot; code - I've done some DTW runs, I've done a couple of different types of clustering, etc. I think I largely understand the direction I'm taking, and what I'm really looking for is related to how I process my data before finding distances, running clustering, etc. Given this, I suspect the answer would be the same whether the distances between series are calculated via DTW or a simpler Euclidean Distance (ED).</li>\n<li>I have found these papers especially informative on time series and DTW and they may be helpful if some background is needed to the topic area: <a href=\"http://www.cs.ucr.edu/%7Eeamonn/selected_publications.htm\" rel=\"noreferrer\">http://www.cs.ucr.edu/~eamonn/selected_publications.htm</a></li>\n</ul>\n"
  },
  {
    "tags": [
      "python",
      "linear-regression",
      "library",
      "software-recommendation"
    ],
    "owner": {
      "account_id": 169656,
      "reputation": 5862,
      "user_id": 843,
      "user_type": "registered",
      "accept_rate": 33,
      "profile_image": "https://i.sstatic.net/Z99mk.jpg?s=256",
      "display_name": "Franck Dernoncourt",
      "link": "https://datascience.stackexchange.com/users/843/franck-dernoncourt"
    },
    "is_answered": true,
    "view_count": 41681,
    "answer_count": 5,
    "score": 27,
    "last_activity_date": 1662385377,
    "creation_date": 1444968462,
    "question_id": 8457,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/8457/python-library-for-segmented-regression-a-k-a-piecewise-regression",
    "title": "Python library for segmented regression (a.k.a. piecewise regression)",
    "body": "<p>I am looking for a Python library that can perform <a href=\"https://en.wikipedia.org/wiki/Segmented_regression#Segmented_linear_regression.2C_two_segments\" rel=\"noreferrer\">segmented regression (a.k.a. piecewise regression)</a>.</p>\n\n<p><a href=\"https://onlinecourses.science.psu.edu/stat501/node/310\" rel=\"noreferrer\">Example</a>:</p>\n\n<p><a href=\"https://i.sstatic.net/ZNoPv.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/ZNoPv.png\" alt=\"enter image description here\"></a></p>\n"
  },
  {
    "tags": [
      "feature-engineering"
    ],
    "owner": {
      "account_id": 8113787,
      "reputation": 271,
      "user_id": 17318,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/62a108575250327b16c061a807b1c77a?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "small dwarf",
      "link": "https://datascience.stackexchange.com/users/17318/small-dwarf"
    },
    "is_answered": true,
    "view_count": 18143,
    "answer_count": 3,
    "score": 27,
    "last_activity_date": 1588020073,
    "creation_date": 1459762274,
    "last_edit_date": 1559965821,
    "question_id": 11024,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/11024/encoding-categorical-variables-using-likelihood-estimation",
    "title": "Encoding categorical variables using likelihood estimation",
    "body": "<p>I am trying to understand how I can encode categorical variables using likelihood estimation, but have had little success so far.</p>\n\n<p>Any suggestions would be greatly appreciated.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "scikit-learn",
      "tensorflow",
      "keras",
      "encoding"
    ],
    "owner": {
      "account_id": 2458632,
      "reputation": 393,
      "user_id": 29865,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/281bb6b2c03f8cce5b6e65e7110f9ab8?s=256&d=identicon&r=PG",
      "display_name": "Dracarys",
      "link": "https://datascience.stackexchange.com/users/29865/dracarys"
    },
    "is_answered": true,
    "view_count": 39043,
    "accepted_answer_id": 17517,
    "answer_count": 3,
    "score": 27,
    "last_activity_date": 1612886290,
    "creation_date": 1489239730,
    "last_edit_date": 1612845606,
    "question_id": 17516,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/17516/how-to-deal-with-string-labels-in-multi-class-classification-with-keras",
    "title": "How to deal with string labels in multi-class classification with keras?",
    "body": "<p>I am newbie on machine learning and keras and now working a multi-class image classification problem using keras. The input is tagged image. After some pre-processing, the training data is represented in Python list as:</p>\n<pre><code>[[&quot;dog&quot;, &quot;path/to/dog/imageX.jpg&quot;],[&quot;cat&quot;, &quot;path/to/cat/imageX.jpg&quot;], \n [&quot;bird&quot;, &quot;path/to/cat/imageX.jpg&quot;]]\n</code></pre>\n<p>the &quot;dog&quot;, &quot;cat&quot;, and &quot;bird&quot; are the class labels. I think one-hot encoding should be used for this problem but I am not very clear on how to deal it with these string labels. I've tried sklearn's <code>LabelEncoder()</code> in this way:</p>\n<pre><code>encoder = LabelEncoder()\ntrafomed_label = encoder.fit_transform([&quot;dog&quot;, &quot;cat&quot;, &quot;bird&quot;])\nprint(trafomed_label)\n</code></pre>\n<p>And the output is [2 1 0], which is different that my expectation output of somthing like [[1,0,0],[0,1,0],[0,0,1]]. It can be done with some coding, but I'd like to know if there is some &quot;standard&quot; or &quot;traditional&quot; way to deal with it?</p>\n"
  },
  {
    "tags": [
      "visualization",
      "pandas",
      "plotting"
    ],
    "owner": {
      "account_id": 5132131,
      "reputation": 641,
      "user_id": 29897,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/oTdfgsmA.jpg?s=256",
      "display_name": "redeemefy",
      "link": "https://datascience.stackexchange.com/users/29897/redeemefy"
    },
    "is_answered": true,
    "view_count": 93595,
    "accepted_answer_id": 17558,
    "answer_count": 6,
    "score": 27,
    "last_activity_date": 1657812212,
    "creation_date": 1489343545,
    "last_edit_date": 1495151880,
    "question_id": 17540,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/17540/make-seaborn-heatmap-bigger",
    "title": "make seaborn heatmap bigger",
    "body": "<p>I create a <code>corr()</code> df out of an original df. The <code>corr()</code> df came out 70 X 70 and it is impossible to visualize the heatmap... <code>sns.heatmap(df)</code>. If I try to display the <code>corr = df.corr()</code>, the table doesn't fit the screen and I can see all the correlations. Is it a way to either print the entire <code>df</code> regardless of its size or to control the size of the heatmap?</p>\n\n<p><a href=\"https://i.sstatic.net/21NBn.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/21NBn.png\" alt=\"enter image description here\"></a></p>\n"
  },
  {
    "tags": [
      "python",
      "pandas",
      "dataframe"
    ],
    "owner": {
      "account_id": 10613061,
      "reputation": 543,
      "user_id": 35420,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/DAGfA.jpg?s=256",
      "display_name": "Kevin",
      "link": "https://datascience.stackexchange.com/users/35420/kevin"
    },
    "is_answered": true,
    "view_count": 92360,
    "accepted_answer_id": 20561,
    "answer_count": 3,
    "score": 27,
    "last_activity_date": 1635725697,
    "creation_date": 1499701652,
    "last_edit_date": 1545619785,
    "question_id": 20308,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/20308/how-to-sum-values-grouped-by-two-columns-in-pandas",
    "title": "How to sum values grouped by two columns in pandas",
    "body": "<p>I have a Pandas DataFrame like this:</p>\n\n<pre><code>df = pd.DataFrame({\n    'Date': ['2017-1-1', '2017-1-1', '2017-1-2', '2017-1-2', '2017-1-3'],\n    'Groups': ['one', 'one', 'one', 'two', 'two'],\n    'data': range(1, 6)})\n\n    Date      Groups     data  \n0  2017-1-1    one       1\n1  2017-1-1    one       2\n2  2017-1-2    one       3\n3  2017-1-2    two       4\n4  2017-1-3    two       5\n</code></pre>\n\n<p>How can I generate a new DataFrame like this:</p>\n\n<pre><code>    Date       one     two \n0  2017-1-1    3        0\n1  2017-1-2    3        4\n2  2017-1-3    0        5\n</code></pre>\n"
  },
  {
    "tags": [
      "machine-learning",
      "regression",
      "categorical-data"
    ],
    "owner": {
      "account_id": 4755642,
      "reputation": 383,
      "user_id": 42205,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/7cOml.jpg?s=256",
      "display_name": "Mithun Sarker",
      "link": "https://datascience.stackexchange.com/users/42205/mithun-sarker"
    },
    "is_answered": true,
    "view_count": 32120,
    "accepted_answer_id": 27993,
    "answer_count": 2,
    "score": 27,
    "last_activity_date": 1582149798,
    "creation_date": 1518975836,
    "last_edit_date": 1560094344,
    "question_id": 27957,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/27957/why-do-we-need-to-discard-one-dummy-variable",
    "title": "Why do we need to discard one dummy variable?",
    "body": "<p>I have learned that, for creating a regression model, we have to take care of categorical variables by converting them into dummy variables. As an example, if, in our data set, there is a variable like location: </p>\n\n<pre><code>Location \n----------\nCalifornian\nNY\nFlorida\n</code></pre>\n\n<p>We have to convert them like:</p>\n\n<pre><code>1  0  0\n0  1  0\n0  0  1\n</code></pre>\n\n<p>However, it was suggested that we have to discard one dummy variable, no matter how many dummy variables are there. </p>\n\n<p>Why do we need to discard one dummy variable?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "sql",
      "nlp"
    ],
    "owner": {
      "account_id": 10774251,
      "reputation": 1471,
      "user_id": 50146,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/W7XNJ.jpg?s=256",
      "display_name": "deepguy",
      "link": "https://datascience.stackexchange.com/users/50146/deepguy"
    },
    "is_answered": true,
    "view_count": 36960,
    "accepted_answer_id": 31713,
    "answer_count": 5,
    "score": 27,
    "last_activity_date": 1632315460,
    "creation_date": 1526271788,
    "last_edit_date": 1550716776,
    "question_id": 31617,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/31617/natural-language-to-sql-query",
    "title": "Natural Language to SQL query",
    "body": "<p>I have been working on developing a system \"Converting Natural Language to SQL Query\".</p>\n\n<p>I have read the answers from the similar questions, but was not able to get the information that I was looking for.</p>\n\n<p>Below is the flowchart for such system which I have got from <strong><a href=\"http://www.iaees.org/publications/journals/selforganizology/articles/2016-3(3)/algorithm-to-transform-natural-language-into-SQL-queries.pdf\" rel=\"noreferrer\">An Algorithm to Transform Natural Language into SQL Queries for Relational Databases by Garima Singh, Arun Solanki</a></strong></p>\n\n<p><a href=\"https://i.sstatic.net/9Ekcd.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/9Ekcd.png\" alt=\"Flowchart\"></a></p>\n\n<p>I have understood till part of speech tagging step. But how do I approach the remaining steps.</p>\n\n<ol>\n<li>Do I need to train all the possible SQL queries?</li>\n<li>Or, once part of speech tagging is done, I have to play with the words and form a SQL query?</li>\n</ol>\n\n<p>Edit: I have successfully implemented the from step \"user query\" to \"Part of speech tagging\". </p>\n\n<p>Thank you.</p>\n"
  },
  {
    "tags": [
      "computer-vision",
      "object-detection"
    ],
    "owner": {
      "account_id": 9685744,
      "reputation": 398,
      "user_id": 74267,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/C1vDq.png?s=256",
      "display_name": "Guilherme Marques",
      "link": "https://datascience.stackexchange.com/users/74267/guilherme-marques"
    },
    "is_answered": true,
    "view_count": 19102,
    "accepted_answer_id": 52019,
    "answer_count": 2,
    "score": 27,
    "last_activity_date": 1669322561,
    "creation_date": 1557932440,
    "question_id": 52015,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/52015/what-is-the-difference-between-semantic-segmentation-object-detection-and-insta",
    "title": "What is the difference between semantic segmentation, object detection and instance segmentation?",
    "body": "<p>I'm fairly new at computer vision and I've read an explanation at a <a href=\"https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46\" rel=\"noreferrer\">medium post</a>, however it still isn't clear for me how they truly differ. </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "cost-function"
    ],
    "owner": {
      "account_id": 12960268,
      "reputation": 373,
      "user_id": 74441,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s256-rj/photo.jpg",
      "display_name": "Marton Langa",
      "link": "https://datascience.stackexchange.com/users/74441/marton-langa"
    },
    "is_answered": true,
    "view_count": 27607,
    "closed_date": 1558205128,
    "accepted_answer_id": 52159,
    "answer_count": 2,
    "score": 27,
    "last_activity_date": 1558187933,
    "creation_date": 1558185212,
    "last_edit_date": 1558187310,
    "question_id": 52157,
    "link": "https://datascience.stackexchange.com/questions/52157/why-do-we-have-to-divide-by-2-in-the-ml-squared-error-cost-function",
    "closed_reason": "Duplicate",
    "title": "Why do we have to divide by 2 in the ML squared error cost function?",
    "body": "<p><a href=\"https://i.sstatic.net/XtBWr.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/XtBWr.png\" alt=\"\"></a></p>\n\n<p>I'm not sure why you need to multiply by <span class=\"math-container\">$\\frac1{2m}$</span> in the beginning. I understand that you would have to divide the whole sum by <span class=\"math-container\">$\\frac1{m}$</span>, but why do we have to multiply <span class=\"math-container\">$m$</span> by two?</p>\n\n<p>Is it because we have two <span class=\"math-container\">$\\theta$</span> here in the example?</p>\n"
  },
  {
    "tags": [
      "word2vec",
      "word-embeddings",
      "bert"
    ],
    "owner": {
      "account_id": 1155215,
      "reputation": 521,
      "user_id": 44707,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/47437b3ce66e4345a72237fac0e03012?s=256&d=identicon&r=PG",
      "display_name": "sovon",
      "link": "https://datascience.stackexchange.com/users/44707/sovon"
    },
    "is_answered": true,
    "view_count": 20523,
    "answer_count": 5,
    "score": 27,
    "last_activity_date": 1584564216,
    "creation_date": 1561134316,
    "last_edit_date": 1583937645,
    "question_id": 54232,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/54232/bert-vs-word2vec-is-bert-disambiguating-the-meaning-of-the-word-vector",
    "title": "BERT vs Word2VEC: Is bert disambiguating the meaning of the word vector?",
    "body": "<p><strong>Word2vec</strong>:\nWord2vec provides a vector for each token/word and those vectors encode the meaning of the word. Although those vectors are not human interpretable, the meaning of the vectors are understandable/interpretable by comparing with other vectors (for example, the vector of <code>dog</code> will be most similar to the vector of <code>cat</code>), and various interesting equations (for example <code>king-men+women=queen</code>, which proves how well those vectors hold the semantic of words).</p>\n\n<p><strong>The problem</strong> with word2vec is that each word has only one vector but in the real world each word has different meaning depending on the context and sometimes the meaning can be totally different (for example, <code>bank as a financial institute</code> vs <code>bank of the river</code>).</p>\n\n<p><strong>Bert:</strong>\nOne important difference between Bert/ELMO (dynamic word embedding) and Word2vec is that these models consider the context and for each token, there is a vector.</p>\n\n<p><strong>Now the question is</strong>, do vectors from Bert hold the behaviors of word2Vec and solve the meaning disambiguation problem (as this is a contextual word embedding)? </p>\n\n<p><strong>Experiments</strong>\nTo get the vectors from google's pre-trained model, I used <a href=\"https://pypi.org/project/bert-embedding/\" rel=\"noreferrer\">bert-embedding-1.0.1</a> library. \nI first tried to see whether it hold the <strong>similarity property</strong>. To test, I took the first paragraphs from wikipedia page of Dog, Cat, and Bank (financial institute). The similar word for <code>dog</code> is: \n    ('dog',1.0)\n    ('wolf', 0.7254540324211121)\n    ('domestic', 0.6261438727378845)\n    ('cat', 0.6036421656608582)\n    ('canis', 0.5722522139549255)\n    ('mammal', 0.5652133226394653)\nHere, the first element is token and second is the similarity.</p>\n\n<p>Now for <strong>disambiguation test</strong>: \nAlong with Dog, Cat and Bank (financtial institute), I added a paragraph of <a href=\"https://en.wikipedia.org/wiki/Bank_(geography)\" rel=\"noreferrer\">River bank</a> from wikipedia. This is to check that bert can differentiate between two different types of <code>Bank</code>. Here the hope is, the vector of token bank (of river) will be close to vector of <code>river</code> or <code>water</code> but far away from <code>bank(financial institute)</code>, <code>credit</code>, <code>financial</code> etc. Here is the result: \nThe second element is the sentence to show the context. </p>\n\n<pre><code>('bank', 'in geography , the word bank generally refers to the land alongside a body of water . different structures are referred to as', 1.0)\n('bank', 'a bank is a financial institution that accepts deposits from the public and creates credit .', 0.7796692848205566)\n('bank', 'in limnology , a stream bank or river bank is the terrain alongside the bed of a river , creek , or', 0.7275459170341492)\n('bank', 'in limnology , a stream bank or river bank is the terrain alongside the bed of a river , creek , or', 0.7121304273605347)\n('bank', 'the bank consists of the sides of the channel , between which the flow is confined .', 0.6965076327323914)\n('banks', 'markets to their importance in the financial stability of a country , banks are highly regulated in most countries .', 0.6590269804000854)\n('banking', 'most nations have institutionalized a system known as fractional reserve banking under which banks hold liquid assets equal to only a', 0.6490173935890198)\n('banks', 'most nations have institutionalized a system known as fractional reserve banking under which banks hold liquid assets equal to only a', 0.6224181652069092)\n('financial', 'a bank is a financial institution that accepts deposits from the public and creates credit .', 0.614281952381134)\n('banks', 'stream banks are of particular interest in fluvial geography , which studies the processes associated with rivers and streams and the deposits', 0.6096583604812622)\n('structures', 'in geography , the word bank generally refers to the land alongside a body of water . different structures are referred to as', 0.5771245360374451)\n('financial', 'markets to their importance in the financial stability of a country , banks are highly regulated in most countries .', 0.5701562166213989)\n('reserve', 'most nations have institutionalized a system known as fractional reserve banking under which banks hold liquid assets equal to only a', 0.5462549328804016)\n('institution', 'a bank is a financial institution that accepts deposits from the public and creates credit .', 0.537483811378479)\n('land', 'in geography , the word bank generally refers to the land alongside a body of water . different structures are referred to as', 0.5331911444664001)\n('of', 'in geography , the word bank generally refers to the land alongside a body of water . different structures are referred to as', 0.527492105960846)\n('water', 'in geography , the word bank generally refers to the land alongside a body of water . different structures are referred to as', 0.5234918594360352)\n('banks', 'bankfull discharge is a discharge great enough to fill the channel and overtop the banks .', 0.5213838815689087)\n('lending', 'lending activities can be performed either directly or indirectly through due capital .', 0.5207482576370239)\n('deposits', 'a bank is a financial institution that accepts deposits from the public and creates credit .', 0.5131596922874451)\n('stream', 'in limnology , a stream bank or river bank is the terrain alongside the bed of a river , creek , or', 0.5108630061149597)\n('bankfull', 'bankfull discharge is a discharge great enough to fill the channel and overtop the banks .', 0.5102289915084839)\n('river', 'in limnology , a stream bank or river bank is the terrain alongside the bed of a river , creek , or', 0.5099104046821594)\n</code></pre>\n\n<p>Here, the result of the most similar vectors of bank (as a river bank, the token is taken from the context of the first row and that is why the similarity score is 1.0. So, the second one is the closest vector). From the result, it can be seen that the first most close token's meaning and context is very different. Even the token <code>river</code>, <code>water and</code>stream` has lower similarity. </p>\n\n<p>So, it seems that the vectors do not really disambiguate the meaning. \nWhy is that?\nIsn't the contextual token vector supposed to disambiguate the meaning of a word?</p>\n"
  },
  {
    "tags": [
      "data-mining",
      "definitions"
    ],
    "owner": {
      "account_id": 84539,
      "reputation": 413,
      "user_id": 66,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e328e5547ae249f222721cba4a7010eb?s=256&d=identicon&r=PG",
      "display_name": "demongolem",
      "link": "https://datascience.stackexchange.com/users/66/demongolem"
    },
    "is_answered": true,
    "view_count": 1973,
    "accepted_answer_id": 29,
    "answer_count": 4,
    "score": 26,
    "last_activity_date": 1597582893,
    "creation_date": 1400030759,
    "last_edit_date": 1403021840,
    "question_id": 14,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/14/is-data-science-the-same-as-data-mining",
    "title": "Is Data Science the Same as Data Mining?",
    "body": "<p>I am sure data science as will be discussed in this forum has several synonyms or at least related fields where large data is analyzed.</p>\n\n<p>My particular question is in regards to Data Mining.  I took a graduate class in Data Mining a few years back.  What are the differences between Data Science and Data Mining and in particular what more would I need to look at to become proficient in Data Mining?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "feature-selection",
      "logistic-regression",
      "information-retrieval"
    ],
    "owner": {
      "account_id": 4902738,
      "reputation": 361,
      "user_id": 2979,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/44bf69e4d833f1cd071af55802d46c24?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "elmille",
      "link": "https://datascience.stackexchange.com/users/2979/elmille"
    },
    "is_answered": true,
    "view_count": 18344,
    "answer_count": 2,
    "score": 26,
    "last_activity_date": 1452054010,
    "creation_date": 1408296584,
    "question_id": 987,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/987/text-categorization-combining-different-kind-of-features",
    "title": "Text categorization: combining different kind of features",
    "body": "<p>The problem I am tackling is categorizing short texts into multiple classes. My current approach is to use tf-idf weighted term frequencies and learn a simple linear classifier (logistic regression). This works reasonably well (around 90% macro F-1 on test set, nearly 100% on training set). A big problem are unseen words/n-grams. </p>\n\n<p>I am trying to improve the classifier by adding other features, e.g. a fixed sized vector computed using distributional similarities (as computed by word2vec) or other categorical features of the examples. My idea was to just add the features to the sparse input features from the bag of words. However, this results in worse performance on the test and training set. The additional features by themselves give about 80% F-1 on the test set, so they aren't garbage. Scaling the features didn't help as well. My current thinking is that these kind of features don't mix well with the (sparse) bag of words features.</p>\n\n<p>So the question is: assuming the additional features provide additional information, what is the best way to incorporate them? Could training separate classifiers and combining them in some kind of ensemble work (this would probably have the drawback that no interaction between the features of the different classifiers could be captured)? Are there other more complex models I should consider?</p>\n"
  },
  {
    "tags": [
      "neural-network"
    ],
    "owner": {
      "account_id": 6457628,
      "reputation": 363,
      "user_id": 10990,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/10153429389833824/picture?type=large",
      "display_name": "Victor Yip",
      "link": "https://datascience.stackexchange.com/users/10990/victor-yip"
    },
    "is_answered": true,
    "view_count": 11745,
    "accepted_answer_id": 6644,
    "answer_count": 5,
    "score": 26,
    "last_activity_date": 1604708385,
    "creation_date": 1438302336,
    "last_edit_date": 1596625595,
    "question_id": 6639,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/6639/extra-output-layer-in-a-neural-network-decimal-to-binary",
    "title": "Extra output layer in a neural network (Decimal to binary)",
    "body": "<p>I'm working through a question from the <a href=\"http://neuralnetworksanddeeplearning.com/chap1.html\" rel=\"noreferrer\">online book</a>.</p>\n<p>I can understand that if the additional output layer is of 5 output neurons, I could probably set bias at 0.5 and weight of 0.5 each for the previous layer. But the question now ask for a new layer of four output neurons - which is more than enough to represent 10 possible outputs at <span class=\"math-container\">$2^{4}$</span>.</p>\n<p>Can someone walk me through the steps involved in understanding and solving this problem?</p>\n<p>The exercise question:</p>\n<p>There is a way of determining the bitwise representation of a digit by adding an extra layer to the three-layer network above. The extra layer converts the output from the previous layer into a binary representation, as illustrated in the figure below. Find a set of weights and biases for the new output layer. Assume that the first 3 layers of neurons are such that the correct output in the third layer (i.e., the old output layer) has activation at least 0.99, and incorrect outputs have activation less than 0.01.</p>\n<p><a href=\"https://i.sstatic.net/OqQ6N.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/OqQ6N.png\" alt=\"enter image description here\" /></a></p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "r",
      "predictive-modeling",
      "decision-trees",
      "xgboost"
    ],
    "owner": {
      "account_id": 4179565,
      "reputation": 2078,
      "user_id": 1151,
      "user_type": "registered",
      "accept_rate": 0,
      "profile_image": "https://www.gravatar.com/avatar/b6c6c1670281f453f3cc9ccf3fbf7071?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "GeorgeOfTheRF",
      "link": "https://datascience.stackexchange.com/users/1151/georgeoftherf"
    },
    "is_answered": true,
    "view_count": 79790,
    "answer_count": 4,
    "score": 26,
    "last_activity_date": 1654506415,
    "creation_date": 1441682049,
    "last_edit_date": 1630125707,
    "question_id": 8032,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/8032/how-to-predict-probabilities-in-xgboost-using-r",
    "title": "How to predict probabilities in xgboost using R?",
    "body": "<p>The below predict function is giving -ve values as well so it cannot be probabilities. </p>\n\n<pre><code>param &lt;- list(max.depth = 5, eta = 0.01,  objective=\"binary:logistic\",subsample=0.9)\nbst &lt;- xgboost(param, data = x_mat, label = y_mat,nround = 3000)\n\npred_s &lt;- predict(bst, x_mat_s2)\n</code></pre>\n\n<p>I google &amp; tried <code>pred_s &lt;- predict(bst, x_mat_s2,type=\"response\")</code>\nbut it didn't work.</p>\n\n<p><strong>Question</strong></p>\n\n<p>How to predict probabilities instead?</p>\n"
  },
  {
    "tags": [
      "search",
      "ranking",
      "xgboost",
      "gbm"
    ],
    "owner": {
      "account_id": 1193188,
      "reputation": 418,
      "user_id": 588,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/58ecfb9d07c391230f94db22a49f3084?s=256&d=identicon&r=PG",
      "display_name": "tokestermw",
      "link": "https://datascience.stackexchange.com/users/588/tokestermw"
    },
    "is_answered": true,
    "view_count": 27977,
    "protected_date": 1558010933,
    "accepted_answer_id": 10308,
    "answer_count": 2,
    "score": 26,
    "last_activity_date": 1627665067,
    "creation_date": 1455122434,
    "last_edit_date": 1627665067,
    "question_id": 10179,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/10179/how-fit-pairwise-ranking-models-in-xgboost",
    "title": "How fit pairwise ranking models in XGBoost?",
    "body": "<p>As far as I know, to train learning to rank models, you need to have three things in the dataset:</p>\n<ul>\n<li>label or relevance</li>\n<li>group or query id</li>\n<li>feature vector</li>\n</ul>\n<p>For example, the <a href=\"http://research.microsoft.com/en-us/um/beijing/projects/letor/letor4dataset.aspx\" rel=\"noreferrer\">Microsoft Learning to Rank dataset</a> uses this format (label, group id, and features).</p>\n<pre><code>1 qid:10 1:0.031310 2:0.666667 ...\n0 qid:10 1:0.078682 2:0.166667 ...\n</code></pre>\n<p>I am trying out XGBoost that utilizes GBMs to do pairwise ranking. They have an <a href=\"https://github.com/dmlc/xgboost/tree/master/demo/rank\" rel=\"noreferrer\">example for a ranking task</a> that uses the C++ program to learn on the Microsoft dataset like above.</p>\n<p>However, I am using their Python wrapper and cannot seem to find where I can input the group id (<code>qid</code> above). I can train the model using just the features and relevance score, but I am missing something.</p>\n<p>Here is a sample script.</p>\n<pre><code>gbm = XGBRegressor(objective=&quot;rank:pairwise&quot;)\n\nX =  np.random.normal(0, 1, 1000).reshape(100, 10)\ny = np.random.randint(0, 5, 100)\n\ngbm.fit(X, y) ### --- no group id needed???\n\nprint gbm.predict(X)\n\n# should be in reverse order of relevance score\nprint y[gbm.predict_proba(X)[:, 1].argsort()][::-1]\n</code></pre>\n"
  },
  {
    "tags": [
      "random-forest",
      "scikit-learn"
    ],
    "owner": {
      "account_id": 1576456,
      "reputation": 623,
      "user_id": 21433,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/291d06d13068ab8e99232dc6fd566d55?s=256&d=identicon&r=PG",
      "display_name": "darXider",
      "link": "https://datascience.stackexchange.com/users/21433/darxider"
    },
    "is_answered": true,
    "view_count": 31355,
    "accepted_answer_id": 30408,
    "answer_count": 1,
    "score": 26,
    "last_activity_date": 1612901797,
    "creation_date": 1470152867,
    "last_edit_date": 1612901797,
    "question_id": 13151,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/13151/randomforestclassifier-oob-scoring-method",
    "title": "RandomForestClassifier OOB scoring method",
    "body": "<p>Does the random forest implementation in scikit-learn use mean accuracy as its scoring method to estimate generalization error with out-of-bag samples? This is not mentioned in the documentation, but the <code>score()</code> method reports the mean accuracy.</p>\n<p>I have a highly unbalanced dataset, and I am using AUC of ROC as my scoring metric in the grid search. Is there a way to tell the classifier to use the same scoring method on the OOB samples as well?</p>\n"
  },
  {
    "tags": [
      "software-recommendation"
    ],
    "owner": {
      "account_id": 340794,
      "reputation": 383,
      "user_id": 3591,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2d735ca95ae6d53fa8fd9fa79814e514?s=256&d=identicon&r=PG",
      "display_name": "Dror Atariah",
      "link": "https://datascience.stackexchange.com/users/3591/dror-atariah"
    },
    "is_answered": true,
    "view_count": 10432,
    "answer_count": 7,
    "score": 26,
    "last_activity_date": 1647515387,
    "creation_date": 1478602802,
    "last_edit_date": 1480919330,
    "question_id": 14998,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/14998/sharing-jupyter-notebooks-within-a-team",
    "title": "Sharing Jupyter notebooks within a team",
    "body": "<p>I would like to set up a server which could support a data science team in the following way: be a central point for storing, versioning, sharing and possible also executing Jupyter notebooks.</p>\n\n<p>Some desired properties:</p>\n\n<ol>\n<li>Different users can access the server and open and execute notebooks that were stored by them or by other team members. The interesting question here is what would be the behavior if user X executes cells in a notebook authored by user Y. I guess the notebook should <em>NOT</em> be changed:</li>\n<li>Solution should be self-hosted.</li>\n<li>Notebooks should be stored either on the server or on Google drive or on self-hosted instance of owncloud.</li>\n<li>(Bonus) Notebooks will be under git versioning control (git may be self-hosted. Cannot be bounded to GitHub or something of that sort).</li>\n</ol>\n\n<p>I looked into <a href=\"https://github.com/jupyterhub\" rel=\"noreferrer\">JupyterHub</a> and <a href=\"https://github.com/binder-project/binder\" rel=\"noreferrer\">Binder</a>. With the former, I didn't understand how to allow cross users access. The latter seems to only support GitHub as the storage of the notebooks.</p>\n\n<p>Do you have experience with either of the solutions? </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "cnn",
      "convolutional-neural-network",
      "backpropagation",
      "kernel"
    ],
    "owner": {
      "account_id": 4577769,
      "reputation": 436,
      "user_id": 45821,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://www.gravatar.com/avatar/bd28015dbb4f4ced5b455ee4d20d46a6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "koryakinp",
      "link": "https://datascience.stackexchange.com/users/45821/koryakinp"
    },
    "is_answered": true,
    "view_count": 30011,
    "accepted_answer_id": 27751,
    "answer_count": 1,
    "score": 26,
    "last_activity_date": 1613234561,
    "creation_date": 1517895511,
    "last_edit_date": 1613234561,
    "question_id": 27506,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/27506/back-propagation-in-cnn",
    "title": "back propagation in CNN",
    "body": "<p>I have the following CNN:</p>\n\n<p><a href=\"https://i.sstatic.net/MehcI.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/MehcI.png\" alt=\"network layour\"></a></p>\n\n<ol>\n<li>I start with an input image of size 5x5</li>\n<li>Then I apply convolution using 2x2 kernel and stride = 1, that produces feature map of size 4x4.</li>\n<li>Then I apply 2x2 max-pooling with stride = 2, that reduces feature map to size 2x2. </li>\n<li>Then I apply logistic sigmoid.</li>\n<li>Then one fully connected layer with 2 neurons.</li>\n<li>And an output layer.</li>\n</ol>\n\n<p>For the sake of simplicity, let's assume I have already completed the forward pass and computed <strong>&delta;H1=0.25</strong> and  <strong>&delta;H2=-0.15</strong></p>\n\n<p>So after the complete forward pass and partially completed backward pass my network looks like this:</p>\n\n<p><a href=\"https://i.sstatic.net/wE1He.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/wE1He.png\" alt=\"network after forward pass\"></a></p>\n\n<p>Then I compute deltas for non-linear layer (logistic sigmoid):</p>\n\n<p>$$\n\\begin{align}\n&amp;\\delta_{11}=(0.25 * 0.61 + -0.15 * 0.02) * 0.58 * (1 - 0.58) = 0.0364182\\\\\n&amp;\\delta_{12}=(0.25 * 0.82 + -0.15 * -0.50) * 0.57 * (1 - 0.57) = 0.068628\\\\\n&amp;\\delta_{21}=(0.25 * 0.96 + -0.15 * 0.23) * 0.65 * (1 - 0.65) = 0.04675125\\\\\n&amp;\\delta_{22}=(0.25 * -1.00 + -0.15 * 0.17) * 0.55 * (1 - 0.55) = -0.06818625\\\\\n\\end{align}\n$$</p>\n\n<p>Then, I propagate deltas to 4x4 layer and set all the values which were filtered out by max-pooling to 0 and gradient map look like this:</p>\n\n<p><a href=\"https://i.sstatic.net/aaEQ9.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/aaEQ9.png\" alt=\"enter image description here\"></a></p>\n\n<p>How do I update kernel weights from there? And if my network had another convolutional layer prior to 5x5, what values should I use to update it kernel weights? And overall, is my calculation correct?</p>\n"
  },
  {
    "tags": [
      "preprocessing"
    ],
    "owner": {
      "account_id": 13028119,
      "reputation": 261,
      "user_id": 46893,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f44b5e0ec24c60916145cfa8b27fea8a?s=256&d=identicon&r=PG",
      "display_name": "Pooja",
      "link": "https://datascience.stackexchange.com/users/46893/pooja"
    },
    "is_answered": true,
    "view_count": 25602,
    "answer_count": 4,
    "score": 26,
    "last_activity_date": 1519819326,
    "creation_date": 1519676962,
    "last_edit_date": 1519678208,
    "question_id": 28331,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/28331/different-test-set-and-training-set-distribution",
    "title": "Different Test Set and Training Set Distribution",
    "body": "<p>I am working on a data science competition for which the distribution of my test set is different from the training set. I want to subsample observations from training set which closely resembles test set. </p>\n\n<p>How can I do this?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "decision-trees"
    ],
    "owner": {
      "account_id": 12460160,
      "reputation": 435,
      "user_id": 52726,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/571232736550600/picture?type=large",
      "display_name": "Sahil Chaturvedi",
      "link": "https://datascience.stackexchange.com/users/52726/sahil-chaturvedi"
    },
    "is_answered": true,
    "view_count": 101427,
    "accepted_answer_id": 32623,
    "answer_count": 4,
    "score": 26,
    "last_activity_date": 1612636012,
    "creation_date": 1528136614,
    "last_edit_date": 1612635883,
    "question_id": 32622,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/32622/how-to-make-a-decision-tree-with-both-continuous-and-categorical-variables-in-th",
    "title": "How to make a decision tree with both continuous and categorical variables in the dataset?",
    "body": "<p>Let's say I have 3 categorical and 2 continuous attributes in a dataset. How do I build a decision tree using these 5 variables?</p>\n<p><strong>Edit:</strong><br />\nFor categorical variables, it is easy to say that we will split them just by <code>{yes/no}</code> and calculate the total gini gain, but my doubt tends to be primarily with the continuous attributes. Let's say I have values for a continuous attribute like <code>{1,2,3,4,5}</code>. What will be my split point choices? Will they be checked at every data point like <code>{&lt;1,&gt;=1......&amp; so on till}</code> or will the splitting point will be something like the mean of column?</p>\n"
  },
  {
    "tags": [
      "python",
      "visualization",
      "jupyter",
      "anaconda",
      "graphviz"
    ],
    "owner": {
      "account_id": 3348911,
      "reputation": 535,
      "user_id": 57429,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0e50f7e8bb3963d1dc749e278c3d8d33?s=256&d=identicon&r=PG",
      "display_name": "psygo",
      "link": "https://datascience.stackexchange.com/users/57429/psygo"
    },
    "is_answered": true,
    "view_count": 145137,
    "protected_date": 1575318305,
    "answer_count": 10,
    "score": 26,
    "last_activity_date": 1628140431,
    "creation_date": 1535219297,
    "last_edit_date": 1587474627,
    "question_id": 37428,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/37428/graphviz-not-working-when-imported-inside-pydotplus-graphvizs-executables-not",
    "title": "GraphViz not working when imported inside PydotPlus (`GraphViz&#39;s executables not found`)",
    "body": "<p>I've been trying to make these packages work for quite some time now but with no success. Basically the error is:</p>\n\n<pre><code>GraphViz's Executables not found\n</code></pre>\n\n<p><strong>EDIT</strong>: I had not posted a terminal <code>log</code> with the error originally. I'm using <code>Ubuntu</code> now so I won't be able to reproduce the exact same error I got in the past (a year ago, so far away in the past...). However, I've been experiencing a similar --- if not the same --- error in my current setup; even while using a virtual environment with <code>pipenv</code>. The error seems to come from lines that were described in <a href=\"https://datascience.stackexchange.com/a/48563/57429\">@'s answer</a>:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"example.py\", line 49, in &lt;module&gt;\n    Image(graph.create_png())\n  File \"/home/philippe/.local/lib/python3.6/site-packages/pydotplus/graphviz.py\", line 1797, in &lt;lambda&gt;\n    lambda f=frmt, prog=self.prog: self.create(format=f, prog=prog)\n  File \"/home/philippe/.local/lib/python3.6/site-packages/pydotplus/graphviz.py\", line 1960, in create\n    'GraphViz\\'s executables not found')\npydotplus.graphviz.InvocationException: GraphViz's executables not found\n</code></pre>\n\n<p>I've tried to install <code>GraphViz</code> via 2 different ways: via <code>pip install graphviz</code> and through the <code>.msi</code> package (and also tried to install <code>pydot</code>, <code>pydotplus</code> and <code>graphviz</code> in many different orders).</p>\n\n<p>The code I'm trying to run is simply a <code>dot-to-png</code> converter for the <a href=\"https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\" rel=\"noreferrer\">Iris Dataset</a>.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sklearn.tree import DecisionTreeClassifier\nimport sklearn.datasets as datasets\nfrom sklearn.externals.six import StringIO\nfrom sklearn.tree import export_graphviz\n\nimport pandas as pd\nimport pydotplus\n\nfrom IPython.display import Image\n\niris = datasets.load_iris()\ndf = pd.DataFrame(iris.data, columns = iris.feature_names)\ny = iris.target\n\ndtree = DecisionTreeClassifier()\ndtree.fit(df,y)\n\ndot_data = StringIO()\nexport_graphviz(\n    dtree, \n    out_file = dot_data,\n    filled = True, \n    rounded = True,\n    special_characters = True\n)\ngraph_1 = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(graph_1.create_png())\n</code></pre>\n\n<p>In <code>Jupyter Notebooks</code> and in <code>Atom</code>, the system seems to be looking for <code>GraphViz</code> inside <code>pydotplus</code>, as it points to <code>~\\Anaconda3\\lib\\site-packages\\pydotplus\\graphviz.py</code>. Shouldn't it be the other way around?</p>\n\n<p>Lastly, I just want to point out that I've already tried adding <code>GraphViz</code>'s path to the system's <code>PATH</code> using <code>C:\\Users\\Philippe\\Anaconda3\\Library\\bin\\graphviz</code>.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "r",
      "statistics",
      "data-analysis"
    ],
    "owner": {
      "account_id": 4371294,
      "reputation": 498,
      "user_id": 87923,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4029afdddcec72a6b5c8385b3605bc60?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "confused",
      "link": "https://datascience.stackexchange.com/users/87923/confused"
    },
    "is_answered": true,
    "view_count": 7451,
    "accepted_answer_id": 76841,
    "answer_count": 7,
    "score": 26,
    "last_activity_date": 1647102222,
    "creation_date": 1593403144,
    "last_edit_date": 1601753030,
    "question_id": 76824,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/76824/is-python-a-viable-language-to-do-statistical-analysis-in",
    "title": "Is Python a viable language to do statistical analysis in?",
    "body": "<p>I originally came from R, but Python seems to be the more common language these days.  Ideally, I would do all my coding in Python as the syntax is easier and I've had more real life experience using it - and switching back and forth is a pain.</p>\n<p>Out side of ML type stuff, all of the statistical analysis I've done have been in R - like regressions, time series, ANOVA, logistic regression etc.  I have never really done that type of stuff in Python.  However, I am trying to create a bunch of code templates for myself, and before I start, I would like to know if Python is deep enough to completely replace R as my language of choice.  I eventually do plan on moving more towards ML, and I know Python can do that, and eventually I would imagine I have to go to a more base language like C++.</p>\n<p>Anyone know what are the limitations of Python when it comes to statistical analysis or has as link to the pros and cons of using R vs. Python as the main language for statistical analysis?</p>\n"
  },
  {
    "tags": [
      "clustering",
      "algorithms",
      "similarity"
    ],
    "owner": {
      "account_id": 2937730,
      "reputation": 503,
      "user_id": 113,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/2Tl2O.jpg?s=256",
      "display_name": "vefthym",
      "link": "https://datascience.stackexchange.com/users/113/vefthym"
    },
    "is_answered": true,
    "view_count": 10943,
    "answer_count": 5,
    "score": 25,
    "last_activity_date": 1624871601,
    "creation_date": 1400250372,
    "question_id": 103,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/103/clustering-based-on-similarity-scores",
    "title": "Clustering based on similarity scores",
    "body": "<p>Assume that we have a set of elements <em>E</em> and a similarity (<strong>not distance</strong>) function <em>sim(ei, ej)</em> between two elements <em>ei,ej  E</em>. </p>\n\n<p>How could we (efficiently) cluster the elements of <em>E</em>, using <em>sim</em>?</p>\n\n<p><em>k</em>-means, for example, requires a given <em>k</em>, Canopy Clustering requires two threshold values. What if we don't want such predefined parameters?</p>\n\n<p>Note, that <em>sim</em> is not neccessarily a metric (i.e. the triangle inequality may, or may not hold). Moreover, it doesn't matter if the clusters are disjoint (partitions of <em>E</em>).</p>\n"
  },
  {
    "tags": [
      "topic-model",
      "lda",
      "parameter"
    ],
    "owner": {
      "account_id": 302279,
      "reputation": 2500,
      "user_id": 122,
      "user_type": "registered",
      "accept_rate": 62,
      "profile_image": "https://www.gravatar.com/avatar/0e9087f2672b0e4f28d91266acf9ce57?s=256&d=identicon&r=PG",
      "display_name": "alvas",
      "link": "https://datascience.stackexchange.com/users/122/alvas"
    },
    "is_answered": true,
    "view_count": 36605,
    "accepted_answer_id": 202,
    "answer_count": 2,
    "score": 25,
    "last_activity_date": 1653063633,
    "creation_date": 1400826350,
    "question_id": 199,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/199/what-does-the-alpha-and-beta-hyperparameters-contribute-to-in-latent-dirichlet-a",
    "title": "What does the alpha and beta hyperparameters contribute to in Latent Dirichlet allocation?",
    "body": "<p>LDA has two hyperparameters, tuning them changes the induced topics. </p>\n\n<p>What does the alpha and beta hyperparameters contribute to LDA? </p>\n\n<p>How does the topic change if one or the other hyperparameters increase or decrease? </p>\n\n<p>Why are they hyperparamters and not just parameters?</p>\n"
  },
  {
    "tags": [
      "performance",
      "accuracy"
    ],
    "owner": {
      "account_id": 1414923,
      "reputation": 664,
      "user_id": 1127,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8fc5b99f9f6ffda1d4ee42964b1ff525?s=256&d=identicon&r=PG",
      "display_name": "neuron",
      "link": "https://datascience.stackexchange.com/users/1127/neuron"
    },
    "is_answered": true,
    "view_count": 535,
    "answer_count": 3,
    "community_owned_date": 1434459111,
    "score": 25,
    "last_activity_date": 1434462167,
    "creation_date": 1434292056,
    "question_id": 6116,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/6116/how-do-you-manage-expectations-at-work",
    "title": "How do you manage expectations at work?",
    "body": "<p>With all the hoopla around Data Science, Machine Learning, and all the success stories around, there are a lot of both justified, as well as overinflated, expectations from Data Scientists and their predictive models.</p>\n\n<p>My question to practicing Statisticians, Machine Learning experts, and Data Scientists is - how do you manage expectations from the businesspeople in you company, particularly with regards to predictive accuracy of models? To put it trivially, if your best model can only achieve 90% accuracy, and upper management expects nothing less than 99%, how do you handle situations like these?</p>\n"
  },
  {
    "tags": [
      "clustering",
      "k-means"
    ],
    "owner": {
      "account_id": 3399912,
      "reputation": 1667,
      "user_id": 989,
      "user_type": "registered",
      "accept_rate": 29,
      "profile_image": "https://www.gravatar.com/avatar/4480675ad0ede0043e7caba04750d974?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "marcodena",
      "link": "https://datascience.stackexchange.com/users/989/marcodena"
    },
    "is_answered": true,
    "view_count": 19313,
    "answer_count": 3,
    "score": 25,
    "last_activity_date": 1552583381,
    "creation_date": 1437379401,
    "last_edit_date": 1495543133,
    "question_id": 6508,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/6508/k-means-incoherent-behaviour-choosing-k-with-elbow-method-bic-variance-explain",
    "title": "K-means incoherent behaviour choosing K with Elbow method, BIC, variance explained and silhouette",
    "body": "<p>I'm trying to cluster some vectors with 90 features with K-means. Since this algorithm asks me the number of clusters, I want to validate my choice with some nice math.\nI expect to have from 8 to 10 clusters. The features are Z-score scaled.</p>\n\n<p><strong>Elbow method and variance explained</strong></p>\n\n<pre><code>from scipy.spatial.distance import cdist, pdist\nfrom sklearn.cluster import KMeans\n\nK = range(1,50)\nKM = [KMeans(n_clusters=k).fit(dt_trans) for k in K]\ncentroids = [k.cluster_centers_ for k in KM]\n\nD_k = [cdist(dt_trans, cent, 'euclidean') for cent in centroids]\ncIdx = [np.argmin(D,axis=1) for D in D_k]\ndist = [np.min(D,axis=1) for D in D_k]\navgWithinSS = [sum(d)/dt_trans.shape[0] for d in dist]\n\n# Total with-in sum of square\nwcss = [sum(d**2) for d in dist]\ntss = sum(pdist(dt_trans)**2)/dt_trans.shape[0]\nbss = tss-wcss\n\nkIdx = 10-1\n\n# elbow curve\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(K, avgWithinSS, 'b*-')\nax.plot(K[kIdx], avgWithinSS[kIdx], marker='o', markersize=12, \nmarkeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\nplt.grid(True)\nplt.xlabel('Number of clusters')\nplt.ylabel('Average within-cluster sum of squares')\nplt.title('Elbow for KMeans clustering')\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(K, bss/tss*100, 'b*-')\nplt.grid(True)\nplt.xlabel('Number of clusters')\nplt.ylabel('Percentage of variance explained')\nplt.title('Elbow for KMeans clustering')\n</code></pre>\n\n<p><img src=\"https://i.sstatic.net/VZZgt.png\" alt=\"Elbow method\">\n<img src=\"https://i.sstatic.net/NIyI2.png\" alt=\"Variance\"></p>\n\n<p>From these two pictures, it seems that the number of clusters never stops :D. Strange! Where is the elbow? How can I choose K?</p>\n\n<p><strong>Bayesian information criterion</strong></p>\n\n<p>This methods comes directly from X-means and uses the <a href=\"http://Bayesian%20information%20criterion\" rel=\"noreferrer\">BIC</a> to choose the number of clusters. <a href=\"https://stackoverflow.com/questions/1793532/how-do-i-determine-k-when-using-k-means-clustering\">another ref</a></p>\n\n<pre><code>    from sklearn.metrics import euclidean_distances\nfrom sklearn.cluster import KMeans\n\ndef bic(clusters, centroids):\n    num_points = sum(len(cluster) for cluster in clusters)\n    num_dims = clusters[0][0].shape[0]\n    log_likelihood = _loglikelihood(num_points, num_dims, clusters, centroids)\n    num_params = _free_params(len(clusters), num_dims)\n    return log_likelihood - num_params / 2.0 * np.log(num_points)\n\n\ndef _free_params(num_clusters, num_dims):\n    return num_clusters * (num_dims + 1)\n\n\ndef _loglikelihood(num_points, num_dims, clusters, centroids):\n    ll = 0\n    for cluster in clusters:\n        fRn = len(cluster)\n        t1 = fRn * np.log(fRn)\n        t2 = fRn * np.log(num_points)\n        variance = _cluster_variance(num_points, clusters, centroids) or np.nextafter(0, 1)\n        t3 = ((fRn * num_dims) / 2.0) * np.log((2.0 * np.pi) * variance)\n        t4 = (fRn - 1.0) / 2.0\n        ll += t1 - t2 - t3 - t4\n    return ll\n\ndef _cluster_variance(num_points, clusters, centroids):\n    s = 0\n    denom = float(num_points - len(centroids))\n    for cluster, centroid in zip(clusters, centroids):\n        distances = euclidean_distances(cluster, centroid)\n        s += (distances*distances).sum()\n    return s / denom\n\nfrom scipy.spatial import distance\ndef compute_bic(kmeans,X):\n    \"\"\"\n    Computes the BIC metric for a given clusters\n\n    Parameters:\n    -----------------------------------------\n    kmeans:  List of clustering object from scikit learn\n\n    X     :  multidimension np array of data points\n\n    Returns:\n    -----------------------------------------\n    BIC value\n    \"\"\"\n    # assign centers and labels\n    centers = [kmeans.cluster_centers_]\n    labels  = kmeans.labels_\n    #number of clusters\n    m = kmeans.n_clusters\n    # size of the clusters\n    n = np.bincount(labels)\n    #size of data set\n    N, d = X.shape\n\n    #compute variance for all clusters beforehand\n    cl_var = (1.0 / (N - m) / d) * sum([sum(distance.cdist(X[np.where(labels == i)], [centers[0][i]], 'euclidean')**2) for i in range(m)])\n\n    const_term = 0.5 * m * np.log(N) * (d+1)\n\n    BIC = np.sum([n[i] * np.log(n[i]) -\n               n[i] * np.log(N) -\n             ((n[i] * d) / 2) * np.log(2*np.pi*cl_var) -\n             ((n[i] - 1) * d/ 2) for i in range(m)]) - const_term\n\n    return(BIC)\n\n\n\nsns.set_style(\"ticks\")\nsns.set_palette(sns.color_palette(\"Blues_r\"))\nbics = []\nfor n_clusters in range(2,50):\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(dt_trans)\n\n    labels = kmeans.labels_\n    centroids = kmeans.cluster_centers_\n\n    clusters = {}\n    for i,d in enumerate(kmeans.labels_):\n        if d not in clusters:\n            clusters[d] = []\n        clusters[d].append(dt_trans[i])\n\n    bics.append(compute_bic(kmeans,dt_trans))#-bic(clusters.values(), centroids))\n\nplt.plot(bics)\nplt.ylabel(\"BIC score\")\nplt.xlabel(\"k\")\nplt.title(\"BIC scoring for K-means cell's behaviour\")\nsns.despine()\n#plt.savefig('figures/K-means-BIC.pdf', format='pdf', dpi=330,bbox_inches='tight')\n</code></pre>\n\n<p><img src=\"https://i.sstatic.net/lon6L.png\" alt=\"enter image description here\"></p>\n\n<p>Same problem here... What is K?</p>\n\n<p><strong>Silhouette</strong></p>\n\n<pre><code>    from sklearn.metrics import silhouette_score\n\ns = []\nfor n_clusters in range(2,30):\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(dt_trans)\n\n    labels = kmeans.labels_\n    centroids = kmeans.cluster_centers_\n\n    s.append(silhouette_score(dt_trans, labels, metric='euclidean'))\n\nplt.plot(s)\nplt.ylabel(\"Silouette\")\nplt.xlabel(\"k\")\nplt.title(\"Silouette for K-means cell's behaviour\")\nsns.despine()\n</code></pre>\n\n<p><img src=\"https://i.sstatic.net/5Znt5.png\" alt=\"enter image description here\"></p>\n\n<p>Alleluja! Here it seems to make sense and this is what I expect. But why is this different from the others?</p>\n"
  },
  {
    "tags": [
      "svm",
      "scikit-learn"
    ],
    "owner": {
      "account_id": 1223553,
      "reputation": 536,
      "user_id": 12580,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6cc245f0126ddcd0629ed9fab1b94173?s=256&d=identicon&r=PG",
      "display_name": "metjush",
      "link": "https://datascience.stackexchange.com/users/12580/metjush"
    },
    "is_answered": true,
    "view_count": 49884,
    "answer_count": 2,
    "score": 25,
    "last_activity_date": 1450425261,
    "creation_date": 1441205373,
    "last_edit_date": 1441205781,
    "question_id": 6987,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/6987/can-you-explain-the-difference-between-svc-and-linearsvc-in-scikit-learn",
    "title": "Can you explain the difference between SVC and LinearSVC in scikit-learn?",
    "body": "<p>I've recently started learning to work with <code>sklearn</code> and have just come across this peculiar result.</p>\n\n<p>I used the <code>digits</code> dataset available in <code>sklearn</code> to try different models and estimation methods. </p>\n\n<p>When I tested a Support Vector Machine model on the data, I found out there are two different classes in <code>sklearn</code> for SVM classification: <code>SVC</code> and <code>LinearSVC</code>, where the former uses <em>one-against-one</em> approach and the other uses <em>one-against-rest</em> approach. </p>\n\n<p>I didn't know what effect that could have on the results, so I tried both. I did a Monte Carlo-style estimation where I ran both models 500 times, each time splitting the sample randomly into 60% training and 40% test and calculating the error of the prediction on the test set.</p>\n\n<p>The regular SVC estimator produced the following histogram of errors:\n<a href=\"https://i.sstatic.net/NkKQM.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/NkKQM.png\" alt=\"SVC Error Rate\"></a>\nWhile the linear SVC estimator produced the following histogram:\n<a href=\"https://i.sstatic.net/ajHfx.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/ajHfx.png\" alt=\"Linear SVC Error Rate\"></a></p>\n\n<p>What could account for such a stark difference? Why does the linear model have such higher accuracy most of the time?</p>\n\n<p>And, relatedly, what could be causing the stark polarization in the results? Either an accuracy close to 1 or an accuracy close to 0, nothing in between.</p>\n\n<p>For comparison, a decision tree classification produced a much more normally distributed error rate with an accuracy of around .85.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 9172,
    "accepted_answer_id": 9237,
    "answer_count": 2,
    "score": 25,
    "last_activity_date": 1564921465,
    "creation_date": 1449402090,
    "last_edit_date": 1559964840,
    "question_id": 9233,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/9233/why-do-activation-functions-have-to-be-monotonic",
    "title": "Why do activation functions have to be monotonic?",
    "body": "<p>I am currently preparing for an exam on neural networks. In several protocols from former exams I read that the activation functions of neurons (in multilayer perceptrons) have to be monotonic.</p>\n\n<p>I understand that activation functions should be differentiable, have a derivative which is not 0 on most points, and be non-linear. I do not understand why being monotonic is important/helpful.</p>\n\n<p>I know the following activation functions and that they are monotonic:</p>\n\n<ul>\n<li>ReLU</li>\n<li>Sigmoid</li>\n<li>Tanh</li>\n<li>Softmax: I'm not sure if the definition of monotonicity is applicable for functions <span class=\"math-container\">$f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$</span> with <span class=\"math-container\">$n, m &gt; 1$</span></li>\n<li>Softplus</li>\n<li>(Identity)</li>\n</ul>\n\n<p>However, I still can't see any reason why for example <span class=\"math-container\">$\\varphi(x) = x^2$</span>.</p>\n\n<p>Why do activation functions have to be monotonic?</p>\n\n<p>(Related side question: is there any reason why the logarithm/exponential function is not used as an activation function?)</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "svm",
      "supervised-learning",
      "unsupervised-learning"
    ],
    "owner": {
      "account_id": 6728422,
      "reputation": 511,
      "user_id": 11044,
      "user_type": "registered",
      "accept_rate": 86,
      "profile_image": "https://www.gravatar.com/avatar/0f7dcd953e9a38f406cd284e9d6a3743?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Ragnar",
      "link": "https://datascience.stackexchange.com/users/11044/ragnar"
    },
    "is_answered": true,
    "view_count": 20552,
    "accepted_answer_id": 9751,
    "answer_count": 2,
    "score": 25,
    "last_activity_date": 1637174809,
    "creation_date": 1452496618,
    "last_edit_date": 1637174809,
    "question_id": 9736,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/9736/what-kinds-of-learning-problems-are-suitable-for-support-vector-machines",
    "title": "What kinds of learning problems are suitable for Support Vector Machines?",
    "body": "<p>What are the hallmarks or properties that indicate that a certain learning problem can be tackled using support vector machines?</p>\n<p>In other words, what is it that, when you see a learning problem, makes you go &quot;oh I should definitely use SVMs for this&quot; rather than neural networks or decision trees or anything else?</p>\n"
  },
  {
    "tags": [
      "r",
      "python",
      "dataset",
      "data-cleaning",
      "pandas"
    ],
    "owner": {
      "account_id": 3155405,
      "reputation": 815,
      "user_id": 2489,
      "user_type": "registered",
      "accept_rate": 60,
      "profile_image": "https://www.gravatar.com/avatar/ce6b6ce667b0c91c0c75a9aad86c7d30?s=256&d=identicon&r=PG",
      "display_name": "cpumar",
      "link": "https://datascience.stackexchange.com/users/2489/cpumar"
    },
    "is_answered": true,
    "view_count": 16533,
    "answer_count": 4,
    "score": 25,
    "last_activity_date": 1624647563,
    "creation_date": 1456908850,
    "last_edit_date": 1612901820,
    "question_id": 10478,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/10478/is-there-any-data-tidying-tool-for-python-pandas-similar-to-r-tidyr-tool",
    "title": "Is there any data tidying tool for python/pandas similar to R tidyr tool?",
    "body": "<p>I'm working on a Kaggle challenge where some variables are represented by rows instead of columns (Telstra Network Disruption). I am currently searching for the equivalent of <code>gather()</code>, <code>separate()</code> and <code>spread()</code>, which can be found in R <code>tidyr</code> tool.</p>\n"
  },
  {
    "tags": [
      "scikit-learn",
      "pandas",
      "xgboost"
    ],
    "owner": {
      "account_id": 3979838,
      "reputation": 451,
      "user_id": 10915,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/cKVkW.jpg?s=256",
      "display_name": "Ghostintheshell",
      "link": "https://datascience.stackexchange.com/users/10915/ghostintheshell"
    },
    "is_answered": true,
    "view_count": 47931,
    "answer_count": 3,
    "score": 25,
    "last_activity_date": 1615490067,
    "creation_date": 1468590489,
    "last_edit_date": 1615490028,
    "question_id": 12799,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/12799/pandas-dataframe-to-dmatrix",
    "title": "Pandas Dataframe to DMatrix",
    "body": "<p>I am trying to run xgboost in scikit learn.  And I am only using Pandas to load the data into a dataframe. How am I supposed to use pandas df with xgboost?  I am confused by the DMatrix routine required to run the xgboost algorithm.</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "keras"
    ],
    "owner": {
      "account_id": 8700074,
      "reputation": 8747,
      "user_id": 21560,
      "user_type": "registered",
      "accept_rate": 36,
      "profile_image": "https://www.gravatar.com/avatar/349cff1d621bb8779315322f202fba5b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Hendrik",
      "link": "https://datascience.stackexchange.com/users/21560/hendrik"
    },
    "is_answered": true,
    "view_count": 67675,
    "answer_count": 9,
    "score": 25,
    "last_activity_date": 1596635900,
    "creation_date": 1471358304,
    "question_id": 13461,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/13461/how-can-i-get-prediction-for-only-one-instance-in-keras",
    "title": "How can I get prediction for only one instance in Keras?",
    "body": "<p>When I request Keras to apply prediction with a fitted model to a new dataset without label like this:</p>\n\n<pre><code>model1.predict_classes(X_test)\n</code></pre>\n\n<p>it works fine. But when I try to make prediction for only one row, it fails:</p>\n\n<pre><code>model1.predict_classes(X_test[10])\n\nException: Error when checking : expected dense_input_6 to have shape (None, 784) but got array with shape (784, 1)\n</code></pre>\n\n<p>I wonder, why?</p>\n"
  },
  {
    "tags": [
      "xgboost",
      "feature-engineering"
    ],
    "owner": {
      "account_id": 2988635,
      "reputation": 635,
      "user_id": 30170,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://i.sstatic.net/uAsJd.png?s=256",
      "display_name": "KevinKim",
      "link": "https://datascience.stackexchange.com/users/30170/kevinkim"
    },
    "is_answered": true,
    "view_count": 43923,
    "answer_count": 4,
    "score": 25,
    "last_activity_date": 1609555879,
    "creation_date": 1490018289,
    "question_id": 17710,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/17710/is-feature-engineering-still-useful-when-using-xgboost",
    "title": "Is feature engineering still useful when using XGBoost?",
    "body": "<p>I was reading the material related to XGBoost. It seems that this method does not require any variable scaling since it is based on trees and this one can capture complex non-linearity pattern, interactions. And it can handle both numerical and categorical variables and it also seems that redundant variables does not affect this method too much. </p>\n\n<p>Usually, in predictive modeling, you may do some selection among all the features you have and you may also create some new features from the set of features you have. So select a subset of features means you think there are some redundancy in your set of features; create some new features from the current feature set means you do some functional transformations on your current features. Then, both of these two points should be covered in XGBoost. Then, does it mean that to use XGBoost, you only need to choose those tunning parameters wisely? What is the value of doing feature engineering using XGBoost?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "backpropagation"
    ],
    "owner": {
      "account_id": 1183320,
      "reputation": 709,
      "user_id": 24119,
      "user_type": "registered",
      "accept_rate": 88,
      "profile_image": "https://www.gravatar.com/avatar/9be49b9db792033fcc86542bd36e8818?s=256&d=identicon&r=PG",
      "display_name": "user1157751",
      "link": "https://datascience.stackexchange.com/users/24119/user1157751"
    },
    "is_answered": true,
    "view_count": 49629,
    "accepted_answer_id": 19332,
    "answer_count": 1,
    "score": 25,
    "last_activity_date": 1546514932,
    "creation_date": 1495962380,
    "last_edit_date": 1496123444,
    "question_id": 19272,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/19272/deep-neural-network-backpropogation-with-relu",
    "title": "Deep Neural Network - Backpropogation with ReLU",
    "body": "<p>I'm having some difficulty in deriving back propagation with ReLU, and I did some work, but I'm not sure if I'm on the right track.</p>\n\n<p>Cost Function: $\\frac{1}{2}(y-\\hat y)^2$ where $y$ is the real value, and $\\hat y$ is a predicted value. Also assume that $x$ > 0 always.</p>\n\n<hr>\n\n<p>1 Layer ReLU, where the weight at the 1st layer is $w_1$</p>\n\n<p><a href=\"https://i.sstatic.net/oNeWz.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/oNeWz.png\" alt=\"enter image description here\"></a></p>\n\n<p>$\\frac{dC}{dw_1}=\\frac{dC}{dR}\\frac{dR}{dw_1}$</p>\n\n<p>$\\frac{dC}{w_1}=(y-ReLU(w_1x))(x)$</p>\n\n<hr>\n\n<p>2 Layer ReLU, where the weights at the 1st layer is $w_2$, and the 2nd layer is $w_1$ And I wanted to updated the 1st layer $w_2$</p>\n\n<p><a href=\"https://i.sstatic.net/ic6L6.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/ic6L6.png\" alt=\"enter image description here\"></a></p>\n\n<p>$\\frac{dC}{dw_2}=\\frac{dC}{dR}\\frac{dR}{dw_2}$</p>\n\n<p>$\\frac{dC}{w_2}=(y-ReLU(w_1*ReLU(w_2x))(w_1x)$</p>\n\n<p>Since $ReLU(w_1*ReLU(w_2x))=w_1w_2x$</p>\n\n<hr>\n\n<p>3 Layer ReLU, where the weights at the 1st layer is $w_3$, 2nd layer $w_2$ and 3rd layer $w_1$</p>\n\n<p><a href=\"https://i.sstatic.net/XIeyw.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/XIeyw.png\" alt=\"enter image description here\"></a></p>\n\n<p>$\\frac{dC}{dw_3}=\\frac{dC}{dR}\\frac{dR}{dw_3}$</p>\n\n<p>$\\frac{dC}{w_3}=(y-ReLU(w_1*ReLU(w_2(*ReLU(w_3)))(w_1w_2x)$</p>\n\n<p>Since $ReLU(w_1*ReLU(w_2(*ReLU(w_3))=w_1w_2w_3x$</p>\n\n<p>Since the chain rule only lasts with 2 derivatives, compared to a sigmoid, which could be as long as $n$ number of layers.</p>\n\n<hr>\n\n<p>Say I wanted to updated all 3 layer weights, where $w_1$ is the 3rd layer, $w_2$ is the 2nd layer, $w_1$ is the 3rd layer</p>\n\n<p>$\\frac{dC}{w_1}=(y-ReLU(w_1x))(x)$</p>\n\n<p>$\\frac{dC}{w_2}=(y-ReLU(w_1*ReLU(w_2x))(w_1x)$</p>\n\n<p>$\\frac{dC}{w_3}=(y-ReLU(w_1*ReLU(w_2(*ReLU(w_3)))(w_1w_2x)$</p>\n\n<p>If this derivation is correct, how does this prevent vanishing? Compared to sigmoid, where we have a lot of multiply by 0.25 in the equation, whereas ReLU does not have any constant value multiplication. If there's thousands of layers, there would be a lot of multiplication due to weights, then wouldn't this cause vanishing or exploding gradient?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "feature-extraction",
      "feature-scaling"
    ],
    "owner": {
      "account_id": 9766027,
      "reputation": 406,
      "user_id": 35549,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-C7SZEhT4lO0/AAAAAAAAAAI/AAAAAAAAB3Y/PjVj-WluA5Y/s256-rj/photo.jpg",
      "display_name": "terenceflow",
      "link": "https://datascience.stackexchange.com/users/35549/terenceflow"
    },
    "is_answered": true,
    "view_count": 13502,
    "answer_count": 2,
    "score": 25,
    "last_activity_date": 1511470013,
    "creation_date": 1500867857,
    "last_edit_date": 1500892612,
    "question_id": 21650,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/21650/feature-transformation-on-input-data",
    "title": "Feature Transformation on Input data",
    "body": "<p>I was reading about the solution to this <a href=\"https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335\" rel=\"noreferrer\">OTTO Kaggle challenge</a> and the first place solution seems to use several transforms for the input data X, for example Log(X+1), sqrt( X + 3/8), etc. Is there a general guideline on when to apply which kind transformations to various classifiers? </p>\n\n<p>I do understand the concepts of mean-var and min-max normalization. However, for the above transformations, my guess is that Log and Sqrt are used to compress the dynamic range of the data. And the x-axis shift is just to recenter the data. However, the author chooses to use different methods of normalization for the same input X when feeding into different classifiers. Any ideas?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "neural-network",
      "convolutional-neural-network",
      "image-recognition",
      "dropout"
    ],
    "owner": {
      "account_id": 958566,
      "reputation": 1227,
      "user_id": 38339,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d4477e899c81356b8f4077bf78d27eab?s=256&d=identicon&r=PG",
      "display_name": "Juan Antonio Gomez Moriano",
      "link": "https://datascience.stackexchange.com/users/38339/juan-antonio-gomez-moriano"
    },
    "is_answered": true,
    "view_count": 11817,
    "accepted_answer_id": 22618,
    "answer_count": 5,
    "score": 25,
    "last_activity_date": 1613436167,
    "creation_date": 1503445946,
    "last_edit_date": 1613436167,
    "question_id": 22494,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/22494/convolutional-neural-network-overfitting-dropout-not-helping",
    "title": "Convolutional neural network overfitting. Dropout not helping",
    "body": "<p>I am playing a little with convnets. Specifically, I am using the kaggle cats-vs-dogs dataset which consists on 25000 images labeled as either cat or dog (12500 each).</p>\n\n<p>I've managed to achieve around 85% classification accuracy on my test set, however I set a goal of achieving 90% accuracy.</p>\n\n<p>My main problem is overfitting. Somehow it always ends up happening (normally after epoch 8-10). The architecture of my network is loosely inspired by VGG-16, more specifically my images are resized to <span class=\"math-container\">$128x128x3$</span>, and then I run:</p>\n\n<pre><code>Convolution 1 128x128x32 (kernel size is 3, strides is 1)\nConvolution 2 128x128x32 (kernel size is 3, strides is 1)\nMax pool    1 64x64x32   (kernel size is 2, strides is 2)\nConvolution 3 64x64x64   (kernel size is 3, strides is 1)\nConvolution 4 64x64x64   (kernel size is 3, strides is 1)\nMax pool    2 32x32x64   (kernel size is 2, strides is 2)\nConvolution 5 16x16x128  (kernel size is 3, strides is 1)\nConvolution 6 16x16x128  (kernel size is 3, strides is 1)\nMax pool    3 8x8x128    (kernel size is 2, strides is 2)\nConvolution 7 8x8x256    (kernel size is 3, strides is 1)\nMax pool    4 4x4x256    (kernel size is 2, strides is 2)\nConvolution 8 4x4x512    (kernel size is 3, strides is 1)\nFully connected layer 1024 (dropout 0.5)\nFully connected layer 1024 (dropout 0.5)\n</code></pre>\n\n<p>All the layers except the last one have relus as activation functions.</p>\n\n<p>Note that I have tried different combinations of convolutions (I started with simpler convolutions).</p>\n\n<p>Also, I have augmented the dataset by mirroring the images, so that in total I have 50000 images.</p>\n\n<p>Also, I am normalizing the images using min max normalization, where X is the image</p>\n\n<p><span class=\"math-container\">$X = X - 0 / 255 - 0$</span></p>\n\n<p>The code is written in tensorflow and the batch sizes are 128.</p>\n\n<p>The mini-batches of training data end up overfitting and having an accuracy of 100% while the validation data seems to stop learning at around 84-85%.</p>\n\n<p>I have also tried to increase/decrease the dropout rate.</p>\n\n<p>The optimizer being used is AdamOptimizer with a learning rate of 0.0001</p>\n\n<p>At the moment I have been playing with this problem for the last 3 weeks and 85% seems to have set a barrier in front of me.</p>\n\n<p>For the record, I know I could use transfer learning to achieve much higher results, but I am interesting on building this network as a self-learning experience.</p>\n\n<p><strong>Update:</strong></p>\n\n<p>I am running the SAME network with a different batch size, in this case I am using a much smaller batch size (16 instead of 128) so far I am achieving 87.5% accuracy (instead of 85%). That said, the network ends up overfitting anyway. Still I do not understand how a dropout of 50% of the units is not helping... obviously I am doing something wrong here. Any ideas?</p>\n\n<p><strong>Update 2:</strong></p>\n\n<p>Seems like the problem had to do with the batch size, as with a smaller size (16 instead of 128) I am achieving now 92.8% accuracy on my test set, with the smaller batch size the network still overfits (the mini batches end up with an accuracy of 100%) however, the loss (error) keeps decreasing and it is in general more stable. The cons are a MUCH slower running time, but it is totally worth the wait.</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "gpu"
    ],
    "owner": {
      "account_id": 1280149,
      "reputation": 361,
      "user_id": 39733,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d24007892142f251ff1328b77c80ca06?s=256&d=identicon&r=PG",
      "display_name": "Dan",
      "link": "https://datascience.stackexchange.com/users/39733/dan"
    },
    "is_answered": true,
    "view_count": 33739,
    "accepted_answer_id": 23377,
    "answer_count": 3,
    "score": 25,
    "last_activity_date": 1606655372,
    "creation_date": 1506463998,
    "last_edit_date": 1506507101,
    "question_id": 23341,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/23341/should-i-use-gpu-or-cpu-for-inference",
    "title": "Should I use GPU or CPU for inference?",
    "body": "<p>I'm running a deep learning neural network that has been trained by a GPU. I now want to deploy this to multiple hosts for inference. The question is what are the conditions to decide whether I should use GPU's or CPUs for inference?</p>\n\n<hr>\n\n<p>Adding more details from comments below. </p>\n\n<p>I'm new to this so guidance is appreciated. </p>\n\n<ul>\n<li><p><strong>Memory</strong>: GPU is K80</p></li>\n<li><p><strong>Framework</strong>: Cuda and cuDNN </p></li>\n<li><p><strong>Data size per workloads</strong>: 20G</p></li>\n<li><p><strong>Computing nodes to consume</strong>: one per job, although would like to consider a scale option </p></li>\n<li><p><strong>Cost</strong>: I can afford a GPU option if the reasons make sense</p></li>\n<li><p><strong>Deployment</strong>: Running on own hosted bare metal servers, not in the cloud.</p></li>\n</ul>\n\n<p>Right now I'm running on CPU simply because the application runs ok. But outside of that reason, I'm unsure why one would even consider GPU. </p>\n"
  },
  {
    "tags": [
      "class-imbalance",
      "text",
      "smote"
    ],
    "owner": {
      "account_id": 6329197,
      "reputation": 369,
      "user_id": 12496,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/I7fpy.jpg?s=256",
      "display_name": "catris25",
      "link": "https://datascience.stackexchange.com/users/12496/catris25"
    },
    "is_answered": true,
    "view_count": 36885,
    "answer_count": 3,
    "score": 25,
    "last_activity_date": 1625369291,
    "creation_date": 1518261505,
    "last_edit_date": 1518267736,
    "question_id": 27671,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/27671/how-do-you-apply-smote-on-text-classification",
    "title": "How do you apply SMOTE on text classification?",
    "body": "<p>Synthetic Minority Oversampling Technique (SMOTE) is an oversampling technique used in an imbalanced dataset problem. So far I have an idea how to apply it on generic, structured data. But is it possible to apply it on text classification problem? Which part of the data do you need to oversample? There is already <a href=\"https://datascience.stackexchange.com/questions/19553/can-smote-be-applied-over-sequence-of-words-sentences\">another question</a> about it, but it doesn't have an answer. Where can I possibly learn to get started with this? </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "neural-network",
      "keras",
      "lstm"
    ],
    "owner": {
      "account_id": 9199997,
      "reputation": 361,
      "user_id": 56960,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e5990932e2bb59e5e051a9a514fa8291?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Dookoto_Sea",
      "link": "https://datascience.stackexchange.com/users/56960/dookoto-sea"
    },
    "is_answered": true,
    "view_count": 98685,
    "answer_count": 4,
    "score": 25,
    "last_activity_date": 1620471306,
    "creation_date": 1533008912,
    "last_edit_date": 1533025419,
    "question_id": 36238,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/36238/what-does-the-output-of-model-predict-function-from-keras-mean",
    "title": "What does the output of model.predict function from Keras mean?",
    "body": "<p>I have built a LSTM model to predict duplicate questions on the Quora official dataset. The test labels are 0 or 1. 1 indicates the question pair is duplicate. After building the model using <code>model.fit</code>, I test the model using <code>model.predict</code> on the test data. The output is an array of values something like below:</p>\n\n<pre><code> [ 0.00514298]\n [ 0.15161049]\n [ 0.27588326]\n [ 0.00236167]\n [ 1.80067325]\n [ 0.01048524]\n [ 1.43425131]\n [ 1.99202418]\n [ 0.54853892]\n [ 0.02514757]\n</code></pre>\n\n<p>I am only showing the first 10 values in the array. I don't understand what do these values mean and what is the predicted label for each question pair?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "lstm",
      "rnn"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user105907"
    },
    "is_answered": true,
    "view_count": 24797,
    "answer_count": 2,
    "score": 25,
    "last_activity_date": 1671187126,
    "creation_date": 1602313632,
    "last_edit_date": 1671187126,
    "question_id": 82808,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/82808/whats-the-difference-between-the-cell-and-hidden-state-in-lstm",
    "title": "What&#39;s the difference between the cell and hidden state in LSTM?",
    "body": "<p>LSTM cells consist of two types of states, the cell state and hidden state.</p>\n<p>How do cell and hidden states differ, in terms of their functionality? What information do they carry?</p>\n<p><a href=\"https://i.sstatic.net/bVXaT.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/bVXaT.png\" alt=\"enter image description here\" /></a></p>\n"
  },
  {
    "tags": [
      "education",
      "definitions",
      "career"
    ],
    "owner": {
      "account_id": 3155405,
      "reputation": 815,
      "user_id": 2489,
      "user_type": "registered",
      "accept_rate": 60,
      "profile_image": "https://www.gravatar.com/avatar/ce6b6ce667b0c91c0c75a9aad86c7d30?s=256&d=identicon&r=PG",
      "display_name": "cpumar",
      "link": "https://datascience.stackexchange.com/users/2489/cpumar"
    },
    "is_answered": true,
    "view_count": 6610,
    "closed_date": 1407046680,
    "accepted_answer_id": 742,
    "answer_count": 3,
    "score": 24,
    "last_activity_date": 1405711745,
    "creation_date": 1405364521,
    "last_edit_date": 1405711745,
    "question_id": 739,
    "link": "https://datascience.stackexchange.com/questions/739/starting-my-career-as-data-scientist-is-software-engineering-experience-require",
    "closed_reason": "Not suitable for this site",
    "title": "Starting my career as Data Scientist, is Software Engineering experience required?",
    "body": "<p>I am an MSc student at the University of Edinburgh, specialized in machine learning and natural language processing. I had some practical courses focused on data mining, and others dealing with machine learning, bayesian statistics and graphical models. My background is a BSc in Computer Science.</p>\n\n<p>I did some software engineering and I learnt the basic concepts, such as design patterns, but I have never been involved in a large software development project. However, I had a data mining project in my MSc. My question is, if I want to go for a career as Data Scientist, should I apply for a graduate data scientist position first, or should I get a position as graduate software engineer first, maybe something related to data science, such as big data infrastructure or machine learning software development?</p>\n\n<p>My concern is that I might need good software engineering skills for data science, and I am not sure if these can be obtained by working as a graduate data scientist directly.</p>\n\n<p>Moreover, at the moment I like Data Mining, but what if I want to change my career to software engineering in the future? It might be difficult if I specialised so much in data science.</p>\n\n<p>I have not been employed yet, so my knowledge is still limited. Any clarification or advice are welcome, as I am about to finish my MSc and I want to start applying for graduate positions in early October.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "dimensionality-reduction"
    ],
    "owner": {
      "account_id": 302279,
      "reputation": 2500,
      "user_id": 122,
      "user_type": "registered",
      "accept_rate": 62,
      "profile_image": "https://www.gravatar.com/avatar/0e9087f2672b0e4f28d91266acf9ce57?s=256&d=identicon&r=PG",
      "display_name": "alvas",
      "link": "https://datascience.stackexchange.com/users/122/alvas"
    },
    "is_answered": true,
    "view_count": 17470,
    "accepted_answer_id": 5698,
    "answer_count": 4,
    "score": 24,
    "last_activity_date": 1664261292,
    "creation_date": 1430848115,
    "last_edit_date": 1587495062,
    "question_id": 5694,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/5694/dimensionality-and-manifold",
    "title": "Dimensionality and Manifold",
    "body": "<p>A commonly heard sentence in unsupervised Machine learning is</p>\n\n<blockquote>\n  <p>High dimensional inputs typically live on or near a low dimensional\n  manifold</p>\n</blockquote>\n\n<p><strong>What is a dimension? What is a manifold? What is the difference?</strong> </p>\n\n<p><strong>Can you give an example to describe both?</strong></p>\n\n<p><strong>Manifold</strong> from Wikipedia:</p>\n\n<blockquote>\n  <p>In mathematics, a manifold is a topological space that resembles\n  Euclidean space near each point. More precisely, each point of an\n  n-dimensional manifold has a neighbourhood that is homeomorphic to the\n  Euclidean space of dimension n.</p>\n</blockquote>\n\n<p><strong>Dimension</strong> from Wikipedia:</p>\n\n<blockquote>\n  <p>In physics and mathematics, the dimension of a mathematical space (or\n  object) is informally defined as the minimum number of coordinates\n  needed to specify any point within it.</p>\n</blockquote>\n\n<p>What does the Wikipedia even mean in layman terms? It sounds like some bizarre definition like most machine learning definition?</p>\n\n<p><strong>They are both spaces, so what's the difference between a Euclidean space (i.e. Manifold) and a dimension space (i.e. feature-based)?</strong> </p>\n"
  },
  {
    "tags": [
      "neural-network",
      "deep-learning",
      "theano",
      "tensorflow",
      "keras"
    ],
    "owner": {
      "account_id": 169656,
      "reputation": 5862,
      "user_id": 843,
      "user_type": "registered",
      "accept_rate": 33,
      "profile_image": "https://i.sstatic.net/Z99mk.jpg?s=256",
      "display_name": "Franck Dernoncourt",
      "link": "https://datascience.stackexchange.com/users/843/franck-dernoncourt"
    },
    "is_answered": true,
    "view_count": 10471,
    "accepted_answer_id": 23399,
    "answer_count": 2,
    "score": 24,
    "last_activity_date": 1596619291,
    "creation_date": 1449506524,
    "last_edit_date": 1449516729,
    "question_id": 9249,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9249/choosing-between-tensorflow-or-theano-as-backend-for-keras",
    "title": "Choosing between TensorFlow or Theano as backend for Keras",
    "body": "<p><a href=\"http://keras.io/\">Keras</a> supports both <strong>TensorFlow</strong> and <strong>Theano</strong> as backend: what are the pros/cons of choosing one versus the other, besides the fact that currently not all operations are implemented with the TensorFlow backend?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "text-mining",
      "deep-learning",
      "beginner",
      "tensorflow"
    ],
    "owner": {
      "account_id": 3925636,
      "reputation": 373,
      "user_id": 13508,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Xolau.jpg?s=256",
      "display_name": "shanky_thebearer",
      "link": "https://datascience.stackexchange.com/users/13508/shanky-thebearer"
    },
    "is_answered": true,
    "view_count": 23959,
    "accepted_answer_id": 15497,
    "answer_count": 3,
    "score": 24,
    "last_activity_date": 1520561006,
    "creation_date": 1454497011,
    "last_edit_date": 1454518831,
    "question_id": 10077,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10077/keyword-phrase-extraction-from-text-using-deep-learning-libraries",
    "title": "Keyword/phrase extraction from Text using Deep Learning libraries",
    "body": "<p>Perhaps this is too broad, but I am looking for references on how to use deep learning in a text summarization task.</p>\n\n<p>I have already implemented text summarization using standard word-frequency approaches and sentence-ranking, but I'd like to explore the possibility of using deep learning techniques for this task. I have also gone through some implementations given on <a href=\"http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\" rel=\"noreferrer\">wildml.com</a> using Convolutional Neural Networks (CNN) for sentiment analysis; I'd like to know how one could use libraries such as TensorFlow or Theano for text summarization and keyword extraction. Its been about a week since I started experimenting with Neural nets, and I am really excited to see how the performance of these libraries compares to my previous approaches to this problem.</p>\n\n<p>I am particularly looking for some interesting papers and github projects related to text summarization using these frameworks. Can anyone provide me with some references?</p>\n"
  },
  {
    "tags": [
      "nlp",
      "nltk"
    ],
    "owner": {
      "account_id": 95846,
      "reputation": 759,
      "user_id": 15613,
      "user_type": "registered",
      "accept_rate": 84,
      "profile_image": "https://i.sstatic.net/D40aN.jpg?s=256",
      "display_name": "gogasca",
      "link": "https://datascience.stackexchange.com/users/15613/gogasca"
    },
    "is_answered": true,
    "view_count": 87687,
    "accepted_answer_id": 12583,
    "answer_count": 6,
    "score": 24,
    "last_activity_date": 1617914357,
    "creation_date": 1467612034,
    "last_edit_date": 1467621679,
    "question_id": 12575,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/12575/similarity-between-two-words",
    "title": "Similarity between two words",
    "body": "<p>I'm looking for a Python library that helps me identify the similarity between two words or sentences. </p>\n\n<p>I will be doing Audio to Text conversion which will result in an English dictionary or non dictionary word(s) ( This could be a Person or Company name) After that, I need to compare it to a known word or words. </p>\n\n<p>Example: </p>\n\n<p>1) Text to audio result: <strong>Thanks for calling America Expansion</strong> \nwill be compared to <strong>American Express</strong>. </p>\n\n<p>Both sentences are somehow similar but not the same. </p>\n\n<p>Looks like I may need to look into how many chars they share. Any ideas will be great. Looks a functionality like Google search \"did you mean\" feature.</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "deep-learning"
    ],
    "owner": {
      "account_id": 6085014,
      "reputation": 343,
      "user_id": 26955,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b5e82d9c261afa2e4ac8b9dae9502969?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "nnrales",
      "link": "https://datascience.stackexchange.com/users/26955/nnrales"
    },
    "is_answered": true,
    "view_count": 15653,
    "protected_date": 1590497775,
    "accepted_answer_id": 15667,
    "answer_count": 3,
    "score": 24,
    "last_activity_date": 1615489883,
    "creation_date": 1481334189,
    "last_edit_date": 1615489883,
    "question_id": 15656,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/15656/how-to-add-a-new-category-to-a-deep-learning-model",
    "title": "How to add a new category to a deep learning model?",
    "body": "<p>Say I have done transfer learning on a pre-trained network to recognize 10 objects. How can I add a <span class=\"math-container\">$11^{th}$</span> item that the network can classify without losing all the 10 categories I already trained and the information from the original pre-trained model? A friend told me that active research is going on in this field, but I cannot find any relevant papers or a names  to search for?</p>\n"
  },
  {
    "tags": [
      "keras"
    ],
    "owner": {
      "account_id": 2026138,
      "reputation": 501,
      "user_id": 36935,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/17347e7edbad326a47aea734d98ff45d?s=256&d=identicon&r=PG",
      "display_name": "aweeeezy",
      "link": "https://datascience.stackexchange.com/users/36935/aweeeezy"
    },
    "is_answered": true,
    "view_count": 29433,
    "answer_count": 6,
    "score": 24,
    "last_activity_date": 1615489050,
    "creation_date": 1501125621,
    "question_id": 21734,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/21734/keras-transfer-learning-changing-input-tensor-shape",
    "title": "Keras -- Transfer learning -- changing Input tensor shape",
    "body": "<p><a href=\"https://stackoverflow.com/questions/42187425/how-to-change-input-shape-in-sequential-model-in-keras\">This post</a> seems to indicate that what I want to accomplish is not possible. However, I'm not convinced of this -- given what I've already done, I don't see why what I want to do can not be achieved...</p>\n\n<p>I have two image datasets where one has images of shape (480, 720, 3) while the other has images of shape (540, 960, 3).</p>\n\n<p>I initialized a model using the following code:</p>\n\n<pre><code>input = Input(shape=(480, 720, 3), name='image_input')\n\ninitial_model = VGG16(weights='imagenet', include_top=False)\n\nfor layer in initial_model.layers:\n    layer.trainable = False\n\nx = Flatten()(initial_model(input))\nx = Dense(1000, activation='relu')(x)\nx = BatchNormalization()(x)\nx = Dropout(0.5)(x)\nx = Dense(1000, activation='relu')(x)\nx = BatchNormalization()(x)\nx = Dropout(0.5)(x)\nx = Dense(14, activation='linear')(x)\n\nmodel = Model(inputs=input, outputs=x)\nmodel.compile(loss='mse', optimizer='adam', metrics=['mae'])\n</code></pre>\n\n<p>Now that I've trained this model on the former dataset, I'd like to pop the input tensor layer off and prepend the model with a new input tensor with a shape that matches the image dimensions of the latter dataset.</p>\n\n<pre><code>model = load_model('path/to/my/trained/model.h5')\nold_input = model.pop(0)\nnew_input = Input(shape=(540, 960, 3), name='image_input')\nx = model(new_input)\nm = Model(inputs=new_input, outputs=x)\nm.save('transfer_model.h5')\n</code></pre>\n\n<p>which yields this error:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/home/aicg2/.local/lib/python2.7/site-packages/keras/engine/topology.py\", line 2506, in save\n    save_model(self, filepath, overwrite, include_optimizer)\n  File \"/home/aicg2/.local/lib/python2.7/site-packages/keras/models.py\", line 106, in save_model\n    'config': model.get_config()\n  File \"/home/aicg2/.local/lib/python2.7/site-packages/keras/engine/topology.py\", line 2322, in get_config\n    layer_config = layer.get_config()\n  File \"/home/aicg2/.local/lib/python2.7/site-packages/keras/engine/topology.py\", line 2370, in get_config\n    new_node_index = node_conversion_map[node_key]\nKeyError: u'image_input_ib-0'\n</code></pre>\n\n<p>In the post that I linked, maz states that there is a dimension mismatch that prevents changing the input layer of a model -- if this was the case, then how is that I put a (480, 720, 3) input layer in front of the VGG16 model which expects (224, 224, 3) images? </p>\n\n<p>I think a more likely issue is that my former model's output is expecting something different than what I'm giving it based on what fchollet is saying in <a href=\"https://github.com/fchollet/keras/issues/2790\" rel=\"noreferrer\">this post</a>. I'm syntactically confused, but I believe the whole <code>x = Layer()(x)</code> segment is constructing the layer piece by piece from input->output and simply throwing a different input in front is breaking it.</p>\n\n<p>I really have no idea though...</p>\n\n<p>Can somebody please enlighten me how to accomplish what I'm trying to do or, if it's not possible, explain to me why not?</p>\n"
  },
  {
    "tags": [
      "python",
      "logistic-regression",
      "cost-function"
    ],
    "owner": {
      "account_id": 3039464,
      "reputation": 353,
      "user_id": 38306,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/ShO0a.jpg?s=256",
      "display_name": "GhostRider",
      "link": "https://datascience.stackexchange.com/users/38306/ghostrider"
    },
    "is_answered": true,
    "view_count": 41561,
    "accepted_answer_id": 22472,
    "answer_count": 3,
    "score": 24,
    "last_activity_date": 1623005171,
    "creation_date": 1503394263,
    "last_edit_date": 1615494268,
    "question_id": 22470,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/22470/python-implementation-of-cost-function-in-logistic-regression-why-dot-multiplic",
    "title": "Python implementation of cost function in logistic regression: why dot multiplication in one expression but element-wise multiplication in another",
    "body": "<p>I have a very basic question which relates to Python, numpy and multiplication of matrices in the setting of logistic regression.</p>\n<p>First, let me apologise for not using math notation.</p>\n<p>I am confused about the use of matrix dot multiplication versus element wise pultiplication. The cost function is given by:</p>\n<p><span class=\"math-container\">$J = - {1\\over m} \\sum_{i=1}^m y^{(i)}log(a^{(i)})+(1 - y^{(i)})log(1-a^{(i)})$</span></p>\n<p>And in python I have written this as</p>\n<pre><code>    cost = -1/m * np.sum(Y * np.log(A) + (1-Y) * (np.log(1-A)))\n</code></pre>\n<p>But for example this expression (the first one - the derivative of J with respect to w)</p>\n<p><span class=\"math-container\">${\\partial J \\over{\\partial w}} = {1 \\over{m}} X(A-Y)^T$</span></p>\n<p><span class=\"math-container\">${\\partial J\\over{\\partial b}} = {1\\over{m}} \\sum \\limits_{i = 1}^m (a^{(i)}-y^{(i)})$</span></p>\n<p>is</p>\n<pre><code>   dw = 1/m * np.dot(X, dz.T)\n</code></pre>\n<p>I don't understand why it is correct to use dot multiplication in the above, but use element wise multiplication in the cost function i.e why not:</p>\n<pre><code>   cost = -1/m * np.sum(np.dot(Y,np.log(A)) + np.dot(1-Y, np.log(1-A)))\n</code></pre>\n<p>I fully get that this is not elaborately explained but I am guessing that the question is so simple that anyone with even basic logistic regression experience will understand my problem.</p>\n"
  },
  {
    "tags": [
      "decision-trees",
      "xgboost"
    ],
    "owner": {
      "account_id": 10896976,
      "reputation": 463,
      "user_id": 32230,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/307cc9aff09a268fa984093fba022eaf?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "kkk",
      "link": "https://datascience.stackexchange.com/users/32230/kkk"
    },
    "is_answered": true,
    "view_count": 20487,
    "accepted_answer_id": 26950,
    "answer_count": 1,
    "score": 24,
    "last_activity_date": 1656004439,
    "creation_date": 1516122277,
    "last_edit_date": 1516184379,
    "question_id": 26699,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/26699/decision-trees-leaf-wise-best-first-and-level-wise-tree-traverse",
    "title": "Decision trees: leaf-wise (best-first) and level-wise tree traverse",
    "body": "<p><strong>Issue 1:</strong></p>\n\n<p>I am confused by the <a href=\"https://github.com/Microsoft/LightGBM/blob/master/docs/Features.rst#references\" rel=\"noreferrer\">description of LightGBM</a> regarding the way the tree is expanded. </p>\n\n<p>They state:</p>\n\n<blockquote>\n  <p>Most decision tree learning algorithms grow tree by level\n  (depth)-wise, like the following image:</p>\n</blockquote>\n\n<p><a href=\"https://i.sstatic.net/e1FWe.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/e1FWe.png\" alt=\"enter image description here\"></a></p>\n\n<p><strong>Questions 1</strong>: Which \"most\" algorithms are implemented this way? \nAs far as I know C4.5 and CART use DFS. XGBoost uses BFS. Which other algorithms or packages use BFS for decision trees?</p>\n\n<p><strong>Issue 2:</strong></p>\n\n<p>LightGBM states:</p>\n\n<blockquote>\n  <p>LightGBM grows tree by leaf-wise (best-first).It will choose the leaf with max delta loss to grow. When growing same\n  leaf, leaf-wise algorithm can reduce more loss than level-wise algorithm.</p>\n</blockquote>\n\n<p><a href=\"https://i.sstatic.net/YOE9y.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/YOE9y.png\" alt=\"enter image description here\"></a></p>\n\n<p><strong>Question 2</strong>: Is it correct to say that level-wise growth trees will have equal depth for all leaves? </p>\n\n<p><strong>Questions 3:</strong> If Question 2 is not correct, then the trees from level-wise and leaf-wise growth will look the same at the end of the traversal (without pruning etc). Is it a correct statement?</p>\n\n<p><strong>Questions 4:</strong> If question 3 is correct, how can \"leaf-wise algorithm can reduce more loss than level-wise algorithm\"? Does it have to do with the post-pruning algorithm?</p>\n"
  },
  {
    "tags": [
      "lstm",
      "backpropagation",
      "mini-batch-gradient-descent"
    ],
    "owner": {
      "account_id": 4410342,
      "reputation": 2756,
      "user_id": 43077,
      "user_type": "registered",
      "accept_rate": 72,
      "profile_image": "https://i.sstatic.net/4IA0y.jpg?s=256",
      "display_name": "Kari",
      "link": "https://datascience.stackexchange.com/users/43077/kari"
    },
    "is_answered": true,
    "view_count": 19506,
    "accepted_answer_id": 27804,
    "answer_count": 2,
    "score": 24,
    "last_activity_date": 1685046659,
    "creation_date": 1518138644,
    "last_edit_date": 1685046659,
    "question_id": 27628,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/27628/sliding-window-leads-to-overfitting-in-lstm",
    "title": "Sliding window leads to overfitting in LSTM?",
    "body": "<p>Will I overfit my LSTM if I train it via the sliding-window approach? Why do people not seem to use it for LSTMs?</p>\n<p>For a simplified example, assume that we have to predict the sequence of characters:</p>\n<pre><code>A B C D E F G H I J K L M N O P Q R S T U V W X Y Z\n</code></pre>\n<p>Is it bad (or better?) if I keep training my LSTM with the following minibatches:</p>\n<pre><code>A B C D E F G H I J K L M N, backprop, erase the cell\n\nB C D E F G H I J K L M N O, backprop, erase the cell\n\n .... and so on, shifting by 1 every time?\n</code></pre>\n<p>Previously, I always trained it as:</p>\n<pre><code>A B C D E F G H I J K L M N,  backprop, erase the cell\n\nO P Q R S T U V W X Y Z,  backprop, erase the cell\n</code></pre>\n<hr />\n<p>Instead of shifting by one, would it be better to slide the window by 2 entries instead, etc? What would that mean (in terms of precision/overfitting)?</p>\n<hr />\n<p>Also, if I were to do the sliding-window approach in a Feed-forward network, would it result in overfitting? I would assume yes, because the network is exposed to the same information regions for a very long time. For example, it is exposed to <code>E F G H I J K</code> for a long time.</p>\n<hr />\n<p><strong>Edit:</strong></p>\n<p>Please remember that cell state is erased between training batches, so the LSTM will have a &quot;hammer to head&quot; at these times. <strong>It's unable to remember</strong> what was before OPQRSTUVWXYZ. This means that the LSTM is unable to ever learn that &quot;O&quot; follows the &quot;M&quot;.</p>\n<p>So, I thought (thus my entire question), why not to give it intermediate (overlapping) batch in between...and in that case why not use multiple overlapping minibatches - to me this would provide a smoother training? Ultimately, that would mean a sliding window for an LSTM.</p>\n<hr />\n<p>Some useful info I've found after answer was accepted:</p>\n<p><a href=\"http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/\" rel=\"nofollow noreferrer\">from here</a></p>\n<blockquote>\n<p>The first word of the English translation is probably highly correlated with the first word of the source sentence. But that means decoder has to consider information from 50 steps ago, and that information needs to be somehow encoded in the vector. Recurrent Neural Networks are known to have problems dealing with such long-range dependencies. In theory, architectures like LSTMs should be able to deal with this, but in practice long-range dependencies are still problematic.</p>\n<p>For example, researchers have found that reversing the source sequence\n(feeding it backwards into the encoder) produces significantly better\nresults because it shortens the path from the decoder to the relevant\nparts of the encoder. <strong>Similarly, feeding an input sequence twice also\nseems to help a network to better memorize things.</strong> For example, if one training example is &quot;John went home&quot;, you would give &quot;John went home John went home&quot; to the network as one input.</p>\n</blockquote>\n<p><strong>Edit after accepting the answer:</strong></p>\n<p>Several months after, I am more inclined to use the sliding window approach, as it uses the data better. But in that case you probably don't want to train BCDEFGHIJKLMNO right after ABCDEFGHIJKLMN.  Instead, shuffle your examples, to gradually and uniformly &quot;brush-in&quot; <strong>all</strong> of the information into your LSTM.  Give it HIJKLMNOPQRSTU after ABCDEFGHIJKLMNO etc.  That's directly related to Catastrophic Forgetting. As always, monitor the Validation and Test set closely, and stop as soon as you see their errors steadily increasing</p>\n<p>Also, the &quot;hammer to head&quot; issue can be improved, by using Synthetic Gradients. See its benefit here: (linked answer discusses its benefit of long sequences) <a href=\"https://datascience.stackexchange.com/a/32425/43077\">https://datascience.stackexchange.com/a/32425/43077</a></p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "tensorflow",
      "cnn"
    ],
    "owner": {
      "account_id": 13406489,
      "reputation": 497,
      "user_id": 51141,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ed109c79b895579f147b504eea4156ad?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Josh",
      "link": "https://datascience.stackexchange.com/users/51141/josh"
    },
    "is_answered": true,
    "view_count": 7158,
    "accepted_answer_id": 31394,
    "answer_count": 1,
    "score": 24,
    "last_activity_date": 1569649057,
    "creation_date": 1525781596,
    "last_edit_date": 1562248131,
    "question_id": 31388,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/31388/how-to-add-non-image-features-along-side-images-as-the-input-of-cnns",
    "title": "How to add non-image features along side images as the input of CNNs",
    "body": "<p>I'm training a convolutional neural network to classify images on fog conditions (3 classes). However, for each of about 150.000 images I also have four meteorological variables available that might help in predicting the classes of the images. I was wondering how I could add the meteorological variables (e.g. temperature, wind speed) to the existing CNN structure so that it can help in the classification.</p>\n\n<p>One way I can already think of is creating another (small) feedforward neural net alongside the CNN and then concatenating the outputs of the CNN layers and the hidden layers of the non-image neural net to each other at the dense layer.</p>\n\n<p>The second way I could think of is just contacting these features to the dense layer. However, in this case, the non-image variables will (I think) only be able to make linear predictions. </p>\n\n<p>Are there any other (better) ways that the non-image features could be included in the model? And what would be the advisable method considering the amount of data I have? </p>\n\n<p>Another question I have is whether or not I should unfreeze the convolutional layers while training with these non-image features? These layers of a Resnet-18 (which were initialized as pre-trained on ImageNet) have already been fine-tuned using the images. My guess is that I should keep them frozen and only unfreeze the dense layer since it is only here that the non-image features come into 'contact' with the image features (not earlier in the CNN). If I'm wrong on this, please say so!</p>\n"
  },
  {
    "tags": [
      "python",
      "time-series",
      "anomaly-detection",
      "bayesian-networks",
      "anomaly"
    ],
    "owner": {
      "account_id": 13448965,
      "reputation": 1266,
      "user_id": 51209,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/3KYBdzFl.jpg?s=256",
      "display_name": "Ryan Ghorbandoost",
      "link": "https://datascience.stackexchange.com/users/51209/ryan-ghorbandoost"
    },
    "is_answered": true,
    "view_count": 24826,
    "accepted_answer_id": 39282,
    "answer_count": 4,
    "score": 24,
    "last_activity_date": 1625136194,
    "creation_date": 1527185962,
    "last_edit_date": 1608607007,
    "question_id": 32126,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/32126/looking-for-a-good-package-for-anomaly-detection-in-time-series",
    "title": "Looking for a good package for anomaly detection in time series",
    "body": "<p>Is there a comprehensive open source package (preferably in python or R) that can be used for anomaly detection in time series?</p>\n<p>There is a one class SVM package in scikit-learn but it is not for the time series data. Im looking for more sophisticated packages that, for example, use Bayesian networks for anomaly detection.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "neural-network",
      "deep-learning",
      "keras"
    ],
    "owner": {
      "account_id": 12563799,
      "reputation": 2485,
      "user_id": 50406,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Eq7j1.jpg?s=256",
      "display_name": "thanatoz",
      "link": "https://datascience.stackexchange.com/users/50406/thanatoz"
    },
    "is_answered": true,
    "view_count": 57029,
    "accepted_answer_id": 37379,
    "answer_count": 2,
    "score": 24,
    "last_activity_date": 1635785166,
    "creation_date": 1535085057,
    "last_edit_date": 1535104713,
    "question_id": 37378,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/37378/what-are-kernel-initializers-and-what-is-their-significance",
    "title": "What are kernel initializers and what is their significance?",
    "body": "<p>I was looking at code and found this:</p>\n\n<pre><code>model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n</code></pre>\n\n<p>I was keen to know about <code>kernel_initializer</code> but wasn't able to understand it's significance?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "keras",
      "dropout",
      "monte-carlo"
    ],
    "owner": {
      "account_id": 1723820,
      "reputation": 600,
      "user_id": 50863,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/e9v0y.jpg?s=256",
      "display_name": "Arka Mallick",
      "link": "https://datascience.stackexchange.com/users/50863/arka-mallick"
    },
    "is_answered": true,
    "view_count": 20187,
    "accepted_answer_id": 44069,
    "answer_count": 1,
    "score": 24,
    "last_activity_date": 1663512441,
    "creation_date": 1547604584,
    "last_edit_date": 1574697739,
    "question_id": 44065,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/44065/what-is-monte-carlo-dropout",
    "title": "What is Monte Carlo dropout?",
    "body": "<p>I understand how to use MC dropout from <a href=\"https://datascience.stackexchange.com/a/42868/10640\">this answer</a>, but I don't understand how MC dropout works, what its purpose is, and how it differs from normal dropout.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "gradient-descent",
      "backpropagation",
      "cost-function"
    ],
    "owner": {
      "account_id": 7250027,
      "reputation": 355,
      "user_id": 66626,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-DdYt0qYU58U/AAAAAAAAAAI/AAAAAAAAAJg/DrlNhir0g3g/s256-rj/photo.jpg",
      "display_name": "Mohamed Mahyoub",
      "link": "https://datascience.stackexchange.com/users/66626/mohamed-mahyoub"
    },
    "is_answered": true,
    "view_count": 26813,
    "accepted_answer_id": 44709,
    "answer_count": 1,
    "score": 24,
    "last_activity_date": 1586371262,
    "creation_date": 1548682481,
    "last_edit_date": 1586371262,
    "question_id": 44703,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/44703/how-does-gradient-descent-and-backpropagation-work-together",
    "title": "How does Gradient Descent and Backpropagation work together?",
    "body": "<p>Please forgive me as I am new to this. I have attached a diagram trying to model my understanding of neural network and Back-propagation? From videos on Coursera and resources online I formed the following understanding of how neural network works:</p>\n\n<ol>\n<li>Input is given, which gets weight assigned to it using a probability distribution.</li>\n<li>The activation functions use the weights to provide the predicted value.</li>\n<li>The cost or loss functions calculate the error of the prediction between the actual class and the predicted value.</li>\n<li>The optimization functions such as gradient descent use the results of the cost function to minimize the error.</li>\n</ol>\n\n<p>If the above is correct then I am struggling to understand the connection between Gradient Descent and Backpropagation?</p>\n\n<p>Here is an image of my understanding so far: <img src=\"https://i.sstatic.net/7Ui1C.png\"></p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "xgboost",
      "kaggle",
      "lightgbm",
      "catboost"
    ],
    "owner": {
      "account_id": 11245218,
      "reputation": 6136,
      "user_id": 50727,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/9Y5b3.jpg?s=256",
      "display_name": "David Masip",
      "link": "https://datascience.stackexchange.com/users/50727/david-masip"
    },
    "is_answered": true,
    "view_count": 20195,
    "accepted_answer_id": 49616,
    "answer_count": 1,
    "score": 24,
    "last_activity_date": 1635432734,
    "creation_date": 1555654096,
    "last_edit_date": 1635432734,
    "question_id": 49567,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/49567/lightgbm-vs-xgboost-vs-catboost",
    "title": "Lightgbm vs xgboost vs catboost",
    "body": "<p>I've seen that in Kaggle competitions people are using lightgbms where they used to use xgboost. My question is: when would you rather use xgboost instead of lightgbm? What about catboost? </p>\n"
  },
  {
    "tags": [
      "scikit-learn",
      "pandas"
    ],
    "owner": {
      "account_id": 5996420,
      "reputation": 405,
      "user_id": 70243,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/SZviV.jpg?s=256",
      "display_name": "radioactive",
      "link": "https://datascience.stackexchange.com/users/70243/radioactive"
    },
    "is_answered": true,
    "view_count": 101287,
    "accepted_answer_id": 71805,
    "answer_count": 4,
    "score": 24,
    "last_activity_date": 1667496622,
    "creation_date": 1586118117,
    "question_id": 71804,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/71804/how-to-perform-one-hot-encoding-on-multiple-categorical-columns",
    "title": "How to perform one hot encoding on multiple categorical columns",
    "body": "<p>I am trying to perform one-hot encoding on some categorical columns. From the tutorial I am following, I am supposed to do LabelEncoding before One hot encoding. I have successfully performed the labelencoding as shown below</p>\n\n<pre><code>#categorical data\ncategorical_cols = ['a', 'b', 'c', 'd'] \nfrom sklearn.preprocessing import LabelEncoder\n# instantiate labelencoder object\nle = LabelEncoder()\n# apply le on categorical feature columns\ndata[categorical_cols] = data[categorical_cols].apply(lambda col: le.fit_transform(col))\n</code></pre>\n\n<p>Now I am stuck with how to perform one hot encoding and then join the encoded columns to the dataframe (data). </p>\n\n<p>Please how do I do this?</p>\n"
  },
  {
    "tags": [
      "nlp",
      "metadata",
      "data-cleaning",
      "text-mining"
    ],
    "owner": {
      "account_id": 210958,
      "reputation": 1393,
      "user_id": 227,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/bf9facac0cf9c57c1c4d1e02c63ab6f5?s=256&d=identicon&r=PG",
      "display_name": "Amir Ali Akbari",
      "link": "https://datascience.stackexchange.com/users/227/amir-ali-akbari"
    },
    "is_answered": true,
    "view_count": 3806,
    "answer_count": 5,
    "score": 23,
    "last_activity_date": 1593695040,
    "creation_date": 1401394276,
    "question_id": 223,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/223/how-to-annotate-text-documents-with-meta-data",
    "title": "How to annotate text documents with meta-data?",
    "body": "<p>Having a lot of text documents (in natural language, unstructured), what are the possible ways of annotating them with some semantic meta-data? For example, consider a short document:</p>\n\n<pre><code>I saw the company's manager last day.\n</code></pre>\n\n<p>To be able to extract information from it, it must be annotated with additional data to be less ambiguous. The process of finding such meta-data is not in question, so assume it is done manually. The question is how to store these data in a way that further analysis on it can be done more conveniently/efficiently?</p>\n\n<p>A possible approach is to use XML tags (see below), but it seems too verbose, and maybe there are better approaches/guidelines for storing such meta-data on text documents.</p>\n\n<pre><code>&lt;Person name=\"John\"&gt;I&lt;/Person&gt; saw the &lt;Organization name=\"ACME\"&gt;company&lt;/Organization&gt;'s\nmanager &lt;Time value=\"2014-5-29\"&gt;last day&lt;/Time&gt;.\n</code></pre>\n"
  },
  {
    "tags": [
      "nlp",
      "text-mining",
      "freebase"
    ],
    "owner": {
      "account_id": 1573122,
      "reputation": 587,
      "user_id": 906,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d95a1f39b50e6554948f926eefbe5a88?s=256&d=identicon&r=PG",
      "display_name": "nassimhddd",
      "link": "https://datascience.stackexchange.com/users/906/nassimhddd"
    },
    "is_answered": true,
    "view_count": 6207,
    "accepted_answer_id": 440,
    "answer_count": 3,
    "score": 23,
    "last_activity_date": 1596730685,
    "creation_date": 1402985139,
    "last_edit_date": 1403156923,
    "question_id": 424,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/424/how-to-grow-a-list-of-related-words-based-on-initial-keywords",
    "title": "How to grow a list of related words based on initial keywords?",
    "body": "<p>I recently saw a cool feature that <a href=\"https://support.google.com/docs/answer/3543688?hl=en\">was once available</a> in Google Sheets: you start by writing a few related keywords in consecutive cells, say: \"blue\", \"green\", \"yellow\", and it automatically generates similar keywords (in this case, other colors). See more examples in <a href=\"http://youtu.be/dlslNhfrQmw\">this YouTube video</a>.</p>\n\n<p>I would like to reproduce this in my own program. I'm thinking of using Freebase, and it would work like this intuitively: </p>\n\n<ol>\n<li>Retrieve the list of given words in Freebase;</li>\n<li>Find their \"common denominator(s)\" and construct a distance metric based on this;</li>\n<li>Rank other concepts based on their \"distance\" to the original keywords;</li>\n<li>Display the next closest concepts.</li>\n</ol>\n\n<p>As I'm not familiar with this area, my questions are:</p>\n\n<ul>\n<li>Is there a better way to do this?</li>\n<li>What tools are available for each step?</li>\n</ul>\n"
  },
  {
    "tags": [
      "algorithms",
      "logistic-regression"
    ],
    "owner": {
      "account_id": 3340621,
      "reputation": 526,
      "user_id": 922,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/nIpt2.jpg?s=256",
      "display_name": "joews",
      "link": "https://datascience.stackexchange.com/users/922/joews"
    },
    "is_answered": true,
    "view_count": 14129,
    "accepted_answer_id": 484,
    "answer_count": 4,
    "score": 23,
    "last_activity_date": 1615493273,
    "creation_date": 1403168206,
    "last_edit_date": 1403175338,
    "question_id": 473,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/473/is-logistic-regression-actually-a-regression-algorithm",
    "title": "Is logistic regression actually a regression algorithm?",
    "body": "<p>The usual definition of regression (as far as I am aware) is <em>predicting a continuous output variable from a given set of input variables</em>. </p>\n\n<p>Logistic regression is a binary classification algorithm, so it produces a categorical output.</p>\n\n<p>Is it really a regression algorithm? If so, why?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-mining",
      "statistics"
    ],
    "owner": {
      "account_id": 1636186,
      "reputation": 333,
      "user_id": 1047,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/6E33O.jpg?s=256",
      "display_name": "Peter Kirby",
      "link": "https://datascience.stackexchange.com/users/1047/peter-kirby"
    },
    "is_answered": true,
    "view_count": 976,
    "accepted_answer_id": 506,
    "answer_count": 4,
    "score": 23,
    "last_activity_date": 1550230240,
    "creation_date": 1403234339,
    "last_edit_date": 1413979653,
    "question_id": 497,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/497/what-statistical-model-should-i-use-to-analyze-the-likelihood-that-a-single-even",
    "title": "What statistical model should I use to analyze the likelihood that a single event influenced longitudinal data",
    "body": "<p>I am trying to find a formula, method, or model to use to analyze the likelihood that a specific event influenced some longitudinal data. I am having difficultly figuring out what to search for on Google.</p>\n\n<p>Here is an example scenario:</p>\n\n<p>Image you own a business that has an average of 100 walk-in customers every day. One day, you decide you want to increase the number of walk-in customers arriving at your store each day, so you pull a crazy stunt outside your store to get attention. Over the next week, you see on average 125 customers a day.</p>\n\n<p>Over the next few months, you again decide that you want to get some more business, and perhaps sustain it a bit longer, so you try some other random things to get more customers in your store. Unfortunately, you are not the best marketer, and some of your tactics have little or no effect, and others even have a negative impact.</p>\n\n<p>What methodology could I use to determine the probability that any one individual event positively or negatively impacted the number of walk-in customers? I am fully aware that correlation does not necessarily equal causation, but what methods could I use to determine the likely increase or decrease in your business's daily walk in client's following a specific event?</p>\n\n<p>I am not interested in analyzing whether or not there is a correlation between your attempts to increase the number of walk-in customers, but rather whether or not any one single event, independent of all others, was impactful.</p>\n\n<p>I realize that this example is rather contrived and simplistic, so I will also give you a brief description of the actual data that I am using:</p>\n\n<p>I am attempting to determine the impact that a particular marketing agency has on their client's website when they publish new content, perform social media campaigns, etc. For any one specific agency, they may have anywhere from 1 to 500 clients. Each client has websites ranging in size from 5 pages to well over 1 million. Over the course of the past 5 year, each agency has annotated all of their work for each client, including the type of work that was done, the number of webpages on a website that were influenced, the number of hours spent, etc.</p>\n\n<p>Using the above data, which I have assembled into a data warehouse (placed into a bunch of star/snowflake schemas), I need to determine how likely it was that any one piece of work (any one event in time) had an impact on the traffic hitting any/all pages influenced by a specific piece of work. I have created models for 40 different types of content that are found on a website that describes the typical traffic pattern a page with said content type might experience from launch date until present. Normalized relative to the appropriate model, I need to determine the highest and lowest number of increased or decreased visitors a specific page received as the result of a specific piece of work.</p>\n\n<p>While I have experience with basic data analysis (linear and multiple regression, correlation, etc), I am at a loss for how to approach solving this problem. Whereas in the past I have typically analyzed data with multiple measurements for a given axis (for example temperature vs thirst vs animal and determined the impact on thirst that increased temperate has across animals), I feel that above, I am attempting to analyze the impact of a single event at some point in time for a non-linear, but predictable (or at least model-able), longitudinal dataset. I am stumped :(</p>\n\n<p>Any help, tips, pointers, recommendations, or directions would be extremely helpful and I would be eternally grateful!</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "distributed",
      "map-reduce",
      "dimensionality-reduction"
    ],
    "owner": {
      "account_id": 413927,
      "reputation": 451,
      "user_id": 1242,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/vl4wT.png?s=256",
      "display_name": "cjauvin",
      "link": "https://datascience.stackexchange.com/users/1242/cjauvin"
    },
    "is_answered": true,
    "view_count": 8770,
    "answer_count": 3,
    "score": 23,
    "last_activity_date": 1499690790,
    "creation_date": 1407977451,
    "question_id": 974,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/974/nearest-neighbors-search-for-very-high-dimensional-data",
    "title": "Nearest neighbors search for very high dimensional data",
    "body": "<p>I have a big sparse matrix of users and items they like (in the order of 1M users and 100K items, with a very low level of sparsity). I'm exploring ways in which I could perform kNN search on it. Given the size of my dataset and some initial tests I performed, my assumption is that the method I will use will need to be either parallel or distributed. So I'm considering two classes of possible solutions: one that is either available (or implementable in a reasonably easy way) on a single multicore machine, the other on a Spark cluster, i.e. as a MapReduce program. Here are three broad ideas that I considered:</p>\n\n<ul>\n<li>Assuming a cosine similarity metric, perform the full multiplication of the normalized matrix by its transpose (implemented as a sum of outer products)</li>\n<li>Using locality-sensitive hashing (LSH)</li>\n<li>Reducing first the dimensionality of the problem with a PCA</li>\n</ul>\n\n<p>I'd appreciate any thoughts or advices about possible other ways in which I could tackle this problem.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "svm"
    ],
    "owner": {
      "account_id": 4700920,
      "reputation": 339,
      "user_id": 8073,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/EnmOh.jpg?s=256",
      "display_name": "Chong Zheng",
      "link": "https://datascience.stackexchange.com/users/8073/chong-zheng"
    },
    "is_answered": true,
    "view_count": 19005,
    "answer_count": 4,
    "score": 23,
    "last_activity_date": 1661098325,
    "creation_date": 1433860744,
    "last_edit_date": 1433936757,
    "question_id": 6054,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/6054/in-svm-algorithm-why-vector-w-is-orthogonal-to-the-separating-hyperplane",
    "title": "In SVM Algorithm, why vector w is orthogonal to the separating hyperplane?",
    "body": "<p>I am a beginner on Machine Learning.\nIn SVM, the separating hyperplane is defined as $y = w^T x + b$.\nWhy we say vector $w$ orthogonal to the separating hyperplane?</p>\n"
  },
  {
    "tags": [
      "community"
    ],
    "owner": {
      "account_id": 6444670,
      "reputation": 8476,
      "user_id": 11097,
      "user_type": "registered",
      "accept_rate": 75,
      "profile_image": "https://i.sstatic.net/z0jss.png?s=256",
      "display_name": "Dawny33",
      "link": "https://datascience.stackexchange.com/users/11097/dawny33"
    },
    "is_answered": true,
    "view_count": 3690,
    "accepted_answer_id": 8945,
    "answer_count": 14,
    "community_owned_date": 1460629379,
    "score": 23,
    "last_activity_date": 1506646273,
    "creation_date": 1448018463,
    "last_edit_date": 1492087460,
    "question_id": 8941,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/8941/data-science-conferences",
    "title": "Data Science conferences?",
    "body": "<p>This is a similar question like the <a href=\"https://stats.stackexchange.com/questions/1904/statistics-conferences\">Statistics Conferences question at CrossValidated</a></p>\n\n<p>What are the most significant annual Data Science conferences?</p>\n\n<p>Rules:</p>\n\n<ul>\n<li>Include a link to the conference</li>\n<li>Please include links for the talks (be it youtube, the conference site or some other video streaming site)</li>\n</ul>\n"
  },
  {
    "tags": [
      "nlp",
      "named-entity-recognition"
    ],
    "owner": {
      "account_id": 2704121,
      "reputation": 388,
      "user_id": 15735,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/X0SR8.jpg?s=256",
      "display_name": "AbtPst",
      "link": "https://datascience.stackexchange.com/users/15735/abtpst"
    },
    "is_answered": true,
    "view_count": 15015,
    "accepted_answer_id": 9967,
    "answer_count": 2,
    "score": 23,
    "last_activity_date": 1560098791,
    "creation_date": 1453747284,
    "last_edit_date": 1560098791,
    "question_id": 9950,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/9950/nlp-is-gazetteer-a-cheat",
    "title": "NLP - Is Gazetteer a cheat?",
    "body": "<p>In NLP, there is the concept of <code>Gazetteer</code> which can be quite useful for creating annotations. As far as I understand: </p>\n\n<blockquote>\n  <p>A gazetteer consists of a set of lists containing names of entities\n  such as cities, organisations, days of the week, etc. These lists are\n  used to nd occurrences of these names in text, e.g. for the task of\n  named entity recognition.</p>\n</blockquote>\n\n<p>So it is essentially a lookup. Isn't this kind of a cheat? If we use a <code>Gazetteer</code> for detecting named entities, then there is not much <code>Natural Language Processing</code> going on. Ideally, I would want to detect named entities using <code>NLP</code> techniques. Otherwise, how is it any better than a regex pattern matcher?</p>\n"
  },
  {
    "tags": [
      "image-classification",
      "computer-vision",
      "convolutional-neural-network",
      "inception"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 32398,
    "answer_count": 4,
    "score": 23,
    "last_activity_date": 1613489368,
    "creation_date": 1479985808,
    "last_edit_date": 1613489368,
    "question_id": 15328,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/15328/what-is-the-difference-between-inception-v2-and-inception-v3",
    "title": "What is the difference between Inception v2 and Inception v3?",
    "body": "<p>The paper <a href=\"https://arxiv.org/pdf/1409.4842v1.pdf\" rel=\"noreferrer\">Going deeper with convolutions</a> describes GoogleNet which contains the original inception modules:</p>\n\n<p><a href=\"https://i.sstatic.net/zTinD.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/zTinD.png\" alt=\"enter image description here\"></a></p>\n\n<p>The change to inception v2 was that they replaced the 5x5 convolutions by two successive 3x3 convolutions and applied pooling:</p>\n\n<p><a href=\"https://i.sstatic.net/eindA.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/eindA.png\" alt=\"enter image description here\"></a></p>\n\n<p>What is the difference between Inception v2 and Inception v3?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "gradient-descent",
      "activation-function"
    ],
    "owner": {
      "account_id": 6631713,
      "reputation": 14308,
      "user_id": 28175,
      "user_type": "registered",
      "accept_rate": 95,
      "profile_image": "https://i.sstatic.net/ojtVl.jpg?s=256",
      "display_name": "Green Falcon",
      "link": "https://datascience.stackexchange.com/users/28175/green-falcon"
    },
    "is_answered": true,
    "view_count": 39485,
    "accepted_answer_id": 23502,
    "answer_count": 2,
    "score": 23,
    "last_activity_date": 1655174559,
    "creation_date": 1507040229,
    "last_edit_date": 1514369581,
    "question_id": 23493,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/23493/why-relu-is-better-than-the-other-activation-functions",
    "title": "Why ReLU is better than the other activation functions",
    "body": "<p><a href=\"https://datascience.stackexchange.com/questions/15484/sigmoid-vs-relu-function-in-convnets\">Here</a> the answer refers to vanishing and exploding gradients that has been in <code>sigmoid</code>-like activation functions but, I guess, <code>Relu</code> has a disadvantage and it is its expected value. there is no limitation for the output of the <code>Relu</code> and so its expected value is not zero. I remember the time before the popularity of <code>Relu</code> that <code>tanh</code> was the most popular amongst machine learning experts rather than <code>sigmoid</code>. The reason was that the expected value of the <code>tanh</code> was equal to zero and and it helped learning in deeper layers to be more rapid in a neural net. <code>Relu</code> does not have this characteristic, but why it is working so good if we put its derivative advantage aside. Moreover, I guess the derivative also may be affected. Because the activations (output of <code>Relu</code>) are involved for calculating the update rules.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "dataset",
      "colab"
    ],
    "owner": {
      "account_id": 7861124,
      "reputation": 341,
      "user_id": 43878,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/tJNog.jpg?s=256",
      "display_name": "chatbot_chakra",
      "link": "https://datascience.stackexchange.com/users/43878/chatbot-chakra"
    },
    "is_answered": true,
    "view_count": 133927,
    "protected_date": 1602430269,
    "answer_count": 6,
    "score": 23,
    "last_activity_date": 1602597226,
    "creation_date": 1521831148,
    "last_edit_date": 1521869814,
    "question_id": 29480,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/29480/uploading-images-folder-from-my-system-into-google-colab",
    "title": "Uploading images folder from my system into Google Colab",
    "body": "<p>I want to train a deep learning model on a dataset containing around 3000 images. Since the dataset is huge, I want to use Google colab since it's GPU supported. How do I upload this full image folder into my notebook and use it?</p>\n"
  },
  {
    "tags": [
      "scikit-learn",
      "hyperparameter-tuning",
      "mlp"
    ],
    "owner": {
      "account_id": 14014190,
      "reputation": 347,
      "user_id": 56593,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/459373447865766/picture?type=large",
      "display_name": "Irving.ren",
      "link": "https://datascience.stackexchange.com/users/56593/irving-ren"
    },
    "is_answered": true,
    "view_count": 102767,
    "accepted_answer_id": 36087,
    "answer_count": 2,
    "score": 23,
    "last_activity_date": 1665392886,
    "creation_date": 1532607849,
    "last_edit_date": 1532702420,
    "question_id": 36049,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/36049/how-to-adjust-the-hyperparameters-of-mlp-classifier-to-get-more-perfect-performa",
    "title": "How to adjust the hyperparameters of MLP classifier to get more perfect performance",
    "body": "<p>I am just getting touch with Multi-layer Perceptron. And, I got this accuracy when classifying the DEAP data with MLP. However, I have no idea how to adjust the hyperparameters for improving the result. </p>\n\n<p>Here is the detail of my code and result:</p>\n\n<p><a href=\"https://i.sstatic.net/nD3Ow.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/nD3Ow.png\" alt=\"enter image description here\"></a>.</p>\n\n<pre><code>from sklearn.neural_network import MLPClassifier\n\nimport numpy as np\nimport scipy.io\nx_vals = data['all_data'][:,0:320]\n\ny_vals_new = np.array([0 if each=='Neg'  else 1 if each =='Neu' else 2 for each in data['all_data'][:,320]])\ny_vals_Arousal = np.array([3 if each=='Pas'  else 4 if each =='Neu' else 5 for each in data['all_data'][:,321]])\n\nDEAP_x_train = x_vals[:-256]        #using 80% of whole data for training\nDEAP_x_test = x_vals[-256:]         #using 20% of whole data for testing\nDEAP_y_train = y_vals_new[:-256]     ##Valence\nDEAP_y_test = y_vals_new[-256:]\nDEAP_y_train_A = y_vals_Arousal[:-256]   ### Arousal\nDEAP_y_test_A = y_vals_Arousal[-256:]\n\nmlp = MLPClassifier(solver='adam', activation='relu',alpha=1e-4,hidden_layer_sizes=(50,50,50), random_state=1,max_iter=11,verbose=10,learning_rate_init=.1)\n\nmlp.fit(DEAP_x_train, DEAP_y_train)\n\nprint (mlp.score(DEAP_x_test,DEAP_y_test))\nprint (mlp.n_layers_)\nprint (mlp.n_iter_)\nprint (mlp.loss_)\n</code></pre>\n"
  },
  {
    "tags": [
      "machine-learning",
      "variance"
    ],
    "owner": {
      "account_id": 10782040,
      "reputation": 1293,
      "user_id": 44083,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/pGLFq.jpg?s=256",
      "display_name": "Sociopath",
      "link": "https://datascience.stackexchange.com/users/44083/sociopath"
    },
    "is_answered": true,
    "view_count": 28568,
    "protected_date": 1599877304,
    "accepted_answer_id": 37350,
    "answer_count": 3,
    "score": 23,
    "last_activity_date": 1608457152,
    "creation_date": 1535016866,
    "question_id": 37345,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/37345/what-is-the-meaning-of-term-variance-in-machine-learning-model",
    "title": "What is the meaning of term Variance in Machine Learning Model?",
    "body": "<p>I am familiar with terms high bias and high variance and their effect on the model.</p>\n\n<p>Basically your model has high variance when it is too complex and sensitive too even outliers.</p>\n\n<p>But recently I was asked the meaning of term <strong>Variance</strong> in machine learning model in one of the interview?</p>\n\n<p>I would like to know what exactly Variance means in ML Model and how does it get introduce in your model? I would really appreciate if someone could explain this with an example.</p>\n"
  },
  {
    "tags": [
      "classification",
      "scikit-learn",
      "metric"
    ],
    "owner": {
      "account_id": 5798358,
      "reputation": 355,
      "user_id": 57028,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/3JaHl.jpg?s=256",
      "display_name": "Fractale",
      "link": "https://datascience.stackexchange.com/users/57028/fractale"
    },
    "is_answered": true,
    "view_count": 26045,
    "accepted_answer_id": 40904,
    "answer_count": 1,
    "score": 23,
    "last_activity_date": 1615677701,
    "creation_date": 1541658348,
    "last_edit_date": 1615677701,
    "question_id": 40900,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/40900/whats-the-difference-between-sklearn-f1-score-micro-and-weighted-for-a-mult",
    "title": "What&#39;s the difference between Sklearn F1 score &#39;micro&#39; and &#39;weighted&#39; for a multi class classification problem?",
    "body": "<p>I have a multi-class classification problem with class imbalance. I searched for the best metric to evaluate my model. Scikit-learn has multiple ways of calculating the F1 score. I would like to understand the differences.</p>\n<p>What do you recommending when there is a class imbalance?</p>\n"
  },
  {
    "tags": [
      "python",
      "dataset",
      "preprocessing",
      "pytorch"
    ],
    "owner": {
      "account_id": 14442246,
      "reputation": 361,
      "user_id": 59855,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/bzMaf.jpg?s=256",
      "display_name": "Amarnath",
      "link": "https://datascience.stackexchange.com/users/59855/amarnath"
    },
    "is_answered": true,
    "view_count": 50051,
    "answer_count": 2,
    "score": 23,
    "last_activity_date": 1573272215,
    "creation_date": 1550697225,
    "last_edit_date": 1552529450,
    "question_id": 45916,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/45916/loading-own-train-data-and-labels-in-dataloader-using-pytorch",
    "title": "Loading own train data and labels in dataloader using pytorch?",
    "body": "<p>I have x_data and labels separately. How can I combine and load them in the model using <code>torch.utils.data.DataLoader</code>?</p>\n\n<p>I have a dataset that I created and the training data has 20k samples and the labels are also separate. Lets say I want to load a dataset in the model, shuffle each time and use the batch size that I prefer. The Dataloader function does that. How can I combine and put them in the function so that I can train it in the model in pytorch?</p>\n"
  },
  {
    "tags": [
      "image-classification",
      "image-recognition"
    ],
    "owner": {
      "account_id": 4135902,
      "reputation": 1187,
      "user_id": 56922,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/uTWkH.jpg?s=256",
      "display_name": "Hunar",
      "link": "https://datascience.stackexchange.com/users/56922/hunar"
    },
    "is_answered": true,
    "view_count": 3731,
    "answer_count": 8,
    "score": 23,
    "last_activity_date": 1634053897,
    "creation_date": 1558597713,
    "last_edit_date": 1608111716,
    "question_id": 52445,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/52445/rationale-behind-most-published-works-in-medical-imaging-trying-to-reduce-false",
    "title": "Rationale behind most published works in medical imaging trying to reduce false positives",
    "body": "<p>In medical image processing, most of the published works try to reduce false positive rate (FPR) while in reality, false negatives are more dangerous than false positives. What is the rationale behind it?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "mathematics",
      "policy-gradients"
    ],
    "owner": {
      "account_id": 7755332,
      "reputation": 280,
      "user_id": 75812,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/O5ksA.jpg?s=256",
      "display_name": "Markus Peschl",
      "link": "https://datascience.stackexchange.com/users/75812/markus-peschl"
    },
    "is_answered": true,
    "view_count": 674,
    "answer_count": 2,
    "score": 23,
    "last_activity_date": 1687482305,
    "creation_date": 1560617914,
    "last_edit_date": 1602673566,
    "question_id": 53858,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/53858/formal-proof-of-vanilla-policy-gradient-convergence",
    "title": "Formal proof of vanilla policy gradient convergence",
    "body": "<p>So I stumbled upon <a href=\"https://datascience.stackexchange.com/questions/36158/convergence-of-vanilla-or-natural-policy-gradients-e-g-reinforce\">this</a> question, where the author asks for a proof of vanilla policy gradient procedures. The answer provided points to some literature, but the formal proof is nowhere to be included. Looking at Sutton,Barto- Reinforcement Learning, they claim that convergence of the REINFORCE Monte Carlo algorithm is guaranteed under stochastic approximation step size requirements, but they do not seem to reference any sources that go into more detail.</p>\n<p>I am curious whether or not anybody actually has a formal proof ready for me to read. I found a paper, which goes into detail for proving convergence of a general online stochastic gradient descent algorithm, see, <a href=\"https://leon.bottou.org/publications/pdf/online-1998.pdf\" rel=\"noreferrer\">section 2.3</a>.</p>\n<p>However, I am not sure if the proof provided in the paper is applicable to the algorithm described in Sutton's book. In the mentioned algorithm, one obtains samples which, assuming that the policy did not change, is in expectation at least proportional to the gradient. However, the analytic expression of the gradient</p>\n<p><span class=\"math-container\">$$\\nabla J(\\theta) \\propto \\sum_s \\mu(s)\\sum_a q_{\\pi}(s,a)\\nabla \\pi(a|s,\\theta)$$</span></p>\n<p>depends on the on policy state distribution <span class=\"math-container\">$\\mu(s)$</span> which changes when we update <span class=\"math-container\">$\\theta$</span>. Therefore, when updating during the algorithm, the distribution changes.</p>\n<p>Any help would be greatly appreciated. Bottou's paper, which I linked above states that the event is drawn from a fixed probability distribution, which is not the case here.</p>\n<p>EDIT:</p>\n<p>So after reading some more papers, I found <a href=\"https://dspace.mit.edu/bitstream/handle/1721.1/3462/P-2404-38121470.pdf\" rel=\"noreferrer\">this</a>, which is a paper of Bertsekas and Tsitsiklis. They argue that under certain assumptions convergence to a stationary point is guaranteed, where one has an update rule of the form</p>\n<p><span class=\"math-container\">$$x_{t+1} = x_t +\\gamma_t (s_t + w_t)$$</span>\nand <span class=\"math-container\">$w_t$</span> is some error with\n<span class=\"math-container\">$$\\mathbb{E}[w_t  | \\mathcal{F}_t] = 0$$</span>\nfor ascending <span class=\"math-container\">$\\sigma$</span>-fields <span class=\"math-container\">$\\mathcal{F}_t$</span>, which can be thought of conditioning on the trajectory <span class=\"math-container\">$x_0,s_0\\dots,x_{t-1},s_{t-1},w_{t-1},x_t,s_t$</span>.\nI believe that this might be a solution since we need an expected gradient update given the past parameter <span class=\"math-container\">$x_t$</span>, which determines the sampling distribution which is exactly what the policy gradient theorem guarantees.\nI'd be happy if someone could verify this.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "scikit-learn",
      "online-learning"
    ],
    "owner": {
      "account_id": 7877664,
      "reputation": 936,
      "user_id": 87594,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/674d04093a101784e7bb3cd93eee913e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Adam",
      "link": "https://datascience.stackexchange.com/users/87594/adam"
    },
    "is_answered": true,
    "view_count": 35856,
    "answer_count": 4,
    "score": 23,
    "last_activity_date": 1681985429,
    "creation_date": 1582539351,
    "last_edit_date": 1592305723,
    "question_id": 68599,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/68599/incremental-learning-with-sklearn-warm-start-partial-fit-fit",
    "title": "Incremental Learning with sklearn: warm_start, partial_fit(), fit()",
    "body": "<p>I have built an ML model with the goal of making predictions for targets of the following week. In general, new data will come in and be processed at the end of each week and be in the <em>same</em> data structure as before. In other words, the same number of features, same classes for classification, etc.</p>\n<p>Instead of re-training the model from scratch for each week's predictions, I am considering applying an incremental learning approach so that past learning is not entirely discarded and the model would (presumably) increase in performance over time. I'm working with <code>sklearn</code>  on Python 3. There were only a handful of posts on StackOverflow regarding this, but many of the answers seem inconsistent (possibly due to updates with sklearn's API?).</p>\n<p>The documentation <a href=\"https://scikit-learn.org/0.15/modules/scaling_strategies.html#incremental-learning\" rel=\"noreferrer\">here</a> and <a href=\"https://scikit-learn.org/stable/glossary.html#term-partial-fit\" rel=\"noreferrer\">here</a> suggests that incremental/online learning is possible with certain ML implementations - implying that the new datasets could be thought of as &quot;mini-batches&quot; and incrementally trained by saving/loading the model and calling <code>.partial_fit()</code> with the same model parameters.</p>\n<blockquote>\n<p>Although all algorithms cannot learn incrementally (i.e. without seeing all the instances at once), all estimators implementing the partial_fit API are candidates. <a href=\"https://scikit-learn.org/0.15/modules/scaling_strategies.html#incremental-learning\" rel=\"noreferrer\">1</a></p>\n<p>Unlike fit, repeatedly calling partial_fit does not clear the model, but updates it with respect to the data provided. The portion of data provided to partial_fit may be called a mini-batch. Each mini-batch must be of consistent shape, etc. In iterative estimators, partial_fit often only performs a single iteration. <a href=\"https://scikit-learn.org/stable/glossary.html#term-partial-fit\" rel=\"noreferrer\">2</a></p>\n</blockquote>\n<p>However, the documentation <a href=\"https://scikit-learn.org/stable/glossary.html#term-warm-start\" rel=\"noreferrer\">here</a> is throwing me off.</p>\n<blockquote>\n<p>partial_fit also retains the model between calls, but differs: with warm_start the parameters change and the data is (more-or-less) constant across calls to fit; with partial_fit, the mini-batch of data changes and model parameters stay fixed. <a href=\"https://scikit-learn.org/stable/glossary.html#term-warm-start\" rel=\"noreferrer\">3</a></p>\n<p>There are cases where you want to use warm_start to fit on different, but closely related data. <a href=\"https://scikit-learn.org/stable/glossary.html#term-warm-start\" rel=\"noreferrer\">3</a></p>\n</blockquote>\n<p>For the problem I am tackling, <em>ideally</em> model parameters should be adjusted based on cross-validation and new datasets should  be weighted more heavily than old ones due to concept drift. However  ignoring this for now,</p>\n<ol>\n<li>In (3), what exactly does &quot;(more-or-less) constant...different but closely related data&quot; mean? Since the data structure of new datasets are the same, should i be calling <code>estimator(warm_start=True).fit(#new df)</code> or <code>estimator.partial_fit(#new df)</code>?</li>\n<li>For iterative estimators such as <code>sklearn.linear_model.SGDClassifier</code>, only one epoch is run when using <code>.partial_fit()</code>. If I want <span class=\"math-container\">$k$</span> epochs, would calling it on the same dataset repeatedly be the same as calling <code>.fit()</code> with <span class=\"math-container\">$k$</span> epochs to begin with?</li>\n<li>Do dedicated libraries such as <code>creme</code> offer any advantage for incremental learniing?</li>\n</ol>\n"
  },
  {
    "tags": [
      "dataset",
      "nlp"
    ],
    "owner": {
      "account_id": 1572823,
      "reputation": 2039,
      "user_id": 684,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://www.gravatar.com/avatar/3e1a7d9ccfa13aed6bc6cb7ec6892518?s=256&d=identicon&r=PG",
      "display_name": "Madison May",
      "link": "https://datascience.stackexchange.com/users/684/madison-may"
    },
    "is_answered": true,
    "view_count": 9856,
    "answer_count": 3,
    "score": 22,
    "last_activity_date": 1610346685,
    "creation_date": 1404162125,
    "last_edit_date": 1559964704,
    "question_id": 641,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/641/dataset-for-named-entity-recognition-on-informal-text",
    "title": "Dataset for Named Entity Recognition on Informal Text",
    "body": "<p>I'm currently searching for labeled datasets to train a model to extract named entities from informal text (something similar to tweets). Because capitalization and grammar are often lacking in the documents in my dataset, I'm looking for out of domain data that's a bit more \"informal\" than the news article and journal entries that many of today's state of the art named entity recognition systems are trained on.</p>\n\n<p>Any recommendations? So far I've only been able to locate 50k tokens from twitter published <a href=\"https://github.com/aritter/twitter_nlp/blob/master/data/annotated/ner.txt\" rel=\"nofollow noreferrer\">here</a>. </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "feature-selection",
      "feature-extraction"
    ],
    "owner": {
      "account_id": 3399912,
      "reputation": 1667,
      "user_id": 989,
      "user_type": "registered",
      "accept_rate": 29,
      "profile_image": "https://www.gravatar.com/avatar/4480675ad0ede0043e7caba04750d974?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "marcodena",
      "link": "https://datascience.stackexchange.com/users/989/marcodena"
    },
    "is_answered": true,
    "view_count": 7476,
    "accepted_answer_id": 718,
    "answer_count": 2,
    "score": 22,
    "last_activity_date": 1615490806,
    "creation_date": 1404986833,
    "last_edit_date": 1596625598,
    "question_id": 716,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/716/how-to-choose-the-features-for-a-neural-network",
    "title": "How to choose the features for a neural network?",
    "body": "<p>I know that there is no a clear answer for this question, but let's suppose that I have a huge neural network, with a lot of data and I want to add a new feature in input. The &quot;best&quot; way would be to test the network with the new feature and see the results, but is there a method to test if the feature IS UNLIKELY helpful? Like <a href=\"http://www3.nd.edu/%7Emclark19/learn/CorrelationComparison.pdf\" rel=\"noreferrer\">correlation measures</a> etc?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "svm",
      "accuracy",
      "random-forest"
    ],
    "owner": {
      "account_id": 4716046,
      "reputation": 323,
      "user_id": 1387,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4480675ad0ede0043e7caba04750d974?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Mika",
      "link": "https://datascience.stackexchange.com/users/1387/mika"
    },
    "is_answered": true,
    "view_count": 12366,
    "accepted_answer_id": 769,
    "answer_count": 2,
    "score": 22,
    "last_activity_date": 1596706691,
    "creation_date": 1405504155,
    "last_edit_date": 1405523384,
    "question_id": 750,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/750/how-to-increase-accuracy-of-classifiers",
    "title": "How to increase accuracy of classifiers?",
    "body": "<p>I am using OpenCV letter_recog.cpp example to experiment on random trees and other classifiers. This example has implementations of six classifiers - random trees, boosting, MLP, kNN, naive Bayes and SVM. UCI letter recognition dataset with 20000 instances and 16 features is used, which I split in half for training and testing. I have experience with SVM so I quickly set its recognition error to 3.3%. After some experimentation what I got was:</p>\n\n<p>UCI letter recognition:</p>\n\n<ul>\n<li>RTrees - 5.3% </li>\n<li>Boost -  13% </li>\n<li>MLP -    7.9% </li>\n<li>kNN(k=3) -   6.5%  </li>\n<li>Bayes -  11.5%  </li>\n<li>SVM -    3.3%</li>\n</ul>\n\n<p>Parameters used:</p>\n\n<ul>\n<li><p>RTrees - max_num_of_trees_in_the_forrest=200, max_depth=20,\nmin_sample_count=1</p></li>\n<li><p>Boost -  boost_type=REAL, weak_count=200, weight_trim_rate=0.95,\nmax_depth=7</p></li>\n<li><p>MLP -    method=BACKPROP, param=0.001, max_iter=300 (default values - too\nslow to experiment) </p></li>\n<li><p>kNN(k=3) -   k=3</p></li>\n<li><p>Bayes -  none</p></li>\n<li><p>SVM -    RBF kernel, C=10, gamma=0.01</p></li>\n</ul>\n\n<p>After that I used same parameters and tested on Digits and MNIST datasets by extracting gradient features first (vector size 200 elements):</p>\n\n<p>Digits:</p>\n\n<ul>\n<li>RTrees - 5.1%</li>\n<li>Boost -  23.4%</li>\n<li>MLP -    4.3%</li>\n<li>kNN(k=3) -   7.3%</li>\n<li>Bayes -  17.7%</li>\n<li>SVM -    4.2%</li>\n</ul>\n\n<p>MNIST:</p>\n\n<ul>\n<li>RTrees - 1.4%</li>\n<li>Boost -  out of memory</li>\n<li>MLP -    1.0%</li>\n<li>kNN(k=3) -   1.2%</li>\n<li>Bayes -  34.33%</li>\n<li>SVM -    0.6%</li>\n</ul>\n\n<p>I am new to all classifiers except SVM and kNN, for these two I can say the results seem fine. What about others? I expected more from random trees, on MNIST kNN gives better accuracy, any ideas how to get it higher? Boost and Bayes give very low accuracy. In the end I'd like to use these classifiers to make a multiple classifier system. Any advice?</p>\n"
  },
  {
    "tags": [
      "education",
      "beginner",
      "career"
    ],
    "owner": {
      "account_id": 4106443,
      "reputation": 343,
      "user_id": 4836,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/100007630451118/picture?type=large",
      "display_name": "user3754366",
      "link": "https://datascience.stackexchange.com/users/4836/user3754366"
    },
    "is_answered": true,
    "view_count": 12608,
    "closed_date": 1415488325,
    "answer_count": 2,
    "score": 22,
    "last_activity_date": 1622195014,
    "creation_date": 1415179498,
    "question_id": 2403,
    "link": "https://datascience.stackexchange.com/questions/2403/data-science-without-knowledge-of-a-specific-topic-is-it-worth-pursuing-as-a-ca",
    "closed_reason": "Not suitable for this site",
    "title": "Data science without knowledge of a specific topic, is it worth pursuing as a career?",
    "body": "<p>I had a conversation with someone recently and mentioned my interest in data analysis and who I intended to learn the necessary skills and tools. They suggested to me that while it is great to learn the tools and build the skills there is little point in doing so unless i have specialized  knowledge in a specific field. </p>\n\n<p>They basically summed it to that I'd just be like a builder with a pile of tools who could build a few wooden boxes and may be build better things (cabins, cupboards etc), but without knowledge in a specific field I'd never be a builder people would come to for a specific product.</p>\n\n<p>Has anyone found this or have any input on what to make of this ? It would seem if it was true one would have to learn the data science aspects of things and then learn  a new field just to  become specialized.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "dataset"
    ],
    "owner": {
      "account_id": 5218800,
      "reputation": 323,
      "user_id": 6465,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/75d2bc138b8f98370d198aea398946bd?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "m-bhole",
      "link": "https://datascience.stackexchange.com/users/6465/m-bhole"
    },
    "is_answered": true,
    "view_count": 13887,
    "accepted_answer_id": 5429,
    "answer_count": 3,
    "score": 22,
    "last_activity_date": 1612902060,
    "creation_date": 1427901797,
    "last_edit_date": 1612902060,
    "question_id": 5427,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/5427/how-to-generate-synthetic-dataset-using-machine-learning-model-learnt-with-origi",
    "title": "How to generate synthetic dataset using machine learning model learnt with original dataset?",
    "body": "<p>Generally, the machine learning model is built on datasets. I'd like to know if there is any way to generate synthetic dataset using such trained machine learning model preserving original dataset characteristics?</p>\n<p>[original data --&gt; build machine learning model --&gt; use ml model to generate synthetic data....!!!]</p>\n<p>Is it possible? Please point me to related resource if possible.</p>\n"
  },
  {
    "tags": [
      "visualization"
    ],
    "owner": {
      "account_id": 6454247,
      "reputation": 323,
      "user_id": 10114,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/80741d12565683c11540150865502cdf?s=256&d=identicon&r=PG",
      "display_name": "Kunal Dharamsi",
      "link": "https://datascience.stackexchange.com/users/10114/kunal-dharamsi"
    },
    "is_answered": true,
    "view_count": 32007,
    "protected_date": 1589582513,
    "accepted_answer_id": 6593,
    "answer_count": 3,
    "score": 22,
    "last_activity_date": 1589578733,
    "creation_date": 1434041933,
    "last_edit_date": 1493055452,
    "question_id": 6084,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/6084/how-do-i-create-a-complex-radar-chart",
    "title": "How do I create a complex Radar Chart?",
    "body": "<p>So, I want to create a Player Profile <strong>Radar Chart</strong> something like this:</p>\n\n<hr>\n\n<p><a href=\"https://i.sstatic.net/neeVU.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/neeVU.png\" alt=\"enter image description here\"></a></p>\n\n<hr>\n\n<p>Not only the scale of each variable different, but also I want a reversed scale for some statistics like the 'dispossessed' stat, where less actually means good.</p>\n\n<p>One solution for the variable scale for each statistic maybe is setting a benchmark and then calculating a score on a scale of 100? </p>\n\n<p>But, How do I display the actual numbers on the chart then? Also, how do I get the reversed scale for some of the statistics.</p>\n\n<p>Currently working in Excel. What is the most powerful tool to create a complex chart like this?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "text-mining",
      "word-embeddings",
      "word2vec"
    ],
    "owner": {
      "account_id": 276079,
      "reputation": 702,
      "user_id": 1138,
      "user_type": "registered",
      "accept_rate": 62,
      "profile_image": "https://www.gravatar.com/avatar/77c97c977d1b7f3881a7149b1e4428a4?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "B_Miner",
      "link": "https://datascience.stackexchange.com/users/1138/b-miner"
    },
    "is_answered": true,
    "view_count": 17947,
    "accepted_answer_id": 10230,
    "answer_count": 2,
    "score": 22,
    "last_activity_date": 1455316074,
    "creation_date": 1455243721,
    "question_id": 10216,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10216/doc2vec-how-to-label-the-paragraphs-gensim",
    "title": "Doc2Vec - How to label the paragraphs (gensim)",
    "body": "<p>I am wondering how to label (tag) sentences / paragraphs / documents with doc2vec in gensim - from a practical standpoint. </p>\n\n<p>Do you need to have each sentence / paragraph / document with its own unique label (e.g. \"Sent_123\")? This seems useful if you want to say \"what words or sentences are most similar to a single specific sentence labeled \"Sent_123\".</p>\n\n<p>Can you have the labels be repeated based on content? For example if each sentence / paragraph / document is about a certain product item (and there are multiple sentence / paragraph / document for a given product item) can you label the sentences based on the item and then compute the similarity between a word or a sentence and this label (which I guess would be like an average of all those sentences that had to do with the product item)?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "logistic-regression"
    ],
    "owner": {
      "account_id": 1671841,
      "reputation": 323,
      "user_id": 16683,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/9912499987e9ef9e6be9b005b280e5b0?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "asPlankBridge",
      "link": "https://datascience.stackexchange.com/users/16683/asplankbridge"
    },
    "is_answered": true,
    "view_count": 10290,
    "accepted_answer_id": 10474,
    "answer_count": 3,
    "score": 22,
    "last_activity_date": 1641900426,
    "creation_date": 1456880979,
    "question_id": 10471,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10471/linear-regression-with-non-symmetric-cost-function",
    "title": "Linear regression with non-symmetric cost function?",
    "body": "<p>I want to predict some value $Y(x)$ and I am trying to get some prediction $\\hat Y(x)$ that optimizes between being as low as possible, but still being larger than $Y(x)$. In other words:\n$$\\text{cost}\\left\\{ Y(x) \\gtrsim \\hat Y(x) \\right\\}  &gt;&gt; \\text{cost}\\left\\{ \\hat Y(x) \\gtrsim Y(x) \\right\\}  $$</p>\n\n<p>I think a simple linear regression should do totally fine.\nSo I somewhat know how to implement this manually, but I guess I'm not the first one with this kind of problem. Are there any packages/libraries (preferably python) out there doing what I want to do?\nWhat's the keyword I need to look for?</p>\n\n<p>What if I knew a function $Y_0(x) &gt; 0$ where $Y(x) &gt; Y_0(x)$. What's the best way to implement these restrictions?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "feature-selection",
      "feature-extraction",
      "feature-engineering",
      "kaggle"
    ],
    "owner": {
      "account_id": 2800709,
      "reputation": 451,
      "user_id": 12377,
      "user_type": "registered",
      "accept_rate": 0,
      "profile_image": "https://www.gravatar.com/avatar/28ee736df1c6de2d9b5d45fb6623d4b9?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user2409011",
      "link": "https://datascience.stackexchange.com/users/12377/user2409011"
    },
    "is_answered": true,
    "view_count": 13228,
    "answer_count": 3,
    "score": 22,
    "last_activity_date": 1631115643,
    "creation_date": 1457638756,
    "question_id": 10640,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10640/how-to-perform-feature-engineering-on-unknown-features",
    "title": "How to perform feature engineering on unknown features?",
    "body": "<p>I am participating on a kaggle competition. The dataset has around 100 features and all are unknown (in terms of what actually they represent). Basically they are just numbers.</p>\n\n<p>People are performing a lot of feature engineering on these features. I am wondering how exactly one is able perform feature engineering on features which are unknown? Can somebody please help me understand this and some tips on how can I perform feature engineering on unknown features?</p>\n"
  },
  {
    "tags": [
      "python",
      "nlp",
      "word-embeddings",
      "word2vec",
      "gensim"
    ],
    "owner": {
      "account_id": 2578672,
      "reputation": 471,
      "user_id": 16964,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/2XDLo.jpg?s=256",
      "display_name": "Nomiluks",
      "link": "https://datascience.stackexchange.com/users/16964/nomiluks"
    },
    "is_answered": true,
    "view_count": 45068,
    "answer_count": 4,
    "score": 22,
    "last_activity_date": 1615489537,
    "creation_date": 1457948848,
    "last_edit_date": 1615489512,
    "question_id": 10695,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/10695/how-to-initialize-a-new-word2vec-model-with-pre-trained-model-weights",
    "title": "How to initialize a new word2vec model with pre-trained model weights?",
    "body": "<p>I am using Gensim Library in python for using and training word2vector model. Recently, I was looking at initializing my model weights with some pre-trained word2vec model such as (GoogleNewDataset pretrained model). I have been struggling with it for a couple of weeks. Now, I just found out that in gesim there is a function that can help me initialize the weights of my model with pre-trained model weights.</p>\n<p>That is mentioned below:</p>\n<pre><code>reset_from(other_model)\n\n    Borrow shareable pre-built structures (like vocab) from the other_model. Useful if testing multiple models in parallel on the same corpus.\n</code></pre>\n<p>I do not know this function can do the same thing or not. Please help!</p>\n"
  },
  {
    "tags": [
      "ipython",
      "jupyter"
    ],
    "owner": {
      "account_id": 3054234,
      "reputation": 419,
      "user_id": 21825,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b81ade0455376da08203abe46a17fe2a?s=256&d=identicon&r=PG",
      "display_name": "Manu H",
      "link": "https://datascience.stackexchange.com/users/21825/manu-h"
    },
    "is_answered": true,
    "view_count": 48661,
    "answer_count": 4,
    "score": 22,
    "last_activity_date": 1613523778,
    "creation_date": 1472136546,
    "last_edit_date": 1512658450,
    "question_id": 13669,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/13669/how-to-export-one-cell-of-a-jupyter-notebook",
    "title": "How to export one cell of a jupyter notebook?",
    "body": "<p>I'm currently working/prototyping into a <em>Jupyter</em> notebook. I want to run some of my code on a standalone <em>iPython</em> shell. </p>\n\n<p>For now, I export my <em>iPython</em> code (file --> download as) and then execute it in my <em>iPython</em> (with %run). It works, but I would like to export only one cell or set of cells. So, that I can run only what I modified in my <em>Jupyter</em> notebook.</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "activation-function"
    ],
    "owner": {
      "account_id": 8700074,
      "reputation": 8747,
      "user_id": 21560,
      "user_type": "registered",
      "accept_rate": 36,
      "profile_image": "https://www.gravatar.com/avatar/349cff1d621bb8779315322f202fba5b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Hendrik",
      "link": "https://datascience.stackexchange.com/users/21560/hendrik"
    },
    "is_answered": true,
    "view_count": 20609,
    "accepted_answer_id": 14359,
    "answer_count": 1,
    "score": 22,
    "last_activity_date": 1542583794,
    "creation_date": 1475579124,
    "last_edit_date": 1542583794,
    "question_id": 14349,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/14349/difference-of-activation-functions-in-neural-networks-in-general",
    "title": "Difference of Activation Functions in Neural Networks in general",
    "body": "<p>I have studied the activation function types for neural networks. The functions themselves are quite straightforward, but the application difference is not entirely clear.</p>\n\n<p>It's reasonable that one differentiates between logical and linear type functions, depending on the desired binary/continuous output but what is the advantage of sigmoid function over the simple linear one?</p>\n\n<p>ReLU is especially difficult to understand for me, for instance: what is the point to use a function that behaves like linear in case of positive inputs but is \"flat\" in case of negatives? What is the intuition behind this? Or is it just a simple trial-error thing, nothing more?</p>\n"
  },
  {
    "tags": [
      "reference-request",
      "books"
    ],
    "owner": {
      "account_id": 2186282,
      "reputation": 320,
      "user_id": 24980,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f1aaf5739ec5e0976cda4fa8eeefc91c?s=256&d=identicon&r=PG",
      "display_name": "Contactomorph",
      "link": "https://datascience.stackexchange.com/users/24980/contactomorph"
    },
    "is_answered": true,
    "view_count": 9065,
    "answer_count": 4,
    "score": 22,
    "last_activity_date": 1703412054,
    "creation_date": 1475754496,
    "last_edit_date": 1703412054,
    "question_id": 14396,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/14396/data-science-machine-learning-books-for-mathematicians",
    "title": "Data science / machine learning books for mathematicians",
    "body": "<p>I have found other requests for references here. In particular in:\n<a href=\"https://datascience.stackexchange.com/questions/5860/where-to-start-which-books\">Where to start, which books</a>\nand\n<a href=\"https://datascience.stackexchange.com/questions/313/books-about-the-science-in-data-science/407#407\">Books about the &quot;Science&quot; in Data Science?</a></p>\n<p>I have given a glance to:</p>\n<ul>\n<li>Artificial Intelligence: A Modern Approach (Russel &amp; Norvig)</li>\n<li>Machine Learning: The Art and Science of Algorithms that Make Sense of Data (Flach)</li>\n<li>Learning From Data (Abu-Mostafa et al.)</li>\n<li>Introduction to Statistical Learning (James et al.)</li>\n<li>Elements of Statistical Learning (Hastie et al.)</li>\n<li>Pattern Recognition and Machine Learning (Bishop)</li>\n</ul>\n<p>Now it is difficult to evaluate if they would fit my needs because only a few pages are generally available online. However my first impression is that they do not. In the appendices of <em>Artificial Intelligence: A Modern Approach</em> I can read:</p>\n<blockquote>\n<p>Mathematicians define a vector as a member of a vector space, but we will use a more concrete definition: a vector is an ordered sequence of values.</p>\n</blockquote>\n<p>This is exactly the kind of approach I am <strong>not</strong> looking for.</p>\n<p>I'm looking for a book which assumes the reader has a good understanding in set theory, abstract algebra, measure and probability theory, statistics, topology, calculus, graph theory, complexity theory, etc and <strong>a preference for formal and axiomatic explanations</strong> rather than lengthy and so-called &quot;intuitive&quot; approaches based on basic mathematical objects and examples. Furthermore I don't want something that looks like a recipe book from the very beginning. I want a book that formalizes the abstract and common shape of all data science methods as well as their common aim first. Only after that it can start to explain the different categories by explicitly stating which further hypotheses each category is assuming and which cases/problems/domains they are known to handle efficiently or not.</p>\n<p>At last, to be clear, I have no problem with being shown concrete examples and their treatment via a specific programming language for example. I just want this to come second as an illustration for the conceptual explanation, not as a substitute.</p>\n"
  },
  {
    "tags": [
      "python",
      "time-series",
      "data-cleaning",
      "pandas"
    ],
    "owner": {
      "account_id": 2280426,
      "reputation": 560,
      "user_id": 13165,
      "user_type": "registered",
      "accept_rate": 88,
      "profile_image": "https://www.gravatar.com/avatar/b9dc66eb52a870f6808366cb1d307028?s=256&d=identicon&r=PG",
      "display_name": "Austin Capobianco",
      "link": "https://datascience.stackexchange.com/users/13165/austin-capobianco"
    },
    "is_answered": true,
    "view_count": 93021,
    "accepted_answer_id": 14646,
    "answer_count": 2,
    "score": 22,
    "last_activity_date": 1646439276,
    "creation_date": 1476912163,
    "question_id": 14645,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/14645/convert-a-pandas-column-of-int-to-timestamp-datatype",
    "title": "Convert a pandas column of int to timestamp datatype",
    "body": "<p>I have a dataframe that among other things, contains a column of the number of milliseconds passed since 1970-1-1. I need to convert this column of ints to timestamp data, so I can then ultimately convert it to a column of datetime data by adding the timestamp column series to a series that consists entirely of datetime values for 1970-1-1.</p>\n\n<p>I know how to convert a series of <em>strings</em> to datetime data (pandas.to_datetime), but I can't find or come up with any solution to convert the entire column of <em>ints</em> to datetime data OR to timestamp data.</p>\n"
  },
  {
    "tags": [
      "python",
      "sequential-pattern-mining"
    ],
    "owner": {
      "account_id": 4962749,
      "reputation": 2785,
      "user_id": 17310,
      "user_type": "registered",
      "accept_rate": 25,
      "profile_image": "https://www.gravatar.com/avatar/6b2ddcbdf322159e5ff9fa987d2af7c9?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Edamame",
      "link": "https://datascience.stackexchange.com/users/17310/edamame"
    },
    "is_answered": true,
    "view_count": 25839,
    "protected_date": 1587578754,
    "answer_count": 7,
    "score": 22,
    "last_activity_date": 1617899772,
    "creation_date": 1478608383,
    "last_edit_date": 1478643942,
    "question_id": 14999,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/14999/good-frequent-sequence-mining-packages-in-python",
    "title": "Good &quot;frequent sequence mining&quot; packages in Python?",
    "body": "<p>Has anyone used (and liked) any good \"frequent sequence mining\" packages in Python other than the FPM in MLLib? I am looking for a stable package, preferable stilled maintained by people. Thank you!</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "model-evaluations",
      "class-imbalance",
      "smote"
    ],
    "owner": {
      "account_id": 4962749,
      "reputation": 2785,
      "user_id": 17310,
      "user_type": "registered",
      "accept_rate": 25,
      "profile_image": "https://www.gravatar.com/avatar/6b2ddcbdf322159e5ff9fa987d2af7c9?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Edamame",
      "link": "https://datascience.stackexchange.com/users/17310/edamame"
    },
    "is_answered": true,
    "view_count": 33289,
    "accepted_answer_id": 15633,
    "answer_count": 4,
    "score": 22,
    "last_activity_date": 1639655692,
    "creation_date": 1481242785,
    "last_edit_date": 1625865814,
    "question_id": 15630,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/15630/train-test-split-after-performing-smote",
    "title": "Train/Test Split after performing SMOTE",
    "body": "<p>I am dealing with a highly unbalanced dataset so I used SMOTE  to resample it.</p>\n<p>After SMOTE resampling, I split the resampled dataset into training/test sets using the training set to build a model and the test set to evaluate it.</p>\n<p>However, I am worried that some data points in the test set might actually be jittered from data points in the training set (i.e. the information is leaking from the training set into the test set) so the test set is not really a clean set for testing.</p>\n<p>Does anyone have any similar experience? Does the information really leak from the training set into the test set? Or does SMOTE actually take care of this and we do not need to worry about it?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "decision-trees",
      "xgboost",
      "efficiency"
    ],
    "owner": {
      "account_id": 1707413,
      "reputation": 315,
      "user_id": 21690,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/5123ca9217a405695f6cb56e4645d84f?s=256&d=identicon&r=PG",
      "display_name": "user1566200",
      "link": "https://datascience.stackexchange.com/users/21690/user1566200"
    },
    "is_answered": true,
    "view_count": 33549,
    "accepted_answer_id": 17293,
    "answer_count": 1,
    "score": 22,
    "last_activity_date": 1488420056,
    "creation_date": 1488395754,
    "question_id": 17282,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/17282/xgbregressor-vs-xgboost-train-huge-speed-difference",
    "title": "XGBRegressor vs. xgboost.train huge speed difference?",
    "body": "<p>If I train my model using the following code:</p>\n\n<pre><code>import xgboost as xg\nparams = {'max_depth':3,\n'min_child_weight':10,\n'learning_rate':0.3,\n'subsample':0.5,\n'colsample_bytree':0.6,\n'obj':'reg:linear',\n'n_estimators':1000,\n'eta':0.3}\n\nfeatures = df[feature_columns]\ntarget = df[target_columns]\ndmatrix = xg.DMatrix(features.values,\n                     target.values,\n                     feature_names=features.columns.values)\nclf = xg.train(params, dmatrix)\n</code></pre>\n\n<p>it finishes in about 1 minute. </p>\n\n<p>If I train my model using the Sci-Kit learn method:</p>\n\n<pre><code>import xgboost as xg\nmax_depth = 3\nmin_child_weight = 10\nsubsample = 0.5\ncolsample_bytree = 0.6\nobjective = 'reg:linear'\nnum_estimators = 1000\nlearning_rate = 0.3\n\nfeatures = df[feature_columns]\ntarget = df[target_columns]\nclf = xg.XGBRegressor(max_depth=max_depth,\n                min_child_weight=min_child_weight,\n                subsample=subsample,\n                colsample_bytree=colsample_bytree,\n                objective=objective,\n                n_estimators=num_estimators,\n                learning_rate=learning_rate)\nclf.fit(features, target)\n</code></pre>\n\n<p>it takes over 30 minutes.</p>\n\n<p>I would think the underlying code is nearly exactly the same (i.e. <code>XGBRegressor</code> calls <code>xg.train</code>) - what's going on here?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "keras",
      "tflearn"
    ],
    "owner": {
      "account_id": 5457062,
      "reputation": 207,
      "user_id": 42069,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-GlO7v4S0eNc/AAAAAAAAAAI/AAAAAAAAD74/sFdjjMIJOz0/s256-rj/photo.jpg",
      "display_name": "Ankit Bindal",
      "link": "https://datascience.stackexchange.com/users/42069/ankit-bindal"
    },
    "is_answered": true,
    "view_count": 10442,
    "answer_count": 1,
    "community_owned_date": 1531354359,
    "score": 22,
    "last_activity_date": 1531354271,
    "creation_date": 1512211583,
    "last_edit_date": 1531293567,
    "question_id": 25317,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/25317/what-are-the-pros-and-cons-of-keras-and-tflearn",
    "title": "What are the pros and cons of Keras and TFLearn?",
    "body": "<p>What are the pros and cons of <a href=\"https://github.com/keras-team/keras\" rel=\"noreferrer\">Keras</a> and <a href=\"https://github.com/tflearn/tflearn\" rel=\"noreferrer\">TFlearn</a>? When is one library preferred over the other?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "correlation"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 15871,
    "answer_count": 3,
    "score": 22,
    "last_activity_date": 1639404544,
    "creation_date": 1519672841,
    "question_id": 28328,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/28328/how-does-multicollinearity-affect-neural-networks",
    "title": "How does multicollinearity affect neural networks?",
    "body": "<p><a href=\"https://en.wikipedia.org/wiki/Multicollinearity\" rel=\"noreferrer\">Multicollinearity</a> is a problem for linear regression because the results become unstable / depend too much on single elements (<a href=\"http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/\" rel=\"noreferrer\">source</a>).</p>\n\n<p>(Also, the inverse of $X^TX$ doesn't exist so the standard OLS estimator does not exist ... I have no idea how, but <a href=\"https://gist.github.com/MartinThoma/6e7714fd4774b5185ac7c3ace9a5b780\" rel=\"noreferrer\">sklearn deals with it just fine</a>)</p>\n\n<p>Is (perfect) multicollinearity also a problem for neural networks?</p>\n"
  },
  {
    "tags": [
      "python",
      "visualization",
      "numpy",
      "seaborn"
    ],
    "owner": {
      "account_id": 11172513,
      "reputation": 797,
      "user_id": 44680,
      "user_type": "registered",
      "accept_rate": 40,
      "profile_image": "https://graph.facebook.com/1446979032025333/picture?type=large",
      "display_name": "Srihari",
      "link": "https://datascience.stackexchange.com/users/44680/srihari"
    },
    "is_answered": true,
    "view_count": 170966,
    "answer_count": 2,
    "score": 22,
    "last_activity_date": 1643737215,
    "creation_date": 1526490287,
    "question_id": 31746,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/31746/how-to-include-labels-in-sns-heatmap",
    "title": "How to include labels in sns heatmap",
    "body": "<p>I got this matrix </p>\n\n<pre><code>    120     100     80      40      20      10      5       0\n120 64.21   58.20   51.20   56.37   47.00   45.61   46.86   2.16\n100 62.84   57.80   50.60   51.32   39.43   39.30   42.80   0.89\n80  62.62   56.20   51.20   51.61   46.23   37.20   42.20   5.32\n40  62.05   52.10   44.20   48.79   42.22   35.16   41.80   1.81\n20  61.65   50.90   42.30   46.23   44.83   32.70   41.50   6.24\n10  59.69   50.20   40.10   40.20   44.28   32.80   39.90   12.31\n5   59.05   49.20   40.60   38.90   44.10   30.80   32.80   9.91\n0   56.20   49.10   40.50   38.60   36.20   32.20   31.50   0.00\n</code></pre>\n\n<p>I know how to plot heatmap for the values inside by specifying it as numpy array and then using</p>\n\n<pre><code>ax = sns.heatmap(nd, annot=True, fmt='g')\n</code></pre>\n\n<p>But can someone help me how do I include the column and row labels? The column labels and row labels are given (120,100,80,42,etc.)</p>\n"
  },
  {
    "tags": [
      "python",
      "classification",
      "training"
    ],
    "owner": {
      "account_id": 5541285,
      "reputation": 423,
      "user_id": 49017,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7b88babbe921fc3b43c3b96e8f5bc1a6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "lads",
      "link": "https://datascience.stackexchange.com/users/49017/lads"
    },
    "is_answered": true,
    "view_count": 81049,
    "accepted_answer_id": 32820,
    "answer_count": 4,
    "score": 22,
    "last_activity_date": 1728267202,
    "creation_date": 1528451353,
    "last_edit_date": 1528455089,
    "question_id": 32818,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/32818/train-test-split-of-unbalanced-dataset-classification",
    "title": "Train, test split of unbalanced dataset classification",
    "body": "<p>I have a model that does binary classification. </p>\n\n<p>My dataset is highly unbalanced, so I thought that I should balance it by undersampling before I train the model. So balance the dataset and then split it randomly. Is this the right way ?  or should I balance also the test and train dataset ? </p>\n\n<p>I tried balancing only the whole dataset and I get train accuracy of 80% but then on the test set I have 30% accuracy. This doesn't seem right ?</p>\n\n<p>But also I don't think that I should balance the test set because it could be considered as bias.</p>\n\n<p>What is the right way to do this? </p>\n\n<p>Thanks</p>\n\n<p><strong>UPDATE</strong>: I have 400 000  samples, 10% are 1s and 90% 0s. I cannot get more data. I tried to keep the whole dataset but I don't know how to split it into train and test set. Do I need the same distribution in the train and test dataset ?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "keras",
      "lstm",
      "rnn",
      "backpropagation"
    ],
    "migrated_from": {
      "other_site": {
        "styling": {
          "tag_background_color": "#FFF",
          "tag_foreground_color": "#000",
          "link_color": "#0077CC"
        },
        "related_sites": [
          {
            "relation": "meta",
            "api_site_parameter": "ai.meta",
            "site_url": "https://ai.meta.stackexchange.com",
            "name": "Artificial Intelligence Meta Stack Exchange"
          },
          {
            "relation": "chat",
            "site_url": "https://chat.stackexchange.com?tab=site&host=ai.stackexchange.com",
            "name": "Chat Stack Exchange"
          }
        ],
        "markdown_extensions": [
          "MathJax"
        ],
        "launch_date": 1639665491,
        "open_beta_date": 1471907237,
        "closed_beta_date": 1470164400,
        "site_state": "normal",
        "high_resolution_icon_url": "https://cdn.sstatic.net/Sites/ai/Img/apple-touch-icon@2.png",
        "favicon_url": "https://cdn.sstatic.net/Sites/ai/Img/favicon.ico",
        "icon_url": "https://cdn.sstatic.net/Sites/ai/Img/apple-touch-icon.png",
        "audience": "people interested in conceptual questions about life and challenges in a world where &quot;cognitive&quot; functions can be mimicked in purely digital environment",
        "site_url": "https://ai.stackexchange.com",
        "api_site_parameter": "ai",
        "logo_url": "https://cdn.sstatic.net/Sites/ai/Img/apple-touch-icon.png",
        "name": "Artificial Intelligence",
        "site_type": "main_site"
      },
      "on_date": 1536256147,
      "question_id": 7767
    },
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "KenMarsu"
    },
    "is_answered": true,
    "view_count": 3551,
    "answer_count": 1,
    "score": 22,
    "last_activity_date": 1717211060,
    "creation_date": 1535718379,
    "last_edit_date": 1600369412,
    "question_id": 37901,
    "link": "https://datascience.stackexchange.com/questions/37901/understanding-timestamps-and-batchsize-of-keras-lstm-considering-hiddenstates-an",
    "title": "Understanding Timestamps and Batchsize of Keras LSTM considering Hiddenstates and TBPTT",
    "body": "<h2>What I'm trying to do</h2>\n\n<p>What I am trying to do is predicting the next data-point <span class=\"math-container\">$x_t$</span> for each point in the timeseries <span class=\"math-container\">$[x_0, x_1, x_2,...,x_T]$</span> in the context of a date-stream in real-time, in theory the series is infinity. If a new value <span class=\"math-container\">$x$</span> is coming, i feed it into the network and predict the next one.\nSo <em>seq-to-seq</em> model with the <em>return_sequenze</em> parameter set to <span class=\"math-container\">$True$</span> does not work, because in practical application, at time <span class=\"math-container\">$t$</span> we do not have the data-points <span class=\"math-container\">$x_i$</span> for <span class=\"math-container\">$i&gt;t$</span>. For a multistep-prediction I use a manual loop back, where I feed the predicted value <span class=\"math-container\">$\\hat{x}$</span> as the new input.(Knowing what comes next - compensation if nothing comes)</p>\n\n<p><a href=\"https://i.sstatic.net/Z3eHn.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/Z3eHn.png\" alt=\"unrolled LSTM\"></a></p>\n\n<h2>My understanding of LSTM</h2>\n\n<p>Maybe there is something basic that I have misunderstood\n:</p>\n\n<h2>Timelags</h2>\n\n<p>If we take a stepsize of 3 (in a <em>one-to-many</em> network), we are ending up with <em>input/output</em> samples <span class=\"math-container\">$([x_0,x_1,x_2], x_3),([x_1,x_2,x_3], x_4), ..., ([x_{T-3},x_{T-2},x_{T-1}], x_T)$</span> (Using Rolling-windows), or <span class=\"math-container\">$([x_0,x_1,x_2], x_3),([x_3,x_4,x_5], x_6), ..., ([x_{T-3},x_{T-2},x_{T-1}], x_T)$</span> (Not using Rolling-windows). Using a batchsize of 1, the networkinput is <span class=\"math-container\">$(1,3,n\\_features)$</span>, where <span class=\"math-container\">$n\\_features$</span> is the dimension of <span class=\"math-container\">$X_i$</span> (<span class=\"math-container\">$1$</span> for an univariat timeseries, <span class=\"math-container\">$&gt;1$</span> for multivariate timeseries).</p>\n\n<p><a href=\"https://i.sstatic.net/Uh6hb.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/Uh6hb.png\" alt=\"LSTM (1,3,x) sample\"></a></p>\n\n<h2>Batches and timelags</h2>\n\n<p>So far so good. But, considering the batchsize and that within the batch the states of the Cells are not reset, the initial state of the second sample is the state at the end of the first sample. So with a batchsize of <span class=\"math-container\">$2$</span> the network gets the input as shown in the picture below (Inputshape would be <span class=\"math-container\">$(2, 3, n\\_features)$</span>).The number in the superscript indicates the sample. Doing so, we are messing up the timeseries, because, as you can see, the series now is <span class=\"math-container\">$[x_0, x_1, x_2, x_1, x_2, x_3]$</span>. (The same problem occurs between to batches, if the network is set to be stateful). \nIf the samples are not generated by rolling-windows, its obvious that there are missing values in the output sequence (<span class=\"math-container\">$y_4$</span> and <span class=\"math-container\">$y_6$</span> would be the outputs). And since there is no useful reason to do a seq-to-seq prediction for my problem (as stated previous), I'll ignore that case for now.\n<a href=\"https://i.sstatic.net/gHfGy.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/gHfGy.png\" alt=\"inputshape (2,3,x)\"></a></p>\n\n<p>As there is a good reason to use batches to train RNNs (there is an article at machinelearningmastery) and not to update the weights after every sample, I am using a single timestep for each sample. The inputshape is now <span class=\"math-container\">$(batch\\_size, 1, n\\_features)$</span>.\nFor example using a batchsize of 6 the networks calculation should look like this. (For stateful networks the next observation would be <span class=\"math-container\">$x_6, ...$</span> together with the state at <span class=\"math-container\">$x_5$</span>, the batchsize determines when to update the weights during training.)</p>\n\n<p><a href=\"https://i.sstatic.net/RbxGf.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/RbxGf.png\" alt=\"enter image description here\"></a></p>\n\n<h2>Truncated Backpropagation Through Time</h2>\n\n<p>As stated <a href=\"https://stackoverflow.com/questions/39457744/backpropagation-through-time-in-stateful-rnns%20%22stackoverflow\">in this stackoverflow question</a> the BPTT for keras RNNs is limited to the timelags of the input. Lets have a look at our input shape considering the BPTT.\nFor a higher order of lags, the error is back-propagated through time.\n<a href=\"https://i.sstatic.net/FTwem.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/FTwem.png\" alt=\"bptt n lags\"></a></p>\n\n<p>For the approach, where the input is \n<span class=\"math-container\">$(batch\\_size, 1, n\\_features)$</span> there is basically no BPTT, because it stops at one. And I think, that the weights of the recurrent connection are not updated at all, are they?\n<a href=\"https://i.sstatic.net/EYl81.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/EYl81.png\" alt=\"BPTT for 1 lag\"></a></p>\n\n<h2>Conclusion and Question</h2>\n\n<p>Learning with a timelag of one might not be a good idea (mainly because of the insufficient BPTT, as the states can be preserved). Working with a higher order of timelags leads into a) holes in the output data using no rolling-windows while generating the samples or b) the time sequence is messed up.</p>\n\n<p>What is a good approach to archive BPTT with Keras LSTMs, predicting future values (one-step and recursive multistep prediction as will) and still keep the input data stream-like (one observation after another)?\nDoes some one have experienced similar problems and if so, how did you solve it? </p>\n\n<p>I'm thankful for every input and maybe I also could help someone who is working on similar stuff ;) </p>\n\n<h2>My theoretical solution</h2>\n\n<p>While writing this question, I had this idea:</p>\n\n<p>As far as i understand the Keras Layers, I can use <em>TimeDistributed</em> Outputlayer to have the Outputlayer applied to every given lag and the networks returns the sequence. (seq-to-seq network)</p>\n\n<pre><code>input = Input(batch_shape=(batch_size, timelags, features))\nlstm = LSTM(units=10, return_sequences=True)(input)\nout = TimeDistributed(Dense(units=features))(lstm)\n</code></pre>\n\n<p>To avoid messing up the sequence, I do not use a rolling-window. The Input/Outputs are (for lags=3) <span class=\"math-container\">$([x_0,x_1,x_2], [x_1, x_2, x_3]),([x_3, x_4, x_4],[x_4, x_5, x_6])...)$</span>\nAfter I have trained the model I build a new one, which has the same architecture, but the inputshpape <span class=\"math-container\">$(1, 1, features)$</span> and copy the trained parameters. This works, because the each LSTM- and outputpart as shown in the image are the actually the same (it shows the unrolled version of the lstm) and there is no need for BPTT while predicting, so no need to keep timelags > one and as the network is stateful the hiddenstates are preserved and the output should be the same as if I use a higher order of timelags with a sequence as output.</p>\n\n<pre><code>prediction_model.set_weights(trained_model.get_weights())\n</code></pre>\n\n<p>This way the BPTT is used and the input can be fed as stream of single points in time while predicting(I'm testing it now).\n <a href=\"https://i.sstatic.net/NhNNn.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/NhNNn.png\" alt=\"bptt\"></a></p>\n"
  },
  {
    "tags": [
      "python",
      "scikit-learn",
      "data-cleaning",
      "normalization"
    ],
    "owner": {
      "account_id": 6275282,
      "reputation": 411,
      "user_id": 67602,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/92b6264bf64c1b48521e9ed509c978a8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Heisenbug",
      "link": "https://datascience.stackexchange.com/users/67602/heisenbug"
    },
    "is_answered": true,
    "view_count": 53721,
    "answer_count": 3,
    "score": 22,
    "last_activity_date": 1650415788,
    "creation_date": 1550680685,
    "last_edit_date": 1562676955,
    "question_id": 45900,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/45900/when-to-use-standard-scaler-and-when-normalizer",
    "title": "When to use Standard Scaler and when Normalizer?",
    "body": "<p>I understand what Standard Scalar does and what Normalizer does, per the scikit documentation: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer\" rel=\"noreferrer\">Normalizer</a>, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\" rel=\"noreferrer\">Standard Scaler</a>.</p>\n\n<p>I know when Standard Scaler is applied. But in which scenario is Normalizer applied? Are there scenarios where one is preferred over the other?</p>\n"
  },
  {
    "tags": [
      "keras",
      "lstm"
    ],
    "owner": {
      "account_id": 4262249,
      "reputation": 1310,
      "user_id": 67743,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ecc8ba04bbf35d83c866bf36496084d5?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user3486308",
      "link": "https://datascience.stackexchange.com/users/67743/user3486308"
    },
    "is_answered": true,
    "view_count": 26816,
    "accepted_answer_id": 46504,
    "answer_count": 2,
    "score": 22,
    "last_activity_date": 1559311777,
    "creation_date": 1551472983,
    "last_edit_date": 1559311777,
    "question_id": 46491,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/46491/what-is-the-job-of-repeatvector-and-timedistributed",
    "title": "What is the job of &quot;RepeatVector&quot; and &quot;TimeDistributed&quot;?",
    "body": "<p>I read about them in Keras documentation and other websites, but I couldn't exactly understand what exactly they do and how should we use them in designing <code>many-to-many</code> or <code>encoder-decoder</code> LSTM networks?</p>\n\n<p>I saw them used in the solution of <a href=\"https://github.com/keras-team/keras/issues/6063\" rel=\"noreferrer\">this problem here</a>.</p>\n\n<blockquote>\n<pre><code>model = Sequential()  \nmodel.add(LSTM(input_dim=1, output_dim=hidden_neurons, return_sequences=False))  \nmodel.add(RepeatVector(10))\nmodel.add(LSTM(output_dim=hidden_neurons, return_sequences=True))  \nmodel.add(TimeDistributed(Dense(1)))\nmodel.add(Activation('linear'))   \nmodel.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['accuracy'])\n</code></pre>\n</blockquote>\n"
  },
  {
    "tags": [
      "python",
      "deep-learning",
      "autoencoder",
      "vae"
    ],
    "owner": {
      "account_id": 12563799,
      "reputation": 2485,
      "user_id": 50406,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Eq7j1.jpg?s=256",
      "display_name": "thanatoz",
      "link": "https://datascience.stackexchange.com/users/50406/thanatoz"
    },
    "is_answered": true,
    "view_count": 28845,
    "accepted_answer_id": 48972,
    "answer_count": 1,
    "score": 22,
    "last_activity_date": 1645407694,
    "creation_date": 1554814221,
    "question_id": 48962,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/48962/what-is-posterior-collapse-phenomenon",
    "title": "What is &quot;posterior collapse&quot; phenomenon?",
    "body": "<p>I was going through this paper on <a href=\"https://papers.nips.cc/paper/7983-towards-text-generation-with-adversarially-learned-neural-outlines.pdf\" rel=\"noreferrer\">Towards Text Generation with Adversarially Learned\nNeural Outlines</a> and it states why the VAEs are hard to train for text generation due to this problem. The paper states </p>\n\n<blockquote>\n  <p>the model ends up\n  relying solely on the auto-regressive properties of the decoder while ignoring the latent variables,\n  which become uninformative.</p>\n</blockquote>\n\n<p>please simplify and explain the problem in a lucid way.</p>\n"
  },
  {
    "tags": [
      "algorithms"
    ],
    "owner": {
      "account_id": 2048845,
      "reputation": 3152,
      "user_id": 26,
      "user_type": "registered",
      "accept_rate": 60,
      "profile_image": "https://i.sstatic.net/S6fiE.png?s=256",
      "display_name": "Alex I",
      "link": "https://datascience.stackexchange.com/users/26/alex-i"
    },
    "is_answered": true,
    "view_count": 767,
    "answer_count": 3,
    "score": 21,
    "last_activity_date": 1400558203,
    "creation_date": 1400061114,
    "question_id": 35,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/35/how-to-scale-up-algorithm-development",
    "title": "How to scale up algorithm development?",
    "body": "<p>In working on exploratory data analysis, and developing algorithms, I find that most of my time is spent in a cycle of visualize, write some code, run on small dataset, repeat.   The data I have tends to be computer vision/sensor fusion type stuff, and algorithms are vision-heavy (for example object detection and tracking, etc), and the off the shelf algorithms don't work in this context.  I find that this takes a lot of iterations (for example, to dial in the type of algorithm or tune the parameters in the algorithm, or to get a visualization right) and also the run times even on a small dataset are quite long, so all together it takes a while. </p>\n\n<p>How can the algorithm development itself be sped up and made more scalable?</p>\n\n<p>Some specific challenges: </p>\n\n<p>How can the number of iterations be reduced?  (Esp. when what kind of algorithm, let alone the specifics of it, does not seem to be easily foreseeable without trying different versions and examining their behavior)</p>\n\n<p>How to run on bigger datasets during development?  (Often going from small to large dataset is when a bunch of new behavior and new issues is seen)</p>\n\n<p>How can algorithm parameters be tuned faster?</p>\n\n<p>How to apply machine learning type tools to algorithm development itself?  (For example, instead of writing the algorithm by hand, write some simple building blocks and combine them in a way learned from the problem, etc)</p>\n"
  },
  {
    "tags": [
      "recommender-system",
      "information-retrieval"
    ],
    "owner": {
      "account_id": 1822136,
      "reputation": 4117,
      "user_id": 84,
      "user_type": "registered",
      "accept_rate": 82,
      "profile_image": "https://www.gravatar.com/avatar/5394d14f632e89b2dfc937e3660f0079?s=256&d=identicon&r=PG",
      "display_name": "Rubens",
      "link": "https://datascience.stackexchange.com/users/84/rubens"
    },
    "is_answered": true,
    "view_count": 324,
    "accepted_answer_id": 97,
    "answer_count": 3,
    "score": 21,
    "last_activity_date": 1448292988,
    "creation_date": 1400164884,
    "question_id": 94,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/94/does-click-frequency-account-for-relevance",
    "title": "Does click frequency account for relevance?",
    "body": "<p>While building a rank, say for a search engine, or a recommendation system, is it valid to rely on click frequency to determine the relevance of an entry?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "time-series"
    ],
    "owner": {
      "account_id": 1151310,
      "reputation": 311,
      "user_id": 886,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8e4a3fe7b05067f52558f6145ab9f0ee?s=256&d=identicon&r=PG",
      "display_name": "user1132959",
      "link": "https://datascience.stackexchange.com/users/886/user1132959"
    },
    "is_answered": true,
    "view_count": 12633,
    "answer_count": 7,
    "score": 21,
    "last_activity_date": 1615489410,
    "creation_date": 1402933795,
    "last_edit_date": 1403021866,
    "question_id": 406,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/406/how-can-i-predict-traffic-based-on-previous-time-series-data",
    "title": "How can I predict traffic based on previous time series data?",
    "body": "<p>If I have a retail store and have a way to measure how many people enter my store every minute, and timestamp that data, how can I predict future foot traffic?  </p>\n\n<p>I have looked into machine learning algorithms, but I'm not sure which one to use. In my test data, a year over year trend is more accurate compared to other things I've tried, like KNN (with what I think are sensible parameters and distance function).</p>\n\n<p>It almost seems like this could be similar to financial modeling, where you deal with time series data. Any ideas?</p>\n"
  },
  {
    "tags": [
      "bigdata",
      "nosql",
      "mongodb"
    ],
    "owner": {
      "account_id": 2534842,
      "reputation": 369,
      "user_id": 2643,
      "user_type": "registered",
      "accept_rate": 25,
      "profile_image": "https://www.gravatar.com/avatar/f08f3691758fea1afc29e965ea4d6788?s=256&d=identicon&r=PG",
      "display_name": "10land",
      "link": "https://datascience.stackexchange.com/users/2643/10land"
    },
    "is_answered": true,
    "view_count": 8219,
    "accepted_answer_id": 797,
    "answer_count": 3,
    "score": 21,
    "last_activity_date": 1446882668,
    "creation_date": 1405950073,
    "last_edit_date": 1406446611,
    "question_id": 793,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/793/uses-of-nosql-database-in-data-science",
    "title": "Uses of NoSQL database in data science",
    "body": "<p>How can <a href=\"http://en.wikipedia.org/wiki/NoSQL\">NoSQL</a> databases like <a href=\"http://en.wikipedia.org/wiki/MongoDB\">MongoDB</a> be used for data analysis? What are the features in them that can make data analysis faster and powerful?</p>\n"
  },
  {
    "tags": [
      "r",
      "visualization"
    ],
    "migrated_from": {
      "other_site": {
        "aliases": [
          "https://statistics.stackexchange.com",
          "https://crossvalidated.com"
        ],
        "styling": {
          "tag_background_color": "#edefed",
          "tag_foreground_color": "#5D5D5D",
          "link_color": "#0077CC"
        },
        "related_sites": [
          {
            "relation": "meta",
            "api_site_parameter": "stats.meta",
            "site_url": "https://stats.meta.stackexchange.com",
            "name": "Cross Validated Meta"
          },
          {
            "relation": "chat",
            "site_url": "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com",
            "name": "Chat Stack Exchange"
          }
        ],
        "markdown_extensions": [
          "MathJax",
          "Prettify"
        ],
        "launch_date": 1288900046,
        "open_beta_date": 1280170800,
        "closed_beta_date": 1279566000,
        "site_state": "normal",
        "high_resolution_icon_url": "https://cdn.sstatic.net/Sites/stats/Img/apple-touch-icon@2.png",
        "favicon_url": "https://cdn.sstatic.net/Sites/stats/Img/favicon.ico",
        "icon_url": "https://cdn.sstatic.net/Sites/stats/Img/apple-touch-icon.png",
        "audience": "people interested in statistics, machine learning, data analysis, data mining, and data visualization",
        "site_url": "https://stats.stackexchange.com",
        "api_site_parameter": "stats",
        "logo_url": "https://cdn.sstatic.net/Sites/stats/Img/logo.png",
        "name": "Cross Validated",
        "site_type": "main_site"
      },
      "on_date": 1407193163,
      "question_id": 110604
    },
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "aiolias"
    },
    "is_answered": true,
    "view_count": 13353,
    "answer_count": 6,
    "score": 21,
    "last_activity_date": 1615489565,
    "creation_date": 1407180105,
    "last_edit_date": 1407454646,
    "question_id": 904,
    "link": "https://datascience.stackexchange.com/questions/904/what-do-you-use-to-generate-a-dashboard-in-r",
    "title": "What do you use to generate a dashboard in R?",
    "body": "<p>I need to generate periodic (daily, monthly) web analytics dashboard reports. They will be static and don't require interaction, so imagine a PDF file as the target output. The reports will mix tables and charts (mainly sparkline and bullet graphs created with ggplot2). Think Stephen Few/Perceptual Edge style dashboards, such as: <img src=\"https://i.sstatic.net/Edh2e.png\" alt=\"sample dashboard\"></p>\n\n<p>but applied to web analytics. </p>\n\n<p>Any suggestions on what packages to use creating these dashboard reports? </p>\n\n<p>My first intuition is to use R markdown and knitr, but perhaps you've found a better solution. I can't seem to find rich examples of dashboards generated from R. </p>\n"
  },
  {
    "tags": [
      "nlp",
      "text-mining"
    ],
    "owner": {
      "account_id": 368960,
      "reputation": 661,
      "user_id": 2750,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://i.sstatic.net/03C6h.jpg?s=256",
      "display_name": "MaticDiba",
      "link": "https://datascience.stackexchange.com/users/2750/maticdiba"
    },
    "is_answered": true,
    "view_count": 24690,
    "accepted_answer_id": 2648,
    "answer_count": 2,
    "score": 21,
    "last_activity_date": 1553007693,
    "creation_date": 1418050287,
    "question_id": 2646,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/2646/extract-most-informative-parts-of-text-from-documents",
    "title": "Extract most informative parts of text from documents",
    "body": "<p>Are there any articles or discussions about extracting part of text that holds the most of information about current document.</p>\n\n<p>For example, I have a large corpus of documents from the same domain. There are parts of text that hold the key information what single document talks about. I want to extract some of those parts and use them as kind of a summary of the text. Is there any useful documentation about how to achieve something like this. </p>\n\n<p>It would be really helpful if someone could point me into the right direction what I should search for or read to get some insight in work that might have already been done in this field of Natural language processing.</p>\n"
  },
  {
    "tags": [
      "python",
      "feature-extraction",
      "image-recognition"
    ],
    "owner": {
      "account_id": 6230581,
      "reputation": 315,
      "user_id": 14112,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/1051859291495305/picture?type=large",
      "display_name": "Jeremy Barnes",
      "link": "https://datascience.stackexchange.com/users/14112/jeremy-barnes"
    },
    "is_answered": true,
    "view_count": 44993,
    "accepted_answer_id": 8849,
    "answer_count": 3,
    "score": 21,
    "last_activity_date": 1596450309,
    "creation_date": 1447548303,
    "last_edit_date": 1454049884,
    "question_id": 8847,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/8847/feature-extraction-of-images-in-python",
    "title": "Feature extraction of images in Python",
    "body": "<p>In my class I have to create an application using two classifiers to decide whether an object in an image is an example of phylum porifera (seasponge) or some other object.</p>\n\n<p>However, I am completely lost when it comes to feature extraction techniques in python. My advisor convinced me to use images which haven't been covered in class.</p>\n\n<p>Can anyone direct me towards meaningful documentation or reading or suggest methods to consider?</p>\n"
  },
  {
    "tags": [
      "python",
      "neural-network",
      "deep-learning",
      "keras",
      "hyperparameter"
    ],
    "owner": {
      "account_id": 2460255,
      "reputation": 3480,
      "user_id": 13023,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://www.gravatar.com/avatar/a15c4546ec5b537e7340c95be39aa52e?s=256&d=identicon&r=PG",
      "display_name": "wacax",
      "link": "https://datascience.stackexchange.com/users/13023/wacax"
    },
    "is_answered": true,
    "view_count": 21782,
    "answer_count": 4,
    "score": 21,
    "last_activity_date": 1596464733,
    "creation_date": 1453055214,
    "last_edit_date": 1493223867,
    "question_id": 9823,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9823/hyperparameter-search-for-lstm-rnn-using-keras-python",
    "title": "Hyperparameter search for LSTM-RNN using Keras (Python)",
    "body": "<p>From Keras RNN Tutorial: <em>\"RNNs are tricky. Choice of batch size is important, choice of loss and optimizer is critical, etc. Some configurations won't converge.\"</em></p>\n\n<p>So this is more a general question about tuning the hyperparameters of a LSTM-RNN on Keras. I would like to know about an approach to finding the best parameters for your RNN. </p>\n\n<p>I began with the <a href=\"https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py\">IMDB example on Keras' Github</a>.</p>\n\n<p>the main model looks like this:</p>\n\n<pre><code>(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n                                                      test_split=0.2)\n\nmax_features = 20000\nmaxlen = 100  # cut texts after this number of words (among top max_features most common words)\nbatch_size = 32\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 128, input_length=maxlen))\nmodel.add(LSTM(128))  \nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\n# try using different optimizers and different optimizer configs\nmodel.compile(loss='binary_crossentropy',\n          optimizer='adam',\n          class_mode=\"binary\")\n\nprint(\"Train...\")\nmodel.fit(X_train, y_train, batch_size=batch_size, nb_epoch=3,\n      validation_data=(X_test, y_test), show_accuracy=True)\nscore, acc = model.evaluate(X_test, y_test,\n                        batch_size=batch_size,\n                        show_accuracy=True)\n\nprint('Test accuracy:', acc)\nTest accuracy:81.54321846\n</code></pre>\n\n<p>81.5 is a fair score and more importantly it means that the model, even though not fully optimized, it works. </p>\n\n<p>My data is Time Series and the task is binary prediction, the same as the example. And now my problem looks like this:</p>\n\n<pre><code>#Training Data\ntrain = genfromtxt(os.getcwd() + \"/Data/trainMatrix.csv\", delimiter=',', skip_header=1)\nvalidation = genfromtxt(os.getcwd() + \"/Data/validationMatrix.csv\", delimiter=',', skip_header=1)\n\n#Targets\nminiTrainTargets = [int(x) for x in genfromtxt(os.getcwd() + \"/Data/trainTarget.csv\", delimiter=',', skip_header=1)]\nvalidationTargets = [int(x) for x in genfromtxt(os.getcwd() + \"/Data/validationTarget.csv\", delimiter=',', skip_header=1)]\n\n#LSTM\nmodel = Sequential()\nmodel.add(Embedding(train.shape[0], 64, input_length=train.shape[1]))\nmodel.add(LSTM(64)) \nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\n# try using different optimizers and different optimizer configs\nmodel.compile(loss='binary_crossentropy',\n          optimizer='adam',\n          class_mode=\"binary\")\n\nmodel.fit(train, miniTrainTargets, batch_size=batch_size, nb_epoch=5,\n      validation_data=(validation, validationTargets), show_accuracy=True)\nvalid_preds = model.predict_proba(validation, verbose=0)\nroc = metrics.roc_auc_score(validationTargets, valid_preds)\nprint(\"ROC:\", roc)\nROC:0.5006526\n</code></pre>\n\n<p>The model is basically the same as the IMDB one. Though the result means it's not learning anything. However, when I use a vanilla MLP-NN I don't have the same problem, the model learns and the score increases. I tried increasing the number of epochs and increasing-decreasing the number of LTSM units but the score won't increase.</p>\n\n<p>So I would like to know a standard approach to tuning the network because in theory the algorithm should perform better than a multilayer perceptron network specially for this time series data.</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "tensorflow"
    ],
    "owner": {
      "account_id": 74064,
      "reputation": 313,
      "user_id": 19172,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e79daf8b7c434fae1f3e6543eb229c5e?s=256&d=identicon&r=PG",
      "display_name": "barbolo",
      "link": "https://datascience.stackexchange.com/users/19172/barbolo"
    },
    "is_answered": true,
    "view_count": 19250,
    "accepted_answer_id": 32078,
    "answer_count": 3,
    "score": 21,
    "last_activity_date": 1681544916,
    "creation_date": 1467899502,
    "last_edit_date": 1596364134,
    "question_id": 12649,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/12649/how-to-calculate-the-mini-batch-memory-impact-when-training-deep-learning-models",
    "title": "How to calculate the mini-batch memory impact when training deep learning models?",
    "body": "<p>I'm trying to calculate the amount of memory needed by a GPU to train my model based on <a href=\"http://cs231n.github.io/convolutional-networks/#computational-considerations\" rel=\"nofollow noreferrer\">this notes</a> from Andrej Karphaty.</p>\n<p>My network has <strong>532,752 activations</strong> and <strong>19,072,984 parameters</strong> (weights and biases). These are all 32 bit floats values, so each takes 4 bytes in memory.</p>\n<p>My input image is 180x50x1 (width x height x depth) = <strong>9,000</strong> float 32 values. I don't use image augmentation, so I think the miscellaneous memory would be only related to the mini-batch size. I'm using a mini-batch size of 128 images.</p>\n<p>Based on Andrej's recommendation, I get the following memory sizes:</p>\n<p><strong>Activations:</strong> 532,752*4/(1024^2) = <strong>2.03 MB</strong></p>\n<p><strong>Parameters:</strong> 19,072,984*4/(1024^2) * 3 = <strong>218.27 MB</strong></p>\n<p><strong>Miscellaneous:</strong> 128<em>9,000</em>4/(1024^2) = <strong>4.39 MB</strong></p>\n<p>So the total memory to train this network would be <strong>224,69 MB</strong>.</p>\n<p>I'm using TensorFlow and I think I'm missing something. I haven't run the training yet, but I'm pretty sure (based on past experiences) that the memory in use will be much higher than what I've calculated.</p>\n<p>If for each image in the mini-batch, TensorFlow keeps their gradients so it can normalize them later for a single weights/biases updates step, then I think the memory should take into account another 532,752 * 128 values (gradients for each image in the mini-batch). If that is the case, then I'd need more 260.13 MB to train this model with 128 images/mini-batch.</p>\n<p>Can you help me understand the memory considerations for training my deep learning model? Are the above considerations right?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "deep-learning",
      "keras",
      "confusion-matrix"
    ],
    "owner": {
      "account_id": 1695359,
      "reputation": 1042,
      "user_id": 14360,
      "user_type": "registered",
      "accept_rate": 56,
      "profile_image": "https://www.gravatar.com/avatar/ae52e7465a397c7740bc8de66af2b9ee?s=256&d=identicon&r=PG",
      "display_name": "pseudomonas",
      "link": "https://datascience.stackexchange.com/users/14360/pseudomonas"
    },
    "is_answered": true,
    "view_count": 78501,
    "accepted_answer_id": 22559,
    "answer_count": 3,
    "score": 21,
    "last_activity_date": 1597965027,
    "creation_date": 1473261296,
    "last_edit_date": 1503612197,
    "question_id": 13894,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/13894/how-to-get-predictions-with-predict-generator-on-streaming-test-data-in-keras",
    "title": "How to get predictions with predict_generator on streaming test data in Keras?",
    "body": "<p>In the <a href=\"https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\" rel=\"noreferrer\">Keras blog on training convnets from scratch</a>, the code shows only the network running on training and validation data. What about test data? Is the validation data the same as test data (I think not). If there was a separate test folder on similar lines as the train and validation folders, how do we get a confusion matrix for the test data. I know that we have to use scikit learn or some other package to do this, but how do I get something along the lines of class wise probabilities for test data? I am hoping to use this for the confusion matrix.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "image-classification",
      "training"
    ],
    "owner": {
      "account_id": 7292252,
      "reputation": 401,
      "user_id": 24392,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/qEpOG.jpg?s=256",
      "display_name": "jlarsch",
      "link": "https://datascience.stackexchange.com/users/24392/jlarsch"
    },
    "is_answered": true,
    "view_count": 25945,
    "protected_date": 1589387869,
    "accepted_answer_id": 14041,
    "answer_count": 6,
    "score": 21,
    "last_activity_date": 1615489755,
    "creation_date": 1474034469,
    "last_edit_date": 1615489755,
    "question_id": 14039,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/14039/tool-to-label-images-for-classification",
    "title": "Tool to label images for classification",
    "body": "<p>Can anyone recommend a tool to quickly label several hundred images as an input for classification?</p>\n<p>I have ~500 microscopy images of cells. I would like to assign categories such as <code>'healthy'</code>, <code>'dead'</code>, <code>'sick'</code> manually for a training set and save those to a csv file.</p>\n<p>Basically, the same as described in <a href=\"https://datascience.stackexchange.com/questions/13335/build-a-tool-for-manually-classifying-training-data-images\">this</a> question, except I do not have proprietary images, so maybe that opens up additional possibilities?</p>\n"
  },
  {
    "tags": [
      "python",
      "r",
      "data-cleaning",
      "data",
      "sql"
    ],
    "owner": {
      "account_id": 2087101,
      "reputation": 373,
      "user_id": 29353,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8e41a9909348f1ab709c6ae2b9077768?s=256&d=identicon&r=PG",
      "display_name": "AffableAmbler",
      "link": "https://datascience.stackexchange.com/users/29353/affableambler"
    },
    "is_answered": true,
    "view_count": 17188,
    "accepted_answer_id": 17212,
    "answer_count": 6,
    "score": 21,
    "last_activity_date": 1729176044,
    "creation_date": 1487964814,
    "question_id": 17157,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/17157/do-modern-r-and-or-python-libraries-make-sql-obsolete",
    "title": "Do modern R and/or Python libraries make SQL obsolete?",
    "body": "<p>I work in an office where SQL Server is the backbone of everything we do, from data processing to cleaning to munging. My colleague specializes in writing complex functions and stored procedures to methodically process incoming data so that it can be standardized and put to work in reports, visualizations, and analytics projects. Prior to starting here, I had very little experience with SQL, aside from writing the most basic of queries. The vast majority of my analysis prep work was all done in R. My boss insists that I improve my SQL skills, even though there seem to be very few assignments that can't be done more efficiently and with far fewer lines of code using R packages like dplyr, data.table, and tidyr (to name a few). My question is--does this make sense?</p>\n\n<p>A couple weeks ago, I found myself faced with the task of getting a list of column names for each row in a table that met certain criteria and concatenating them into a vector of strings. There was a tight deadline and at the time, I was experiencing some blockage and couldn't quite wrap my head around the problem. I asked my boss, who in turn asked my colleague to write a script TSQL to solve the problem. While he was working on it, I figured out a way to do it in R writing a fairly simple function and applying it over the data frame. My colleague came back with his script about two hours later. It was at least 75 lines comprising two nested for loops. I asked him to tell notify when it was finished running and he said it would take several hours. Meanwhile my R script was able to loop over the ~45,000 records in about 30 seconds.</p>\n\n<p>Am I right to assume that R is a much better choice for cleaning and munging data?  Maybe the SQL developer in my office is just inept?  I'm curious if anyone who has worked with both R and SQL (or Python and SQL for that matter) have any thoughts on this.</p>\n"
  },
  {
    "tags": [
      "nlp",
      "scikit-learn",
      "text-mining",
      "tfidf",
      "hashingvectorizer"
    ],
    "owner": {
      "account_id": 4642185,
      "reputation": 815,
      "user_id": 2647,
      "user_type": "registered",
      "accept_rate": 56,
      "profile_image": "https://i.sstatic.net/QZGJB.jpg?s=256",
      "display_name": "Minu",
      "link": "https://datascience.stackexchange.com/users/2647/minu"
    },
    "is_answered": true,
    "view_count": 26389,
    "accepted_answer_id": 22258,
    "answer_count": 4,
    "score": 21,
    "last_activity_date": 1639301910,
    "creation_date": 1502728927,
    "last_edit_date": 1639297535,
    "question_id": 22250,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/22250/what-is-the-difference-between-a-hashing-vectorizer-and-a-tfidf-vectorizer",
    "title": "What is the difference between a hashing vectorizer and a tfidf vectorizer",
    "body": "<p>I'm converting a corpus of text documents into word vectors for each document. I've tried this using a <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\" rel=\"noreferrer\">TfidfVectorizer</a> and a <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html\" rel=\"noreferrer\">HashingVectorizer</a></p>\n\n<p>I understand that a <code>HashingVectorizer</code> does not take into consideration the <code>IDF</code> scores like a <code>TfidfVectorizer</code> does. The reason I'm still working with a <code>HashingVectorizer</code> is the flexibility it gives while dealing with huge datasets, as explained <a href=\"https://stackoverflow.com/questions/30024122/what-is-the-difference-between-hashing-vectorizer-and-count-vectorizer-when-eac\">here</a> and <a href=\"https://stackoverflow.com/questions/25145552/tfidf-for-large-dataset\">here</a>. (My original dataset has 30 million documents)</p>\n\n<p>Currently, I am working with a sample of 45339 documents, so, I have the ability to work with a <code>TfidfVectorizer</code> also. When I use these two vectorizers on the same 45339 documents, the matrices that I get are different. </p>\n\n<blockquote>\n<pre><code>hashing = HashingVectorizer()\nwith LSM('corpus.db')) as corpus:\n    hashing_matrix = hashing.fit_transform(corpus)\nprint(hashing_matrix.shape) \n</code></pre>\n  \n  <p>hashing matrix shape (45339, 1048576)</p>\n\n<pre><code>tfidf = TfidfVectorizer()\nwith LSM('corpus.db')) as corpus:\n    tfidf_matrix = tfidf.fit_transform(corpus)\nprint(tfidf_matrix.shape) \n</code></pre>\n  \n  <p>tfidf matrix shape (45339, 663307)</p>\n</blockquote>\n\n<p>I want to understand better the differences between a <code>HashingVectorizer</code> and a <code>TfidfVectorizer</code>, and the reason why these matrices are in different sizes - particularly in the number of words/terms. </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "nlp",
      "scikit-learn"
    ],
    "owner": {
      "account_id": 1801314,
      "reputation": 800,
      "user_id": 30359,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/81743cdfed553ba8ef2524664481c36b?s=256&d=identicon&r=PG",
      "display_name": "Cybernetic",
      "link": "https://datascience.stackexchange.com/users/30359/cybernetic"
    },
    "is_answered": true,
    "view_count": 42934,
    "accepted_answer_id": 25582,
    "answer_count": 3,
    "score": 21,
    "last_activity_date": 1607600477,
    "creation_date": 1513032717,
    "last_edit_date": 1576170962,
    "question_id": 25581,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/25581/what-is-the-difference-between-countvectorizer-token-counts-and-tfidftransformer",
    "title": "What is the difference between CountVectorizer token counts and TfidfTransformer with use_idf set to False?",
    "body": "<p>We can use CountVectorizer to count the number of times a word occurs in a corpus:</p>\n\n<pre><code># Tokenizing text\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(twenty_train.data)\n</code></pre>\n\n<p>If we convert this to a data frame, we can see what the tokens look like:</p>\n\n<p><a href=\"https://i.sstatic.net/eB4Uh.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/eB4Uh.png\" alt=\"enter image description here\"></a></p>\n\n<p>For example, the 35,780th word of the 3rd document occurs twice. </p>\n\n<p>We can use TfidfTransformer to count the number of times a word occurs in a corpus (only the term frequency and not the inverse) as follows:</p>\n\n<pre><code>from sklearn.feature_extraction.text import TfidfTransformer\ntf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\nX_train_tf = tf_transformer.transform(X_train_counts)\n</code></pre>\n\n<p>Converting this to a data frame, we get:</p>\n\n<p><a href=\"https://i.sstatic.net/3VnIe.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/3VnIe.png\" alt=\"enter image description here\"></a></p>\n\n<p>We can see the representation is different. The TF is shown as 0.15523. Why is this different than the token count using CountVectorizer?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "rnn",
      "lstm"
    ],
    "owner": {
      "account_id": 11556970,
      "reputation": 731,
      "user_id": 37986,
      "user_type": "registered",
      "accept_rate": 33,
      "profile_image": "https://www.gravatar.com/avatar/3ef1322d57967f65e05d0ed8fbb0eafb?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Volka",
      "link": "https://datascience.stackexchange.com/users/37986/volka"
    },
    "is_answered": true,
    "view_count": 55933,
    "accepted_answer_id": 25657,
    "answer_count": 3,
    "score": 21,
    "last_activity_date": 1544177128,
    "creation_date": 1513216402,
    "last_edit_date": 1514476092,
    "question_id": 25650,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/25650/what-is-lstm-bilstm-and-when-to-use-them",
    "title": "What is LSTM, BiLSTM and when to use them?",
    "body": "<p>I am very new to Deep learning and I am particularly interested in knowing what are LSTM and BiLSTM and when to use them (major application areas). Why are LSTM and BILSTM more popular than RNN?</p>\n\n<p>Can we use these deep learning architectures in unsupervised problems?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "keras",
      "image-classification"
    ],
    "owner": {
      "account_id": 12136242,
      "reputation": 404,
      "user_id": 47078,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/dc5ec32a220f272677d102a8c4f63a3b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Louis",
      "link": "https://datascience.stackexchange.com/users/47078/louis"
    },
    "is_answered": true,
    "view_count": 37885,
    "accepted_answer_id": 28897,
    "answer_count": 4,
    "score": 21,
    "last_activity_date": 1619205238,
    "creation_date": 1520609765,
    "last_edit_date": 1614852514,
    "question_id": 28874,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/28874/how-to-maximize-recall",
    "title": "How to maximize recall?",
    "body": "<p>I'm a little bit new to machine learning.</p>\n<p>I am using a neural network to classify images. There are two possible classes. I am using a <code>Sigmoid</code> activation at the last layer so the scores of images are between 0 to 1.</p>\n<p>I expected the scores to be sometimes close to 0.5 when the neural net is not sure about the class of the image, but all scores are either 1.0000000e+00 (due to rounding I guess) or very close to zero (for exemple 2.68440009e-15). In general, is that a good or bad thing ? How can this behaviour be avoided?</p>\n<p>In my use case I wanted to optimize for recall by setting a lower threshold but this has no impact because of what I described above.</p>\n<p>More generally, how can I minimize the number of false negatives when in training the neural net only cares about my not ad-hoc loss ? I am ok with decreasing accuracy a little bit to increase recall.</p>\n"
  },
  {
    "tags": [
      "feature-selection",
      "feature-extraction",
      "dimensionality-reduction"
    ],
    "owner": {
      "account_id": 9258481,
      "reputation": 677,
      "user_id": 32288,
      "user_type": "registered",
      "accept_rate": 25,
      "profile_image": "https://www.gravatar.com/avatar/8f72367733f53b4e97d8b9f6b71d6399?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Sid",
      "link": "https://datascience.stackexchange.com/users/32288/sid"
    },
    "is_answered": true,
    "view_count": 36092,
    "accepted_answer_id": 29018,
    "answer_count": 5,
    "score": 21,
    "last_activity_date": 1582420652,
    "creation_date": 1520919135,
    "last_edit_date": 1548988991,
    "question_id": 29006,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/29006/feature-selection-vs-feature-extraction-which-to-use-when",
    "title": "Feature selection vs Feature extraction. Which to use when?",
    "body": "<p>Feature extraction and feature selection essentially reduce the dimensionality of the data, but feature extraction also makes the data more separable, if I am  right. </p>\n\n<p>Which technique would be <em>preferred</em> over the other and when? </p>\n\n<p>I was thinking, since feature selection does not modify the original data and it's properties, I assume that you will use feature selection when it's important that the features you're training on be unchanged. But I can't imagine why you would want something like this.. </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "logistic-regression",
      "multilabel-classification"
    ],
    "owner": {
      "account_id": 7077818,
      "reputation": 351,
      "user_id": 39423,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7b24144d6f8c8af8713ef3aaebc20c19?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "joydeep bhattacharjee",
      "link": "https://datascience.stackexchange.com/users/39423/joydeep-bhattacharjee"
    },
    "is_answered": true,
    "view_count": 3642,
    "answer_count": 1,
    "score": 21,
    "last_activity_date": 1561325978,
    "creation_date": 1524294388,
    "last_edit_date": 1561325978,
    "question_id": 30605,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/30605/what-does-it-mean-to-share-parameters-between-features-and-classes",
    "title": "What does it mean to &quot;share parameters between features and classes&quot;",
    "body": "<p>When reading <a href=\"https://arxiv.org/abs/1607.01759\" rel=\"noreferrer\">this paper</a> there is a line which says \"linear  classifiers  do not  share  parameters  among  features  and  classes.\" What is the meaning of this statement? Does it mean that linear classifiers such as logistic regression need features that are mutually independent?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "image-classification",
      "preprocessing",
      "image-recognition"
    ],
    "owner": {
      "account_id": 2181927,
      "reputation": 333,
      "user_id": 27369,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/JGvfm.jpg?s=256",
      "display_name": "Odgiiv",
      "link": "https://datascience.stackexchange.com/users/27369/odgiiv"
    },
    "is_answered": true,
    "view_count": 47298,
    "accepted_answer_id": 30825,
    "answer_count": 3,
    "score": 21,
    "last_activity_date": 1649747936,
    "creation_date": 1524664007,
    "last_edit_date": 1592475177,
    "question_id": 30819,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/30819/image-resizing-and-padding-for-cnn",
    "title": "Image resizing and padding for CNN",
    "body": "<p>I want to train a CNN for image recognition. Images for training have not fixed size. I want the input size for the CNN to be 50x100 (height x width), for example. When I resize some small sized images (for example 32x32) to input size, the content of the image is stretched horizontally too much, but for some medium size images it looks okay.</p>\n\n<p>What is the proper method for resizing images while avoiding the content being destroyed? </p>\n\n<p>(I am thinking about padding images with 0s to complete size after resizing them to some degree keeping ratio of width and height. Would it be okay with this method?)</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network"
    ],
    "owner": {
      "account_id": 11168678,
      "reputation": 119,
      "user_id": 64895,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-qHaA1VycdBY/AAAAAAAAAAI/AAAAAAAAAQo/3ACoJM3L9UM/s256-rj/photo.jpg",
      "display_name": "Furkan Toprak",
      "link": "https://datascience.stackexchange.com/users/64895/furkan-toprak"
    },
    "is_answered": true,
    "view_count": 4471,
    "protected_date": 1579190869,
    "accepted_answer_id": 43177,
    "answer_count": 9,
    "community_owned_date": 1545922797,
    "score": 21,
    "last_activity_date": 1667658182,
    "creation_date": 1545806731,
    "last_edit_date": 1632178502,
    "question_id": 43148,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/43148/how-do-i-learn-neural-networks",
    "title": "How Do I Learn Neural Networks?",
    "body": "<p>I'm a freshman undergraduate student (mentioning this so you may forgive my unfamiliarity) who is currently doing research using neural networks. I've coded a three-node neural network (that works) based on my professor's guidance. However, I'd like to pursue a career in AI and Data Science, and I'd like to teach myself more about these properly in-depth. Are there any books or resources that will teach me more about neural network structures, deep learning, etc? Are there any recommendations?</p>\n<p>Note: I'm proficient in Java, Python, Bash, JavaScript, Matlab, and know a bit of C++.</p>\n"
  },
  {
    "tags": [
      "keras",
      "prediction",
      "backpropagation",
      "cost-function",
      "methods"
    ],
    "owner": {
      "account_id": 4262249,
      "reputation": 1310,
      "user_id": 67743,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ecc8ba04bbf35d83c866bf36496084d5?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user3486308",
      "link": "https://datascience.stackexchange.com/users/67743/user3486308"
    },
    "is_answered": true,
    "view_count": 26014,
    "accepted_answer_id": 46127,
    "answer_count": 1,
    "score": 21,
    "last_activity_date": 1615677536,
    "creation_date": 1550993267,
    "last_edit_date": 1615677536,
    "question_id": 46124,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/46124/what-do-compile-fit-and-predict-do-in-keras-sequential-models",
    "title": "What do &quot;compile&quot;, &quot;fit&quot;, and &quot;predict&quot; do in Keras sequential models?",
    "body": "<p>I am a little confused between these two parts of <code>Keras sequential models</code> functions. May someone explains what is exactly the job of each one? I mean <code>compile</code> doing forward pass and calculating <code>cost function</code> then pass it through <code>fit</code> to do backward pass and calculating <code>derivatives and updating weights</code>? Or what?</p>\n\n<p>I have seen in some codes, they only used <code>compile</code> function for some of their <code>LSTM</code>s and <code>fit</code> for some other ones! So I need to know each of these functions do what part of the work(training a neural network). </p>\n\n<p>It's also interesting for me to know what exactly do <code>predict</code> function as well.</p>\n\n<p>Very thank you in advanced!</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "deep-learning",
      "attention-mechanism",
      "transformer",
      "bert"
    ],
    "owner": {
      "account_id": 2859532,
      "reputation": 721,
      "user_id": 62846,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/pgOHOCfg.jpg?s=256",
      "display_name": "CoderOnly",
      "link": "https://datascience.stackexchange.com/users/62846/coderonly"
    },
    "is_answered": true,
    "view_count": 12319,
    "accepted_answer_id": 46382,
    "answer_count": 1,
    "score": 21,
    "last_activity_date": 1616003842,
    "creation_date": 1551343062,
    "last_edit_date": 1551922661,
    "question_id": 46377,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/46377/can-bert-do-the-next-word-predict-task",
    "title": "Can BERT do the next-word-predict task?",
    "body": "<p>As BERT is bidirectional (uses bi-directional transformer), is it possible to use it for the next-word-predict task? If yes, what needs to be tweaked?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "keras",
      "cnn"
    ],
    "owner": {
      "account_id": 8658944,
      "reputation": 357,
      "user_id": 50204,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-8yPdfuEgiyQ/AAAAAAAAAAI/AAAAAAAAACc/CDfou3nvYNM/s256-rj/photo.jpg",
      "display_name": "Saurabh",
      "link": "https://datascience.stackexchange.com/users/50204/saurabh"
    },
    "is_answered": true,
    "view_count": 14943,
    "accepted_answer_id": 51609,
    "answer_count": 2,
    "score": 21,
    "last_activity_date": 1557392046,
    "creation_date": 1557125990,
    "last_edit_date": 1557392046,
    "question_id": 51470,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/51470/what-are-the-differences-between-convolutional1d-convolutional2d-and-convoluti",
    "title": "What are the differences between Convolutional1D, Convolutional2D, and Convolutional3D?",
    "body": "<p>I've been learning about Convolutional Neural Networks. When looking at <code>Keras</code> examples, I came across three different convolution methods. Namely, 1D, 2D &amp; 3D. What are the differences between these three layers? What are their use cases? Are there some links or references to show their use cases?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "feature-selection"
    ],
    "owner": {
      "account_id": 3835201,
      "reputation": 393,
      "user_id": 43765,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/87172d77484417f9dbba385a53a0dfa6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "cloudscomputes",
      "link": "https://datascience.stackexchange.com/users/43765/cloudscomputes"
    },
    "is_answered": true,
    "view_count": 33266,
    "accepted_answer_id": 54045,
    "answer_count": 6,
    "score": 21,
    "last_activity_date": 1630590274,
    "creation_date": 1560845358,
    "last_edit_date": 1560884079,
    "question_id": 53995,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/53995/what-does-embedding-mean-in-machine-learning",
    "title": "What does embedding mean in machine learning?",
    "body": "<p>I just met a terminology called \"embedding\" in a paper regarding deep learning. The context is \"multi-modal embedding\"</p>\n\n<p>My guess: embedding of something is extract some feature of sth,to form a vector.</p>\n\n<p>I couldn't get the explicit meaning for this terminology and that stops me from fully understanding the author's idea and model mechanism</p>\n\n<p>I check the dictionary and search on line,but the explanation is based more on the real life meaning rather than meaning as a machine learning terminology.</p>\n\n<p>And that raise a more generalized and frequently met question, when you find some machine learning terminology/word that you can't understand well, where can you get the solution, some specific way to google? join a machine learning group? raise a question in stack exchange?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "keras",
      "tensorflow",
      "cnn",
      "convolution"
    ],
    "owner": {
      "account_id": 1117437,
      "reputation": 333,
      "user_id": 52350,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-e58_Nsa70nU/AAAAAAAAAAI/AAAAAAAAHT0/TqqhvRUvb3w/s256-rj/photo.jpg",
      "display_name": "Sanjay",
      "link": "https://datascience.stackexchange.com/users/52350/sanjay"
    },
    "is_answered": true,
    "view_count": 39540,
    "accepted_answer_id": 55547,
    "answer_count": 2,
    "score": 21,
    "last_activity_date": 1691380727,
    "creation_date": 1562914323,
    "question_id": 55545,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/55545/in-cnn-why-do-we-increase-the-number-of-filters-in-deeper-convolution-layers-fo",
    "title": "In CNN, why do we increase the number of filters in deeper Convolution layers for complex images?",
    "body": "<p>I have been doing this online course <em>Introduction to TensorFlow for AI, ML and DL</em>. Here in one part, they were showing a CNN model for classifying human and horses. In this model, the first <code>Conv2D</code> layer had <strong>16 filters</strong>, followed by two more <code>Conv2D</code> layers with <strong>32 and 64 filters</strong> respectively. I am not sure how the number of filters is correlated with the deeper convolution layers.</p>\n\n<p><a href=\"https://i.sstatic.net/JFpxA.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/JFpxA.jpg\" alt=\"Declaration of the model\"></a></p>\n"
  },
  {
    "tags": [
      "nlp",
      "machine-translation"
    ],
    "owner": {
      "account_id": 13655170,
      "reputation": 806,
      "user_id": 77914,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b6003aa2006bbab69f3d3ef26580fdfa?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Amit Keinan",
      "link": "https://datascience.stackexchange.com/users/77914/amit-keinan"
    },
    "is_answered": true,
    "view_count": 5076,
    "accepted_answer_id": 68577,
    "answer_count": 3,
    "score": 21,
    "last_activity_date": 1660285343,
    "creation_date": 1582477730,
    "last_edit_date": 1582488819,
    "question_id": 68562,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/68562/what-is-the-bleu-score-of-professional-human-translators",
    "title": "What is the bleu score of professional human translators?",
    "body": "<p>Machine translation models are usually evaluated using bleu score. I want to get some intuition for this score. What is the bleu score of professional human translator? </p>\n\n<p>I know it depends on the languages, the translator ect. I just want to get the scale.</p>\n\n<p>edit: I want to make it clear - I talk about the expected bleu. It's not a theoretical question, it is an experimental one.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "statistics",
      "linear-regression",
      "overfitting"
    ],
    "owner": {
      "account_id": 18286732,
      "reputation": 379,
      "user_id": 103269,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Ggkwr7Jg8qG1gwz8WK9IUpo7dTK8RgmaQqm2lXuUw=k-s256",
      "display_name": "Sachin Krishna",
      "link": "https://datascience.stackexchange.com/users/103269/sachin-krishna"
    },
    "is_answered": true,
    "view_count": 22964,
    "answer_count": 3,
    "score": 21,
    "last_activity_date": 1733259663,
    "creation_date": 1598518325,
    "last_edit_date": 1598525306,
    "question_id": 80868,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/80868/overfitting-in-linear-regression",
    "title": "Overfitting in Linear Regression",
    "body": "<p>I'm just getting started with machine learning and I have trouble understanding how overfitting can happen in a linear regression model.</p>\n<p>Considering we use only 2 feature variables to train a model, how can a flat plane possibly be overfitted to a set of data points?</p>\n<p>I assume linear regression uses only a line to describe the linear relationship between 2 variables and a flat plane to describe the relationship between 3 variables, I have trouble understanding (or rather imagining) how overfitting in a line or a plane can happen?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "neural-network",
      "activation-function"
    ],
    "owner": {
      "account_id": 12159311,
      "reputation": 413,
      "user_id": 125901,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0f5ebd4c8e7e53f49a24fd4fd9eb0202?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Konstantinos Skoularikis",
      "link": "https://datascience.stackexchange.com/users/125901/konstantinos-skoularikis"
    },
    "is_answered": true,
    "view_count": 13674,
    "accepted_answer_id": 102810,
    "answer_count": 2,
    "score": 21,
    "last_activity_date": 1638129561,
    "creation_date": 1633202251,
    "question_id": 102724,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/102724/why-deep-learning-models-still-use-relu-instead-of-selu-as-their-activation-fun",
    "title": "Why deep learning models still use RELU instead of SELU, as their activation function?",
    "body": "<p>I am a trying to understand the <a href=\"https://arxiv.org/pdf/1706.02515v5.pdf\" rel=\"noreferrer\">SELU</a> activation function and I was wondering why deep learning practitioners keep using RELU, with all its issues, instead of SELU, which enables a neural network to converge faster and internally normalizes each layer?</p>\n"
  },
  {
    "tags": [
      "statistics",
      "bayesian",
      "pvalue",
      "statistical-significance"
    ],
    "owner": {
      "account_id": 993147,
      "reputation": 3363,
      "user_id": 7730,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/fhpik.jpg?s=256",
      "display_name": "Robert Long",
      "link": "https://datascience.stackexchange.com/users/7730/robert-long"
    },
    "is_answered": true,
    "view_count": 3963,
    "protected_date": 1732180261,
    "accepted_answer_id": 130750,
    "answer_count": 2,
    "score": 21,
    "last_activity_date": 1732471884,
    "creation_date": 1731850836,
    "last_edit_date": 1731860736,
    "question_id": 130749,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/130749/what-is-the-correct-meaning-and-interpretation-of-p-values",
    "title": "What is the correct meaning and interpretation of p-values?",
    "body": "<p>Im posting this question, <a href=\"https://meta.stackexchange.com/help/self-answer\">and an answer</a>, to help dispel a few misunderstandings about what p-values are. As a hiring manager interviewing mid-level and senior data scientists, I have noticed these misunderstandings many times. I have also noticed several posts here on DS.SE where the poster has misinterpreted, or misunderstood, p-values, so rather than point it out every time with details, I thought it better to make a question and answer on the topic. The intention is to hopefully create a &quot;canonical&quot; Q&amp;A that the community can refer to, if/when appropriate. I would welcome any comments in case I have made any mistakes.</p>\n<p>What follows is very heavily based on the paper by Haller and Krauss (2002) who investigated misconceptions about null hypothesis significance testing (NHST) among three groups: psychology students, scientific psychologists, and methodology instructors teaching statistics to Psychology students German universities. They used a survey consisting of six statements about the interpretation of a p-value derived from a t-test. Participants were asked to classify each statement as &quot;true&quot; or &quot;false&quot;.</p>\n<hr />\n<h3>A questionnaire:</h3>\n<p>Suppose you have a treatment that you suspect may alter performance on a task. You compare the means of your control and experimental groups (say 20 subjects in each sample). Then, you use an <a href=\"https://en.wikipedia.org/wiki/Student%27s_t-test#Independent_(unpaired)_samples\" rel=\"noreferrer\">independent means t-test</a> and your result is (t = 2.7, d.f. = 18, p = 0.01). Please choose &quot;true&quot; or &quot;false&quot;, and  note that several or none of the statements may be correct.</p>\n<ol>\n<li>You have disproved the null hypothesis (that is, there is no difference between the population means).</li>\n<li>You have found the probability of the null hypothesis being true.</li>\n<li>You have proved your experimental hypothesis (that there is a difference between the population means).</li>\n<li>You can deduce the probability of the experimental hypothesis being true.</li>\n<li>You know, if you decide to reject the null hypothesis, the probability that you are making the wrong decision.</li>\n<li>You have a reliable experimental finding in the sense that if, hypothetically, the experiment were repeated a great number of times, you would obtain a significant result on 99% of occasions.</li>\n</ol>\n<p>I would encourage everyone reading this to think deeply about these questions and come up with their answers, <strong>before</strong> reading my answer which follows. I would also encourage people to read the Haller and Krauss (2002) paper on which this is based (there is a link to the pdf in the References section)</p>\n<h3>References</h3>\n<ul>\n<li>Haller, H., &amp; Krauss, S. (2002). Misinterpretations of significance: A problem students share with their teachers. <em>Methods of Psychological Research</em>, 7(1), 120. <a href=\"http://www.krigolsonteaching.com/uploads/4/3/8/4/43848243/haller_krauss_2002.pdf\" rel=\"noreferrer\">PDF</a></li>\n</ul>\n"
  },
  {
    "tags": [
      "classification",
      "binary",
      "svm",
      "random-forest",
      "logistic-regression"
    ],
    "owner": {
      "account_id": 241216,
      "reputation": 5474,
      "user_id": 97,
      "user_type": "registered",
      "accept_rate": 77,
      "profile_image": "https://www.gravatar.com/avatar/68acdc33e278ab19debf230662a7ad96?s=256&d=identicon&r=PG",
      "display_name": "IgorS",
      "link": "https://datascience.stackexchange.com/users/97/igors"
    },
    "is_answered": true,
    "view_count": 28365,
    "accepted_answer_id": 395,
    "answer_count": 5,
    "score": 20,
    "last_activity_date": 1428855159,
    "creation_date": 1402840898,
    "last_edit_date": 1402927362,
    "question_id": 384,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/384/choose-binary-classification-algorithm",
    "title": "Choose binary classification algorithm",
    "body": "<p>I have a binary classification problem:</p>\n\n<ul>\n<li>Approximately 1000 samples in training set</li>\n<li>10 attributes, including binary, numeric and categorical</li>\n</ul>\n\n<p>Which algorithm is the best choice for this type of problem?</p>\n\n<p>By default I'm going to start with SVM (preliminary having nominal attributes values converted to binary features), as it is considered the best for relatively clean and  not noisy data. </p>\n"
  },
  {
    "tags": [
      "data-mining",
      "clustering",
      "k-means"
    ],
    "owner": {
      "account_id": 2179908,
      "reputation": 333,
      "user_id": 9388,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Meyb8.jpg?s=256",
      "display_name": "ngub05",
      "link": "https://datascience.stackexchange.com/users/9388/ngub05"
    },
    "is_answered": true,
    "view_count": 23540,
    "accepted_answer_id": 5662,
    "answer_count": 4,
    "score": 20,
    "last_activity_date": 1597582740,
    "creation_date": 1430401325,
    "last_edit_date": 1430574032,
    "question_id": 5656,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/5656/k-means-what-are-some-good-ways-to-choose-an-efficient-set-of-initial-centroids",
    "title": "K-means: What are some good ways to choose an efficient set of initial centroids?",
    "body": "<p>When a random initialization of centroids is used, different runs of K-means produce different total SSEs. And it is crucial in the performance of the algorithm. \nWhat are some effective approaches toward solving this problem? Recent approaches are appreciated.</p>\n"
  },
  {
    "tags": [
      "python",
      "time-series",
      "markov-process"
    ],
    "owner": {
      "account_id": 6165335,
      "reputation": 1803,
      "user_id": 13450,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://www.gravatar.com/avatar/7b4fe85f09404d96a10df58489d834bc?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "neural-nut",
      "link": "https://datascience.stackexchange.com/users/13450/neural-nut"
    },
    "is_answered": true,
    "view_count": 16909,
    "answer_count": 5,
    "score": 20,
    "last_activity_date": 1596446615,
    "creation_date": 1444977935,
    "question_id": 8460,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/8460/python-library-to-implement-hidden-markov-models",
    "title": "Python library to implement Hidden Markov Models",
    "body": "<p>What <em>stable</em> Python library can I use to implement Hidden Markov Models? I need it to be reasonably well documented, because I've never really used this model before.</p>\n\n<p>Alternatively, is there a more direct approach to performing a time-series analysis on a data-set using HMM?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-mining",
      "feature-selection",
      "feature-extraction",
      "feature-engineering"
    ],
    "owner": {
      "account_id": 3509867,
      "reputation": 539,
      "user_id": 2504,
      "user_type": "registered",
      "accept_rate": 0,
      "profile_image": "https://www.gravatar.com/avatar/b12a8d7b2ccb4114402efd37c3c13023?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "icm",
      "link": "https://datascience.stackexchange.com/users/2504/icm"
    },
    "is_answered": true,
    "view_count": 14476,
    "answer_count": 1,
    "score": 20,
    "last_activity_date": 1597597380,
    "creation_date": 1458703553,
    "last_edit_date": 1559964887,
    "question_id": 10839,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/10839/what-is-difference-between-one-hot-encoding-and-leave-one-out-encoding",
    "title": "What is difference between one hot encoding and leave one out encoding?",
    "body": "<p>I am reading a presentation and it recommends not using leave one out encoding, but it is okay with one hot encoding. I thought they both were the same. Can anyone describe what the differences between them are?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "nlp",
      "rnn"
    ],
    "owner": {
      "account_id": 2252789,
      "reputation": 2177,
      "user_id": 22012,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://i.sstatic.net/4SJZF.jpg?s=256",
      "display_name": "tastyminerals",
      "link": "https://datascience.stackexchange.com/users/22012/tastyminerals"
    },
    "is_answered": true,
    "view_count": 17473,
    "accepted_answer_id": 13179,
    "answer_count": 4,
    "score": 20,
    "last_activity_date": 1661500092,
    "creation_date": 1470091096,
    "question_id": 13138,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/13138/what-is-the-difference-between-word-based-and-char-based-text-generation-rnns",
    "title": "What is the difference between word-based and char-based text generation RNNs?",
    "body": "<p>While reading about text generation with Recurrent Neural Networks I noticed that some examples were implemented to generate text <strong>word by word</strong> and others <strong>character by character</strong> without actually stating why.</p>\n\n<p>So, what is the difference between RNN models that predict text <strong>per-word</strong> basis and the ones that predict text <strong>per-char</strong> basis? Do word-based RNN require a bigger corpus size? Do char-based RNN generalize better? Maybe the only difference is input representation (one-hot encoding, word embeddings)? Which ones to choose for text generation? </p>\n"
  },
  {
    "tags": [
      "dimensionality-reduction",
      "tsne"
    ],
    "owner": {
      "account_id": 2350822,
      "reputation": 407,
      "user_id": 28384,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/hcrgw.jpg?s=256",
      "display_name": "Nitro",
      "link": "https://datascience.stackexchange.com/users/28384/nitro"
    },
    "is_answered": true,
    "view_count": 2047,
    "accepted_answer_id": 17971,
    "answer_count": 1,
    "score": 20,
    "last_activity_date": 1491008470,
    "creation_date": 1488473165,
    "question_id": 17314,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/17314/are-t-sne-dimensions-meaningful",
    "title": "Are t-sne dimensions meaningful?",
    "body": "<p>Are there any meanings for the dimensions of a t-sne embedding? Like with PCA we have this sense of linearly transformed variance maximizations but for t-sne is there intuition besides just the space we define for mapping and minimization of the KL-distance?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "dimensionality-reduction",
      "autoencoder"
    ],
    "owner": {
      "account_id": 509508,
      "reputation": 261,
      "user_id": 37475,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/62d85c362bf553bdf1d14e68c3c069cf?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "dcl",
      "link": "https://datascience.stackexchange.com/users/37475/dcl"
    },
    "is_answered": true,
    "view_count": 10611,
    "accepted_answer_id": 23744,
    "answer_count": 3,
    "score": 20,
    "last_activity_date": 1615493432,
    "creation_date": 1507872323,
    "question_id": 23739,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/23739/why-are-autoencoders-for-dimension-reduction-symmetrical",
    "title": "Why are autoencoders for dimension reduction symmetrical?",
    "body": "<p>I'm not an expert in autoencoders or neural networks by any means, so forgive me if this is a silly question. </p>\n\n<p>For the purpose of dimension reduction or visualizing clusters in high dimensional data, we can use an autoencoder to create a (lossy) 2 dimensional representation by inspecting the output of the network layer with 2 nodes. For example, with the following architecture, we would inspect the output of the third layer</p>\n\n<p>$[X] \\rightarrow N_1=100 \\rightarrow N_2=25 \\rightarrow (N_3=2) \\rightarrow N_4=25 \\rightarrow N_5=100 \\rightarrow [X]$</p>\n\n<p>where $X$ is the input data and $N_l$ is the number of nodes in the $l$th layer.</p>\n\n<p>Now, my question is, why do we want a symmetrical architecture? Doesn't a mirror of the deep 'compression' phase mean we might have a similarly complex 'decompression' phase resulting in a 2 node output which is not forced to be very intuitive? In other words, wouldn't having a simpler decoding phase result in the output of the layer with 2 nodes necessarily being simpler too?</p>\n\n<p>My thinking here is that the less complex the decompression phase, the simpler (more linear?) the 2D representation has to be. A more complex decompression phase would allow a more complex 2D representation.</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "deep-learning",
      "loss-function",
      "parameter-estimation"
    ],
    "owner": {
      "account_id": 5974351,
      "reputation": 9438,
      "user_id": 14904,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://i.sstatic.net/vh2Hd.jpg?s=256",
      "display_name": "Jan van der Vegt",
      "link": "https://datascience.stackexchange.com/users/14904/jan-van-der-vegt"
    },
    "is_answered": true,
    "view_count": 9744,
    "accepted_answer_id": 25102,
    "answer_count": 2,
    "score": 20,
    "last_activity_date": 1692706481,
    "creation_date": 1511278380,
    "question_id": 24986,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/24986/parameterization-regression-of-rotation-angle",
    "title": "Parameterization regression of rotation angle",
    "body": "<p>Let's say I have a top-down picture of an arrow, and I want to predict the angle this arrow makes. This would be between $0$ and $360$ degrees, or between $0$ and $2\\pi$. The problem is that this target is circular, $0$ and $360$ degrees are exactly the same which is an invariance I would like to incorporate in my target, which should help generalization significantly (this is my assumption). The problem is that I don't see a clean way of solving this, are there any papers that try to tackle this problem (or similar ones)? I do have some ideas with their potential downsides:</p>\n\n<ul>\n<li><p>Use a sigmoid or tanh activation, scale it to the ($0, 2\\pi)$ range and incorporate the circular property in the loss function. I think this will fail fairly hard, because if it's on the border (worst prediction) only a tiny bit of noise will push the weights to go one way or the other. Also, values closer to the border of $0$ and $2\\pi$ will be more difficult to reach because the absolute pre-activation value will need to be close to infinite.</p></li>\n<li><p>Regress to two values, a $x$ and $y$ value and calculate the loss based on the angle these two values make. I think this one has more potential but the norm of this vector is unbounded, which could lead to numeric instability and could lead to blow ups or going to 0 during training. This could potentially be solved by using some weird regularizer to prevent this norm from going too far away from 1.</p></li>\n</ul>\n\n<p>Other options would be doing something with sine and cosine functions but I feel like the fact that multiple pre-activations map to the same output will also make optimization and generalizations very difficult.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "neural-network",
      "convolutional-neural-network",
      "overfitting"
    ],
    "owner": {
      "account_id": 8220803,
      "reputation": 369,
      "user_id": 45898,
      "user_type": "registered",
      "accept_rate": 80,
      "profile_image": "https://www.gravatar.com/avatar/bd28015dbb4f4ced5b455ee4d20d46a6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user121",
      "link": "https://datascience.stackexchange.com/users/45898/user121"
    },
    "is_answered": true,
    "view_count": 50051,
    "accepted_answer_id": 27562,
    "answer_count": 2,
    "score": 20,
    "last_activity_date": 1613234545,
    "creation_date": 1518010445,
    "last_edit_date": 1613234545,
    "question_id": 27561,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/27561/can-the-number-of-epochs-influence-overfitting",
    "title": "Can the number of epochs influence overfitting?",
    "body": "<p>I am using a convolution neural network ,<code>CNN</code>. At a specific epoch, I only save the best CNN model weights based on improved validation accuracy over previous epochs.</p>\n\n<p>Does increasing the number of epochs also increase over-fitting for <code>CNNs</code> and deep learning in general?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "keras",
      "weighted-data"
    ],
    "owner": {
      "account_id": 2460255,
      "reputation": 3480,
      "user_id": 13023,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://www.gravatar.com/avatar/a15c4546ec5b537e7340c95be39aa52e?s=256&d=identicon&r=PG",
      "display_name": "wacax",
      "link": "https://datascience.stackexchange.com/users/13023/wacax"
    },
    "is_answered": true,
    "view_count": 32750,
    "accepted_answer_id": 31358,
    "answer_count": 2,
    "score": 20,
    "last_activity_date": 1526061568,
    "creation_date": 1525284278,
    "question_id": 31129,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/31129/sample-importance-training-weights-in-keras",
    "title": "Sample Importance (Training Weights) in Keras",
    "body": "<p>How do you add more importance to some samples than others (sample weights) in Keras? </p>\n\n<p>I'm not looking for <code>class_weight</code>which is a fix for unbalanced datasets.</p>\n\n<p>What I currently have is:</p>\n\n<p><code>trainingWeights</code> which is the desired importance I want to give to each sample.</p>\n\n<pre><code>epochs = 30\nbatchSize = 512\n\n# Fit model with selected data\nmodel.fit(trainingMatrix, trainingTargets,\n          batch_size=batchSize, epochs=epochs, \n          sample_weight=trainingWeights)\n</code></pre>\n\n<p>However the training error is much lower than before, and according to <a href=\"https://keras.io/models/sequential/\" rel=\"noreferrer\">Keras' documentation</a>:</p>\n\n<blockquote>\n  <p>sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only).</p>\n</blockquote>\n\n<p>As I understand it, this option only calculates the loss function differently without training the model with weights (sample importance) so how do I train a Keras model with different importance (weights) for different samples. </p>\n\n<p>PD. This is a similar question <a href=\"https://datascience.stackexchange.com/questions/9488/xgboost-give-more-importance-to-recent-samples/9493#9493\">xgboost: give more importance to recent samples</a> but I would like an applicable answer to Keras.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "model-evaluations",
      "class-imbalance"
    ],
    "owner": {
      "account_id": 273295,
      "reputation": 303,
      "user_id": 38864,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/dd1fc7de6d96bd3b538b3f5346e3dd90?s=256&d=identicon&r=PG",
      "display_name": "Krrr",
      "link": "https://datascience.stackexchange.com/users/38864/krrr"
    },
    "is_answered": true,
    "view_count": 26847,
    "accepted_answer_id": 36870,
    "answer_count": 4,
    "score": 20,
    "last_activity_date": 1587461204,
    "creation_date": 1534154257,
    "last_edit_date": 1534164232,
    "question_id": 36862,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/36862/macro-or-micro-average-for-imbalanced-class-problems",
    "title": "Macro- or micro-average for imbalanced class problems",
    "body": "<p>The question of whether to use macro- or micro-averages when the data is imbalanced comes up all the time.</p>\n\n<p>Some googling shows that many bloggers tend to say that micro-average is the preferred way to go, e.g.:</p>\n\n<ol>\n<li><a href=\"https://medium.com/usf-msds/choosing-the-right-metric-for-evaluating-machine-learning-models-part-2-86d5649a5428\" rel=\"noreferrer\">Micro-average is preferable if there is a class imbalance problem.</a></li>\n<li><a href=\"http://rushdishams.blogspot.com/2011/08/micro-and-macro-average-of-precision.html\" rel=\"noreferrer\">On the other hand, micro-average can be a useful measure when your dataset varies in size.</a></li>\n</ol>\n\n<p>A <a href=\"https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin\">similar question in this forum</a> suggests a similar answer.</p>\n\n<p>However, this seems quite counter-intuitive. For example if we have a data set with 90%-10% class distribution then a baseline classifier can achieve 90% mico-averaged accuracy by assigning the majority class label. </p>\n\n<p>This is corroborated by books, e.g. <a href=\"https://nlp.stanford.edu/IR-book/\" rel=\"noreferrer\">An Introduction to Information Retrieval</a> says (page 282) \"Microaveraged results are therefore really a measure of effectiveness on the large classes in a test collection.  To get a sense of effectiveness on small classes, you should compute macroaveraged results.\"</p>\n\n<p>In the end the real decision about which measure to use should be based on the relative mis-classification costs for the classes. But a quick look at the internet seems to suggest use of micro-averaging. Is this correct or misleading?</p>\n"
  },
  {
    "tags": [
      "xgboost",
      "regularization",
      "lightgbm"
    ],
    "owner": {
      "account_id": 15081138,
      "reputation": 466,
      "user_id": 70169,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0d3257c81dcfe7dd6e415917cda8804b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Vikrant Arora",
      "link": "https://datascience.stackexchange.com/users/70169/vikrant-arora"
    },
    "is_answered": true,
    "view_count": 32196,
    "accepted_answer_id": 57450,
    "answer_count": 3,
    "score": 20,
    "last_activity_date": 1633083044,
    "creation_date": 1565284124,
    "last_edit_date": 1565296669,
    "question_id": 57255,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/57255/l1-l2-regularization-in-light-gbm",
    "title": "L1 &amp; L2 Regularization in Light GBM",
    "body": "<p>This question pertains to L1 &amp; L2 regularization parameters in Light GBM. As per official documentation:</p>\n\n<p><code>reg_alpha (float, optional (default=0.))</code>  L1 regularization term on weights.</p>\n\n<p><code>reg_lambda (float, optional (default=0.))</code>  L2 regularization term on weights</p>\n\n<p>I have seen data scientists using both of these parameters at the same time, ideally either you use L1 or L2 not both together. </p>\n\n<p>While reading about tuning LGBM parameters I cam across one such case: Kaggle official GBDT Specification and Optimization Workshop in Paris where Instructors are ML experts. And these experts have used positive values of both L1 &amp; L2 params in LGBM model. Link below (Ctrl+F 'search_spaces' to directly reach parameter grid in this long kernel) </p>\n\n<p><a href=\"http://www.kaggle.com/lucamassaron/kaggle-days-paris-gbdt-workshop\" rel=\"noreferrer\">http://www.kaggle.com/lucamassaron/kaggle-days-paris-gbdt-workshop</a> </p>\n\n<p>I have seen same in XGBoost implementations.</p>\n\n<p>My question is why use both at the same time in LGBM/XGBoost.</p>\n\n<p>Thanks.</p>\n"
  },
  {
    "tags": [
      "keras",
      "tensorflow",
      "activation-function"
    ],
    "owner": {
      "account_id": 16660018,
      "reputation": 201,
      "user_id": 80878,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/41513b567302b9e0ca5a1026156d1ff6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Basta",
      "link": "https://datascience.stackexchange.com/users/80878/basta"
    },
    "is_answered": true,
    "view_count": 22987,
    "answer_count": 3,
    "score": 20,
    "last_activity_date": 1676725708,
    "creation_date": 1568014492,
    "last_edit_date": 1568063613,
    "question_id": 58884,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/58884/how-to-create-custom-activation-functions-in-keras-tensorflow",
    "title": "How to create custom Activation functions in Keras / TensorFlow?",
    "body": "<p>I'm using keras and I wanted to add my own activation function myf to tensorflow backend. how to define the new function and make it operational. so instead of the line of code: </p>\n\n<pre><code>model.add(layers.Conv2D(64, (3, 3), activation='relu')) \n</code></pre>\n\n<p>I'll write </p>\n\n<pre><code>model.add(layers.Conv2D(64, (3, 3), activation='myf')). \n</code></pre>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-product"
    ],
    "owner": {
      "account_id": 11245218,
      "reputation": 6136,
      "user_id": 50727,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/9Y5b3.jpg?s=256",
      "display_name": "David Masip",
      "link": "https://datascience.stackexchange.com/users/50727/david-masip"
    },
    "is_answered": true,
    "view_count": 4258,
    "protected_date": 1590498594,
    "accepted_answer_id": 64808,
    "answer_count": 8,
    "score": 20,
    "last_activity_date": 1677842412,
    "creation_date": 1576239359,
    "question_id": 64764,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/64764/monitoring-machine-learning-models-in-production",
    "title": "Monitoring machine learning models in production",
    "body": "<p>I am looking for tools that allow me to monitor machine learning models once they are gone to production. I would like to monitor:</p>\n\n<ol>\n<li>Long term changes: changes of distribution in the features with respect to training time, that would suggest retraining the model.</li>\n<li>Short term changes: bugs in the features (radical changes of distribution).</li>\n<li>Changes in the performance of the model with respect to a given metric.</li>\n</ol>\n\n<p>I have been looking over the Internet, but I don't see any in-depth analysis of any of the cases. Can you provide me with techniques, books, references or Software?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "terminology",
      "explainable-ai",
      "interpretation"
    ],
    "owner": {
      "account_id": 2035987,
      "reputation": 635,
      "user_id": 30238,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/665ac068b26409d8c68e16bdeeb3fd97?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Funkwecker",
      "link": "https://datascience.stackexchange.com/users/30238/funkwecker"
    },
    "is_answered": true,
    "view_count": 6322,
    "answer_count": 5,
    "score": 20,
    "last_activity_date": 1613467078,
    "creation_date": 1585036583,
    "last_edit_date": 1598301566,
    "question_id": 70164,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/70164/what-is-the-difference-between-explainable-and-interpretable-machine-learning",
    "title": "What is the difference between explainable and interpretable machine learning?",
    "body": "<p><a href=\"https://statmodeling.stat.columbia.edu/2018/10/30/explainable-ml-versus-interpretable-ml/\" rel=\"noreferrer\">ORourke</a> says that explainable ML uses a black box model and explains it afterwards, whereas interpretable ML uses models that are no black boxes.</p>\n\n<p><a href=\"https://christophm.github.io/interpretable-ml-book/interpretability.html\" rel=\"noreferrer\">Christoph Molnar</a> says interpretable ML refers to the degree to which a human can understand the cause of a decision (of a model). He then uses interpretable ML and explainable ML interchangably.</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Explainable_artificial_intelligence\" rel=\"noreferrer\">Wikipedia</a> says on the topic \"Explainable artificial intelligence\" that it refers to AI methods and techniques such that the results of the solution can be understood by human experts. It contrasts with the concept of the black box in machine learning where even their designers cannot explain why the AI arrived at a specific decision. The technical challenge of explaining AI decisions is sometimes known as the interpretability problem.</p>\n\n<p><a href=\"https://arxiv.org/abs/1702.08608\" rel=\"noreferrer\">Doshi-Velez and Kim</a> say that that interpretable machine learning systems provide explanation for their outputs.</p>\n\n<p>Obviously, there are a lot of definitions but they do not totally agree. Ultimatively, what should be explained: The results of the model, the model itself or how the model makes decissions? And what is the difference between interpret and explain?</p>\n"
  },
  {
    "tags": [
      "nosql",
      "relational-dbms"
    ],
    "owner": {
      "account_id": 361515,
      "reputation": 293,
      "user_id": 96,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/MV9ND.jpg?s=256",
      "display_name": "ePezhman",
      "link": "https://datascience.stackexchange.com/users/96/epezhman"
    },
    "is_answered": true,
    "view_count": 471,
    "accepted_answer_id": 26,
    "answer_count": 6,
    "score": 19,
    "last_activity_date": 1657787428,
    "creation_date": 1400045866,
    "last_edit_date": 1567880637,
    "question_id": 20,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/20/the-data-in-our-relational-dbms-is-getting-big-is-it-the-time-to-move-to-nosql",
    "title": "The data in our relational DBMS is getting big, is it the time to move to NoSQL?",
    "body": "<p>We created a social network application for eLearning purposes. It's an experimental project that we are researching on in our lab. It has been used in some case studies for a while and the data in our relational DBMS (SQL Server 2008) is getting big. It's a few gigabytes now and the tables are highly connected to each other. The performance is still fine, but when should we consider other options? Is it the matter of performance?  </p>\n"
  },
  {
    "tags": [
      "clustering",
      "algorithms",
      "k-means"
    ],
    "owner": {
      "account_id": 1822136,
      "reputation": 4117,
      "user_id": 84,
      "user_type": "registered",
      "accept_rate": 82,
      "profile_image": "https://www.gravatar.com/avatar/5394d14f632e89b2dfc937e3660f0079?s=256&d=identicon&r=PG",
      "display_name": "Rubens",
      "link": "https://datascience.stackexchange.com/users/84/rubens"
    },
    "is_answered": true,
    "view_count": 12188,
    "accepted_answer_id": 459,
    "answer_count": 2,
    "score": 19,
    "last_activity_date": 1493223861,
    "creation_date": 1403120934,
    "last_edit_date": 1493223861,
    "question_id": 458,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/458/k-means-vs-online-k-means",
    "title": "K-means vs. online K-means",
    "body": "<p><a href=\"http://en.wikipedia.org/wiki/K-means_clustering\">K-means</a> is a well known algorithm for clustering, but there is also an online variation of such algorithm (online K-means). What are the pros and cons of these approaches, and when should each be preferred?</p>\n"
  },
  {
    "tags": [
      "python",
      "regression",
      "library",
      "software-recommendation"
    ],
    "owner": {
      "account_id": 169656,
      "reputation": 5862,
      "user_id": 843,
      "user_type": "registered",
      "accept_rate": 33,
      "profile_image": "https://i.sstatic.net/Z99mk.jpg?s=256",
      "display_name": "Franck Dernoncourt",
      "link": "https://datascience.stackexchange.com/users/843/franck-dernoncourt"
    },
    "is_answered": true,
    "view_count": 33728,
    "accepted_answer_id": 8632,
    "answer_count": 2,
    "score": 19,
    "last_activity_date": 1503961678,
    "creation_date": 1445998879,
    "last_edit_date": 1446175828,
    "question_id": 8625,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/8625/multivariate-linear-regression-in-python",
    "title": "Multivariate linear regression in Python",
    "body": "<p>I'm looking for a Python package that implements multivariate linear regression.</p>\n\n<p>(Terminological note: <em>multivariate</em> regression deals with the case where there are more than one dependent variables while <em>multiple</em> regression deals with the case where there is one dependent variable but more than one independent variables.)</p>\n"
  },
  {
    "tags": [
      "python",
      "scikit-learn"
    ],
    "owner": {
      "account_id": 2811843,
      "reputation": 2023,
      "user_id": 13736,
      "user_type": "registered",
      "accept_rate": 45,
      "profile_image": "https://www.gravatar.com/avatar/9fe7def9462f2e9e696d8d5c367af484?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user",
      "link": "https://datascience.stackexchange.com/users/13736/user"
    },
    "is_answered": true,
    "view_count": 86586,
    "accepted_answer_id": 10775,
    "answer_count": 2,
    "score": 19,
    "last_activity_date": 1615493054,
    "creation_date": 1458297285,
    "question_id": 10773,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10773/how-does-selectkbest-work",
    "title": "How does SelectKBest work?",
    "body": "<p>I am looking at this tutorial: <a href=\"https://www.dataquest.io/mission/75/improving-your-submission\" rel=\"noreferrer\">https://www.dataquest.io/mission/75/improving-your-submission</a></p>\n\n<p>At section 8, finding the best features, it shows the following code.  </p>\n\n<pre><code>import numpy as np\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\npredictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n\n# Perform feature selection\nselector = SelectKBest(f_classif, k=5)\nselector.fit(titanic[predictors], titanic[\"Survived\"])\n\n# Get the raw p-values for each feature, and transform from p-values into scores\nscores = -np.log10(selector.pvalues_)\n\n# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\nplt.bar(range(len(predictors)), scores)\nplt.xticks(range(len(predictors)), predictors, rotation='vertical')\nplt.show()\n</code></pre>\n\n<p>What is k=5 doing, since it is never used (the graph still lists all of the features, whether I use k=1 or k=\"all\")?\nHow does it determine the best features, are they independent of the method one wants to use (whether logistic regression, random forests, or whatever)?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning"
    ],
    "owner": {
      "account_id": 10144777,
      "reputation": 191,
      "user_id": 28473,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/3ebf4f54720b96b1698258826e15e43b?s=256&d=identicon&r=PG",
      "display_name": "user28473",
      "link": "https://datascience.stackexchange.com/users/28473/user28473"
    },
    "is_answered": true,
    "view_count": 17554,
    "answer_count": 2,
    "score": 19,
    "last_activity_date": 1625074492,
    "creation_date": 1485840756,
    "question_id": 16639,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/16639/could-deep-learning-be-used-to-crack-encryption",
    "title": "Could Deep Learning be used to crack encryption?",
    "body": "<p>Say you have a dataset with millions of rows and the attributes Plain Text, Key, and Output Ciphertext. Could Deep Learning, theoretically, be used to find patterns in the outputs that help decipher the ciphertext? Are there any other potential approaches?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "lstm"
    ],
    "owner": {
      "account_id": 2661655,
      "reputation": 656,
      "user_id": 18418,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f0e4a5f52cfcbaa7b27d957da309c8eb?s=256&d=identicon&r=PG",
      "display_name": "Vadim Smolyakov",
      "link": "https://datascience.stackexchange.com/users/18418/vadim-smolyakov"
    },
    "is_answered": true,
    "view_count": 13756,
    "answer_count": 3,
    "score": 19,
    "last_activity_date": 1567768406,
    "creation_date": 1504025320,
    "last_edit_date": 1504040900,
    "question_id": 22690,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/22690/advantages-of-stacking-lstms",
    "title": "Advantages of stacking LSTMs?",
    "body": "<p>I'm wondering in what situations it is advantageous to stack LSTMs?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "nlp",
      "word2vec",
      "language-model",
      "tfidf"
    ],
    "owner": {
      "account_id": 5337194,
      "reputation": 311,
      "user_id": 47149,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6927fc09abe845246c8afc183525d381?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "SFD",
      "link": "https://datascience.stackexchange.com/users/47149/sfd"
    },
    "is_answered": true,
    "view_count": 34486,
    "answer_count": 2,
    "score": 19,
    "last_activity_date": 1718243775,
    "creation_date": 1520165253,
    "last_edit_date": 1520285101,
    "question_id": 28598,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/28598/word2vec-embeddings-with-tf-idf",
    "title": "Word2Vec embeddings with TF-IDF",
    "body": "<p>When you train the word2vec model (using for instance, gensim) you supply a list of words/sentences.  But there does not seem to be a way to specify weights for the words calculated for instance using TF-IDF.  </p>\n\n<p>Is the usual practice to multiply the word vector embeddings with the associated TF-IDF weight? Or can word2vec organically take advantage of these somehow?</p>\n"
  },
  {
    "tags": [
      "python",
      "scikit-learn"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user29151"
    },
    "is_answered": true,
    "view_count": 37369,
    "answer_count": 1,
    "score": 19,
    "last_activity_date": 1545634855,
    "creation_date": 1531476419,
    "last_edit_date": 1542975913,
    "question_id": 34416,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/34416/scikit-learn-n-jobs-parameter-on-cpu-usage-memory",
    "title": "scikit-learn n_jobs parameter on CPU usage &amp; memory",
    "body": "<p>In most estimators on scikit-learn, there is an <code>n_jobs</code> parameter in <code>fit</code>/<code>predict</code> methods for creating parallel jobs using <code>joblib</code>.\nI noticed that setting it to <code>-1</code> creates just 1 Python process and maxes out the cores, causing CPU usage to hit 2500 % on top.\nThis is quite different from setting it to some positive integer >1, which creates multiple Python processes at ~100 % usage.</p>\n\n<p>How does setting it affects CPU &amp; core usage on a multi-CPU Linux server? \n(e.g. if <code>n_jobs=8</code> then are 8 CPUs fully locked up or do the CPUs still reserve some cores for other tasks/processes?)</p>\n\n<p>Additionally, I do get <code>MemoryError</code> occasionally when setting <code>n_jobs=-1</code> for large datasets. \nHowever, the memory usage usually hovers at around 30-40 % for the single Python process. \nHow is the data &amp; memory being managed/copied depending on the value of <code>n_jobs</code>?</p>\n"
  },
  {
    "tags": [
      "python",
      "pandas"
    ],
    "owner": {
      "account_id": 9897607,
      "reputation": 333,
      "user_id": 57214,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/64d3853542b511e3ca8c1e1176d5d810?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user633599",
      "link": "https://datascience.stackexchange.com/users/57214/user633599"
    },
    "is_answered": true,
    "view_count": 205647,
    "accepted_answer_id": 45315,
    "answer_count": 3,
    "score": 19,
    "last_activity_date": 1652370276,
    "creation_date": 1549736690,
    "last_edit_date": 1549861614,
    "question_id": 45314,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/45314/dataframe-has-no-column-names-how-to-add-a-header",
    "title": "Dataframe has no column names. How to add a header?",
    "body": "<p>I am using a dataset to practice for building a decision tree classifier.</p>\n\n<p>Here is my code:</p>\n\n<pre><code>import pandas as pd \ntdf = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data', sep = ',', header=0)\ntdf.info()\n</code></pre>\n\n<p>The column has no name, and i have problem to add the column name, already tried reindex, pd.melt, rename, etc.</p>\n\n<p>The column names  want to assign are:</p>\n\n<ol>\n<li>Sample code number: id number </li>\n<li>Clump Thickness: 1 - 10 </li>\n<li>Uniformity of Cell Size: 1 - 10 </li>\n<li>Uniformity of Cell Shape: 1 - 10 </li>\n<li>Marginal Adhesion: 1 - 10 </li>\n<li>Single Epithelial Cell Size: 1 - 10 </li>\n<li>Bare Nuclei: 1 - 10 </li>\n<li>Bland Chromatin: 1 - 10 </li>\n<li>Normal Nucleoli: 1 - 10 </li>\n<li>Mitoses: 1 - 10 </li>\n<li>Class: (2 for benign, 4 for malignant)</li>\n</ol>\n\n<p>Thanks,</p>\n"
  },
  {
    "tags": [
      "python",
      "boosting",
      "lightgbm"
    ],
    "owner": {
      "account_id": 13186506,
      "reputation": 7866,
      "user_id": 71442,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/4obaD.jpg?s=256",
      "display_name": "Peter",
      "link": "https://datascience.stackexchange.com/users/71442/peter"
    },
    "is_answered": true,
    "view_count": 57212,
    "accepted_answer_id": 53997,
    "answer_count": 5,
    "score": 19,
    "last_activity_date": 1706084182,
    "creation_date": 1560783981,
    "last_edit_date": 1643024984,
    "question_id": 53954,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/53954/how-to-make-lightgbm-to-suppress-output",
    "title": "How to make LightGBM to suppress output?",
    "body": "<p>I have tried for a while to figure out how to &quot;shut up&quot; LightGBM. Especially, I would like to suppress the output of LightGBM during training (i.e. feedback on the boosting steps).</p>\n<p>My model:</p>\n<pre><code>params = {\n            'objective': 'regression',\n            'learning_rate' :0.9,\n            'max_depth' : 1,\n            'metric': 'mean_squared_error',\n            'seed': 7,\n            'boosting_type' : 'gbdt'\n        }\n\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=100000,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=100)\n</code></pre>\n<p>I tried to add <code>verbose=0</code> as suggested in the docs, but this does not work.\n<a href=\"https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst\" rel=\"noreferrer\">https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst</a></p>\n<p>Does anyone know how to suppress LightGBM output during training?</p>\n"
  },
  {
    "tags": [
      "xgboost",
      "cross-validation",
      "lightgbm",
      "early-stopping"
    ],
    "owner": {
      "account_id": 13334782,
      "reputation": 191,
      "user_id": 97300,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s256-rj/photo.jpg",
      "display_name": "amine456",
      "link": "https://datascience.stackexchange.com/users/97300/amine456"
    },
    "is_answered": true,
    "view_count": 12696,
    "answer_count": 3,
    "score": 19,
    "last_activity_date": 1660975057,
    "creation_date": 1589728541,
    "question_id": 74351,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/74351/what-is-the-proper-way-to-use-early-stopping-with-cross-validation",
    "title": "What is the proper way to use early stopping with cross-validation?",
    "body": "<p>I am not sure what is the proper way to use early stopping with cross-validation for a gradient boosting algorithm. For a simple train/valid split, we can use the valid dataset as the evaluation dataset for the early stopping and when refitting we use the best number of iterations.</p>\n\n<p>But in case of cross-validation like k-fold, my intuition would be to use each valid set of each fold as evaluation dataset for the early stopping but that means the best number of iterations would be different from a fold to another. So when refitting, what do we use as the final best number of iterations ? the mean ?</p>\n\n<p>Thanks !</p>\n"
  },
  {
    "tags": [
      "nlp",
      "language-model",
      "gpt"
    ],
    "owner": {
      "account_id": 26605808,
      "reputation": 361,
      "user_id": 141493,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/44b077491c6c5a6bf51e1e62bd08e11e?s=256&d=identicon&r=PG",
      "display_name": "user141493",
      "link": "https://datascience.stackexchange.com/users/141493/user141493"
    },
    "is_answered": true,
    "view_count": 24945,
    "answer_count": 7,
    "score": 19,
    "last_activity_date": 1702911691,
    "creation_date": 1675414641,
    "last_edit_date": 1675446840,
    "question_id": 118260,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/118260/chatgpts-architecture-decoder-only-or-encoder-decoder",
    "title": "ChatGPT&#39;s Architecture - Decoder Only? Or Encoder-Decoder?",
    "body": "<p>Does ChatGPT use an encoder-decoder architecture, or a decoder-only architecture? I have been coming across Medium and TowardsDataScience articles suggesting that it has an encoder-decoder architecture (see sources below):</p>\n<p>-- <a href=\"https://medium.com/@sreedevr/chatgpt-architecture-explained-7fc0cbb7426c\" rel=\"noreferrer\">https://medium.com/@sreedevr/chatgpt-architecture-explained-7fc0cbb7426c</a></p>\n<p>-- <a href=\"https://towardsdatascience.com/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286\" rel=\"noreferrer\">https://towardsdatascience.com/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286</a></p>\n<p>-- <a href=\"https://leoneperdigao.medium.com/chatgpt-a-deep-dive-1feade9c4d77\" rel=\"noreferrer\">https://leoneperdigao.medium.com/chatgpt-a-deep-dive-1feade9c4d77</a></p>\n<p>I even asked ChatGPT myself whether it uses an encoder and decoder, or just a decoder, and it told me it has both an encoder and decoder. Here is one of its responses to the question:</p>\n<p><em>&quot;As a language model, ChatGPT does not have a decoder-only architecture, it has an encoder-decoder architecture. Having an encoder-decoder architecture enables language models like ChatGPT to process input sequences and generate outputs based on that input. The encoder processes the input sequence and generates a set of hidden states, which are then used by the decoder to generate the output sequence.\nHaving both an encoder and a decoder allows ChatGPT to handle a variety of tasks that involve generating text based on input text, such as language translation, summarization, and answering questions. Having an encoder-decoder architecture allows ChatGPT to maintain context and consistency in the generated output, which is important for many NLP tasks.\nIt is possible to train a decoder-only language model, but it would be limited in its ability to generate coherent text as it would not have access to the context provided by the encoder.&quot;</em></p>\n<p>However, I have been under the definite impression for quite some time now that GPT-3 (from which ChatGPT was in part derived) is a decoder-only model. And I take with a grain of salt ChatGPT's explanation of its own architecture given that it seems prone to generating incorrect answers sometimes. Also, with the huge fanfare of ChatGPT and the potential for misinformed authors writing about the model, I wonder if anyone knows of a reliable source that can clarify this question.</p>\n<p>Thanks</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "statistics",
      "class-imbalance",
      "supervised-learning"
    ],
    "owner": {
      "account_id": 15896855,
      "reputation": 4532,
      "user_id": 73930,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2d2e852a5b039b0d1968a16bef768d49?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Dave",
      "link": "https://datascience.stackexchange.com/users/73930/dave"
    },
    "is_answered": true,
    "view_count": 955,
    "accepted_answer_id": 126469,
    "answer_count": 2,
    "score": 19,
    "last_activity_date": 1706110158,
    "creation_date": 1704825248,
    "last_edit_date": 1704829563,
    "question_id": 126337,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/126337/why-does-data-science-see-class-imbalance-as-a-problem-for-supervised-learning-w",
    "title": "Why does data science see class imbalance as a problem for supervised learning when statistics does not?",
    "body": "<p>Why does data science see class imbalance as a problem in supervised learning when statistics says it is not?</p>\n<p>Data science seems to seem class imbalance as problematic and needing special techniques to remedy this problem.</p>\n<p>For instance, <a href=\"https://datascience.stackexchange.com/q/65886/73930\">this DS.SE question</a> takes it as self-evident that class imbalance is problematic, and no answers or comments push back on this notion; <a href=\"https://datascience.stackexchange.com/q/18516/73930\">the answer to this DS.SE question</a> believes class imbalance to be a problem in need of remedy; and <a href=\"https://datascience.stackexchange.com/q/28360/73930\">this</a>, <a href=\"https://datascience.stackexchange.com/questions/44755/why-doesnt-class-weight-resolve-the-imbalanced-classification-problem\">this</a>, and <a href=\"https://datascience.stackexchange.com/questions/36285/what-is-the-best-performance-metric-used-in-balancing-dataset-using-smote-techni\">this</a> advocate for data adjustments in order to solve what is taken as a self-evident problem.</p>\n<p>Statisticians do not see class imbalance as a problem for supervised learning that requires special remedy. Multiple posts on Cross Validated (Statistics) Stack Exchange argue against this except for specific circumstances.</p>\n<p><a href=\"https://stats.stackexchange.com/questions/357466/are-unbalanced-datasets-problematic-and-how-does-oversampling-purport-to-he\">Are unbalanced datasets problematic, and (how) does oversampling (purport to) help?</a></p>\n<p><a href=\"https://stats.meta.stackexchange.com/questions/6349/profusion-of-threads-on-imbalanced-data-can-we-merge-deem-canonical-any\">Profusion of threads on imbalanced data - can we merge/deem canonical any?</a></p>\n<p>Vanderbilt's founder of the Department of Biostatistics, <a href=\"https://stats.stackexchange.com/users/4253/frank-harrell\">Frank Harrell</a>, has at least two good blog posts and several tweets that argue against class imbalance being a problem, even if some of the arguments are indirect (especially in the blog).</p>\n<p><a href=\"https://www.fharrell.com/post/classification/\" rel=\"noreferrer\">Classification vs. Prediction</a></p>\n<blockquote>\n<p>For this reason the odd practice of subsampling the controls is used in an attempt to balance the frequencies and get some variation that will lead to sensible looking classifiers (users of regression models would never exclude good data to get an answer). Then they have to, in some ill-defined way, construct the classifier to make up for biasing the sample. It is simply the case that a classifier trained to a 1/2 prevalence situation will not be applicable to a population with a 1/1000 prevalence. The classifier would have to be re-trained on the new sample, and the patterns detected may change greatly.</p>\n</blockquote>\n<p><a href=\"https://www.fharrell.com/post/class-damage/\" rel=\"noreferrer\">Damage Caused by Classification Accuracy and Other Discontinuous Improper Accuracy Scoring Rules</a></p>\n<blockquote>\n<p>As discusssed here, fans of classifiers sometimes subsample from observations in the most frequent outcome category (here Y=1) to get an artificial 50/50 balance of Y=0 and Y=1 when developing their classifier. Fans of such deficient notions of accuracy fail to realize that their classifier will not apply to a population when a much different prevalence of Y=1 than 0.5.</p>\n</blockquote>\n<p><a href=\"https://twitter.com/f2harrell/status/1499369999890370566\" rel=\"noreferrer\">&quot;People are still pushing ridiculous methods like SMOTE.&quot;</a></p>\n<p><a href=\"https://twitter.com/f2harrell/status/1062424969366462473?lang=en\" rel=\"noreferrer\">&quot;Don't mess with the data.  Use the right stat methods.&quot;</a></p>\n<p><a href=\"https://twitter.com/f2harrell/status/1656620486581796864\" rel=\"noreferrer\">&quot;#MachineLearning advocates are amazingly still using SMOTE to ruin 'imbalanced' data before analysis and invalidate 'classifications' they develop.&quot;</a></p>\n<p>(Notice that these criticisms of SMOTE are not specific to SMOTE and just want a better generator of synthetic data. The criticisms are of doing something to &quot;ruin&quot; the data, with SMOTE just being one way to &quot;ruin&quot; the natural class ratio.)</p>\n<p>Why does data science see class imbalance as a problem when statistics says it is not? What do the statisticians miss about class imbalance that makes it problematic in data science?</p>\n<p>An answer like, &quot;You could get 99% accuracy just by classifying everything in the majority class,&quot; really speaks to why accuracy is a problematic measure of performance, which is most obvious in, <a href=\"https://stats.stackexchange.com/a/312787/247274\">but not exclusive to</a>, situations with imbalance. Also, &quot;Imbalance often leads to everything being classified as the majority category,&quot; is a criticism of the <a href=\"https://datascience.stackexchange.com/a/118215/73930\">decision rule used on top of the raw predictions</a> from most (but I concede not all) models, such as the probability values returned by logistic regressions (think <code>model.predict_proba</code> instead of <code>model.predict</code> in <code>sklearn</code>), which might be reasonable when they all fall below some software-default threshold and would be rounded to the majority category by some kind of <code>predict</code> method.</p>\n"
  },
  {
    "tags": [
      "social-network-analysis"
    ],
    "owner": {
      "account_id": 735810,
      "reputation": 355,
      "user_id": 95,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/3f9be2c2958e208c8d9b629ac43c9c42?s=256&d=identicon&r=PG",
      "display_name": "EdChum",
      "link": "https://datascience.stackexchange.com/users/95/edchum"
    },
    "is_answered": true,
    "view_count": 324,
    "accepted_answer_id": 1162,
    "answer_count": 2,
    "score": 18,
    "last_activity_date": 1464357754,
    "creation_date": 1406124271,
    "last_edit_date": 1406637599,
    "question_id": 823,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/823/how-should-ethics-be-applied-in-data-science",
    "title": "How should ethics be applied in data science",
    "body": "<p>There was a recent furore with <a href=\"http://online.wsj.com/articles/furor-erupts-over-facebook-experiment-on-users-1404085840\">facebook experimenting on their users to see if they could alter user's emotions</a> and now <a href=\"http://www.bbc.co.uk/news/technology-28542642\">okcupid</a>.</p>\n\n<p>Whilst I am not a professional data scientist I read about <a href=\"http://columbiadatascience.com/2013/11/25/data-science-ethics/\">data science ethics</a> from <a href=\"http://shop.oreilly.com/product/0636920028529.do\">Cathy O'Neill's book 'Doing Data Science'</a> and would like to know if this is something that professionals are taught at academic level (I would expect so) or something that is ignored or is lightly applied in the professional world. Particularly for those who ended up doing data science <em>accidentally</em>.</p>\n\n<p>Whilst the linked article touched on data integrity, the book also discussed the moral ethics behind understanding the impact of the data models that are created and the impact of those models which can have adverse effects when used inappropriately (sometimes unwittingly) or when the models are inaccurate, again producing adverse results.</p>\n\n<p>The article discusses a code of practice and mentions the <a href=\"http://www.datascienceassn.org/code-of-conduct.html\">Data Science Association's Code of conduct</a>, is this something that is in use? Rule 7 is of particular interest (quoted from their website):</p>\n\n<blockquote>\n  <p>(a) A person who consults with a data scientist about the possibility\n  of forming a client-data scientist relationship with respect to a\n  matter is a prospective client.</p>\n  \n  <p>(b) Even when no client-data scientist relationship ensues, a data\n  scientist who has learned information from a prospective client shall\n  not use or reveal that information.</p>\n  \n  <p>(c) A data scientist subject to paragraph (b) shall not provide\n  professional data science services for a client with interests\n  materially adverse to those of a prospective client in the same or a\n  substantially related industry if the data scientist received\n  information from the prospective client that could be significantly\n  harmful to that person in the matter</p>\n</blockquote>\n\n<p>Is this something that is practiced professionally? Many users blindly accept that we get some free service (mail, social network, image hosting, blog platform etc..) and agree to an EULA in order to have ads pushed at us. </p>\n\n<p>Finally how is this regulated, I often read about users being up in arms when the terms of a service change but it seems that it requires some liberty organisation, class action or a <a href=\"http://www.cnet.com/news/senator-asks-ftc-to-investigate-facebooks-mood-study/\">senator</a> to react to such things before something happens.</p>\n\n<p>By the way I am not making any judgements here or saying that all data scientists behave like this, I'm interested in what is taught academically and practiced professionally.</p>\n"
  },
  {
    "tags": [
      "dataset",
      "aws"
    ],
    "owner": {
      "account_id": 4955815,
      "reputation": 183,
      "user_id": 9249,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/eOqUY.png?s=256",
      "display_name": "Will Stedden",
      "link": "https://datascience.stackexchange.com/users/9249/will-stedden"
    },
    "is_answered": true,
    "view_count": 27065,
    "accepted_answer_id": 5641,
    "answer_count": 5,
    "score": 18,
    "last_activity_date": 1591030948,
    "creation_date": 1429725627,
    "last_edit_date": 1516030961,
    "question_id": 5589,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/5589/downloading-a-large-dataset-on-the-web-directly-into-aws-s3",
    "title": "Downloading a large dataset on the web directly into AWS S3",
    "body": "<p>Does anyone know if it's possible to import a large dataset into Amazon S3 from a URL?  </p>\n\n<p>Basically, I want to avoid downloading a huge file and then reuploading it to S3 through the web portal. I just want to supply the download URL to S3 and wait for them to download it to their filesystem. It seems like an easy thing to do, but I just can't find the documentation on it.</p>\n"
  },
  {
    "tags": [
      "classification",
      "logistic-regression",
      "decision-trees"
    ],
    "owner": {
      "account_id": 4784762,
      "reputation": 727,
      "user_id": 9793,
      "user_type": "registered",
      "accept_rate": 62,
      "profile_image": "https://www.gravatar.com/avatar/8ef7ff8cf6ed433b1f1f6dd7b75e0324?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Arun",
      "link": "https://datascience.stackexchange.com/users/9793/arun"
    },
    "is_answered": true,
    "view_count": 52607,
    "accepted_answer_id": 6059,
    "answer_count": 5,
    "score": 18,
    "last_activity_date": 1662948691,
    "creation_date": 1433842631,
    "last_edit_date": 1594484593,
    "question_id": 6048,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/6048/should-i-use-a-decision-tree-or-logistic-regression-for-classification",
    "title": "Should I use a decision tree or logistic regression for classification?",
    "body": "<p>I am working on a classification problem. I have a dataset containing equal numbers of categorical variables and continuous variables. How do I decide which technique to use, between a decision tree and logistic regression?</p>\n<p>Is it right to assume that logistic regression will be more suitable for continuous variables and that decision trees will be more suitable for both continuous and categorical variables?</p>\n"
  },
  {
    "tags": [
      "beginner",
      "open-source"
    ],
    "owner": {
      "account_id": 241216,
      "reputation": 5474,
      "user_id": 97,
      "user_type": "registered",
      "accept_rate": 77,
      "profile_image": "https://www.gravatar.com/avatar/68acdc33e278ab19debf230662a7ad96?s=256&d=identicon&r=PG",
      "display_name": "IgorS",
      "link": "https://datascience.stackexchange.com/users/97/igors"
    },
    "is_answered": true,
    "view_count": 22269,
    "answer_count": 5,
    "community_owned_date": 1465391862,
    "score": 18,
    "last_activity_date": 1615490110,
    "creation_date": 1451468260,
    "last_edit_date": 1451505696,
    "question_id": 9554,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9554/open-source-data-science-projects-to-contribute",
    "title": "Open source data science projects to contribute",
    "body": "<p>Contribution into open source projects is typically a good way to get some practice for newbies, and try a new area for experienced data scientists and analysts.</p>\n\n<p>Which projects do you contribute? Please provide some intro + link on Github.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "predictive-modeling",
      "scikit-learn",
      "supervised-learning"
    ],
    "owner": {
      "account_id": 2971936,
      "reputation": 231,
      "user_id": 17619,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e5813edf47aec03308bcebc826908ce4?s=256&d=identicon&r=PG",
      "display_name": "Sagar Waghmode",
      "link": "https://datascience.stackexchange.com/users/17619/sagar-waghmode"
    },
    "is_answered": true,
    "view_count": 7244,
    "answer_count": 5,
    "score": 18,
    "last_activity_date": 1595510635,
    "creation_date": 1459919651,
    "last_edit_date": 1460954578,
    "question_id": 11060,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/11060/merging-sparse-and-dense-data-in-machine-learning-to-improve-the-performance",
    "title": "Merging sparse and dense data in machine learning to improve the performance",
    "body": "<p>I have sparse features which are predictive, also I have some dense features which are also predictive. I need to combine these features together to improve the overall performance of the classifier.</p>\n\n<p>Now, the thing is when I try to combine these together, the dense features tend to dominate more over sparse features, hence giving only 1% improvement in AUC compared to model with only dense features.</p>\n\n<p>Has somebody come across similar problems? Really appreciate the inputs, kind of stuck. I have already tried lot of different classifiers, combination of classifiers, feature transformations and processing with different algorithms.</p>\n\n<p>Thanks in advance for the help.</p>\n\n<p><strong>Edit</strong>:</p>\n\n<p>I have already tried the suggestions which are given in the comments. What I have observed is, for almost 45% of the data, sparse features perform really well, I get the AUC of around 0.9 with only sparse features, but for the remaining ones dense features perform well with AUC of around 0.75. I kind of tried separating out these datasets, but I get the AUC of 0.6, so, I can't simply train a model and decide which features to use.</p>\n\n<p>Regarding the code snippet, I have tried out so many things, that I am not sure what exactly to share :(</p>\n"
  },
  {
    "tags": [
      "classification",
      "dataset",
      "sampling",
      "class-imbalance"
    ],
    "owner": {
      "account_id": 1460293,
      "reputation": 604,
      "user_id": 13891,
      "user_type": "registered",
      "accept_rate": 70,
      "profile_image": "https://i.sstatic.net/GTmeh.jpg?s=256",
      "display_name": "Rami",
      "link": "https://datascience.stackexchange.com/users/13891/rami"
    },
    "is_answered": true,
    "view_count": 24768,
    "accepted_answer_id": 11789,
    "answer_count": 3,
    "score": 18,
    "last_activity_date": 1528030039,
    "creation_date": 1463398574,
    "last_edit_date": 1528030039,
    "question_id": 11788,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/11788/when-should-we-consider-a-dataset-as-imbalanced",
    "title": "When should we consider a dataset as imbalanced?",
    "body": "<p>I'm facing a situation where the numbers of positive and negative examples in a dataset are imbalanced.</p>\n\n<p>My question is, are there any rules of thumb that tell us when we should subsample the large category in order to force some kind of balancing in the dataset.</p>\n\n<p>Examples:</p>\n\n<ul>\n<li>If the number of positive examples is 1,000 and the number of negative examples is 10,000, should I go for training my classifier on the full dataset or I should subsample the negative examples?</li>\n<li>The same question for 1,000 positive example and 100,000 negative.</li>\n<li>The same question for 10,000 positive and 1,000 negative.</li>\n<li>etc...</li>\n</ul>\n"
  },
  {
    "tags": [
      "deep-learning",
      "convolutional-neural-network",
      "backpropagation"
    ],
    "owner": {
      "account_id": 2811843,
      "reputation": 2023,
      "user_id": 13736,
      "user_type": "registered",
      "accept_rate": 45,
      "profile_image": "https://www.gravatar.com/avatar/9fe7def9462f2e9e696d8d5c367af484?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user",
      "link": "https://datascience.stackexchange.com/users/13736/user"
    },
    "is_answered": true,
    "view_count": 55206,
    "accepted_answer_id": 11870,
    "answer_count": 4,
    "score": 18,
    "last_activity_date": 1613490509,
    "creation_date": 1463765350,
    "last_edit_date": 1613490509,
    "question_id": 11853,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/11853/question-about-bias-in-convolutional-networks",
    "title": "Question about bias in Convolutional Networks",
    "body": "<p>I am trying to figure out how many weights and biases are needed for CNN.</p>\n\n<p>Say I have a (3, 32, 32)-image and want to apply a (32, 5, 5)-filter.\nFor each feature map I have 5x5 weights, so I should have 3 x (5x5) x 32 parameters. Now I need to add the bias. I believe I only have (3 x (5x5) + 1) x 32 parameters, so is the bias the same across all colors (RGB)? </p>\n\n<p>Is this correct? Do I keep the same bias for each image across its depth (in this case 3) while I use different weights? Why is that?</p>\n"
  },
  {
    "tags": [
      "feature-selection",
      "feature-extraction",
      "feature-engineering",
      "feature-construction",
      "featurization"
    ],
    "owner": {
      "account_id": 3509867,
      "reputation": 539,
      "user_id": 2504,
      "user_type": "registered",
      "accept_rate": 0,
      "profile_image": "https://www.gravatar.com/avatar/b12a8d7b2ccb4114402efd37c3c13023?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "icm",
      "link": "https://datascience.stackexchange.com/users/2504/icm"
    },
    "is_answered": true,
    "view_count": 18254,
    "protected_date": 1673028245,
    "answer_count": 2,
    "score": 18,
    "last_activity_date": 1672919531,
    "creation_date": 1469472953,
    "last_edit_date": 1615666303,
    "question_id": 12984,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/12984/list-of-feature-engineering-techniques",
    "title": "List of feature engineering techniques",
    "body": "<p>Is there any resource with a list of feature engineering techniques? A mapping of type of data, model and feature engineering technique would be a gold mine.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "image-classification",
      "convolutional-neural-network",
      "image-recognition"
    ],
    "owner": {
      "account_id": 2639718,
      "reputation": 301,
      "user_id": 14150,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/536ecae1c0efad7acf061ebc817896cb?s=256&d=identicon&r=PG",
      "display_name": "Feynman27",
      "link": "https://datascience.stackexchange.com/users/14150/feynman27"
    },
    "is_answered": true,
    "view_count": 28765,
    "answer_count": 2,
    "score": 18,
    "last_activity_date": 1613490021,
    "creation_date": 1470261794,
    "last_edit_date": 1613490021,
    "question_id": 13181,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/13181/how-many-images-per-class-are-sufficient-for-training-a-cnn",
    "title": "How many images per class are sufficient for training a CNN",
    "body": "<p>I'm starting a project where the task is to identify sneaker types from images.  I'm currently reading into <a href=\"https://www.tensorflow.org/versions/r0.10/tutorials/image_recognition/index.html\" rel=\"noreferrer\">TensorFlow</a> and <a href=\"https://github.com/karpathy/neuraltalk2\" rel=\"noreferrer\">Torch</a> implementations. My question is: how many images per class are required to reach a reasonable classification performance?      </p>\n"
  },
  {
    "tags": [
      "xgboost"
    ],
    "owner": {
      "account_id": 7921415,
      "reputation": 303,
      "user_id": 26409,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e1af8ff5bcfa7e8bf487fe49cd8b0fe1?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "mathnoob",
      "link": "https://datascience.stackexchange.com/users/26409/mathnoob"
    },
    "is_answered": true,
    "view_count": 28414,
    "accepted_answer_id": 15306,
    "answer_count": 1,
    "score": 18,
    "last_activity_date": 1479900712,
    "creation_date": 1479894410,
    "last_edit_date": 1479898902,
    "question_id": 15305,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/15305/how-does-xgboost-learn-what-are-the-inputs-for-missing-values",
    "title": "How does Xgboost learn what are the inputs for missing values?",
    "body": "<p>So from Algorithm 3 of <a href=\"https://arxiv.org/pdf/1603.02754v3.pdf\" rel=\"noreferrer\">https://arxiv.org/pdf/1603.02754v3.pdf</a>, it says that an optimum default direction is determined and the missing values will go in that direction. However, or perhaps I have misunderstood/missed the explanation from the article, it doesn't say what exactly is the input. </p>\n\n<p>For example, I have (parent) node A (with 50 inputs) splitting into node B and node C. Now, say of the 50 inputs there are 7 missing values. </p>\n\n<p>The other 43 inputs are split into B and C accordingly. What I seem to be understanding is that, it will allocate the remaining 7 into B and C and determine which one gives a higher gain score; that will be the optimal direction. </p>\n\n<p>However, given the 7 values are missing (Which means I don't know what are these 7 values), how does allocating missing values into any of the child nodes change the gain score, or rather minimize the loss function? This seems to suggest that Xgboost is inputting something for the missing values. I can't seem to find out what is Xgboost inputting for these missing values. I hope this question isn't too vague/general and easy. </p>\n\n<p>Edit: I think \"Missing values\" may be a vague term. What I meant here is (From wiki) \"In statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation.\" </p>\n\n<p>From the author himself (<a href=\"https://github.com/dmlc/xgboost/issues/21\" rel=\"noreferrer\">https://github.com/dmlc/xgboost/issues/21</a>), he said\n\" tqchen commented on Aug 13, 2014\nxgboost naturally accepts sparse feature format, you can directly feed data in as sparse matrix, and only contains non-missing value.</p>\n\n<p>i.e. features that are not presented in the sparse feature matrix are treated as 'missing'. XGBoost will handle it internally and you do not need to do anything on it.\"</p>\n\n<p>And,</p>\n\n<p>\" tqchen commented on Aug 13, 2014\nInternally, XGBoost will automatically learn what is the best direction to go when a value is missing. Equivalently, this can be viewed as automatically \"learn\" what is the best imputation value for missing values based on reduction on training loss.\"</p>\n"
  },
  {
    "tags": [
      "classification",
      "naive-bayes-classifier"
    ],
    "owner": {
      "account_id": 6473719,
      "reputation": 333,
      "user_id": 26799,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/77491da2986814a236f76ad8e05eb667?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "fragant",
      "link": "https://datascience.stackexchange.com/users/26799/fragant"
    },
    "is_answered": true,
    "view_count": 22768,
    "accepted_answer_id": 15536,
    "answer_count": 1,
    "score": 18,
    "last_activity_date": 1525381866,
    "creation_date": 1480946482,
    "last_edit_date": 1525381866,
    "question_id": 15526,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/15526/how-to-handle-a-zero-factor-in-naive-bayes-classifier-calculation",
    "title": "How to handle a zero factor in Naive Bayes Classifier calculation?",
    "body": "<p>If I have a training data set and I train a Naive Bayes Classifier on it and I have an attribute value which has probability zero. How do I handle this if I later want to predict the classification on new data? The problem is, if there is a zero in the calculation the whole product becomes zero, no matter how many other values I got which maybe would find another solution.</p>\n\n<p>Example:</p>\n\n<p>$P(x|spam=yes) = P(TimeZone = US | spam=yes) \\cdot P(GeoLocation  = EU | spam = yes)  \\cdot  ~ ... ~  = 0.004 $</p>\n\n<p>$P(x|spam=no) = P(TimeZone = US | spam=no) \\cdot P(GeoLocation  = EU | spam = no)  \\cdot  ~ ... ~  = 0 $ </p>\n\n<p>The whole product becomes $0$ because in the training data the attribute TimeZone US is always Yes in our small training data set. How can I handle this? Should I use a bigger set of training data or is there another possibility to overcome this problem?</p>\n"
  },
  {
    "tags": [
      "xgboost"
    ],
    "owner": {
      "account_id": 233335,
      "reputation": 505,
      "user_id": 25359,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/fb66ef41feb64e9aac7a7661e3d29c56?s=256&d=identicon&r=PG",
      "display_name": "davidjhp",
      "link": "https://datascience.stackexchange.com/users/25359/davidjhp"
    },
    "is_answered": true,
    "view_count": 29583,
    "accepted_answer_id": 16263,
    "answer_count": 2,
    "score": 18,
    "last_activity_date": 1540591977,
    "creation_date": 1484065751,
    "question_id": 16232,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/16232/in-xgboost-would-we-evaluate-results-with-a-precision-recall-curve-vs-roc",
    "title": "In XGBoost would we evaluate results with a Precision Recall curve vs ROC?",
    "body": "<p>I am using XGBoost for payment fraud detection.  The objective is binary classification, and the data is very unbalanced.  One out of every 3-4k transactions is fraud.</p>\n\n<p>I would expect the best way to evaluate the results is a Precision-Recall (PR) curve, not a ROC curve, since the data is so unbalanced.</p>\n\n<p>However in the eval_metric options I see only area under the ROC curve (AUC), and there is no PR option.  <a href=\"https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\" rel=\"noreferrer\">https://github.com/dmlc/xgboost/blob/master/doc/parameter.md</a></p>\n\n<p>Also the documentation recommends AUC  <a href=\"http://xgboost.readthedocs.io/en/latest/how_to/param_tuning.html\" rel=\"noreferrer\">http://xgboost.readthedocs.io/en/latest/how_to/param_tuning.html</a></p>\n\n<p>Does it make sense to not use a Precision-Recall (PR) curve?</p>\n"
  },
  {
    "tags": [
      "convolutional-neural-network"
    ],
    "owner": {
      "account_id": 14926,
      "reputation": 533,
      "user_id": 34165,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/1sz79.png?s=256",
      "display_name": "geometrikal",
      "link": "https://datascience.stackexchange.com/users/34165/geometrikal"
    },
    "is_answered": true,
    "view_count": 25207,
    "answer_count": 2,
    "score": 18,
    "last_activity_date": 1652881696,
    "creation_date": 1504253104,
    "last_edit_date": 1613427873,
    "question_id": 22760,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/22760/number-and-size-of-dense-layers-in-a-cnn",
    "title": "Number and size of dense layers in a CNN",
    "body": "<p>Most networks I've seen have one or two dense layers before the final softmax layer.</p>\n\n<ul>\n<li>Is there any principled way of choosing the number and size of the dense layers?</li>\n<li>Are two dense layers more representative than one, for the same number of parameters?</li>\n<li>Should dropout be applied before each dense layer, or just once?</li>\n</ul>\n"
  },
  {
    "tags": [
      "machine-learning",
      "dataset",
      "dataframe",
      "dimensionality-reduction",
      "encoding"
    ],
    "owner": {
      "account_id": 6367773,
      "reputation": 283,
      "user_id": 40879,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f7fe4d11cc99883a9224213278827910?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "vinaykva",
      "link": "https://datascience.stackexchange.com/users/40879/vinaykva"
    },
    "is_answered": true,
    "view_count": 30615,
    "accepted_answer_id": 24731,
    "answer_count": 4,
    "score": 18,
    "last_activity_date": 1615666349,
    "creation_date": 1510680058,
    "last_edit_date": 1606927082,
    "question_id": 24729,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/24729/one-hot-encoding-alternatives-for-large-categorical-values",
    "title": "One hot encoding alternatives for large categorical values",
    "body": "<p>I have a data frame with large categorical values over 1600 categories. Is there any way I can find alternatives so that I don't have over 1600 columns?</p>\n<p>I found this interesting <a href=\"http://amunategui.github.io/feature-hashing/#sourcecode\" rel=\"noreferrer\">link</a>.</p>\n<p>But they are converting to class/object which I don't want. I want my final output as a data frame so that I can test with different machine learning models? Or, is there any way I can use the generated matrix to train the other machine learning models other than Logistic regression or XGBoost?</p>\n<p>Is there anyway I can implement it?</p>\n"
  },
  {
    "tags": [
      "python",
      "pandas",
      "visualization",
      "dataframe"
    ],
    "owner": {
      "account_id": 7533563,
      "reputation": 291,
      "user_id": 43200,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/972432679514063/picture?type=large",
      "display_name": "Bilal Butt",
      "link": "https://datascience.stackexchange.com/users/43200/bilal-butt"
    },
    "is_answered": true,
    "view_count": 142939,
    "answer_count": 2,
    "score": 18,
    "last_activity_date": 1615666065,
    "creation_date": 1513083899,
    "last_edit_date": 1615666052,
    "question_id": 25596,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/25596/how-to-plot-two-columns-of-single-dataframe-on-y-axis",
    "title": "How to plot two columns of single DataFrame on Y axis",
    "body": "<p>I have two data frames (Action, Comedy). Action contains two columns (year, rating) ratings columns contains average rating with respect to year. The Comedy data frame contains the same two columns with different mean values.</p>\n<p>I merged both data frames into a <code>total_year</code> data frame.</p>\n<p>The output of <code>total_year</code>:</p>\n<p><a href=\"https://i.sstatic.net/BAs6x.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/BAs6x.png\" alt=\"Totle year Output\" /></a></p>\n<p>Now, I would like to plot <code>total_year</code> on a line graph in which the X axis should contain the year column and the Y axis should contain both the action and the comedy columns.</p>\n<p>I can plot only 1 column at a time on Y axis using following code:</p>\n<pre><code>total_year[-15:].plot(x='year', y='action' ,figsize=(10,5), grid=True)\n</code></pre>\n<p>How can I plot both columns on the Y axis?</p>\n<p>I took this photo from google just to let you know guys I want to draw graph in this way:</p>\n<p><a href=\"https://i.sstatic.net/h5luU.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/h5luU.png\" alt=\"enter image description here\" /></a></p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "xgboost",
      "probability",
      "probability-calibration"
    ],
    "owner": {
      "account_id": 10938163,
      "reputation": 181,
      "user_id": 45076,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6b4ba6e728ce27c02a211e56f65026ec?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "alwayslearning",
      "link": "https://datascience.stackexchange.com/users/45076/alwayslearning"
    },
    "is_answered": true,
    "view_count": 2197,
    "answer_count": 4,
    "score": 18,
    "last_activity_date": 1592476148,
    "creation_date": 1516755122,
    "last_edit_date": 1573766115,
    "question_id": 26980,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/26980/xgboost-outputs-tend-towards-the-extremes",
    "title": "XGBoost outputs tend towards the extremes",
    "body": "<p>I am currently using XGBoost for risk prediction, it seems to be doing a good job in the binary classification department but the probability outputs are way off, i.e., changing the value of a feature in an observation by a very small amount can make the probability output jump from 0.5 to 0.99.</p>\n\n<p>I barely see outputs in the 0.6-0.8 range. In all cases, the probability is less than 0.99 or 1.</p>\n\n<p>I am aware of post training calibration methods such as Platt Scaling and Logistic Correction, but I was wondering if there is anything I can tweak in the XGBoost training process.</p>\n\n<p>I call XGBoost from different languages using FFI, so it would be nice if I can fix this issue without introducing other calibration libraries, e.g., changing eval metric from AUC to log loss.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "data",
      "labels"
    ],
    "owner": {
      "account_id": 6951225,
      "reputation": 283,
      "user_id": 58905,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/a705d9743ebcecb8e48de8f9fbd45e4d?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "mibrl12",
      "link": "https://datascience.stackexchange.com/users/58905/mibrl12"
    },
    "is_answered": true,
    "view_count": 16373,
    "protected_date": 1592414280,
    "accepted_answer_id": 61732,
    "answer_count": 7,
    "score": 18,
    "last_activity_date": 1616625749,
    "creation_date": 1536646783,
    "question_id": 38080,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/38080/interactive-labeling-annotating-of-time-series-data",
    "title": "Interactive labeling/annotating of time series data",
    "body": "<p>I have a data set of time series data. I'm looking for an annotation (or labeling) tool to visualize it and to be able to interactively add labels on it, in order to get annotated data that I can use for supervised ML.</p>\n\n<p>E.g. the input data is a csv-file and the output is another csv-file of the format timestamp,label.</p>\n\n<p>Therefore I need something like this:</p>\n\n<ol>\n<li>to visualize data </li>\n<li>to select a specific area</li>\n<li>output the labels with timestamps</li>\n</ol>\n\n<p>As an example:</p>\n\n<p><a href=\"https://i.sstatic.net/QWTEz.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/QWTEz.png\" alt=\"An example\"></a></p>\n\n<p>Building such a tool in python will not take too long, however I was just wondering how other people solve this problem and maybe there are already nice OS tools for doing this. Thank you!</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-science-model"
    ],
    "owner": {
      "account_id": 10920872,
      "reputation": 377,
      "user_id": 54580,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/a6JUW.jpg?s=256",
      "display_name": "aspiring1",
      "link": "https://datascience.stackexchange.com/users/54580/aspiring1"
    },
    "is_answered": true,
    "view_count": 35927,
    "answer_count": 3,
    "score": 18,
    "last_activity_date": 1694546176,
    "creation_date": 1539969969,
    "last_edit_date": 1559928791,
    "question_id": 39932,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/39932/feature-scaling-both-training-and-test-data",
    "title": "Feature Scaling both training and test data",
    "body": "<p>It is stated that for:\nFeature Normalization - </p>\n\n<blockquote>\n  <p>The test set must use identical scaling to the training set.</p>\n</blockquote>\n\n<p>And the point is given that:</p>\n\n<blockquote>\n  <p>Do not scale the training and test sets using different scalars: this\n  could lead to random skew in the data.</p>\n</blockquote>\n\n<p>Could someone explain what that means?</p>\n"
  },
  {
    "tags": [
      "regression",
      "metric"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 24917,
    "answer_count": 3,
    "score": 18,
    "last_activity_date": 1702461449,
    "creation_date": 1545058072,
    "last_edit_date": 1545058789,
    "question_id": 42760,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/42760/mad-vs-rmse-vs-mae-vs-msle-vs-r%c2%b2-when-to-use-which",
    "title": "MAD vs RMSE vs MAE vs MSLE vs R&#178;: When to use which?",
    "body": "<p>In regression problems, you can use various different metrics to check how well your model is doing:</p>\n\n<ul>\n<li>Mean Absolute Deviation (MAD): In <span class=\"math-container\">$[0, \\infty)$</span>, the smaller the better</li>\n<li>Root Mean Squared Error (RMSE): In <span class=\"math-container\">$[0, \\infty)$</span>, the smaller the better</li>\n<li>Median Absolute Error (MAE): In <span class=\"math-container\">$[0, \\infty)$</span>, the smaller the better</li>\n<li>Mean Squared Log Error (MSLE): In <span class=\"math-container\">$[0, \\infty)$</span>, the smaller the better</li>\n<li>R, coefficient of determination: In <span class=\"math-container\">$(-\\infty, 1]$</span> <a href=\"http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit\" rel=\"noreferrer\">not necessarily the bigger the better</a></li>\n</ul>\n\n<p>Are there any strong reasons not to use one or the other?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "feature-selection",
      "feature-scaling",
      "classifier"
    ],
    "owner": {
      "account_id": 14506410,
      "reputation": 611,
      "user_id": 62528,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/674e3f3f17f0061a3c8774eb2b593dd4?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "jochen6677",
      "link": "https://datascience.stackexchange.com/users/62528/jochen6677"
    },
    "is_answered": true,
    "view_count": 43735,
    "answer_count": 3,
    "score": 18,
    "last_activity_date": 1658111073,
    "creation_date": 1547474302,
    "question_id": 43972,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/43972/when-should-i-use-standardscaler-and-when-minmaxscaler",
    "title": "When should I use StandardScaler and when MinMaxScaler?",
    "body": "<p>I have a feature vector with One-Hot-Encoded features and with continous features.</p>\n\n<p>How can I decide now, which data I shall scale with StandardScaler and which data scale with MinMaxScaler? I think I do not have to scale the one-hot-encoded anyway because they are already between 0 and 1. </p>\n\n<p>(I use afterwards a MLPClassifier)</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "keras",
      "feature-selection",
      "rnn",
      "sequence-to-sequence"
    ],
    "owner": {
      "account_id": 298371,
      "reputation": 458,
      "user_id": 57434,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/03c3a4e855b186ab51c7c996c3087d0a?s=256&d=identicon&r=PG",
      "display_name": "Aesir",
      "link": "https://datascience.stackexchange.com/users/57434/aesir"
    },
    "is_answered": true,
    "view_count": 51836,
    "accepted_answer_id": 51442,
    "answer_count": 3,
    "score": 18,
    "last_activity_date": 1642036830,
    "creation_date": 1548597713,
    "question_id": 44644,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/44644/how-to-determine-feature-importance-in-a-neural-network",
    "title": "How to determine feature importance in a neural network?",
    "body": "<p>I have a neural network to solve a time series forecasting problem.  It is a sequence-to-sequence neural network and currently it is trained on samples each with ten features.  The performance of the model is average and I would like to investigate whether adding or removing features will improve the performance.  I have constructed the neural network using keras.</p>\n\n<p>The features I have included are:</p>\n\n<ol>\n<li>The historical data</li>\n<li>quarterly lagged series of the historical data (4 series)</li>\n<li>A series of the change in value each week</li>\n<li>Four time invariant features tiled to extend the length of the series. (another 4 series)</li>\n</ol>\n\n<p>I am aware I could run the model many times changing the combination of features included each time. However, along with tuning the hyperparameters (for it might be that 8 features works really well with one set of hyperparameters but not with another set) this is really a lot of possible combinations.</p>\n\n<p>Is there any separate way that I can use to guage if a feature is likely to add value to the model or not?</p>\n\n<p>I am particuarly concerned that I have four time-invariant features being fed into the model which is designed to work with time varying data and I would like a way to measure their impact and if they add anything or not?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "learnability"
    ],
    "owner": {
      "account_id": 1440165,
      "reputation": 337,
      "user_id": 59901,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/az10K.jpg?s=256",
      "display_name": "Boris Burkov",
      "link": "https://datascience.stackexchange.com/users/59901/boris-burkov"
    },
    "is_answered": true,
    "view_count": 6723,
    "accepted_answer_id": 47789,
    "answer_count": 4,
    "score": 18,
    "last_activity_date": 1720840141,
    "creation_date": 1553259760,
    "last_edit_date": 1720252907,
    "question_id": 47787,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/47787/can-a-neural-network-compute-y-x2",
    "title": "Can a neural network compute $y = x^2$?",
    "body": "<p>In spirit of the famous <a href=\"http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/\" rel=\"noreferrer\">Tensorflow Fizz Buzz</a> joke and <a href=\"https://medium.com/@jayeshbahire/the-xor-problem-in-neural-networks-50006411840b\" rel=\"noreferrer\">XOr problem</a> I started to think, if it's possible to design a neural network that implements <span class=\"math-container\">$y = x^2$</span> function?</p>\n\n<p>Given some representation of a number (e.g. as a vector in binary form, so that number <code>5</code> is represented as <code>[1,0,1,0,0,0,0,...]</code>), the neural network should learn to return its square - 25 in this case.</p>\n\n<p>If I could implement <span class=\"math-container\">$y=x^2$</span>, I could probably implement <span class=\"math-container\">$y=x^3$</span> and generally any polynomial of x, and then with Taylor series I could approximate <span class=\"math-container\">$y=\\sin(x)$</span>, which would solve the Fizz Buzz problem - a neural network that can find remainder of the division.</p>\n\n<p>Clearly, just the linear part of NNs won't be able to perform this task, so if we could do the multiplication, it would be happening thanks to activation function.</p>\n\n<p>Can you suggest any ideas or reading on subject?</p>\n"
  },
  {
    "tags": [
      "python",
      "nlp",
      "word2vec",
      "gensim"
    ],
    "owner": {
      "account_id": 3747607,
      "reputation": 267,
      "user_id": 62989,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0576c74e5e0a23a055c3d91d63b77987?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Neil",
      "link": "https://datascience.stackexchange.com/users/62989/neil"
    },
    "is_answered": true,
    "view_count": 36554,
    "accepted_answer_id": 51557,
    "answer_count": 3,
    "score": 18,
    "last_activity_date": 1673460709,
    "creation_date": 1557012572,
    "last_edit_date": 1558094248,
    "question_id": 51404,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/51404/word2vec-how-to-choose-the-embedding-size-parameter",
    "title": "Word2Vec how to choose the embedding size parameter",
    "body": "<p>I'm running word2vec over collection of documents. I understand that the size of the model is the number of dimensions of the vector space that the word is embedded into. And that different dimensions are somewhat related to different, independent \"concepts\" that a word could be grouped into. But beyond this I can't find any decent heuristics for how exactly to pick the number. There's some discussion here about the vocabulary size: <a href=\"https://stackoverflow.com/questions/45444964/python-what-is-the-size-parameter-in-gensim-word2vec-model-class\">https://stackoverflow.com/questions/45444964/python-what-is-the-size-parameter-in-gensim-word2vec-model-class</a>  However, I suspect that vocabulary size is not most important, but more important is how many sample documents you have and how long they are. Surely each \"dimension\" should have sufficient examples to be learnt? </p>\n\n<p>I have a collection of 200 000 documents, averaging about 20 pages in length each, covering a vocabulary of most of the English language. I'm using the word2vec embedding as a basis for finding distances between sentences and the documents. I'm using Gensim, if it matters. I'm using a size of 240. Is this reasonable? Are there any studies on what heuristics to use to choose the size parameter? Thanks.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "nlp",
      "transformer",
      "bert"
    ],
    "owner": {
      "account_id": 15886350,
      "reputation": 181,
      "user_id": 73769,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-JONgkzN-pm0/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rc0qD0FLi4n2ITtPMG5k2TGT6YJmQ/mo/s256-rj/photo.jpg",
      "display_name": "xiangqing shen",
      "link": "https://datascience.stackexchange.com/users/73769/xiangqing-shen"
    },
    "is_answered": true,
    "view_count": 26713,
    "answer_count": 2,
    "score": 18,
    "last_activity_date": 1621622146,
    "creation_date": 1557204798,
    "last_edit_date": 1557346313,
    "question_id": 51522,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/51522/what-is-the-use-of-sep-in-paper-bert",
    "title": "What is the use of [SEP] in paper BERT?",
    "body": "<p>I know that [CLS] means the start of a sentence and [SEP] makes BERT know the second sentence has begun. </p>\n\n<p>However, I have a question.</p>\n\n<p>If I have 2 sentences, which are s1 and s2, and our fine-tuning task is the same. </p>\n\n<p>In one way, I add special tokens and the input looks like [CLS]+s1+[SEP] + s2 + [SEP]. </p>\n\n<p>In another, I make the input look like [CLS] + s1 + s2 + [SEP]. </p>\n\n<p>When I input them to BERT respectively, what is the difference between them? </p>\n\n<ul>\n<li><p>Will the s1 in second one integrate more information from s2 than the s1 in first one does?</p></li>\n<li><p>Will the token embeddings change a lot between the 2 methods?</p></li>\n</ul>\n\n<p>Thanks for any help!</p>\n"
  },
  {
    "tags": [
      "keras",
      "tensorflow"
    ],
    "owner": {
      "account_id": 7939178,
      "reputation": 1511,
      "user_id": 83473,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/feab58f314adf1beb699a3548726a73c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "star",
      "link": "https://datascience.stackexchange.com/users/83473/star"
    },
    "is_answered": true,
    "view_count": 15232,
    "answer_count": 7,
    "score": 18,
    "last_activity_date": 1712297712,
    "creation_date": 1577979089,
    "last_edit_date": 1578175986,
    "question_id": 65736,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/65736/why-does-keras-need-tensorflow-as-backend",
    "title": "Why does Keras need TensorFlow as backend?",
    "body": "<p>Why does Keras need the TensorFlow engine? I am not getting correct directions on why we need Keras. We can use TensorFlow to build a neural network model, but why do most people use Keras with TensorFlow as backend?</p>\n"
  },
  {
    "tags": [
      "graphs",
      "gaussian",
      "rstudio",
      "anaconda"
    ],
    "owner": {
      "account_id": 15862240,
      "reputation": 193,
      "user_id": 100408,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-wFEmqPQdtW0/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3regTSKxAFiLxRJXKV9zdzRv-Cwthg/mo/s256-rj/photo.jpg",
      "display_name": "Saranya Prakash",
      "link": "https://datascience.stackexchange.com/users/100408/saranya-prakash"
    },
    "is_answered": true,
    "view_count": 57844,
    "answer_count": 5,
    "score": 18,
    "last_activity_date": 1624833824,
    "creation_date": 1594171097,
    "last_edit_date": 1594180252,
    "question_id": 77335,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/77335/anconda-r-version-how-to-upgrade-to-4-0-and-later",
    "title": "Anconda R version - How to upgrade to 4.0 and later",
    "body": "<p>I use R through the anaconda navigator, which manages all my package installations. I need to use <a href=\"https://cran.r-project.org/web/packages/qgraph/index.html\" rel=\"noreferrer\">qgraph</a> for a project, which is dependent on <a href=\"https://cran.r-project.org/web/packages/mnormt/index.html\" rel=\"noreferrer\">mnormt</a> library, which in turn needs RStudio verion &gt;4.0</p>\n<p>I think the solution to my problem would be to upgrade Anaconda R to the latest R version, but the r-base search in conda has no R version greater than 3.6. Any help in upgrading Anaconda R to newer versions would be very useful.</p>\n<p>The problem is in installing qgraph, which stops with errors in installing dependencies (pysch)</p>\n<p>During startup - Warning messages:</p>\n<pre><code>1: Setting LC_CTYPE failed, using &quot;C&quot; \n2: Setting LC_TIME failed, using &quot;C&quot; \n3: Setting LC_MESSAGES failed, using &quot;C&quot; \n4: Setting LC_MONETARY failed, using &quot;C&quot; \nError: .onLoad failed in loadNamespace() for 'mnormt', details:\n  call: library.dynam(&quot;mnormt&quot;, pkg, library)\n  error: shared object 'mnormt.dylib' not found\nExecution halted\nERROR: lazy loading failed for package 'psych'\n</code></pre>\n"
  },
  {
    "tags": [
      "machine-learning",
      "nlp",
      "terminology",
      "gpt"
    ],
    "owner": {
      "account_id": 10157804,
      "reputation": 183,
      "user_id": 148720,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d88f76370fc9b503e76f8d01329ed3aa?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "slim_wizard",
      "link": "https://datascience.stackexchange.com/users/148720/slim-wizard"
    },
    "is_answered": true,
    "view_count": 18739,
    "accepted_answer_id": 120766,
    "answer_count": 1,
    "score": 18,
    "last_activity_date": 1684638249,
    "creation_date": 1680818034,
    "last_edit_date": 1684548186,
    "question_id": 120764,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/120764/how-does-an-llm-parameter-relate-to-a-weight-in-a-neural-network",
    "title": "How does an LLM &quot;parameter&quot; relate to a &quot;weight&quot; in a neural network?",
    "body": "<p>I keep reading about how the latest and greatest LLMs have billions of parameters. As someone who is more familiar with standard neural nets but is trying to better understand LLMs, I'm curious if a LLM parameter is the same as a NN weight i.e. is it basically a number that starts as a random coefficient and is adjusted in a way that reduces loss as the model learns? If so, why do so many researches working in the LLM space refer to these as parameters instead of just calling them weights?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "bigdata",
      "libsvm"
    ],
    "owner": {
      "account_id": 2640702,
      "reputation": 283,
      "user_id": 63,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/AQO95.jpg?s=256",
      "display_name": "Puffin GDI",
      "link": "https://datascience.stackexchange.com/users/63/puffin-gdi"
    },
    "is_answered": true,
    "view_count": 447,
    "accepted_answer_id": 46,
    "answer_count": 2,
    "score": 17,
    "last_activity_date": 1400343854,
    "creation_date": 1400032676,
    "last_edit_date": 1400343854,
    "question_id": 16,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/16/use-liblinear-on-big-data-for-semantic-analysis",
    "title": "Use liblinear on big data for semantic analysis",
    "body": "<p>I use <a href=\"http://www.csie.ntu.edu.tw/~cjlin/libsvm/\">Libsvm</a> to train data and predict classification on <strong>semantic analysis</strong> problem. But it has a <strong>performance</strong> issue on large-scale data, because semantic analysis concerns <strong><em>n-dimension</em></strong> problem.</p>\n\n<p>Last year, <a href=\"http://www.csie.ntu.edu.tw/~cjlin/liblinear/\">Liblinear</a> was release, and it can solve performance bottleneck.\nBut it cost too much <strong>memory</strong>. Is <strong>MapReduce</strong> the only way to solve semantic analysis problem on big data? Or are there any other methods that can improve memory bottleneck on <strong>Liblinear</strong>?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-mining",
      "python",
      "classification"
    ],
    "owner": {
      "account_id": 1682291,
      "reputation": 366,
      "user_id": 555,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f28d38430aa19897d8cc35708f49ffc1?s=256&d=identicon&r=PG",
      "display_name": "GrimSqueaker",
      "link": "https://datascience.stackexchange.com/users/555/grimsqueaker"
    },
    "is_answered": true,
    "view_count": 3023,
    "answer_count": 3,
    "score": 17,
    "last_activity_date": 1597582845,
    "creation_date": 1402481519,
    "question_id": 310,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/310/one-class-discriminatory-classification-with-imbalanced-heterogenous-negative-b",
    "title": "One-Class discriminatory classification with imbalanced, heterogenous Negative background?",
    "body": "<p>I'm working on improving an existing supervised classifier, for classifying {protein} sequences as belonging to a specific class (Neuropeptide hormone precursors), or not.</p>\n\n<p>There are about 1,150 known \"positives\", against a background of about 13 million protein sequences (\"Unknown/poorly annotated background\"), or about 100,000 reviewed, relevant proteins, annotated with a variety of properties (but very few annotated in an explicitly \"negative\" way). </p>\n\n<p>My previous implementation looked at this as a binary classification problem: \nPositive set = Proteins marked as Neuropeptides.\nNegative set: Random sampling of 1,300 samples (total) from among the remaining proteins of a roughly similar length-wise distribution. </p>\n\n<p>That worked, but I want to greatly improve the machines discriminatory abilities (Currently, it's at about 83-86% in terms of accuracy, AUC, F1, measured by CV, on multiple randomly sampled negative sets).</p>\n\n<p>My thoughts were to:\n1) Make this a multiclass problem, choosing 2-3 different classes of protein that will definetly be negatives, by their properties/functional class, along with (maybe) another randomly sampled set.\n (Priority here would be negative sets that are similar in their characteristics/features to the positive set, while still having defining characteristics) . \n2) One class learning - Would be nice, but as I understand it, it's meant just for anomaly detection, and has poorer performance than discriminatory approaches.</p>\n\n<p>*) I've heard of P-U learning, which sounds neat, but I'm a programming N00b, and I don't know of any existing implementations for it. (In Python/sci-kit learn).</p>\n\n<p>So, does approach 1 make sense in a theoretical POV? Is there a best way to make multiple negative sets? (I could also simply use a massive [50K] pick of the \"negative\" proteins, but they're all very very different from each other, so I don't know how well the classifier would handle them as one big , unbalanced mix).\nThanks!</p>\n"
  },
  {
    "tags": [
      "machine-learning"
    ],
    "owner": {
      "account_id": 134457,
      "reputation": 273,
      "user_id": 1147,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/dc730ac7300ee5d01caaf9631eaffdde?s=256&d=identicon&r=PG",
      "display_name": "Frost",
      "link": "https://datascience.stackexchange.com/users/1147/frost"
    },
    "is_answered": true,
    "view_count": 3197,
    "accepted_answer_id": 561,
    "answer_count": 5,
    "score": 17,
    "last_activity_date": 1403667019,
    "creation_date": 1403612890,
    "question_id": 559,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/559/detecting-cats-visually-by-means-of-anomaly-detection",
    "title": "Detecting cats visually by means of anomaly detection",
    "body": "<p>I have a hobby project which I am contemplating committing to as a way of increasing my so far limited experience of machine learning. I have taken and completed the Coursera MOOC on the topic. My question is with regards to the feasibility of the project.</p>\n\n<p>The task is the following:</p>\n\n<p>Neighboring cats are from time to time visiting my garden, which I dislike since they tend to defecate on my lawn. I would like to have a warning system that alerts me when there's a cat present so that I may go chase it off using my super soaker. For simplicity's sake, say that I only care about a cat with black and white coloring.</p>\n\n<p>I have setup a raspberry pi with camera module that can capture video and/or pictures of a part of the garden. </p>\n\n<p>Sample image:</p>\n\n<p><img src=\"https://i.sstatic.net/cNqus.jpg\" alt=\"Sample garden image\"></p>\n\n<p>My first idea was to train a classifier to identify cat or cat-like objects, but after realizing that I will be unable to obtain a large enough number of positive samples, I have abandoned that in favor of anomaly detection.</p>\n\n<p>I estimate that if I captured a photo every second of the day, I would end up with maybe five photos containing cats (out of about 60,000 with sunlight) per day. </p>\n\n<p>Is this feasible using anomaly detection? If so, what features would you suggest? My ideas so far would be to simply count the number of pixels with that has certain colors; do some kind of blob detection/image segmenting (which I do not know how do to, and would thus like to avoid) and perform the same color analysis on them.</p>\n"
  },
  {
    "tags": [
      "python",
      "r",
      "recommender-system"
    ],
    "owner": {
      "account_id": 2799708,
      "reputation": 397,
      "user_id": 1131,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/ToWAD.png?s=256",
      "display_name": "Sidhha",
      "link": "https://datascience.stackexchange.com/users/1131/sidhha"
    },
    "is_answered": true,
    "view_count": 8947,
    "answer_count": 2,
    "score": 17,
    "last_activity_date": 1613681005,
    "creation_date": 1406249892,
    "last_edit_date": 1559965672,
    "question_id": 834,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/834/recommending-movies-with-additional-features-using-collaborative-filtering",
    "title": "Recommending movies with additional features using collaborative filtering",
    "body": "<p>I am trying to build a recommendation system using collaborative filtering. I have the usual <code>[user, movie, rating]</code> information. I would like to incorporate an additional feature like 'language' or 'duration of movie'. I am not sure what techniques I could use for such a problem.</p>\n\n<p>Please suggest references or packages in python/R. </p>\n"
  },
  {
    "tags": [
      "clustering",
      "text-mining",
      "algorithms",
      "scikit-learn"
    ],
    "owner": {
      "account_id": 69353,
      "reputation": 303,
      "user_id": 2958,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/GPeuLGrQ.png?s=256",
      "display_name": "Andrey Rubliov",
      "link": "https://datascience.stackexchange.com/users/2958/andrey-rubliov"
    },
    "is_answered": true,
    "view_count": 12396,
    "accepted_answer_id": 980,
    "answer_count": 1,
    "score": 17,
    "last_activity_date": 1624216893,
    "creation_date": 1408108220,
    "last_edit_date": 1615676968,
    "question_id": 979,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/979/algorithms-for-text-clustering",
    "title": "Algorithms for text clustering",
    "body": "<p>I have a problem of clustering huge amount of sentences into groups by their meanings. This is similar to a problem when you have lots of sentences and want to group them by their meanings.</p>\n<p>What algorithms are suggested to do this? I don't know number of clusters in advance (and as more data is coming clusters can change as well), what features are normally used to represent each sentence?</p>\n<p>I'm trying now the simplest features with just list of words and distance between sentences defined as:</p>\n<p><span class=\"math-container\">$|A \\cup B$</span> \\ <span class=\"math-container\">$A \\cap B|$</span>/<span class=\"math-container\">$|A \\cup B|$</span></p>\n<p>(A and B are corresponding sets of words in sentence A and B)</p>\n<p>Does it make sense at all?</p>\n<p>I'm trying to apply <a href=\"http://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html#example-cluster-plot-mean-shift-py\" rel=\"nofollow noreferrer\">Mean-Shift</a> algorithm from scikit library to this distance, as it does not require number of clusters in advance.</p>\n<p>If anyone will advise better methods/approaches for the problem - it will be very much appreciated as I'm still new to the topic.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "statistics",
      "dimensionality-reduction"
    ],
    "owner": {
      "account_id": 985917,
      "reputation": 461,
      "user_id": 5203,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/9d8b6657ed0cc7c26aaebc42cdab0323?s=256&d=identicon&r=PG",
      "display_name": "ASX",
      "link": "https://datascience.stackexchange.com/users/5203/asx"
    },
    "is_answered": true,
    "view_count": 11246,
    "answer_count": 2,
    "score": 17,
    "last_activity_date": 1487089911,
    "creation_date": 1422226343,
    "last_edit_date": 1422226745,
    "question_id": 4942,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/4942/high-dimensional-data-what-are-useful-techniques-to-know",
    "title": "High-dimensional data: What are useful techniques to know?",
    "body": "<p>Due to various <a href=\"http://en.wikipedia.org/wiki/Curse_of_dimensionality\">curses of dimensionality</a>, the accuracy and speed of many of the common predictive techniques degrade on high dimensional data. What are some of the most useful techniques/tricks/heuristics that help deal with high-dimensional data effectively? For example,</p>\n\n<ul>\n<li>Do certain statistical/modeling methods perform well on high-dimensional datasets?</li>\n<li>Can we improve the performance of our predictive models on high-dimensional data by using certain (that define alternative notions of distance) or <a href=\"http://en.wikipedia.org/wiki/Kernel_method\">kernels</a> (that define alternative notions of dot product)?</li>\n<li>What are the most useful techniques of dimensionality reduction for high-dimensional data?</li>\n</ul>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "feature-scaling",
      "normalization",
      "javascript"
    ],
    "owner": {
      "account_id": 6345661,
      "reputation": 323,
      "user_id": 9802,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/VBD4B.jpg?s=256",
      "display_name": "Jonathan Shobrook",
      "link": "https://datascience.stackexchange.com/users/9802/jonathan-shobrook"
    },
    "is_answered": true,
    "view_count": 49396,
    "accepted_answer_id": 5888,
    "answer_count": 4,
    "score": 17,
    "last_activity_date": 1615493326,
    "creation_date": 1432433496,
    "last_edit_date": 1541538873,
    "question_id": 5885,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/5885/how-to-scale-an-array-of-signed-integers-to-range-from-0-to-1",
    "title": "How to scale an array of signed integers to range from 0 to 1?",
    "body": "<p>I'm using <a href=\"https://www.npmjs.com/package/brain\" rel=\"noreferrer\">Brain</a> to train a neural network on a feature set that includes both positive and negative values. But Brain requires input values between 0 and 1. What's the best way to normalize my data?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning"
    ],
    "owner": {
      "account_id": 315385,
      "reputation": 415,
      "user_id": 10938,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/bee37ff3471620d9a17e619e022b547d?s=256&d=identicon&r=PG",
      "display_name": "emanuele",
      "link": "https://datascience.stackexchange.com/users/10938/emanuele"
    },
    "is_answered": true,
    "view_count": 15083,
    "accepted_answer_id": 8866,
    "answer_count": 3,
    "score": 17,
    "last_activity_date": 1539894864,
    "creation_date": 1447684868,
    "question_id": 8860,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/8860/bagging-vs-dropout-in-deep-neural-networks",
    "title": "Bagging vs Dropout in Deep Neural Networks",
    "body": "<p>Bagging is the generation of multiple predictors that works as ensamble as a single predictor. Dropout is a technique that teach to a neural networks to average all possible subnetworks. Looking at the most important Kaggle's competitions seem that this two techniques are used together very often. I can't see any theoretical difference besides the actual implementation. Who can explain me why we should use both of them in any real application? and why performance improve when we use both of them?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "dataset",
      "sampling"
    ],
    "owner": {
      "account_id": 7322944,
      "reputation": 629,
      "user_id": 14200,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://lh6.googleusercontent.com/-xWUFTu_pds0/AAAAAAAAAAI/AAAAAAAAALA/P9U9tQBilgM/s256-rj/photo.jpg",
      "display_name": "jmvllt",
      "link": "https://datascience.stackexchange.com/users/14200/jmvllt"
    },
    "is_answered": true,
    "view_count": 5697,
    "accepted_answer_id": 8897,
    "answer_count": 3,
    "score": 17,
    "last_activity_date": 1447916758,
    "creation_date": 1447877670,
    "last_edit_date": 1447879637,
    "question_id": 8895,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/8895/with-unbalanced-class-do-i-have-to-use-under-sampling-on-my-validation-testing",
    "title": "With unbalanced class, do I have to use under sampling on my validation/testing datasets?",
    "body": "<p>Im a beginner in machine learning and Im facing a situation. Im working on a Real Time Bidding problem, with the IPinYou dataset and Im trying to do a click prediction.</p>\n\n<p>The thing is that, as you may know, the dataset is very unbalanced : Around 1300 negative examples (non click) for 1 positive example (click).</p>\n\n<p>This is what I do:</p>\n\n<ol>\n<li>Load the data</li>\n<li>Split the dataset into 3 datasets : \n    A = Training (60%)\n    B = Validating (20%)\n    C = Testing (20%)</li>\n<li>For each dataset (A, B, C), do an under-sampling on each negative class in order to have a ratio of 5 (5 negative example for 1 positive example). This give me 3 new datasets which are more balanced:\n    A\n    B\n    C</li>\n</ol>\n\n<p>Then I train my model with the dataset A and logistic regression. </p>\n\n<p>My question are:</p>\n\n<ol>\n<li><p>Which dataset do I have to use for validation ? B or B ?</p></li>\n<li><p>Which dataset do I have to use for testing ? C or C</p></li>\n<li><p>Which metrics are the most relevant to evaluate my model? F1Score seems to be a well used metric. But here due to the unbalanced class (if I use the datasets B and C), the precision is low (under 0.20) and the F1Score is very influenced by low recall/precision. Would that be more accurate to use aucPR or aucROC ?</p></li>\n<li><p>If I want to plot the learning curve, which metrics should I use ? (knowing that the %error isnt relevant if I use the B dataset for validating)</p></li>\n</ol>\n\n<p>Thanks in advance for your time !</p>\n\n<p>Regards.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "algorithms",
      "random-forest",
      "linear-regression",
      "decision-trees"
    ],
    "owner": {
      "account_id": 4869459,
      "reputation": 331,
      "user_id": 14384,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/14f7bd035afecfefb4000c5542937fc7?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Jason Donnald",
      "link": "https://datascience.stackexchange.com/users/14384/jason-donnald"
    },
    "is_answered": true,
    "view_count": 83634,
    "closed_date": 1449067494,
    "answer_count": 2,
    "score": 17,
    "last_activity_date": 1550896864,
    "creation_date": 1449018388,
    "question_id": 9159,
    "link": "https://datascience.stackexchange.com/questions/9159/when-to-choose-linear-regression-or-decision-tree-or-random-forest-regression",
    "closed_reason": "Needs more focus",
    "title": "When to choose linear regression or Decision Tree or Random Forest regression?",
    "body": "<p>I am working on a project and I am having difficulty in deciding which algorithm to choose for <code>regression</code>. I want to know under what conditions should one choose a <code>linear regression</code> or <code>Decision Tree regression</code> or <code>Random Forest regression</code>? Are there any specific characteristics of the data that would make the decision to go towards a specific algorithm amongst the tree mentioned above? What are those characteristics that I should look in my dataset to make the decision? And are there some reasons that would make one choose a <code>decision tree</code> or <code>random forest</code> algorithm even if the same correctness can be achieved by <code>linear regression</code>? </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-mining",
      "decision-trees"
    ],
    "owner": {
      "account_id": 7139451,
      "reputation": 173,
      "user_id": 14566,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e738ff59d0b73a4358f6a576098de55c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "gchavez1",
      "link": "https://datascience.stackexchange.com/users/14566/gchavez1"
    },
    "is_answered": true,
    "view_count": 46725,
    "accepted_answer_id": 9229,
    "answer_count": 5,
    "score": 17,
    "last_activity_date": 1615344921,
    "creation_date": 1449354269,
    "question_id": 9228,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9228/decision-tree-vs-knn",
    "title": "Decision tree vs. KNN",
    "body": "<p>In which cases is it better to use a Decision tree and other cases a KNN?</p>\n\n<p>Why use one of them in certain cases? And the other in different cases? (By looking at its functionality, not at the algorithm)</p>\n\n<p>Anyone have some explanations or references about this?</p>\n"
  },
  {
    "tags": [
      "gensim",
      "word2vec",
      "convergence"
    ],
    "owner": {
      "account_id": 302279,
      "reputation": 2500,
      "user_id": 122,
      "user_type": "registered",
      "accept_rate": 62,
      "profile_image": "https://www.gravatar.com/avatar/0e9087f2672b0e4f28d91266acf9ce57?s=256&d=identicon&r=PG",
      "display_name": "alvas",
      "link": "https://datascience.stackexchange.com/users/122/alvas"
    },
    "is_answered": true,
    "view_count": 26680,
    "answer_count": 5,
    "score": 17,
    "last_activity_date": 1670016751,
    "creation_date": 1453036484,
    "last_edit_date": 1615493929,
    "question_id": 9819,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/9819/number-of-epochs-in-gensim-word2vec-implementation",
    "title": "Number of epochs in Gensim Word2Vec implementation",
    "body": "<p>There's an <code>iter</code> parameter in the <code>gensim</code> Word2Vec implementation</p>\n<pre><code>class gensim.models.word2vec.Word2Vec(sentences=None, size=100,\nalpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0,\nseed=1, workers=1, min_alpha=0.0001, sg=1, hs=1, negative=0,\ncbow_mean=0, hashfxn=&lt;built-in function hash&gt;, **iter=1**, null_word=0,\ntrim_rule=None, sorted_vocab=1)\n</code></pre>\n<p>that specifies the number of epochs, i.e.:</p>\n<pre><code>iter = number of iterations (epochs) over the corpus.\n</code></pre>\n<p>Does anyone know whether that helps in improving the model over the corpus?</p>\n<p>Is there any reason why the <code>iter</code> is set to 1 by default? Is there not much effect in increasing the no. of epochs?</p>\n<p>Is there any scientific/empirical evaluation of how to set the no. of epochs?</p>\n<p>Unlike classification/regression task, the grid search method wouldn't really work since the vectors are generated in an unsupervised manner and the objective function is simply by either hierarchical softmax or negative sampling.</p>\n<p>Is there an early stopping mechanism to cut short the no. of epochs once vectors converges? And can the hierarchical softmax or negative sampling objective converge?</p>\n"
  },
  {
    "tags": [
      "performance",
      "terminology"
    ],
    "owner": {
      "account_id": 3133355,
      "reputation": 447,
      "user_id": 3436,
      "user_type": "registered",
      "accept_rate": 0,
      "profile_image": "https://i.sstatic.net/MtRZl.jpg?s=256",
      "display_name": "Ahmad",
      "link": "https://datascience.stackexchange.com/users/3436/ahmad"
    },
    "is_answered": true,
    "view_count": 3405,
    "answer_count": 4,
    "score": 17,
    "last_activity_date": 1617943914,
    "creation_date": 1466191146,
    "last_edit_date": 1617943914,
    "question_id": 12253,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/12253/performance-measure-why-is-it-called-recall-and-sensitivity",
    "title": "Performance measure: Why is it called recall and sensitivity?",
    "body": "<blockquote>\n  <p><strong>precision</strong> is the fraction of retrieved instances that are relevant, while recall (also known as sensitivity) is the fraction of relevant instances that are retrieved. </p>\n</blockquote>\n\n<p>I know their meaning but I don't know why it is called <strong>recall</strong>? I am not a native-speaker of English. I know recall means remember, then I don't know the relevance of this meaning to this concept! maybe <strong>coverage</strong> was better because it shows how many instances were covered...or any other term. </p>\n\n<p>Moreover <strong>sensitivity</strong> is also insensible to me!</p>\n\n<p>Could you please help me to associate these words to the concept and have a sense of them?</p>\n"
  },
  {
    "tags": [
      "regression",
      "linear-regression"
    ],
    "owner": {
      "account_id": 1389967,
      "reputation": 273,
      "user_id": 20655,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/QFW8g.jpg?s=256",
      "display_name": "Albert Gao",
      "link": "https://datascience.stackexchange.com/users/20655/albert-gao"
    },
    "is_answered": true,
    "view_count": 55822,
    "accepted_answer_id": 12285,
    "answer_count": 4,
    "score": 17,
    "last_activity_date": 1594038572,
    "creation_date": 1466313631,
    "question_id": 12274,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/12274/what-does-linear-in-parameters-mean",
    "title": "What does &quot;linear in parameters&quot; mean?",
    "body": "<blockquote>\n  <p>The model of linear regression is linear in parameters.</p>\n</blockquote>\n\n<p>What does this actually mean?</p>\n"
  },
  {
    "tags": [
      "python",
      "r",
      "visualization",
      "markov-process",
      "simulation"
    ],
    "owner": {
      "account_id": 5697375,
      "reputation": 279,
      "user_id": 15343,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/8EXRw.jpg?s=256",
      "display_name": "Andrew Brown",
      "link": "https://datascience.stackexchange.com/users/15343/andrew-brown"
    },
    "is_answered": true,
    "view_count": 1683,
    "answer_count": 1,
    "score": 17,
    "last_activity_date": 1583392922,
    "creation_date": 1475162167,
    "last_edit_date": 1557852709,
    "question_id": 14282,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/14282/visualization-of-multiple-markov-models",
    "title": "Visualization of multiple Markov models",
    "body": "<p>I am working on a project where we compare over 10 different Markov models, each representing a different treatment plan.</p>\n\n<p>Most often single models are visualized with a decision tree or <a href=\"http://guizzetti.ca/blogs/lenny/wp-content/uploads/2012/04/3state_markov.jpg\" rel=\"noreferrer\">transition state diagram</a>. However, with multiple different models what are potential visualizations that could communicate the transition states that differentiate each model?   </p>\n\n<p>I have seen other people use a <a href=\"http://www.bmj.com/highwire/markup/587855/expansion?width=1000&amp;height=500&amp;iframe=true&amp;postprocessors=highwire_figures%2Chighwire_math\" rel=\"noreferrer\">table</a> to depict different models and the transition states.  </p>\n\n<p>For clarity, I am not referring to a transition probabilities chart but a method of communicating the differences between multiple models.   </p>\n"
  },
  {
    "tags": [
      "unsupervised-learning",
      "rbm"
    ],
    "owner": {
      "account_id": 2181592,
      "reputation": 347,
      "user_id": 26872,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/vhRiE.png?s=256",
      "display_name": "Born2Code",
      "link": "https://datascience.stackexchange.com/users/26872/born2code"
    },
    "is_answered": true,
    "view_count": 2585,
    "accepted_answer_id": 15609,
    "answer_count": 3,
    "score": 17,
    "last_activity_date": 1483725415,
    "creation_date": 1481126793,
    "last_edit_date": 1481128145,
    "question_id": 15595,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/15595/intuition-behind-restricted-boltzmann-machine-rbm",
    "title": "Intuition Behind Restricted Boltzmann Machine (RBM)",
    "body": "<p>I went through Geoff Hinton's Neural Networks course on Coursera and also through <a href=\"http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/\" rel=\"noreferrer\">introduction to restricted boltzmann machines</a>, still I didn't understand the intuition behind RBMs. </p>\n\n<p>Why do we need to compute energy in this machine? And what is the use of the probability in this machine? I also saw this <a href=\"https://www.youtube.com/watch?v=-l1QTbgLTyQ\" rel=\"noreferrer\">video</a>. In the video, he just wrote the probability and energy equations before the computation steps and didn't appear to use it anywhere.</p>\n\n<p>Adding to the above, I am not sure what the likelihood function is for?</p>\n"
  },
  {
    "tags": [
      "nlp",
      "text-mining",
      "similarity",
      "cosine-distance"
    ],
    "owner": {
      "account_id": 9755143,
      "reputation": 171,
      "user_id": 27559,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/44843275a9618d27d017481589096562?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Richard Knoche",
      "link": "https://datascience.stackexchange.com/users/27559/richard-knoche"
    },
    "is_answered": true,
    "view_count": 15373,
    "answer_count": 4,
    "score": 17,
    "last_activity_date": 1551335039,
    "creation_date": 1483389673,
    "question_id": 16036,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/16036/alternatives-to-tf-idf-and-cosine-similarity-when-comparing-documents-of-differi",
    "title": "Alternatives to TF-IDF and Cosine Similarity when comparing documents of differing formats",
    "body": "<p>I've been working on a small, personal project which takes a user's job skills and suggests the most ideal career for them based on those skills. I use a database of job listings to achieve this. At the moment, the code works as follows:</p>\n\n<p>1) Process the text of each job listing to extract skills that are mentioned in the listing</p>\n\n<p>2) For each career (e.g. \"Data Analyst\"), combine the processed text of the job listings for that career into one document</p>\n\n<p>3) Calculate the TF-IDF of each skill within the career documents</p>\n\n<p>After this, I'm not sure which method I should use to rank careers based on a list of a user's skills. The most popular method that I've seen would be to treat the user's skills as a document as well, then to calculate the TF-IDF for the skill document, and use something like cosine similarity to calculate the similarity between the skill document and each career document.</p>\n\n<p>This doesn't seem like the ideal solution to me, since cosine similarity is best used when comparing two documents of the same format. For that matter, TF-IDF doesn't seem like the appropriate metric to apply to the user's skill list at all. For instance, if a user adds additional skills to their list, the TF for each skill will drop. In reality, I don't care what the frequency of the skills are in the user's skills list -- I just care that they have those skills (and maybe how well they know those skills).</p>\n\n<p>It seems like a better metric would be to do the following:</p>\n\n<p>1) For each skill that the user has, calculate the TF-IDF of that skill in the career documents</p>\n\n<p>2) For each career, sum the TF-IDF results for all of the user's skill</p>\n\n<p>3) Rank career based on the above sum</p>\n\n<p>Am I thinking along the right lines here? If so, are there any algorithms that work along these lines, but are more sophisticated than a simple sum? Thanks for the help!</p>\n"
  },
  {
    "tags": [
      "python",
      "dataset"
    ],
    "owner": {
      "account_id": 2738792,
      "reputation": 911,
      "user_id": 8432,
      "user_type": "registered",
      "accept_rate": 57,
      "profile_image": "https://www.gravatar.com/avatar/72bd5e3f73d1cd1774d2235b6bfab11d?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Blaszard",
      "link": "https://datascience.stackexchange.com/users/8432/blaszard"
    },
    "is_answered": true,
    "view_count": 9419,
    "accepted_answer_id": 17600,
    "answer_count": 3,
    "score": 17,
    "last_activity_date": 1662468435,
    "creation_date": 1489563400,
    "last_edit_date": 1489563806,
    "question_id": 17598,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/17598/why-are-variables-of-train-and-test-data-defined-using-the-capital-letter-in-py",
    "title": "Why are variables of train and test data defined using the capital letter (in Python)?",
    "body": "<p>I hope this question is the most suitable in this site...</p>\n\n<p>In Python, usually the class name is defined using the capital letter as its first character, for example</p>\n\n<pre><code>class Vehicle:\n    ...\n</code></pre>\n\n<p>However, in machine learning field, often times train and test data are defined as <code>X</code> and <code>Y</code> - not <code>x</code> and <code>y</code>. For example, I'm now reading <a href=\"https://www.packtpub.com/books/content/keras\" rel=\"noreferrer\">this tutorial on Keras</a>, but it uses the <code>X</code> and <code>Y</code> as its variables:</p>\n\n<pre><code>from sklearn import datasets\n\nmnist = datasets.load_digits()\nX = mnist.data\nY = mnist.target\n</code></pre>\n\n<p>Why are these defined as capital letters? Is there any convention (at least in Python) among machine learning field that it is better to use the capital letter to define these variables?</p>\n\n<p>Or maybe do people distinguish the upper vs lower case variables in machine learning?</p>\n\n<p>In fact the same tutorial later distinguish these variables like the following:</p>\n\n<pre><code>from sklearn.cross_validation import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, Y, train_size=0.7, random_state=0)\n</code></pre>\n"
  },
  {
    "tags": [
      "neural-network"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 19443,
    "answer_count": 1,
    "score": 17,
    "last_activity_date": 1596635896,
    "creation_date": 1490848843,
    "last_edit_date": 1490849145,
    "question_id": 17987,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/17987/how-should-the-bias-be-initialized-and-regularized",
    "title": "How should the bias be initialized and regularized?",
    "body": "<p>I've read a couple of papers about kernel initialization and many papers mention that they use L2 regularization of the kernel (often with $\\lambda = 0.0001$).</p>\n\n<p>Does anybody do something different than initializing the bias with constant zero and not regularizing it?</p>\n\n<h2>Kernel initialization papers</h2>\n\n<ul>\n<li>Mishkin and Matas: <a href=\"https://arxiv.org/abs/1511.06422\" rel=\"noreferrer\">All you need is a good init</a></li>\n<li>Xavier Glorot and Yoshua Bengio: <a href=\"http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\" rel=\"noreferrer\">Understanding the difficulty of training deep feedforward neural networks</a></li>\n<li>He et al: <a href=\"https://arxiv.org/abs/1502.01852\" rel=\"noreferrer\">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>\n</ul>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "topic-model",
      "lda"
    ],
    "owner": {
      "account_id": 11480337,
      "reputation": 171,
      "user_id": 37512,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/IzzDU.jpg?s=256",
      "display_name": "sariii",
      "link": "https://datascience.stackexchange.com/users/37512/sariii"
    },
    "is_answered": true,
    "view_count": 25928,
    "answer_count": 3,
    "score": 17,
    "last_activity_date": 1691653479,
    "creation_date": 1501819008,
    "last_edit_date": 1691652373,
    "question_id": 21950,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/21950/why-should-we-not-feed-lda-with-tf-idf-input",
    "title": "Why should we not feed LDA with TF-IDF input?",
    "body": "<p>Can someone explain why we can not feed <a href=\"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\" rel=\"noreferrer\">LDA</a> topic model with <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" rel=\"noreferrer\">TFIDF</a>?  What is wrong with this approach conceptually?</p>\n"
  },
  {
    "tags": [
      "time-series",
      "rnn"
    ],
    "owner": {
      "account_id": 9591149,
      "reputation": 343,
      "user_id": 40182,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/313d37f49535447d580cae4e40456096?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Ploo",
      "link": "https://datascience.stackexchange.com/users/40182/ploo"
    },
    "is_answered": true,
    "view_count": 10876,
    "accepted_answer_id": 23747,
    "answer_count": 1,
    "score": 17,
    "last_activity_date": 1507885462,
    "creation_date": 1507478250,
    "last_edit_date": 1507479102,
    "question_id": 23615,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/23615/rnn-using-multiple-time-series",
    "title": "RNN using multiple time series",
    "body": "<p>I am trying to create a neural network using time series as input, in order to train it based on the type of each series.\nI read that using RNNs you can split the input into batches and use every point of the time series into individual neurons and eventually train the network.</p>\n\n<p>What I am trying to do though is use multiple time series as an input. So for example you might receive input from two sensors. (So two time series), but I want to use both of them in order to get a final result.</p>\n\n<p>Also I am not trying to predict future values of the time series, I am trying to a get a classification based on all of them.</p>\n\n<p>How should I approach this problem?</p>\n\n<ul>\n<li><p>Is there a way to use multiple time series as an input to an RNN?</p></li>\n<li><p>Should I try to aggregate the time series into one?</p></li>\n<li><p>Or should i just use two different neural networks? And if this last approach is correct, if the number of time series increases wouldn't that be too computer intensive?</p></li>\n</ul>\n"
  },
  {
    "tags": [
      "neural-network",
      "deep-learning",
      "regression",
      "data-cleaning",
      "normalization"
    ],
    "owner": {
      "account_id": 7888593,
      "reputation": 343,
      "user_id": 31316,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/743c389e4c05d0573d91f201d9e1f9d6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Euler_Salter",
      "link": "https://datascience.stackexchange.com/users/31316/euler-salter"
    },
    "is_answered": true,
    "view_count": 26002,
    "accepted_answer_id": 24218,
    "answer_count": 1,
    "score": 17,
    "last_activity_date": 1702978104,
    "creation_date": 1509442790,
    "last_edit_date": 1702978104,
    "question_id": 24214,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/24214/why-should-i-normalize-also-the-output-data",
    "title": "Why should I normalize also the output data?",
    "body": "<p>I'm new to data science and Neural Networks in general.\nLooking around, many people say it is better to normalize the data before doing anything with the NN. I understand how normalizing the <strong>input</strong> data can be useful.</p>\n<p>However, I really don't see how normalizing the <strong>output</strong> data can help.</p>\n<p>I've also tried both cases with an easy dataset, and I achieved the same results. The only difference is that in some weird problems, it is really hard to then re-convert the output back.</p>\n<p>Can you give me some intuition on why we should also normalize the output?</p>\n<p>Or maybe why it is indifferent?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "time-series",
      "prediction",
      "lstm"
    ],
    "owner": {
      "account_id": 3094055,
      "reputation": 308,
      "user_id": 29109,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/082c681b40e47b5c15eebb0d60c52a34?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "4Oh4",
      "link": "https://datascience.stackexchange.com/users/29109/4oh4"
    },
    "is_answered": true,
    "view_count": 17868,
    "accepted_answer_id": 24404,
    "answer_count": 5,
    "score": 17,
    "last_activity_date": 1693754105,
    "creation_date": 1509970599,
    "question_id": 24403,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/24403/prediction-interval-around-lstm-time-series-forecast",
    "title": "Prediction interval around LSTM time series forecast",
    "body": "<p>Is there a method to calculate the prediction interval (probability distribution) around a time series forecast from an LSTM (or other recurrent) neural network? </p>\n\n<p>Say, for example, I am predicting 10 samples into the future (t+1 to t+10), based on the last 10 observed samples (t-9 to t), I would expect the prediction at t+1 to be more accurate than the prediction at t+10. Typically, one might draw error bars around the prediction to show the interval. With an ARIMA model (under the assumption of normally distributed errors), I can calculate a prediction interval (e.g. 95%) around each predicted value. Can I calculate the same, (or something that relates to the prediction interval) from an LSTM model?</p>\n\n<p>I'm been working with LSTMs in Keras/Python, following lots of examples from <a href=\"https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\" rel=\"noreferrer\">machinelearningmastery.com</a>, from which my example code (below) is based on. I'm considering reframing the problem as classification into discrete bins, as that produces a confidence per class, but that seems a poor solution.</p>\n\n<p>There are a couple of similar topics (such as the below), but nothing seems to directly address the issue of prediction intervals from LSTM (or indeed other) neural networks:</p>\n\n<p><a href=\"https://stats.stackexchange.com/questions/25055/how-to-calculate-the-confidence-interval-for-time-series-prediction\">https://stats.stackexchange.com/questions/25055/how-to-calculate-the-confidence-interval-for-time-series-prediction</a></p>\n\n<p><a href=\"https://datascience.stackexchange.com/questions/12721/time-series-prediction-using-arima-vs-lstm\">Time series prediction using ARIMA vs LSTM</a></p>\n\n<pre><code>from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom math import sin\nfrom matplotlib import pyplot\nimport numpy as np\n\n# Build an LSTM network and train\ndef fit_lstm(X, y, batch_size, nb_epoch, neurons):\n    X = X.reshape(X.shape[0], 1, X.shape[1]) # add in another dimension to the X data\n    y = y.reshape(y.shape[0], y.shape[1])      # but don't add it to the y, as Dense has to be 1d?\n    model = Sequential()\n    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n    model.add(Dense(y.shape[1]))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    for i in range(nb_epoch):\n        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n        model.reset_states()\n    return model\n\n# Configuration\nn = 5000    # total size of dataset\nSLIDING_WINDOW_LENGTH = 30\nSLIDING_WINDOW_STEP_SIZE = 1\nbatch_size = 10\ntest_size = 0.1 # fraction of dataset to hold back for testing\nnb_epochs = 100 # for training\nneurons = 8 # LSTM layer complexity\n\n# create dataset\n#raw_values = [sin(i/2) for i in range(n)]  # simple sine wave\nraw_values = [sin(i/2)+sin(i/6)+sin(i/36)+np.random.uniform(-1,1) for i in range(n)]  # double sine with noise\n#raw_values = [(i%4) for i in range(n)] # saw tooth\n\nall_data = np.array(raw_values).reshape(-1,1) # make into array, add anothe dimension for sci-kit compatibility\n\n# data is segmented using a sliding window mechanism\nall_data_windowed = [np.transpose(all_data[idx:idx+SLIDING_WINDOW_LENGTH]) for idx in np.arange(0,len(all_data)-SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP_SIZE)]\nall_data_windowed = np.concatenate(all_data_windowed, axis=0).astype(np.float32)\n\n# split data into train and test-sets\n# round datasets down to a multiple of the batch size\ntest_length = int(round((len(all_data_windowed) * test_size) / batch_size) * batch_size)\ntrain, test = all_data_windowed[:-test_length,:], all_data_windowed[-test_length:,:]\ntrain_length = int(np.floor(train.shape[0] / batch_size)*batch_size) \ntrain = train[:train_length,...]\n\nhalf_size = int(SLIDING_WINDOW_LENGTH/2) # split the examples half-half, to forecast the second half\nX_train, y_train = train[:,:half_size], train[:,half_size:]\nX_test, y_test = test[:,:half_size], test[:,half_size:]\n\n# fit the model\nlstm_model = fit_lstm(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epochs, neurons=neurons)\n\n# forecast the entire training dataset to build up state for forecasting\nX_train_reshaped = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\nlstm_model.predict(X_train_reshaped, batch_size=batch_size)\n\n# predict from test dataset\nX_test_reshaped = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\nyhat = lstm_model.predict(X_test_reshaped, batch_size=batch_size)\n\n#%% Plot prediction vs actual\n\nx_axis_input = range(half_size)\nx_axis_output = [x_axis_input[-1]] + list(half_size+np.array(range(half_size)))\n\nfig = pyplot.figure()\nax = fig.add_subplot(111)\nline1, = ax.plot(x_axis_input,np.zeros_like(x_axis_input), 'r-')\nline2, = ax.plot(x_axis_output,np.zeros_like(x_axis_output), 'o-')\nline3, = ax.plot(x_axis_output,np.zeros_like(x_axis_output), 'g-')\nax.set_xlim(np.min(x_axis_input),np.max(x_axis_output))\nax.set_ylim(-4,4)\npyplot.legend(('Input','Actual','Predicted'),loc='upper left')\npyplot.show()\n\n# update plot in a loop\nfor idx in range(y_test.shape[0]):\n\n    sample_input = X_test[idx]\n    sample_truth = [sample_input[-1]] + list(y_test[idx]) # join lists\n    sample_predicted = [sample_input[-1]] + list(yhat[idx])\n\n    line1.set_ydata(sample_input)\n    line2.set_ydata(sample_truth)\n    line3.set_ydata(sample_predicted)\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\n    pyplot.pause(.25)\n</code></pre>\n"
  },
  {
    "tags": [
      "gan"
    ],
    "owner": {
      "account_id": 5581116,
      "reputation": 381,
      "user_id": 33183,
      "user_type": "registered",
      "accept_rate": 75,
      "profile_image": "https://i.sstatic.net/IYUfs.png?s=256",
      "display_name": "Open Food Broker",
      "link": "https://datascience.stackexchange.com/users/33183/open-food-broker"
    },
    "is_answered": true,
    "view_count": 12968,
    "accepted_answer_id": 24883,
    "answer_count": 3,
    "score": 17,
    "last_activity_date": 1615493601,
    "creation_date": 1510993901,
    "last_edit_date": 1615493601,
    "question_id": 24878,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/24878/gans-generative-adversarial-networks-possible-for-text-as-well",
    "title": "GANs (generative adversarial networks) possible for text as well?",
    "body": "<p>Are GANs (generative adversarial networks) good just for images or can they be used for text as well?</p>\n<p>Like training a network to generate meaningful text from a summary.</p>\n<p>UPD - quotes from the GAN inventor Ian Goodfellow.</p>\n<blockquote>\n<p>GANs have not been applied to NLP because GANs are only defined for\nreal-valued data. (<a href=\"https://www.quora.com/Is-text-generation-using-GANs-a-fundamentally-flawed-idea\" rel=\"nofollow noreferrer\">2016</a>) source</p>\n<p>It is not a fundamentally flawed idea. It should be possible to do at least one of the following... (2017) <a href=\"https://www.quora.com/Is-text-generation-using-GANs-a-fundamentally-flawed-idea\" rel=\"nofollow noreferrer\">source</a></p>\n</blockquote>\n"
  },
  {
    "tags": [
      "perceptron",
      "pytorch"
    ],
    "owner": {
      "account_id": 27349,
      "reputation": 273,
      "user_id": 42255,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/9c4e43d1c358143a5cc388a586391142?s=256&d=identicon&r=PG",
      "display_name": "Bai Li",
      "link": "https://datascience.stackexchange.com/users/42255/bai-li"
    },
    "is_answered": true,
    "view_count": 16310,
    "accepted_answer_id": 25027,
    "answer_count": 2,
    "score": 17,
    "last_activity_date": 1525035434,
    "creation_date": 1511384582,
    "last_edit_date": 1511385787,
    "question_id": 25024,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/25024/strange-behavior-with-adam-optimizer-when-training-for-too-long",
    "title": "Strange behavior with Adam optimizer when training for too long",
    "body": "<p>I'm trying to train a single perceptron (1000 input units, 1 output, no hidden layers) on 64 randomly generated data points. I'm using Pytorch using the Adam optimizer:</p>\n\n<pre><code>import torch\nfrom torch.autograd import Variable\n\ntorch.manual_seed(545345)\nN, D_in, D_out = 64, 1000, 1\n\nx = Variable(torch.randn(N, D_in))\ny = Variable(torch.randn(N, D_out))\n\nmodel = torch.nn.Linear(D_in, D_out)\nloss_fn = torch.nn.MSELoss(size_average=False)\n\noptimizer = torch.optim.Adam(model.parameters())\nfor t in xrange(5000):\n  y_pred = model(x)\n  loss = loss_fn(y_pred, y)\n\n  print(t, loss.data[0])\n\n  optimizer.zero_grad()\n  loss.backward()\n  optimizer.step()\n</code></pre>\n\n<p>Initially, the loss quickly decreases, as expected:</p>\n\n<pre><code>(0, 91.74887084960938)\n(1, 76.85824584960938)\n(2, 63.434078216552734)\n(3, 51.46927261352539)\n(4, 40.942893981933594)\n(5, 31.819372177124023)\n</code></pre>\n\n<p>Around 300 iterations, the error reaches near zero:</p>\n\n<pre><code>(300, 2.1734419819452455e-12)\n(301, 1.90354676465887e-12)\n(302, 2.3347573874232808e-12)\n</code></pre>\n\n<p>This goes on for a few thousand iterations. However, after training for too long, the error starts to increase again:</p>\n\n<pre><code>(4997, 0.002102422062307596)\n(4998, 0.0020302983466535807)\n(4999, 0.0017039275262504816)\n</code></pre>\n\n<p>Why is this happening?</p>\n"
  },
  {
    "tags": [
      "tensorflow",
      "keras",
      "loss-function"
    ],
    "owner": {
      "account_id": 6721244,
      "reputation": 661,
      "user_id": 32860,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://i.sstatic.net/j8M6A.jpg?s=256",
      "display_name": "Nickpick",
      "link": "https://datascience.stackexchange.com/users/32860/nickpick"
    },
    "is_answered": true,
    "view_count": 39011,
    "accepted_answer_id": 28045,
    "answer_count": 2,
    "score": 17,
    "last_activity_date": 1519141900,
    "creation_date": 1511389468,
    "last_edit_date": 1511712499,
    "question_id": 25029,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/25029/custom-loss-function-with-additional-parameter-in-keras",
    "title": "Custom loss function with additional parameter in Keras",
    "body": "<p>I'm looking for a way to create a loss function that looks like this:\nThe function should then maximize for the reward. Is this possible to achieve in Keras?\nAny suggestions how this can be achieved are highly appreciated.</p>\n\n<pre><code>def special_loss_function(y_true, y_pred, reward_if_correct, punishment_if_false):\n    loss = if binary classification is correct apply reward for that training item in accordance with the weight\n           if binary classification is wrong, apply punishment for that training item in accordance with the weight\n                    )\n    return K.mean(loss, axis=-1)\n</code></pre>\n\n<p>The approach I've been looking at for my example is to pass in the weights along with y_true and then cut the tensor into two, separating out the weights and the y_true as seen below. Would an approach like this be possible at all or would this interfere with the normalization process etc?</p>\n\n<pre><code>def decompose_y_true(y_true_and_weights):\n    y_true = y_true_and_weights[:,1]\n    weights= y_true_and_weights[:,1:]\n    return y_true, weights\n\n\ndef custom_loss(y_true_and_weights, y_pred):\n    y_true, y_weights = decompose_y_true(y_true_and_weights)\n     loss = # some loss operation\n    return K.mean(loss, axis=-1)\n</code></pre>\n\n<p>Much more elegant would be if I could pass in my weights over the sample_weights parameter in the fit function, but it seems there are some limits what shape those weights can have, and also there's no way to retrieve them within the loss function as far as I can tell. Or is there any way to pass this into the loss function somehow so I can operate on them from there?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "image-classification",
      "cnn"
    ],
    "owner": {
      "account_id": 7183617,
      "reputation": 173,
      "user_id": 43441,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7eadbad31af9e5cea99395ba47576412?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Felix",
      "link": "https://datascience.stackexchange.com/users/43441/felix"
    },
    "is_answered": true,
    "view_count": 24687,
    "accepted_answer_id": 25759,
    "answer_count": 2,
    "score": 17,
    "last_activity_date": 1513559191,
    "creation_date": 1513547517,
    "last_edit_date": 1513548588,
    "question_id": 25754,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/25754/updating-the-weights-of-the-filters-in-a-cnn",
    "title": "Updating the weights of the filters in a CNN",
    "body": "<p>I am currently trying to understand the architecture of a CNN. I understand the convolution, the ReLU layer, pooling layer, and fully connected layer. However, I am still confused about the weights. </p>\n\n<p>In a normal neural network, each neuron has its own weight. In the fully connected layer, each neuron would also have its own weight. But what I dont know is if each filter has its own weight. Do I just have to update the weights in the fully connected layer during back propagation? Or do the filters all have a separate weight that I need to update?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "rnn",
      "sequence-to-sequence",
      "nlp"
    ],
    "owner": {
      "account_id": 302279,
      "reputation": 2500,
      "user_id": 122,
      "user_type": "registered",
      "accept_rate": 62,
      "profile_image": "https://www.gravatar.com/avatar/0e9087f2672b0e4f28d91266acf9ce57?s=256&d=identicon&r=PG",
      "display_name": "alvas",
      "link": "https://datascience.stackexchange.com/users/122/alvas"
    },
    "is_answered": true,
    "view_count": 13101,
    "answer_count": 1,
    "score": 17,
    "last_activity_date": 1519317109,
    "creation_date": 1516700357,
    "last_edit_date": 1516710838,
    "question_id": 26947,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/26947/why-do-we-need-to-add-start-s-end-s-symbols-when-using-recurrent-neural-n",
    "title": "Why do we need to add START &lt;s&gt; + END &lt;/s&gt; symbols when using Recurrent Neural Nets for Sequence-to-Sequence Models?",
    "body": "<p>In the Sequence-to-Sequence models, we often see that the START (e.g. <code>&lt;s&gt;</code>) and END (e.g. <code>&lt;/s&gt;</code>) symbols are added to the inputs and outputs before training the model and before inference/decoding unseen data.</p>\n\n<p>E.g. <a href=\"http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\" rel=\"noreferrer\">http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html</a> </p>\n\n<pre><code>SOS_token = 0\nEOS_token = 1\n\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n        self.n_words = 2  # Count SOS and EOS\n\n    def addSentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1\n</code></pre>\n\n<ul>\n<li><p><strong>Is there a technical definition or academic explanation of why that is necessary?</strong></p></li>\n<li><p><strong>Or does that need to add the END symbol only applies to Natural Language Processing task where the sentence generation needs to end?</strong></p></li>\n<li><p><strong>But what does the START symbol do? Other than to take the initial state where the trained network will start inferrring.</strong> </p></li>\n</ul>\n"
  },
  {
    "tags": [
      "python",
      "keras",
      "rnn",
      "lstm"
    ],
    "owner": {
      "account_id": 3764237,
      "reputation": 273,
      "user_id": 45892,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/123866fee5cd36f780883741c12403cf?s=256&d=identicon&r=PG",
      "display_name": "Bastien",
      "link": "https://datascience.stackexchange.com/users/45892/bastien"
    },
    "is_answered": true,
    "view_count": 20094,
    "accepted_answer_id": 27572,
    "answer_count": 1,
    "score": 17,
    "last_activity_date": 1589044247,
    "creation_date": 1518014960,
    "question_id": 27563,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/27563/multi-dimentional-and-multivariate-time-series-forecast-rnn-lstm-keras",
    "title": "Multi-dimentional and multivariate Time-Series forecast (RNN/LSTM) Keras",
    "body": "<p>I have been trying to understand how to represent and shape data to make a <strong>multidimentional</strong> and <strong>multivariate</strong> time series forecast using Keras (or TensorFlow) but I am still very unclear after reading many blog posts/tutorials/documentation about how to present the data in the correct shape (most examples being of slightly less </p>\n\n<p><strong>My Dataset:</strong></p>\n\n<ul>\n<li>several cities</li>\n<li>for which I have info about say temperature, car traffic, humidity</li>\n<li>for say the last 2 years (one record for each day)</li>\n</ul>\n\n<p><strong>What I want to do:</strong>\nI'd like to forecast for each city the temperatures I can expect for the next year using a possibly lagged version of temperature, car traffic and humidity (of course there would be several more features but this is just an example for thought).</p>\n\n<p><strong>What I am confused about:</strong>\nIf I have 2 cities, for which I recorded 3 features for 365 days. How should I shape my input so that the model can output a forecast for 365 days for these two cities (i.e. 2 time series of temperatures for 365 days)?</p>\n\n<p>Intuitively the tensor shape would be <code>(?, 365, 3)</code> for 365 days and 3 features. But I'm not sure what to stick into the first dimension and, most importantly, I would be surprised if it had to be for the number of cities. But at the same time, I have no idea how to specify into the model that it has to understand the dimensions properly. </p>\n\n<p>Any pointers will be helpful. I'm pretty familiar with the rest of the problem (i.e. how you build a network in Keras etc since I have done this for other neural networks but more specifically how best to encode the sequence for the desired input.)</p>\n\n<p><em>Oh and also</em>, I guess I could train and predict for each city independently, but I'm sure everyone will agree there are probably things to be learned that are not particular to any city but that can only be seen if considering several of them, hence why I think it is important to encode it in the model.</p>\n"
  },
  {
    "tags": [
      "preprocessing",
      "word-embeddings",
      "embeddings",
      "encoding"
    ],
    "owner": {
      "account_id": 8613515,
      "reputation": 610,
      "user_id": 39198,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-YX-fKMoU-ps/AAAAAAAAAAI/AAAAAAAAACE/qDO4Xj_LSFk/s256-rj/photo.jpg",
      "display_name": "Jonathan DEKHTIAR",
      "link": "https://datascience.stackexchange.com/users/39198/jonathan-dekhtiar"
    },
    "is_answered": true,
    "view_count": 34144,
    "accepted_answer_id": 29853,
    "answer_count": 2,
    "score": 17,
    "last_activity_date": 1621593524,
    "creation_date": 1522764808,
    "last_edit_date": 1621593459,
    "question_id": 29851,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/29851/one-hot-encoding-vs-word-embedding-when-to-choose-one-or-another",
    "title": "One Hot Encoding vs Word Embedding - When to choose one or another?",
    "body": "<p>A colleague of mine is having an interesting situation, he has quite a large set of possibilities for a defined categorical feature (+/- 300 different values)</p>\n<p>The usual data science approach would be to perform a One-Hot Encoding.\nHowever, wouldn't it be a bit extreme to perform some One-Hot Encoding with a dictionary quite large (+/- 300 values)? Is there any best practice on when to choose Embedding vectors or One-Hot Encoding?</p>\n<hr />\n<p>Additional, information: how would you handle the previous case if new values can be added to the dictionary. Re-training seems the only solution, however with One-Hot Encoding, the data dimension will simultaniously increase which may lead to additional troubles, embedding vectors, on the opposite side, can keep the same dimension even if new values appears.</p>\n<p>How would you handle such a case ? Embedding vectors clearly seem more appropriate to me, however I would like to validate my opinion and check if there is another solution that could be more apporiate.</p>\n"
  },
  {
    "tags": [
      "python",
      "deep-learning",
      "classification",
      "keras",
      "overfitting"
    ],
    "owner": {
      "account_id": 13272475,
      "reputation": 173,
      "user_id": 49944,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/da50a69fb27bafac37c3de37a05c3f64?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Amy.Dj",
      "link": "https://datascience.stackexchange.com/users/49944/amy-dj"
    },
    "is_answered": true,
    "view_count": 72785,
    "accepted_answer_id": 29896,
    "answer_count": 5,
    "score": 17,
    "last_activity_date": 1629254050,
    "creation_date": 1522832547,
    "last_edit_date": 1522917881,
    "question_id": 29893,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/29893/high-model-accuracy-vs-very-low-validation-accuarcy",
    "title": "High model accuracy vs very low validation accuarcy",
    "body": "<p>I'm building a sentiment analysis program in python using Keras Sequential model for deep learning</p>\n\n<p>my data is 20,000 tweets:</p>\n\n<ul>\n<li>positive tweets: 9152 tweets</li>\n<li>negative tweets: 10849 tweets</li>\n</ul>\n\n<p>I wrote a sequential model script to make the binary classification as follows:</p>\n\n<pre><code>model=Sequential()\nmodel.add(Embedding(vocab_size, 100, input_length=max_words))\nmodel.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(250, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Fit the model\n\nprint(model.summary())\nhistory=model.fit(X_train[train], y1[train], validation_split=0.30,epochs=2, batch_size=128,verbose=2)\n</code></pre>\n\n<p>however I get very strange results! The model accuracy is almost perfect (>90)\nwhereas the validation accuracy is very low (&lt;1) (shown bellow)</p>\n\n<pre><code>Train on 9417 samples, validate on 4036 samples\n Epoch 1/2\n- 13s - loss: 0.5478 - acc: 0.7133 - val_loss: 3.6157 - val_acc: 0.0243\n Epoch 2/2\n- 11s - loss: 0.2287 - acc: 0.8995 - val_loss: 5.4746 - val_acc: 0.0339\n</code></pre>\n\n<p>I tried to increase the number of epoch, and it only increases the model accuracy and lowers the validation accuracy</p>\n\n<p>Any advice on how to overcome this issue?</p>\n\n<p>Update:</p>\n\n<p>this is how I handle my data</p>\n\n<pre><code>#read training data\npos_file=open('pos2.txt', 'r', encoding=\"Latin-1\")\n neg_file=open('neg3.txt', 'r', encoding=\"Latin-1\")\n# Load data from files\npos = list(pos_file.readlines())\nneg = list(neg_file.readlines()) \nx = pos + neg\ndocs = numpy.array(x)\n#read Testing Data\npos_test=open('posTest2.txt', 'r',encoding=\"Latin-1\")\nposT = list(pos_test.readlines())\nneg_test=open('negTest2.txt', 'r',encoding=\"Latin-1\")\nnegT = list(neg_test.readlines())\nxTest = posT + negT\ntotal2 = numpy.array(xTest)\n\nCombinedDocs=numpy.append(total2,docs)\n\n# Generate labels\npositive_labels = [1 for _ in pos]\nnegative_labels = [0 for _ in neg]\nlabels = numpy.concatenate([positive_labels, negative_labels], 0)\n\n# prepare tokenizer\nt = Tokenizer()\nt.fit_on_texts(CombinedDocs)\nvocab_size = len(t.word_index) + 1\n# integer encode the documents\nencoded_docs = t.texts_to_sequences(docs)\n#print(encoded_docs)\n\n# pad documents to a max length of 140 words\nmax_length = 140\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n</code></pre>\n\n<p>Here I used Google public word2vec</p>\n\n<pre><code># load the whole embedding into memory\nembeddings_index = dict()\nf = open('Google28.bin',encoding=\"latin-1\")\nfor line in f:\nvalues = line.split()\nword = values[0]\ncoefs = asarray(values[1:], dtype='str')\nembeddings_index[word] = coefs\nf.close()\n\n# create a weight matrix for words in training docs\nembedding_matrix = zeros((vocab_size, 100))\n\nfor word, i in t.word_index.items():\nembedding_vector = embeddings_index.get(word)\nif embedding_vector is not None:\n    embedding_matrix[i] = embedding_vector\n\n\n#Convert to numpy\nNewTraining=numpy.array(padded_docs)\nNewLabels=numpy.array(labels)\nencoded_docs2 = t.texts_to_sequences(total2)\n\n# pad documents to a max length of 140 words\n\npadded_docs2 = pad_sequences(encoded_docs2, maxlen=max_length, padding='post')\n\n\n# Generate labels\npositive_labels2 = [1 for _ in posT]\nnegative_labels2 = [0 for _ in negT]\nyTest = numpy.concatenate([positive_labels2, negative_labels2], 0)\nNewTesting=numpy.array(padded_docs2)\nNewLabelsTsting=numpy.array(yTest)\n</code></pre>\n"
  },
  {
    "tags": [
      "python",
      "dataset",
      "csv"
    ],
    "owner": {
      "account_id": 5434043,
      "reputation": 3618,
      "user_id": 54395,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/qcAy4.png?s=256",
      "display_name": "Bruno Lubascher",
      "link": "https://datascience.stackexchange.com/users/54395/bruno-lubascher"
    },
    "is_answered": true,
    "view_count": 113889,
    "accepted_answer_id": 81361,
    "answer_count": 2,
    "score": 17,
    "last_activity_date": 1695089712,
    "creation_date": 1532269878,
    "question_id": 35868,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/35868/how-to-store-strings-in-csv-with-new-line-characters",
    "title": "How to store strings in CSV with new line characters?",
    "body": "<p>My question is: what are ways I can store strings in a CSV that contain newline characters (i.e. <code>\\n</code>), where each data point is in one line?</p>\n\n<h2>Sample data</h2>\n\n<p>This is a sample of the data I have:</p>\n\n<pre><code>data = [\n    ['some text in one line', 1],\n    ['text with\\nnew line character', 0],\n    ['another new\\nline character', 1]\n]\n</code></pre>\n\n<h2>Target CSV</h2>\n\n<p>I want a CSV file, where the first line is <code>\"text,category\"</code> and every subsequent line is an entry from <code>data</code>.</p>\n\n<h2>What I tried</h2>\n\n<p>Using the <code>csv</code> package from Python.</p>\n\n<pre><code>import csv\nfield_names = ['text', 'category']\n\n# Writing\nwith open('file.csv', 'w+', encoding='utf-8') as file:\n    csvwriter = csv.DictWriter(file, field_names)\n    csvwriter.writeheader()\n    for d in data:\n        csvwriter.writerow({'text': d[0], 'category':d[1]})\n\n# Reading\nwith open('file.csv', 'r', encoding='utf-8') as file:\n    csvreader = csv.DictReader(file, field_names)\n    data = []\n    for line in csvreader:\n        data.append([line['text'], line['category']])\n</code></pre>\n\n<p>I can read and write, but the output <code>file.csv</code> is the following:</p>\n\n<blockquote>\n  <p>text,category</p>\n  \n  <p>some text in one line,1</p>\n  \n  <p>\"text with</p>\n  \n  <p>new line character\",0</p>\n  \n  <p>\"another new</p>\n  \n  <p>line character\",1</p>\n</blockquote>\n\n<p>So not one line per data point.</p>\n"
  },
  {
    "tags": [
      "python",
      "pandas",
      "dataframe"
    ],
    "owner": {
      "account_id": 13763679,
      "reputation": 837,
      "user_id": 53575,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b8623828cc8f6de92695ec97993d539b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "a_a_a",
      "link": "https://datascience.stackexchange.com/users/53575/a-a-a"
    },
    "is_answered": true,
    "view_count": 83699,
    "closed_date": 1583440449,
    "accepted_answer_id": 37229,
    "answer_count": 2,
    "score": 17,
    "last_activity_date": 1662948678,
    "creation_date": 1534846937,
    "last_edit_date": 1662948678,
    "question_id": 37227,
    "link": "https://datascience.stackexchange.com/questions/37227/how-to-remove-rows-from-a-dataframe-that-are-identical-to-another-dataframe",
    "closed_reason": "Not suitable for this site",
    "title": "How to remove rows from a dataframe that are identical to another dataframe?",
    "body": "<p>I have two data frames <code>df1</code> and <code>df2</code>.\nFor my analysis, I need to remove rows from <code>df1</code> that have identical column values (<code>Email</code>) in <code>df2</code>?</p>\n<pre><code>&gt;&gt;df1\n   First  Last  Email\n0 Adam   Smith  email@email.com\n1 John   Brown  email2@email.com\n2 Joe    Max    email3@email.com\n3 Will   Bill   email4@email.com\n\n\n&gt;&gt;df2\n  First  Last   Email\n0 Adam   Smith  email@email.com\n1 John   Brown  email2@email.com\n</code></pre>\n"
  },
  {
    "tags": [
      "python",
      "nlp",
      "language-model"
    ],
    "owner": {
      "account_id": 12570973,
      "reputation": 413,
      "user_id": 59385,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0dc5c3f09b1b1cb7418fcc214e7b849b?s=256&d=identicon&r=PG",
      "display_name": "Fred",
      "link": "https://datascience.stackexchange.com/users/59385/fred"
    },
    "is_answered": true,
    "view_count": 8213,
    "accepted_answer_id": 121044,
    "answer_count": 6,
    "score": 17,
    "last_activity_date": 1681967082,
    "creation_date": 1537450462,
    "question_id": 38540,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/38540/are-there-any-good-out-of-the-box-language-models-for-python",
    "title": "Are there any good out-of-the-box language models for python?",
    "body": "<p>I'm prototyping an application and I need a language model to compute perplexity on some generated sentences.</p>\n\n<p>Is there any trained language model in python I can readily use? Something simple like</p>\n\n<pre><code>model = LanguageModel('en')\np1 = model.perplexity('This is a well constructed sentence')\np2 = model.perplexity('Bunny lamp robert junior pancake')\nassert p1 &lt; p2\n</code></pre>\n\n<p>I've looked at some frameworks but couldn't find what I want. I know I can use something like:</p>\n\n<pre><code>from nltk.model.ngram import NgramModel\nlm = NgramModel(3, brown.words(categories='news'))\n</code></pre>\n\n<p>This uses a good turing probability distribution on Brown Corpus, but I was looking for some well-crafted model on some big dataset, like the 1b words dataset. Something that I can actually trust the results for a general domain (not only news)</p>\n"
  },
  {
    "tags": [
      "scikit-learn",
      "graphs"
    ],
    "owner": {
      "account_id": 15319063,
      "reputation": 171,
      "user_id": 67561,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-3411ROAwTsc/AAAAAAAAAAI/AAAAAAAAAAA/ACevoQMU0brGH07lah0Z-sPfo1fNuaFODA/mo/s256-rj/photo.jpg",
      "display_name": "Thibaud Martinez",
      "link": "https://datascience.stackexchange.com/users/67561/thibaud-martinez"
    },
    "is_answered": true,
    "view_count": 3764,
    "answer_count": 1,
    "score": 17,
    "last_activity_date": 1563227630,
    "creation_date": 1549991705,
    "last_edit_date": 1552788526,
    "question_id": 45459,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/45459/how-to-use-scikit-learn-label-propagation-on-graph-structured-data",
    "title": "How to use Scikit-Learn Label Propagation on graph structured data?",
    "body": "<p>As part of my research, I am interested in performing label propagation on a graph. I am especially interested in those two methods:</p>\n\n<ul>\n<li>Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon University, 2002 <a href=\"http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf\" rel=\"noreferrer\">http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf</a></li>\n<li>Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, Bernhard Schoelkopf. Learning with local and global consistency (2004) <a href=\"http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219\" rel=\"noreferrer\">http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219</a></li>\n</ul>\n\n<p>I saw that scikit-learn offer a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation\" rel=\"noreferrer\">model</a> to do that. However, this model is supposed to be applied on vector structured data (<em>i.e.</em> data points).</p>\n\n<p>The model builds an affinity matrix from the data points using a kernel, and then run the algorithm on the constructed matrix. I would like to be able to directly input the adjacency matrix of my graph in place of the similarity matrix.</p>\n\n<p>Any idea on how to achieve that? Or do you know any Python library that will allow to run label propagation directly on graph structured data for the two aforementioned methods?</p>\n\n<p>Thanks in advance for your help!</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "model-selection",
      "hyperparameter-tuning"
    ],
    "owner": {
      "account_id": 4034392,
      "reputation": 271,
      "user_id": 36943,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/686df1f4367033782c57aa8131fc3ab6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user3319400",
      "link": "https://datascience.stackexchange.com/users/36943/user3319400"
    },
    "is_answered": true,
    "view_count": 31491,
    "answer_count": 3,
    "score": 17,
    "last_activity_date": 1559856072,
    "creation_date": 1559383134,
    "last_edit_date": 1559856072,
    "question_id": 53033,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/53033/why-my-network-needs-so-many-epochs-to-learn",
    "title": "Why my network needs so many epochs to learn?",
    "body": "<p>I'm working on a relation classification task for natural language processing and I have some questions about the learning process. I implemented a convolutional neural network using PyTorch, and I'm trying to select the best hyper-parameters.</p>\n\n<p>The common behaviour I noticed is that even after 1000 epochs, my validation loss is still slowly decreasing, and my metric (macro F1 score) is slowly increasing. Both the metrics are oscillating, perhaps due to the imbalance of the 4 classes to classify (5%, 3%, 30%, 62%). I'm using Adam, so the learning rate is not set by myself (it is adaptive). I'm using dropout 0.5 after max pooling, and mini batches of size 50.</p>\n\n<p><a href=\"https://i.sstatic.net/X3Vpc.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/X3Vpc.png\" alt=\"The train and validation loss during the 1000 epochs\"></a>\n<a href=\"https://i.sstatic.net/H7SQW.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/H7SQW.png\" alt=\"The macro f1 score during the 1000 epochs\"></a></p>\n\n<p>On books and online tutorials I have seen plots that are so clear in when to stop training. For instance, when the train loss becomes less that the validation loss, or when a plateau is reached in the validation loss. This seems not to be the case, so I'm asking for an advice to more navigated neural network users:</p>\n\n<ul>\n<li>Why the network is still learning after so many epochs (and so slowly)? It is a reasonable behaviour? Do I need to run the model for 2000, or even 3000 epochs to get the best macro f1 score? It risks to overfit, doesn't it?</li>\n<li>My network is fed by word embeddings and position embeddings only, so I'm wondering if this behaviour can be motivated by (i) a network architecture that is too complex for the task, (ii) a network architecture that is too simple to model the complexity for the task, (iii) the inputs are not so informative to discriminate the classes, so the network learns with difficulty (and thus slowly). My network architecture is not deep: it has an embedding layer + conv layer + max-pool layer + softmax.</li>\n<li>A side question: suppose 1000 epochs are enough. When I need to compare performance of many models (or to compare different hyper-parameter combinations), which score I need to pick and compare? The \"last best\" f1 score (say at epoch 980) or the last reported score (i.e., the one at epoch 1000)?</li>\n</ul>\n\n<p>Do you have any suggestions? Let me know if something is unclear!</p>\n\n<p>=====</p>\n\n<p><strong>Update</strong></p>\n\n<p>Following the advice by <strong>Djib2011</strong> I trained the network with different learning rates, in particular <code>lr=0.001</code> (the default), <code>lr=0.01</code>, <code>lr=0.1</code>. I trained the network using even more epochs (i.e., 3000), to identify the moment in which I could stop the training. What I noticed is that increasing the learning rate doesn't help, and the best results are given by the default <code>lr=0.001</code>. However it is still unclear when to stop.</p>\n\n<p>Do you have other advices to tackle this problem? Besides learning rates, there could be other issues behind the optimizer (e.g., network with a small capacity)? Thank you in advance!</p>\n\n<p><a href=\"https://i.sstatic.net/kjual.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/kjual.png\" alt=\"Report using different learning rates\"></a></p>\n\n<p>=====</p>\n\n<p><strong>Update #2</strong></p>\n\n<p>After experimenting with different optimization algorithms (i.e., RMSProp, AdaDelta, Adamax, SGD) with different learning rates and 3000 epochs, I noticed that the behaviour across these different settings was still the same (no convergence, and many epochs with little decreasing training and validation losses).</p>\n\n<p>I thus modified the pytorch implementation by moving the 3 lines: <code>optimizer.zero_grad()</code>, <code>loss.backward()</code>, <code>optimizer.step()</code> inside the (training) batches loop instead of running them after the loop. As a result, this is the edited code that performs training and testing at each epoch:</p>\n\n<pre><code>#Iterate over epochs\nfor epoch in range(1, n_epochs+1):\n    train_loss = 0\n    model.train()\n\n    train_predictions = []\n    train_true_labels = []\n\n    # Iterate over training batches\n    for i, (inputs, labels) in enumerate(train_loader):\n        inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n\n        optimizer.zero_grad() # set gradients to zero\n\n        preds = model(inputs)\n        preds.to(device)\n\n        # Compute the loss and accumulate it to print it afterwards\n        loss = loss_criterion(preds, labels)\n        train_loss += loss.detach()\n\n        pred_values, pred_encoded_labels = torch.max(preds.data, 1)\n        pred_encoded_labels = pred_encoded_labels.cpu().numpy()\n\n        train_predictions.extend(pred_encoded_labels)\n        train_true_labels.extend(labels)\n\n        loss.backward()       # backpropagate and compute gradients\n        optimizer.step()      # perform a parameter update\n\n\n    # Evaluate on development test\n    predictions = []\n    true_labels = []\n    dev_loss = 0\n\n    model.eval()\n    for i, (inputs, labels) in enumerate(dev_loader):\n        inputs, labels = Variable(inputs).to(device), Variable(labels).to(device)\n\n        preds = model(inputs)\n        preds.to(device)\n\n        loss = loss_criterion(preds, labels)\n        dev_loss += loss.detach()\n\n        pred_values, pred_encoded_labels = torch.max(preds.data, 1)\n        pred_encoded_labels = pred_encoded_labels.cpu().numpy()\n\n        predictions.extend(pred_encoded_labels)\n        true_labels.extend(labels)\n</code></pre>\n\n<p>I thus trained again the network using my default configuration (Adam, lr=0.001) and surprisingly I obtained a convergence at epoch 22 (see images below). I think the issue was there, do you agree? Do you have any additional advice? Thanks again!</p>\n\n<p><a href=\"https://i.sstatic.net/RYUhc.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/RYUhc.png\" alt=\"enter image description here\"></a>\n<a href=\"https://i.sstatic.net/i3tCX.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/i3tCX.png\" alt=\"enter image description here\"></a></p>\n"
  },
  {
    "tags": [
      "neural-network",
      "autoencoder"
    ],
    "owner": {
      "account_id": 15154612,
      "reputation": 644,
      "user_id": 66117,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/sFhyn.jpg?s=256",
      "display_name": "Kahina",
      "link": "https://datascience.stackexchange.com/users/66117/kahina"
    },
    "is_answered": true,
    "view_count": 14640,
    "accepted_answer_id": 53981,
    "answer_count": 2,
    "score": 17,
    "last_activity_date": 1630148000,
    "creation_date": 1560817436,
    "last_edit_date": 1560819574,
    "question_id": 53979,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/53979/what-is-the-difference-between-an-autoencoder-and-an-encoder-decoder",
    "title": "What is the difference between an autoencoder and an encoder-decoder?",
    "body": "<p>I want to know if there is a difference between an autoencoder and an encoder-decoder.</p>\n"
  },
  {
    "tags": [
      "gan",
      "generative-models"
    ],
    "owner": {
      "account_id": 10413543,
      "reputation": 335,
      "user_id": 74281,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/805286292952051/picture?type=large",
      "display_name": "Muhammad Hanif Sarwari",
      "link": "https://datascience.stackexchange.com/users/74281/muhammad-hanif-sarwari"
    },
    "is_answered": true,
    "view_count": 14282,
    "accepted_answer_id": 55094,
    "answer_count": 1,
    "score": 17,
    "last_activity_date": 1606046977,
    "creation_date": 1562267516,
    "question_id": 55090,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/55090/what-is-the-main-difference-between-gan-and-autoencoder",
    "title": "what is the main difference between GAN and autoencoder?",
    "body": "<p>what is the main difference between GAN and other older generative models? what were the characteristics of GAN that made it more successful than other generative models?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "deep-learning",
      "nlp",
      "attention-mechanism"
    ],
    "owner": {
      "account_id": 3318358,
      "reputation": 273,
      "user_id": 88799,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/9c648753a3b8ed84ea61a9b05591a3df?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user2790103",
      "link": "https://datascience.stackexchange.com/users/88799/user2790103"
    },
    "is_answered": true,
    "view_count": 5399,
    "accepted_answer_id": 67188,
    "answer_count": 3,
    "score": 17,
    "last_activity_date": 1639653667,
    "creation_date": 1579759527,
    "question_id": 66913,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/66913/how-does-attention-mechanism-learn",
    "title": "How does attention mechanism learn?",
    "body": "<p>I know how to build an attention in neural networks. But I dont understand how attention layers learn the weights that pay attention to some specific embedding.</p>\n\n<p>I have this question because Im tackling a NLP task using attention layer. I believe it should be very easy to learn (the most important part is to learn alignments). However, my neural networks only achieve 50% test set accuracy. And the attention matrix is weird.\nI dont know how to improve my networks.</p>\n\n<p>To give a example:<br>\nEnglish: Who are you?<br>\nChinese:   </p>\n\n<p>The alignments are<br>\nWho to <br>\nare to <br>\nyou to   </p>\n\n<p>How does attention learn that?</p>\n\n<p>Thank you!</p>\n"
  },
  {
    "tags": [
      "python",
      "neural-network",
      "keras",
      "loss-function"
    ],
    "owner": {
      "account_id": 11409527,
      "reputation": 433,
      "user_id": 90006,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/292868d26b3326351d58960748e3fe7c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "pairon",
      "link": "https://datascience.stackexchange.com/users/90006/pairon"
    },
    "is_answered": true,
    "view_count": 67183,
    "answer_count": 3,
    "score": 17,
    "last_activity_date": 1661400663,
    "creation_date": 1582107491,
    "question_id": 68331,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/68331/keras-sequential-model-returns-loss-nan",
    "title": "Keras Sequential model returns loss &#39;nan&#39;",
    "body": "<p>I'm implementing a neural network with Keras, but the <code>Sequential</code> model returns <code>nan</code> as loss value.\nI have sigmoid activation function in the output layer to squeeze output between 0 and 1, but maybe doesn't work properly.</p>\n\n<p>This is the code:</p>\n\n<pre><code>def data_generator(batch_count, training_dataset, training_dataset_labels):\n  while True:\n    start_range = 0\n    for batch in batch_count:\n      end_range = (start_range + batch[1])\n      batch_dataset = training_dataset[start_range:end_range]\n      batch_labels = training_dataset_labels[start_range:end_range]\n      start_range = end_range\n      yield batch_dataset, batch_dataset\n\nmlp = keras.models.Sequential()\n\n# add input layer\nmlp.add(\n    keras.layers.Input(\n        shape = (training_dataset.shape[1], )\n    )\n)\n# add hidden layer\nmlp.add(\n    keras.layers.Dense(\n        units=training_dataset.shape[1] + 10,\n        input_shape = (training_dataset.shape[1] + 10,),\n        kernel_initializer='random_uniform',\n        bias_initializer='zeros',\n        activation='relu')\n    )\n# add output layer\nmlp.add(\n    keras.layers.Dense(\n        units=1,\n        input_shape = (1, ),\n        kernel_initializer='glorot_uniform',\n        bias_initializer='zeros',\n        activation='sigmoid')\n    )\n\nprint('Compiling model...\\n')\n\nmlp.compile(\n    optimizer='adam',\n    loss=listnet_loss\n)\n\nmlp.summary() # print model settings\n\n\n# Training\nwith tf.device('/GPU:0'):\n  print('Start training')\n  #mlp.fit(training_dataset, training_dataset_labels, epochs=50, verbose=2, batch_size=3, workers=10)\n  mlp.fit_generator(data_generator(groups_id_count, training_dataset, training_dataset_labels),\n                    steps_per_epoch=len(training_dataset), epochs=50, verbose=2, workers=10, use_multiprocessing=True)\n</code></pre>\n\n<p>How can I do?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "reinforcement-learning",
      "active-learning",
      "difference"
    ],
    "owner": {
      "account_id": 2814496,
      "reputation": 285,
      "user_id": 107401,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/51ql6gjH.jpg?s=256",
      "display_name": "Moradnejad",
      "link": "https://datascience.stackexchange.com/users/107401/moradnejad"
    },
    "is_answered": true,
    "view_count": 14172,
    "accepted_answer_id": 85362,
    "answer_count": 2,
    "score": 17,
    "last_activity_date": 1605282270,
    "creation_date": 1605272070,
    "question_id": 85358,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/85358/what-is-the-difference-between-active-learning-and-reinforcement-learning",
    "title": "What is the difference between active learning and reinforcement learning?",
    "body": "<p>From Wikipedia:</p>\n<blockquote>\n<p>Active learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs.</p>\n</blockquote>\n<blockquote>\n<p>Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward.</p>\n</blockquote>\n<p>How to distinguish them? What are the exact differences?</p>\n"
  },
  {
    "tags": [
      "vector-database"
    ],
    "owner": {
      "account_id": 56742,
      "reputation": 75,
      "user_id": 46630,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/bef70693918041a94dfefc01cc48d956?s=256&d=identicon&r=PG",
      "display_name": "Lance Pollard",
      "link": "https://datascience.stackexchange.com/users/46630/lance-pollard"
    },
    "is_answered": true,
    "view_count": 8118,
    "accepted_answer_id": 123189,
    "answer_count": 3,
    "score": 17,
    "last_activity_date": 1706035032,
    "creation_date": 1691726506,
    "question_id": 123181,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/123181/how-do-vector-databases-work-for-the-lay-coder",
    "title": "How do vector databases work (for the lay-coder)?",
    "body": "<blockquote>\n<p>A vector database is a type of database that stores and manages unstructured data, such as text, images, or audio, in vector embeddings (high-dimensional vectors) to make it easy to find and retrieve similar objects quickly.</p>\n</blockquote>\n<p>How do you make that more tangible, to a software engineer?</p>\n<p>Some parts to answering this question:</p>\n<ul>\n<li>What is meant by &quot;high dimensional&quot; vectors? Does that just mean the array of numbers is a big array? Why is that important / highlighted?</li>\n<li>How is text, for example, converted to a vector of numbers (at a high level)?</li>\n<li>Why do they call them vector &quot;embeddings&quot;?</li>\n<li>I don't need to learn the theory of vectors, how can I understand vector databases practical function and use (as someone who wants to build an AI assistant), without learning about the theory of vectors exactly?</li>\n</ul>\n<p>I am an experienced software developer with experience with relational databases like PostgreSQL, and other databases like MongoDB (a doc database) or Neo4j (a graph database). So how does a vector database work exactly, at a high level? I just would like to have a practical sense of how it works, so I know what I'm saving to when saving to pinecone, and what I'm querying and maybe how a query roughly works.</p>\n<p>Relational databases are easy to understand. You just look at a spreadsheet, and the columns have names. Graphs are also easy to understand, you have your named models/types, and they have links between them. But vector databases? Is it literally a hashmap of key = something and value = array of numbers? Or what is it, how to get a practical sense like these other database types?</p>\n<p>References I've used to get a slightly better understanding so far:</p>\n<ul>\n<li><a href=\"https://towardsdatascience.com/explaining-vector-databases-in-3-levels-of-difficulty-fc392e48ab78\" rel=\"noreferrer\">https://towardsdatascience.com/explaining-vector-databases-in-3-levels-of-difficulty-fc392e48ab78</a> (unfortunately, pay-walled)</li>\n<li><a href=\"https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture?ref=blog.apify.com\" rel=\"noreferrer\">https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture?ref=blog.apify.com</a></li>\n<li><a href=\"https://www.pinecone.io/learn/vector-embeddings-for-developers/\" rel=\"noreferrer\">https://www.pinecone.io/learn/vector-embeddings-for-developers/</a></li>\n<li><a href=\"https://blog.apify.com/what-is-a-vector-database/\" rel=\"noreferrer\">https://blog.apify.com/what-is-a-vector-database/</a></li>\n<li><a href=\"https://blog.apify.com/what-is-pinecone-why-use-it-with-llms/\" rel=\"noreferrer\">https://blog.apify.com/what-is-pinecone-why-use-it-with-llms/</a></li>\n</ul>\n"
  },
  {
    "tags": [
      "definitions",
      "parallel",
      "distributed"
    ],
    "owner": {
      "account_id": 1822136,
      "reputation": 4117,
      "user_id": 84,
      "user_type": "registered",
      "accept_rate": 82,
      "profile_image": "https://www.gravatar.com/avatar/5394d14f632e89b2dfc937e3660f0079?s=256&d=identicon&r=PG",
      "display_name": "Rubens",
      "link": "https://datascience.stackexchange.com/users/84/rubens"
    },
    "is_answered": true,
    "view_count": 1626,
    "accepted_answer_id": 82,
    "answer_count": 3,
    "score": 16,
    "last_activity_date": 1681209684,
    "creation_date": 1400129994,
    "last_edit_date": 1400146311,
    "question_id": 81,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/81/parallel-and-distributed-computing",
    "title": "Parallel and distributed computing",
    "body": "<p>What is(are) the difference(s) between parallel and distributed computing? When it comes to scalability and efficiency, it is very common to see solutions dealing with computations in clusters of machines, and sometimes it is referred to as a parallel processing, or as distributed processing.</p>\n\n<p>In a certain way, the computation seems to be always parallel, since there are things running concurrently. But is the distributed computation simply related to the use of more than one machine, or are there any further specificities that distinguishes these two kinds of processing? Wouldn't it be redundant to say, for example, that a computation is <em>parallel AND distributed</em>?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "pandas",
      "scikit-learn"
    ],
    "owner": {
      "account_id": 4494374,
      "reputation": 871,
      "user_id": 250,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/cDteV.jpg?s=256",
      "display_name": "Therriault",
      "link": "https://datascience.stackexchange.com/users/250/therriault"
    },
    "is_answered": true,
    "view_count": 1087,
    "answer_count": 2,
    "score": 16,
    "last_activity_date": 1402486333,
    "creation_date": 1401224868,
    "question_id": 215,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/215/where-in-the-workflow-should-we-deal-with-missing-data",
    "title": "Where in the workflow should we deal with missing data?",
    "body": "<p>I'm building a workflow for creating machine learning models (in my case, using Python's <code>pandas</code> and <code>sklearn</code> packages) from data pulled from a very large database (here, Vertica by way of SQL and <code>pyodbc</code>), and a critical step in that process involves imputing missing values of the predictors. This is straightforward within a single analytics or stats platform---be it Python, R, Stata, etc.---but I'm curious where best to locate this step in a multi-platform workflow.</p>\n\n<p>It's simple enough to do this in Python, either with the <a href=\"http://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values\"><code>sklearn.preprocessing.Imputer</code></a> class, using the <a href=\"http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.DataFrame.fillna.html\"><code>pandas.DataFrame.fillna</code></a> method, or by hand (depending upon the complexity of the imputation method used). But since I'm going to be using this for dozens or hundreds of columns across hundreds of millions of records, I wonder if there's a more efficient way to do this directly through SQL ahead of time. Aside from the potential efficiencies of doing this in a distributed platform like Vertica, this would have the added benefit of allowing us to create an automated pipeline for building \"complete\" versions of tables, so we don't need to fill in a new set of missing values from scratch every time we want to run a model.</p>\n\n<p>I haven't been able to find much guidance about this, but I imagine that we could:</p>\n\n<ol>\n<li>create a table of substitute values (e.g., mean/median/mode, either overall or by group) for each incomplete column</li>\n<li>join the substitute value table with the original table to assign a substitute value for each row and incomplete column</li>\n<li>use a series of case statements to take the original value if available and the substitute value otherwise</li>\n</ol>\n\n<p>Is this a reasonable thing to do in Vertica/SQL, or is there a good reason not to bother and just handle it in Python instead? And if the latter, is there a strong case for doing this in pandas rather than sklearn or vice-versa? Thanks!</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification"
    ],
    "owner": {
      "account_id": 4621839,
      "reputation": 161,
      "user_id": 900,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2db23bf4bcc981f923d423e4d4653a49?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user900",
      "link": "https://datascience.stackexchange.com/users/900/user900"
    },
    "is_answered": true,
    "view_count": 14527,
    "answer_count": 4,
    "score": 16,
    "last_activity_date": 1681327131,
    "creation_date": 1402964184,
    "question_id": 418,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/418/best-way-to-classify-datasets-with-mixed-types-of-attributes",
    "title": "Best way to classify datasets with mixed types of attributes",
    "body": "<p>I would like to know what is the best way to classify a data set composed of mixed types of attributes, for example, textual and numerical. I know I can convert textual to boolean, but the vocabulary is diverse and data become too sparse. I also tried to classify the types of attributes separately and combine the results through meta-learning techniques, but it did not work well.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "feature-selection",
      "class-imbalance"
    ],
    "owner": {
      "account_id": 389606,
      "reputation": 428,
      "user_id": 403,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/159eab465191c050821e4337b692a27a?s=256&d=identicon&r=PG",
      "display_name": "gallamine",
      "link": "https://datascience.stackexchange.com/users/403/gallamine"
    },
    "is_answered": true,
    "view_count": 2498,
    "accepted_answer_id": 620,
    "answer_count": 4,
    "score": 16,
    "last_activity_date": 1469218742,
    "creation_date": 1403106499,
    "last_edit_date": 1448209506,
    "question_id": 454,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/454/what-are-the-implications-for-training-a-tree-ensemble-with-highly-biased-datase",
    "title": "What are the implications for training a Tree Ensemble with highly biased datasets?",
    "body": "<p>I have a highly biased binary dataset - I have 1000x more examples of the negative class than the positive class. I would like to train a Tree Ensemble (like Extra Random Trees or a Random Forest) on this data but it's difficult to create training datasets that contain enough examples of the positive class.</p>\n\n<p>What would be the implications of doing a stratified sampling approach to normalize the number of positive and negative examples? In other words, is it a bad idea to, for instance, artificially inflate (by resampling) the number of positive class examples in the training set?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "classification",
      "logistic-regression"
    ],
    "owner": {
      "account_id": 2901835,
      "reputation": 4125,
      "user_id": 793,
      "user_type": "registered",
      "accept_rate": 60,
      "profile_image": "https://i.sstatic.net/N5kqP.jpg?s=256",
      "display_name": "tejaskhot",
      "link": "https://datascience.stackexchange.com/users/793/tejaskhot"
    },
    "is_answered": true,
    "view_count": 6360,
    "accepted_answer_id": 533,
    "answer_count": 2,
    "score": 16,
    "last_activity_date": 1561716112,
    "creation_date": 1403506995,
    "last_edit_date": 1557202924,
    "question_id": 531,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/531/binary-classification-model-for-unbalanced-data",
    "title": "Binary classification model for unbalanced data",
    "body": "<p>I have a dataset with the following specifications:</p>\n\n<ul>\n<li>Training dataset with 193,176 samples with 2,821 positives</li>\n<li>Test Dataset with 82,887 samples with 673 positives</li>\n<li>There are 10 features.</li>\n</ul>\n\n<p>I want to perform a binary classification (0 or 1). The issue I am facing is that the data is very unbalanced. After normalization and scaling the data along with some feature engineering and using a couple of different algorithms, these are the best results I could achieve:</p>\n\n<pre><code>mean square error : 0.00804710026904\nConfusion matrix : [[82214   667]\n                   [    0     6]]\n</code></pre>\n\n<p>i.e only 6 correct positive hits. This is using logistic regression. Here are the various things I tried with this:</p>\n\n<ul>\n<li>Different algorithms like RandomForest, DecisionTree, SVM</li>\n<li>Changing parameters value to call the function</li>\n<li>Some intuition based feature engineering to include compounded features</li>\n</ul>\n\n<p>Now, my questions are:</p>\n\n<ol>\n<li>What can I do to improve the number of positive hits ? </li>\n<li>How can one determine if there is an overfit in such a case ? ( I have tried plotting etc. )</li>\n<li>At what point could one conclude if maybe this is the best possible fit I could have? ( which seems sad considering only 6 hits out of 673 )</li>\n<li>Is there a way I could make the positive sample instances weigh more so the pattern recognition improves leading to more hits ?</li>\n<li>Which graphical plots could help detect outliers or some intuition about which pattern would fit the best?</li>\n</ol>\n\n<p>I am using the scikit-learn library with Python and all implementations are library functions.</p>\n\n<p><strong>edit:</strong></p>\n\n<p>Here are the results with a few other algorithms:</p>\n\n<p>Random Forest Classifier(n_estimators=100)</p>\n\n<pre><code>[[82211   667]\n[    3     6]]\n</code></pre>\n\n<p>Decision Trees:</p>\n\n<pre><code>[[78611   635]\n[ 3603    38]]\n</code></pre>\n"
  },
  {
    "tags": [
      "neural-network",
      "time-series",
      "regression"
    ],
    "owner": {
      "account_id": 1119445,
      "reputation": 430,
      "user_id": 3159,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/24d6e99d07504fcd67cdc61cb3121618?s=256&d=identicon&r=PG",
      "display_name": "doublebyte",
      "link": "https://datascience.stackexchange.com/users/3159/doublebyte"
    },
    "is_answered": true,
    "view_count": 7538,
    "answer_count": 3,
    "score": 16,
    "last_activity_date": 1594431327,
    "creation_date": 1415033507,
    "question_id": 2395,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/2395/modelling-unevenly-spaced-time-series",
    "title": "Modelling Unevenly Spaced Time Series",
    "body": "<p>I have a continuous variable, sampled over a period of a year at irregular intervals. Some days have more than one observation per hour, while other periods have nothing for days. This makes it particularly difficult to detect patterns in the time series, because some months (for instance October) are highly sampled, while others are not.</p>\n\n<p><img src=\"https://i.sstatic.net/7MEXt.png\" alt=\"enter image description here\"></p>\n\n<p>My question is what would be the best approach to model this time series?</p>\n\n<ul>\n<li>I believe most time series analysis techniques (like ARMA) need a fixed frequency. I could aggregate the data, in order to have a constant sample or choose a sub-set of the data that is very detailed. With both options I would be missing some information from the original dataset, that could unveil distinct patterns.</li>\n<li>Instead of decomposing the series in cycles, I could feed the model\nwith the entire dataset and expect it to pick up the patterns. For\ninstance, I transformed the hour, weekday and month in categorical\nvariables and tried a multiple regression with good results (R2=0.71)</li>\n</ul>\n\n<p>I have the idea that machine learning techniques such as ANN can also pick these patterns from uneven time series, but I was wondering if anybody has tried that, and could provide me some advice about the best way of representing time patterns in a Neural network.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-mining",
      "algorithms",
      "recommender-system"
    ],
    "owner": {
      "account_id": 5088209,
      "reputation": 1940,
      "user_id": 5091,
      "user_type": "registered",
      "accept_rate": 38,
      "profile_image": "https://i.sstatic.net/zzgFQ.jpg?s=256",
      "display_name": "Sreejithc321",
      "link": "https://datascience.stackexchange.com/users/5091/sreejithc321"
    },
    "is_answered": true,
    "view_count": 31331,
    "accepted_answer_id": 2599,
    "answer_count": 2,
    "score": 16,
    "last_activity_date": 1597582923,
    "creation_date": 1417670283,
    "last_edit_date": 1448367621,
    "question_id": 2598,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/2598/item-based-and-user-based-recommendation-difference-in-mahout",
    "title": "Item based and user based recommendation difference in Mahout",
    "body": "<p>I would like to know how exactly mahout user based and item based recommendation differ from each other.</p>\n\n<p>It defines that</p>\n\n<p><a href=\"https://mahout.apache.org/users/recommender/userbased-5-minutes.html\">User-based</a>: Recommend items by finding similar users. This is often harder to scale because of the dynamic nature of users.</p>\n\n<p><a href=\"https://mahout.apache.org/users/recommender/intro-itembased-hadoop.html\">Item-based</a>: Calculate similarity between items and make recommendations. Items usually don't change much, so this often can be computed off line.</p>\n\n<p>But though there are two kind of recommendation available, what I understand is that both these will take some data model ( say 1,2 or 1,2,.5 as item1,item2,value or user1,user2,value where value is not mandatory) and will perform all calculation as the similarity measure and recommender build-in function we chose and we can run both user/item based recommendation on the same data ( is this a correct assumption ?? ).</p>\n\n<p>So I would like to know how exactly and in which all aspects these two type of algorithm differ. </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification"
    ],
    "owner": {
      "account_id": 5649150,
      "reputation": 183,
      "user_id": 7873,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f9546a0ca555f38e4d2d29f646b89e9a?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Saratha Priya",
      "link": "https://datascience.stackexchange.com/users/7873/saratha-priya"
    },
    "is_answered": true,
    "view_count": 18852,
    "accepted_answer_id": 4904,
    "answer_count": 1,
    "score": 16,
    "last_activity_date": 1615493957,
    "creation_date": 1421677617,
    "last_edit_date": 1615493957,
    "question_id": 4903,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/4903/what-is-the-difference-between-feature-generation-and-feature-extraction",
    "title": "What is the difference between feature generation and feature extraction?",
    "body": "<p>Can anybody tell me what the purpose of feature generation is? And why feature space enrichment is needed before classifying an image? Is it a necessary step?</p>\n<p>Is there any method to enrich feature space?</p>\n"
  },
  {
    "tags": [
      "r",
      "beginner"
    ],
    "owner": {
      "account_id": 5806799,
      "reputation": 163,
      "user_id": 8318,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/3dewu.gif?s=256",
      "display_name": "Jae",
      "link": "https://datascience.stackexchange.com/users/8318/jae"
    },
    "is_answered": true,
    "view_count": 31890,
    "accepted_answer_id": 5278,
    "answer_count": 1,
    "score": 16,
    "last_activity_date": 1425471310,
    "creation_date": 1425456345,
    "last_edit_date": 1425456957,
    "question_id": 5277,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/5277/do-you-have-to-normalize-data-when-building-decision-trees-using-r",
    "title": "Do you have to normalize data when building decision trees using R?",
    "body": "<p>So, our data set this week has 14 attributes and each column has very different values. One column has values below 1 while another column has values that go from three to four whole digits.</p>\n\n<p>We learned normalization last week and it seems like you're supposed to normalize data when they have very different values. For decision trees, is the case the same?</p>\n\n<p>I'm not sure about this but would normalization affect the resulting decision tree from the same data set? It doesn't seem like it should but...</p>\n"
  },
  {
    "tags": [
      "reference-request"
    ],
    "owner": {
      "account_id": 6444670,
      "reputation": 8476,
      "user_id": 11097,
      "user_type": "registered",
      "accept_rate": 75,
      "profile_image": "https://i.sstatic.net/z0jss.png?s=256",
      "display_name": "Dawny33",
      "link": "https://datascience.stackexchange.com/users/11097/dawny33"
    },
    "is_answered": true,
    "view_count": 3321,
    "protected_date": 1674547668,
    "accepted_answer_id": 9004,
    "answer_count": 7,
    "community_owned_date": 1460629369,
    "score": 16,
    "last_activity_date": 1674535931,
    "creation_date": 1448361378,
    "last_edit_date": 1492087460,
    "question_id": 9000,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9000/data-science-podcasts",
    "title": "Data Science Podcasts?",
    "body": "<p>What are some podcasts which are related to data science?</p>\n\n<p>This is a similar question to the <a href=\"https://stats.stackexchange.com/questions/927/statistical-podcasts\">reference request question on CrossValidated</a>.</p>\n\n<p>Details/rules:</p>\n\n<ul>\n<li>The podcasts (the theme and the episodes) should be related to data science. (For example: A podcast which is about some other domain, with an episode which speaks about data science in that domain, is not a good reference/answer.)</li>\n<li>Personal opinions/reviews (if any) would be very helpful too.</li>\n</ul>\n"
  },
  {
    "tags": [
      "beginner",
      "self-study"
    ],
    "owner": {
      "account_id": 990072,
      "reputation": 263,
      "user_id": 8960,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/r8vtI.jpg?s=256",
      "display_name": "xyhhx",
      "link": "https://datascience.stackexchange.com/users/8960/xyhhx"
    },
    "is_answered": true,
    "view_count": 4963,
    "closed_date": 1450517137,
    "accepted_answer_id": 9436,
    "answer_count": 3,
    "score": 16,
    "last_activity_date": 1450524893,
    "creation_date": 1450467092,
    "last_edit_date": 1450517144,
    "question_id": 9435,
    "link": "https://datascience.stackexchange.com/questions/9435/how-to-self-learn-data-science",
    "closed_reason": "Needs more focus",
    "title": "How to self-learn data science?",
    "body": "<p>I am a self-taught web developer and am interested in teaching myself data science, but I'm unsure of how to begin. In particular, I'm wondering:</p>\n\n<ol>\n<li>What fields are there within data science? (e.g., Artificial Intelligence, machine learning, data analysis, etc.)</li>\n<li>Are there online classes people can recommend?</li>\n<li>Are there projects available out there that I can practice on (e.g., open datasets).</li>\n<li>Are there certifications I can apply for or complete?</li>\n</ol>\n"
  },
  {
    "tags": [
      "pandas"
    ],
    "owner": {
      "account_id": 4182268,
      "reputation": 813,
      "user_id": 3024,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/a44990369cda3d3d95a565f28a4d984d?s=256&d=identicon&r=PG",
      "display_name": "Michael Hooreman",
      "link": "https://datascience.stackexchange.com/users/3024/michael-hooreman"
    },
    "is_answered": true,
    "view_count": 47220,
    "accepted_answer_id": 9463,
    "answer_count": 4,
    "score": 16,
    "last_activity_date": 1596207286,
    "creation_date": 1450696711,
    "last_edit_date": 1519514021,
    "question_id": 9460,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9460/pandas-how-can-i-create-multi-level-columns",
    "title": "Pandas: how can I create multi-level columns",
    "body": "<p>I have a pandas DataFrame which has the following columns:</p>\n\n<pre><code>n_0\nn_1\np_0\np_1\ne_0\ne_1\n</code></pre>\n\n<p>I want to transform it to have columns and sub-columns:</p>\n\n<pre><code>0\n    n\n    p\n    e\n1\n    n\n    p\n    e\n</code></pre>\n\n<p>I've searched in the documentation, and I'm completely lost on how to implement this. Does anyone have any suggestions?</p>\n"
  },
  {
    "tags": [
      "r",
      "logistic-regression",
      "xgboost"
    ],
    "owner": {
      "account_id": 2979855,
      "reputation": 327,
      "user_id": 15430,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ba44010c18f3bfc83fc6c6a1fab3382d?s=256&d=identicon&r=PG",
      "display_name": "user2530062",
      "link": "https://datascience.stackexchange.com/users/15430/user2530062"
    },
    "is_answered": true,
    "view_count": 9599,
    "accepted_answer_id": 14550,
    "answer_count": 1,
    "score": 16,
    "last_activity_date": 1476572082,
    "creation_date": 1452855641,
    "question_id": 9802,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9802/what-is-the-difference-in-xgboost-binarylogistic-and-reglogistic",
    "title": "What is the difference in xgboost binary:logistic and reg:logistic",
    "body": "<p>What is the difference in R in xgboost between binary:logistic and reg:logistic? Is it only in evaluation metric? </p>\n\n<p>If yes, how does RMSE on binary classification compare to error rate? Is the relationship between the metrics more or less monotonic, output from tuning on one metric should not differ significantly between those two approaches?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "r",
      "gpu"
    ],
    "owner": {
      "account_id": 3076402,
      "reputation": 1071,
      "user_id": 12206,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://i.sstatic.net/9RN4n.png?s=256",
      "display_name": "Simon",
      "link": "https://datascience.stackexchange.com/users/12206/simon"
    },
    "is_answered": true,
    "view_count": 11290,
    "answer_count": 5,
    "score": 16,
    "last_activity_date": 1615493733,
    "creation_date": 1453737475,
    "last_edit_date": 1495937984,
    "question_id": 9945,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9945/r-machine-learning-on-gpu",
    "title": "R: machine learning on GPU",
    "body": "<p>Are there any machine learning packages for R that can make use of the GPU to improve training speed (something like theano from the python world)?</p>\n\n<p>I see that there is a package called gputools which allows execution of code on the gpu, but I'm looking for a more complete library for machine learning.</p>\n"
  },
  {
    "tags": [
      "gensim"
    ],
    "owner": {
      "account_id": 7740748,
      "reputation": 163,
      "user_id": 16865,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/wHPzK.jpg?s=256",
      "display_name": "Seongho",
      "link": "https://datascience.stackexchange.com/users/16865/seongho"
    },
    "is_answered": true,
    "view_count": 10107,
    "accepted_answer_id": 21662,
    "answer_count": 3,
    "score": 16,
    "last_activity_date": 1626072682,
    "creation_date": 1457512640,
    "question_id": 10612,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10612/doc2vecgensim-how-can-i-infer-unseen-sentences-label",
    "title": "Doc2vec(gensim) - How can I infer unseen sentences label?",
    "body": "<p><a href=\"https://radimrehurek.com/gensim/models/doc2vec.html\" rel=\"noreferrer\">https://radimrehurek.com/gensim/models/doc2vec.html</a></p>\n\n<p>For example, if we have trained doc2vec with</p>\n\n<p>\"aaaaaAAAAAaaaaaa\" - \"label 1\"</p>\n\n<p>bbbbbbBBBBBbbbb\" - \"label 2\"</p>\n\n<p>can we infer aaaaAAAAaaaaAA is label 1 using Doc2vec?</p>\n\n<p>I know Doc2vec can train word vectors and label vectors.\nUsing this vectors, can we infer unseen sentences(combination of trained words) in which label?</p>\n"
  },
  {
    "tags": [
      "text-mining",
      "data-cleaning"
    ],
    "owner": {
      "account_id": 5756260,
      "reputation": 281,
      "user_id": 10275,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/3b9860fa47781b7269b7f13853c54633?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "St&#233;phanie C",
      "link": "https://datascience.stackexchange.com/users/10275/st%c3%a9phanie-c"
    },
    "is_answered": true,
    "view_count": 27269,
    "answer_count": 4,
    "score": 16,
    "last_activity_date": 1572546810,
    "creation_date": 1458561683,
    "last_edit_date": 1458578492,
    "question_id": 10810,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10810/how-to-do-postal-addresses-fuzzy-matching",
    "title": "How to do postal addresses fuzzy matching?",
    "body": "<p>I would like to know how to match postal addresses when their format differ or when one of them is mispelled. </p>\n\n<p>So far I've found different solutions but I think that they are quite old and not very efficient. I'm sure some better methods exist, so if you have references for me to read, I'm sure it is a subject that may interest several persons.</p>\n\n<p>The solution I found (examples are in R) :</p>\n\n<ul>\n<li><p>Levenshtein distance, which equals the number of characters you have to insert, delete or change to transform one word into another. </p>\n\n<p><code>agrep(\"acusait\", c(\"accusait\", \"abusait\"), max = 2, value = TRUE)</code>\n<code>## [1] \"accusait\" \"abusait\"</code></p></li>\n<li><p>The comparison of phonemes</p>\n\n<p><code>library(RecordLinkage)</code>\n<code>soundex(x&lt;-c('accusait','acusait','abusait'))</code>\n<code>## [1] \"A223\" \"A223\" \"A123\"</code></p></li>\n<li><p>The use of a spelling corrector <a href=\"http://norvig.com/spell-correct.html\" rel=\"noreferrer\">(eventually a bayesian one like Peter Norvig's)</a>, but not very efficient on address I guess.</p></li>\n<li><p>I thought about using the suggestions of Google suggest, but likewise, it is not very efficient on personal postal addresses. </p></li>\n<li><p>You can imagine using a machine learning supervised approach but you need to have stored the mispelled requests of users to do so which is not an option for me. </p></li>\n</ul>\n"
  },
  {
    "tags": [
      "xgboost",
      "gbm"
    ],
    "owner": {
      "account_id": 279172,
      "reputation": 1357,
      "user_id": 16050,
      "user_type": "registered",
      "accept_rate": 12,
      "profile_image": "https://www.gravatar.com/avatar/0afae9fd8525ee0e8c7016769da5b472?s=256&d=identicon&r=PG",
      "display_name": "ihadanny",
      "link": "https://datascience.stackexchange.com/users/16050/ihadanny"
    },
    "is_answered": true,
    "view_count": 5635,
    "answer_count": 4,
    "score": 16,
    "last_activity_date": 1617316393,
    "creation_date": 1459521306,
    "last_edit_date": 1459697654,
    "question_id": 10997,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10997/need-help-understanding-xgboosts-approximate-split-points-proposal",
    "title": "Need help understanding xgboost&#39;s approximate split points proposal",
    "body": "<p>background:</p>\n\n<p>in <a href=\"http://arxiv.org/abs/1603.02754\" rel=\"noreferrer\">xgboost</a> the $t$ iteration tries to fit a tree $f_t$ over all $n$ examples which minimizes the following objective:</p>\n\n<p>$$\\sum_{i=1}^n[g_if_t(x_i) + \\frac{1}{2}h_if_t^2(x_i)]$$</p>\n\n<p>where $g_i, h_i$ are first order and second order derivatives over our previous best estimation $\\hat{y}$ (from iteration $t-1$): </p>\n\n<ul>\n<li>$g_i=d_{\\hat{y}}l(y_i, \\hat{y}) $</li>\n<li>$h_i=d^2_{\\hat{y}}l(y_i, \\hat{y}) $</li>\n</ul>\n\n<p>and $l$ is our loss function. </p>\n\n<hr>\n\n<p>The question (finally):</p>\n\n<p>When building $f_t$ and considering a specific feature $k$ in a specific split, they use the following heuristic to assess only some split candidates: They sort all examples by their $x_k$, pass over the sorted list and sum their second derivative $h_i$. They consider a split candidate only when the sum changes more than $\\epsilon$. Why is that???</p>\n\n<p>The explanation they give eludes me:</p>\n\n<p>They claim we can rewrite the previous equation like so:</p>\n\n<p>$$\\sum_{i=1}^n\\frac{1}{2}h_i[f_t(x_i) - g_i/h_i]^2 + constant$$</p>\n\n<p>and I fail to follow the algebra - can you show why is it equal? </p>\n\n<p>And then they claim that \"this is exactly weighted squared loss with labels $gi/hi$ and weights $h_i$\" - a statement I agree with, but I don't understand how does it relate to the split candidate algorithm they are using...</p>\n\n<p>Thanks and sorry if this is too long for this forum.  </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-mining",
      "predictive-modeling"
    ],
    "owner": {
      "account_id": 7764728,
      "reputation": 899,
      "user_id": 18843,
      "user_type": "registered",
      "accept_rate": 80,
      "profile_image": "https://www.gravatar.com/avatar/7c27abe54a513e6224ed7e90a86683c8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Robert de Graaf",
      "link": "https://datascience.stackexchange.com/users/18843/robert-de-graaf"
    },
    "is_answered": true,
    "view_count": 798,
    "accepted_answer_id": 11923,
    "answer_count": 3,
    "score": 16,
    "last_activity_date": 1465037457,
    "creation_date": 1464181686,
    "last_edit_date": 1465037457,
    "question_id": 11919,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/11919/why-are-ensembles-so-unreasonably-effective",
    "title": "Why are ensembles so unreasonably effective",
    "body": "<p>It seems to have become axiomatic that an ensemble of learners leads to the best possible model results - and it is becoming far rarer, for example, for single models to win competitions such as Kaggle. Is there a theoretical explanation for why ensembles are so darn effective?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "convolutional-neural-network",
      "backpropagation"
    ],
    "owner": {
      "account_id": 1441866,
      "reputation": 263,
      "user_id": 23640,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/919dbf528b526dc132b6f2ecb435bb7a?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Majster",
      "link": "https://datascience.stackexchange.com/users/23640/majster"
    },
    "is_answered": true,
    "view_count": 6161,
    "accepted_answer_id": 14224,
    "answer_count": 1,
    "score": 16,
    "last_activity_date": 1613490011,
    "creation_date": 1471772694,
    "last_edit_date": 1613490011,
    "question_id": 13587,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/13587/back-propagation-through-max-pooling-layers",
    "title": "Back-propagation through max pooling layers",
    "body": "<p>I have a small sub-question to <a href=\"https://datascience.stackexchange.com/questions/11699/backprop-through-max-pooling-layers?newreg=a19724e3c92a4b71ad72b7128751da1c\">this question</a>. </p>\n\n<p>I understand that when back-propagating through a max pooling layer the gradient is routed back in a way that the neuron in the previous layer which was selected as max gets all the gradient. What I'm not a 100% sure of is how the gradient in the next layer gets routed back to the pooling layer.</p>\n\n<p>So the first question is if I have a pooling layer connected to a fully connected layer - like the image below.</p>\n\n<p><a href=\"https://i.sstatic.net/hJUT8.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/hJUT8.png\" alt=\"example1\"></a></p>\n\n<p>When computing the gradient for the cyan \"neuron\" of the pooling layer do I sum all the gradients from the FC layer neurons? If this is correct then every \"neuron\" of the pooling layer has the same gradient?</p>\n\n<p>For example if the first neuron of FC layer has a gradient of 2, second has a gradient of 3, and third a gradient of 6. What are the gradients of the blue and purple \"neurons\" in the pooling layer and why?</p>\n\n<p>And the second question is when the pooling layer is connected to another convolution layer. How do I compute the gradient then? See the example below.</p>\n\n<p><a href=\"https://i.sstatic.net/7N4nM.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/7N4nM.png\" alt=\"example2\"></a></p>\n\n<p>For the topmost rightmost \"neuron\" of the pooling layer (the outlined green one) I just take the gradient of the purple neuron in the next conv layer and route it back, right?</p>\n\n<p>How about the filled green one? I need to multiply together the first column of neurons in the next layer because of the chain rule? Or do I need to add them?</p>\n\n<p>Please do not post a bunch of equations and tell me that my answer is right in there because I've been trying to wrap my head around equations and I still don't understand it perfectly that's why I'm asking this question in a simple way.</p>\n"
  },
  {
    "tags": [
      "tensorflow",
      "keras",
      "model-evaluations"
    ],
    "owner": {
      "account_id": 8700074,
      "reputation": 8747,
      "user_id": 21560,
      "user_type": "registered",
      "accept_rate": 36,
      "profile_image": "https://www.gravatar.com/avatar/349cff1d621bb8779315322f202fba5b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Hendrik",
      "link": "https://datascience.stackexchange.com/users/21560/hendrik"
    },
    "is_answered": true,
    "view_count": 35051,
    "answer_count": 1,
    "score": 16,
    "last_activity_date": 1596364140,
    "creation_date": 1472547160,
    "last_edit_date": 1472547588,
    "question_id": 13746,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/13746/how-to-define-a-custom-performance-metric-in-keras",
    "title": "How to define a custom performance metric in Keras?",
    "body": "<p>I tried to define a custom metric fuction (F1-Score) in Keras (Tensorflow backend) according to the following:</p>\n\n<pre><code>def f1_score(tags, predicted):\n\n    tags = set(tags)\n    predicted = set(predicted)\n\n    tp = len(tags &amp; predicted)\n    fp = len(predicted) - tp \n    fn = len(tags) - tp\n\n    if tp&gt;0:\n        precision=float(tp)/(tp+fp)\n        recall=float(tp)/(tp+fn)\n        return 2*((precision*recall)/(precision+recall))\n    else:\n        return 0\n</code></pre>\n\n<p>So far, so good, but when I try to apply it in model compilation:</p>\n\n<pre><code>model1.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=[f1_score])\n</code></pre>\n\n<p>it gives error:</p>\n\n<pre><code>TypeError                                 Traceback (most recent call last)\n&lt;ipython-input-85-4eca4def003f&gt; in &lt;module&gt;()\n      5 model1.add(Dense(output_dim=10, activation=\"sigmoid\"))\n      6 \n----&gt; 7 model1.compile(loss=\"binary_crossentropy\", optimizer=Adam(), metrics=[f1_score])\n      8 \n      9 h=model1.fit(X_train, Y_train, batch_size=500, nb_epoch=5, verbose=True, validation_split=0.1)\n\n/home/buda/anaconda2/lib/python2.7/site-packages/keras/models.pyc in compile(self, optimizer, loss, metrics, sample_weight_mode, **kwargs)\n    522                            metrics=metrics,\n    523                            sample_weight_mode=sample_weight_mode,\n--&gt; 524                            **kwargs)\n    525         self.optimizer = self.model.optimizer\n    526         self.loss = self.model.loss\n\n/home/buda/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, **kwargs)\n    664                 else:\n    665                     metric_fn = metrics_module.get(metric)\n--&gt; 666                     self.metrics_tensors.append(metric_fn(y_true, y_pred))\n    667                     if len(self.output_names) == 1:\n    668                         self.metrics_names.append(metric_fn.__name__)\n\n&lt;ipython-input-84-b8a5752b6d55&gt; in f1_score(tags, predicted)\n      4     #tf.convert_to_tensor(img.eval())\n      5 \n----&gt; 6     tags = set(tags)\n      7     predicted = set(predicted)\n      8 \n\n/home/buda/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in __iter__(self)\n    493       TypeError: when invoked.\n    494     \"\"\"\n--&gt; 495     raise TypeError(\"'Tensor' object is not iterable.\")\n    496 \n    497   def __bool__(self):\n\nTypeError: 'Tensor' object is not iterable.\n</code></pre>\n\n<p>What is the problem here? The fact that my f1_score function inputs are not Tensorflow arrays? If so, where/how can I convert them correctly?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "vc-theory"
    ],
    "owner": {
      "account_id": 9979998,
      "reputation": 173,
      "user_id": 27682,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-5iZ2dpftp5w/AAAAAAAAAAI/AAAAAAAAACg/S1QYjTI31KM/s256-rj/photo.jpg",
      "display_name": "",
      "link": "https://datascience.stackexchange.com/users/27682/%e9%93%ad%e5%a3%b0%e5%ad%99"
    },
    "is_answered": true,
    "view_count": 38339,
    "accepted_answer_id": 16146,
    "answer_count": 2,
    "score": 16,
    "last_activity_date": 1690323137,
    "creation_date": 1483698185,
    "last_edit_date": 1559964006,
    "question_id": 16140,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/16140/how-to-calculate-vc-dimension",
    "title": "How to calculate VC-dimension?",
    "body": "<p>Im studying machine learning, and I would like to know how to calculate VC-dimension.</p>\n\n<p>For example:</p>\n\n<p><span class=\"math-container\">$h(x)=\\begin{cases} 1 &amp;\\mbox{if } a\\leq x \\leq b \\\\ \n 0 &amp; \\mbox{else } \\end{cases} $</span>, with parameters <span class=\"math-container\">$(a,b)  R^2$</span>.</p>\n\n<p>What is the VC-dimension of it?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-mining",
      "dataset",
      "data-cleaning",
      "data"
    ],
    "owner": {
      "account_id": 1086411,
      "reputation": 281,
      "user_id": 17305,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e229d00ce8dc85556c2bc2978bb1b7c4?s=256&d=identicon&r=PG",
      "display_name": "DavideChicco.it",
      "link": "https://datascience.stackexchange.com/users/17305/davidechicco-it"
    },
    "is_answered": true,
    "view_count": 15989,
    "answer_count": 3,
    "score": 16,
    "last_activity_date": 1713103938,
    "creation_date": 1498512364,
    "last_edit_date": 1499438607,
    "question_id": 19980,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/19980/how-much-data-are-sufficient-to-train-my-machine-learning-model",
    "title": "How much data are sufficient to train my machine learning model?",
    "body": "<p>I've been working on machine learning and bioinformatics for a while, and today I had a conversation with a colleague about the main general issues of data mining.</p>\n\n<p>My colleague (who is a machine learning expert) said that, in his opinion, the arguably most important practical aspect of machine learning is <strong>how to understand whether you have collected enough data to train your machine learning model</strong>.</p>\n\n<p>This statement surprised me, because I had never given that much importance to this aspect...</p>\n\n<p>I then looked for more information on the internet, and I found this post on <a href=\"http://fastml.com/how-much-data-is-enough/\" rel=\"noreferrer\">FastML.com</a> reporting as rule of thumb that you need roughly <strong>10 times as many data instances as there are features</strong>.</p>\n\n<p>Two questions:</p>\n\n<p>1 - Is this issue really particularly <strong>relevant</strong> in machine learning?</p>\n\n<p>2 - <strong>Is the 10 times rule working?</strong> Are there any other relevant sources for this theme?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning"
    ],
    "owner": {
      "account_id": 279172,
      "reputation": 1357,
      "user_id": 16050,
      "user_type": "registered",
      "accept_rate": 12,
      "profile_image": "https://www.gravatar.com/avatar/0afae9fd8525ee0e8c7016769da5b472?s=256&d=identicon&r=PG",
      "display_name": "ihadanny",
      "link": "https://datascience.stackexchange.com/users/16050/ihadanny"
    },
    "is_answered": true,
    "view_count": 15055,
    "answer_count": 2,
    "score": 16,
    "last_activity_date": 1559767622,
    "creation_date": 1498917579,
    "question_id": 20098,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/20098/why-do-we-normalize-the-discounted-rewards-when-doing-policy-gradient-reinforcem",
    "title": "Why do we normalize the discounted rewards when doing policy gradient reinforcement learning?",
    "body": "<p>I'm trying to understand the <a href=\"http://adv-ml-2017.wdfiles.com/local--files/course-schedule/rl_class1.pdf\" rel=\"noreferrer\">policy gradient</a> approach for solving the <a href=\"https://gym.openai.com/envs/CartPole-v0\" rel=\"noreferrer\">cartpole</a> problem.\nIn this approach, we're expressing the gradient of the loss w.r.t each parameter of our policy as an expectation of the sum of gradients of our policy gradient for all actions in a sequence, weighted by the sum of discounted rewards in that sequence:</p>\n\n<p>$$\\nabla_\\theta L(\\theta) = E[ G(S_{0:T}, A_{0:T})\\sum_{t=0}^{T}\\nabla_\\theta log\\pi_\\theta (A_t|S_t) ]$$</p>\n\n<p>and we estimate it using an empirical average across all samples in an episode - which makes sense intuitively. </p>\n\n<p>BUT the less intuitive part is that I saw a common practice to normalize advantages in between episodes in several implementations (and indeed it works better). So after they calculate the they wouldn't directly use the advantage, but rather would normalize it, e.g. <a href=\"https://github.com/ashutoshkrjha/Cartpole-OpenAI-Tensorflow/blob/master/cartpole.py\" rel=\"noreferrer\">here</a> they do after every episode:</p>\n\n<pre><code>discounted_epr = discount_rewards(epr)\ndiscounted_epr -= np.mean(discounted_epr)\ndiscounted_epr /= np.std(discounted_epr)\n</code></pre>\n\n<p>what's the justification for that - both in theory and in intuition? It seems to me that if an episode is long and as such has large advantages, it's worth learning more from that episode than from a 3 moves episode. What am I missing? </p>\n"
  },
  {
    "tags": [
      "python",
      "scikit-learn",
      "pandas",
      "tfidf"
    ],
    "owner": {
      "account_id": 3460817,
      "reputation": 1379,
      "user_id": 21254,
      "user_type": "registered",
      "accept_rate": 57,
      "profile_image": "https://www.gravatar.com/avatar/9e2014846b008a5b75a2fc3f76b9f72d?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "lte__",
      "link": "https://datascience.stackexchange.com/users/21254/lte"
    },
    "is_answered": true,
    "view_count": 25963,
    "accepted_answer_id": 22817,
    "answer_count": 3,
    "score": 16,
    "last_activity_date": 1620322720,
    "creation_date": 1504524619,
    "last_edit_date": 1620322720,
    "question_id": 22813,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/22813/using-tf-idf-with-other-features-in-scikit-learn",
    "title": "Using TF-IDF with other features in scikit-learn",
    "body": "<p>What is the best/correct way to combine text analysis with other features? For example, I have a dataset with some text but also other features/categories. scikit-learn's TF-IDF vectorizer transforms text data into sparse matrices. I can use these sparse matrices directly with a Naive Bayes classifier for example. But what's the way to also take into account the other features? Should I de-sparsify the tf-idf representation of the text and combine the features and the text into one DataFrame? Or can I keep the sparse matrix as a separate column for example? What's the correct way to do this?</p>\n"
  },
  {
    "tags": [
      "statistics",
      "random-forest",
      "optimization",
      "model-evaluations",
      "sampling"
    ],
    "owner": {
      "account_id": 5581780,
      "reputation": 6116,
      "user_id": 38887,
      "user_type": "registered",
      "accept_rate": 60,
      "profile_image": "https://i.sstatic.net/Ecyth.jpg?s=256",
      "display_name": "Valentin Calomme",
      "link": "https://datascience.stackexchange.com/users/38887/valentin-calomme"
    },
    "is_answered": true,
    "view_count": 16519,
    "accepted_answer_id": 23677,
    "answer_count": 1,
    "score": 16,
    "last_activity_date": 1507668536,
    "creation_date": 1507632622,
    "last_edit_date": 1507668536,
    "question_id": 23666,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/23666/how-many-features-to-sample-using-random-forests",
    "title": "How many features to sample using Random Forests",
    "body": "<p>The <a href=\"https://en.wikipedia.org/wiki/Random_forest#From_bagging_to_random_forests\" rel=\"noreferrer\">Wikipedia page</a> which quotes <a href=\"https://statistics.stanford.edu/~tibs/ElemStatLearn\" rel=\"noreferrer\">\"The Elements of Statistical Learning\"</a> says:</p>\n\n<blockquote>\n  <p>Typically, for a classification problem with $p$ features, $\\lfloor \\sqrt{p}\\rfloor$  features are used in each split.</p>\n</blockquote>\n\n<p>I understand that this is a fairly good educated guess and it was probably confirmed by empirical evidence, but are there other reasons why one would pick the square root? Is there a statistical phenomenon happening there?</p>\n\n<p>Does this somehow help decrease the variance of the errors?</p>\n\n<p>Is this the same for regression and classification?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "statistics",
      "reference-request",
      "mathematics",
      "esl"
    ],
    "owner": {
      "account_id": 12596585,
      "reputation": 261,
      "user_id": 44101,
      "user_type": "registered",
      "accept_rate": 20,
      "profile_image": "https://www.gravatar.com/avatar/d8b9707328a741312f944293b981d638?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Tantaros",
      "link": "https://datascience.stackexchange.com/users/44101/tantaros"
    },
    "is_answered": true,
    "view_count": 4360,
    "accepted_answer_id": 26464,
    "answer_count": 5,
    "community_owned_date": 1515555196,
    "score": 16,
    "last_activity_date": 1615493011,
    "creation_date": 1515522488,
    "last_edit_date": 1587750268,
    "question_id": 26449,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/26449/beginner-math-books-for-machine-learning",
    "title": "Beginner math books for Machine Learning",
    "body": "<p>I'm a Computer Science engineer with no background in statistics or advanced math.</p>\n\n<p>I'm studying the book <a href=\"https://sebastianraschka.com/books.html#python-machine-learning-3rd-edition\" rel=\"nofollow noreferrer\">Python Machine Learning</a> by Raschka and Mirjalili, but when I tried to understand the math of the Machine Learning, I wasn't able to understand the great book that a friend suggest me <a href=\"https://web.stanford.edu/~hastie/ElemStatLearn/\" rel=\"nofollow noreferrer\">The Elements of Statistical Learning</a>.</p>\n\n<p>Do you know any easier statistics and math books for Machine Learning? If you don't, how should I move?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "mathematics"
    ],
    "owner": {
      "account_id": 6942424,
      "reputation": 173,
      "user_id": 41149,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-QHvlx4UQ1iE/AAAAAAAAAAI/AAAAAAAAAQQ/naxyguhBALc/s256-rj/photo.jpg",
      "display_name": "Rohit Kumar Singh",
      "link": "https://datascience.stackexchange.com/users/41149/rohit-kumar-singh"
    },
    "is_answered": true,
    "view_count": 6141,
    "accepted_answer_id": 27390,
    "answer_count": 3,
    "score": 16,
    "last_activity_date": 1580726362,
    "creation_date": 1517583496,
    "last_edit_date": 1542124815,
    "question_id": 27388,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/27388/what-does-it-mean-when-we-say-most-of-the-points-in-a-hypercube-are-at-the-bound",
    "title": "What does it mean when we say most of the points in a hypercube are at the boundary?",
    "body": "<p>If I have a 50 dimensional hypercube. And I define it's boundary by $0&lt;x_j&lt;0.05$ or $0.95&lt;x_j&lt;1$ where $x_j$ is dimension of the hypercube. Then calculating the proportion of points on the boundary of the hypercube will be $0.995$. What does it mean? Does it mean that rest of the space is empty? If $99\\%$ of the points are  at the boundary then the points inside the cube must not be uniformly distributed?</p>\n"
  },
  {
    "tags": [
      "neural-network"
    ],
    "owner": {
      "account_id": 7745588,
      "reputation": 699,
      "user_id": 43870,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://i.sstatic.net/7ZHOR.png?s=256",
      "display_name": "shakedzy",
      "link": "https://datascience.stackexchange.com/users/43870/shakedzy"
    },
    "is_answered": true,
    "view_count": 14445,
    "accepted_answer_id": 27724,
    "answer_count": 3,
    "score": 16,
    "last_activity_date": 1596639475,
    "creation_date": 1518434981,
    "question_id": 27722,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/27722/is-there-a-thumb-rule-for-designing-neural-networks",
    "title": "Is there a thumb-rule for designing neural-networks?",
    "body": "<p>I know that a neural-network architecture is mostly based on the problem itself and the types of input/output, but still - there's always a \"square one\" when starting to build one. So my question is - given a input dataset of <em>MxN</em> (M  is the number of records, N is the number of features) and a C possible output classes - is there a thumb-rule to how many layers/units should we start with? </p>\n"
  },
  {
    "tags": [
      "python",
      "predictive-modeling",
      "accuracy",
      "confusion-matrix",
      "classifier"
    ],
    "owner": {
      "account_id": 7905381,
      "reputation": 367,
      "user_id": 24719,
      "user_type": "registered",
      "accept_rate": 88,
      "profile_image": "https://www.gravatar.com/avatar/b71397ab9f40f2df9b760fcacbead671?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Pedro Alves",
      "link": "https://datascience.stackexchange.com/users/24719/pedro-alves"
    },
    "is_answered": true,
    "view_count": 83946,
    "protected_date": 1547217042,
    "accepted_answer_id": 28427,
    "answer_count": 1,
    "score": 16,
    "last_activity_date": 1547205999,
    "creation_date": 1519852052,
    "question_id": 28426,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/28426/train-accuracy-vs-test-accuracy-vs-confusion-matrix",
    "title": "Train Accuracy vs Test Accuracy vs Confusion matrix",
    "body": "<p>After I developed my predictive model using Random Forest I get the following metrics:</p>\n\n<pre><code>        Train Accuracy ::  0.9764634601043997\n        Test Accuracy  ::  0.7933284397683713\n         Confusion matrix  [[28292  1474]\n                            [ 6128   889]]\n</code></pre>\n\n<p>This is the results from this code:</p>\n\n<pre><code>  training_features, test_features, training_target, test_target, = train_test_split(df.drop(['bad_loans'], axis=1),\n                                                  df['target'],\n                                                  test_size = .3,\n                                                  random_state=12)\nclf = RandomForestClassifier()\ntrained_model = clf.fit(training_features, training_target)\ntrained_model.fit(training_features, training_target)\npredictions = trained_model.predict(test_features)      \n\nTrain Accuracy: accuracy_score(training_target, trained_model.predict(training_features))\nTest Accuracy: accuracy_score(test_target, predictions)\nConfusion Matrix: confusion_matrix(test_target, predictions)\n</code></pre>\n\n<p>However I'm getting a little confuse to interpret and explain this values.</p>\n\n<p>What exactly this 3 measures tell me about my model? </p>\n\n<p>Thanks!</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "feature-scaling",
      "normalization"
    ],
    "owner": {
      "account_id": 12930544,
      "reputation": 705,
      "user_id": 46375,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/65bb434a8ad3687f701d8f0f896dfb5a?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Qwerto",
      "link": "https://datascience.stackexchange.com/users/46375/qwerto"
    },
    "is_answered": true,
    "view_count": 24847,
    "answer_count": 3,
    "score": 16,
    "last_activity_date": 1558764744,
    "creation_date": 1527172101,
    "last_edit_date": 1527259576,
    "question_id": 32109,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/32109/zero-mean-and-unit-variance",
    "title": "Zero Mean and Unit Variance",
    "body": "<p>I'm studying Data Scaling, and in particular the Standardization method.\nI've understood the math behind it, but it's not clear to me why it's important to give the features zero mean and unit variance.</p>\n\n<p>Can you explain me ?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "feature-selection",
      "data-science-model"
    ],
    "owner": {
      "account_id": 2685258,
      "reputation": 163,
      "user_id": 49737,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2ead8aacde9e8d2568f6359f64c5d4ae?s=256&d=identicon&r=PG",
      "display_name": "bp89",
      "link": "https://datascience.stackexchange.com/users/49737/bp89"
    },
    "is_answered": true,
    "view_count": 30297,
    "accepted_answer_id": 36411,
    "answer_count": 5,
    "score": 16,
    "last_activity_date": 1533926513,
    "creation_date": 1533272461,
    "question_id": 36404,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/36404/when-to-remove-correlated-variables",
    "title": "When to remove correlated variables",
    "body": "<p>Can somebody please suggest what is the correct stage to remove correlated variables before feature engineering or after feature engineering ?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "keras",
      "regularization",
      "dropout"
    ],
    "owner": {
      "account_id": 1891013,
      "reputation": 1455,
      "user_id": 57724,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/dc4597c8aedf386296097d025d0c1c78?s=256&d=identicon&r=PG",
      "display_name": "user781486",
      "link": "https://datascience.stackexchange.com/users/57724/user781486"
    },
    "is_answered": true,
    "view_count": 9019,
    "accepted_answer_id": 37024,
    "answer_count": 5,
    "score": 16,
    "last_activity_date": 1594506957,
    "creation_date": 1534421934,
    "last_edit_date": 1571923402,
    "question_id": 37021,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/37021/why-does-adding-a-dropout-layer-improve-deep-machine-learning-performance-given",
    "title": "Why does adding a dropout layer improve deep/machine learning performance, given that dropout suppresses some neurons from the model?",
    "body": "<p>If removing some neurons results in a better performing model, why not use a simpler neural network with fewer layers and fewer neurons in the first place? Why build a bigger, more complicated model in the beginning and suppress parts of it later?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "lstm",
      "rnn",
      "dropout",
      "stacked-lstm"
    ],
    "owner": {
      "account_id": 1504846,
      "reputation": 760,
      "user_id": 51125,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/edde992468b0953f35bca35d98395306?s=256&d=identicon&r=PG",
      "display_name": "BigBadMe",
      "link": "https://datascience.stackexchange.com/users/51125/bigbadme"
    },
    "is_answered": true,
    "view_count": 31358,
    "answer_count": 2,
    "score": 16,
    "last_activity_date": 1721401866,
    "creation_date": 1536844666,
    "last_edit_date": 1536869047,
    "question_id": 38205,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/38205/dropout-on-which-layers-of-lstm",
    "title": "Dropout on which layers of LSTM?",
    "body": "<p>Using a multi-layer <code>LSTM</code> with dropout, is it advisable to put dropout on all hidden layers as well as the output Dense layers?  In Hinton's paper (which proposed Dropout) he only put Dropout on the Dense layers, but that was because the hidden inner layers were convolutional.</p>\n\n<p>Obviously, I can test for my specific model, but I wondered if there was a consensus on this?</p>\n"
  },
  {
    "tags": [
      "keras",
      "tensorflow"
    ],
    "owner": {
      "account_id": 14359730,
      "reputation": 362,
      "user_id": 59193,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/08fcd512a92d45ff3a9574ce603a8f03?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Javier",
      "link": "https://datascience.stackexchange.com/users/59193/javier"
    },
    "is_answered": true,
    "view_count": 683,
    "accepted_answer_id": 38386,
    "answer_count": 5,
    "score": 16,
    "last_activity_date": 1537217857,
    "creation_date": 1537138243,
    "last_edit_date": 1537205958,
    "question_id": 38354,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/38354/what-more-does-tensorflow-offer-to-keras",
    "title": "What more does TensorFlow offer to keras?",
    "body": "<p>I'm aware that keras serves as a high-level interface to TensorFlow. </p>\n\n<p>But it seems to me that keras can do many functionalities on its own (data input, model creation, training, evaluation).</p>\n\n<p>Furthermore, some of TensorFlow's functionality can be ported directly to keras (e.g. it is possible to use a tf metric or loss function in keras).</p>\n\n<p>My question is, what does TensorFlow offer that can't be reproduced in keras? </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "machine-learning-model",
      "dropout",
      "deep-learning"
    ],
    "owner": {
      "account_id": 7086680,
      "reputation": 2520,
      "user_id": 35644,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/1XpWT.png?s=256",
      "display_name": "Aditya",
      "link": "https://datascience.stackexchange.com/users/35644/aditya"
    },
    "is_answered": true,
    "view_count": 15494,
    "accepted_answer_id": 38508,
    "answer_count": 2,
    "score": 16,
    "last_activity_date": 1718244504,
    "creation_date": 1537387164,
    "last_edit_date": 1574361400,
    "question_id": 38507,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/38507/why-should-we-use-or-not-dropout-on-the-input-layer",
    "title": "Why should we use (or not) dropout on the input layer?",
    "body": "<p>People generally avoid using dropout at the input layer itself. But wouldn't it be better to use it?</p>\n\n<p>Adding dropout (given that it's randomized it will probably end up acting like another regularizer) should make the model more robust. It will make it more independent of a given set of features, which matter always, and let the NN find other patterns too, and then the model generalizes better even though we might be missing some important features, but that's randomly decided per epoch.</p>\n\n<p>Is this an incorrect interpretation? What am I missing?</p>\n\n<p>Isn't this equivalent to what we generally do by removing features one by one and then rebuilding the non-NN-based model to see the importance of it?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "keras",
      "cnn"
    ],
    "owner": {
      "account_id": 13968532,
      "reputation": 311,
      "user_id": 56233,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-wC-FPjf74Nk/AAAAAAAAAAI/AAAAAAAAAAA/AAnnY7rkJl43mG6KIeVnO8I5lg3bK8ek0A/mo/s256-rj/photo.jpg",
      "display_name": "kainamanama",
      "link": "https://datascience.stackexchange.com/users/56233/kainamanama"
    },
    "is_answered": true,
    "view_count": 57552,
    "answer_count": 6,
    "score": 16,
    "last_activity_date": 1620310986,
    "creation_date": 1540916836,
    "last_edit_date": 1540920442,
    "question_id": 40462,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/40462/how-to-prepare-the-varied-size-input-in-cnn-prediction",
    "title": "How to prepare the varied size input in CNN prediction",
    "body": "<p>I want to make a CNN model in <code>Keras</code> which can be fed images of different sizes. According to other questions, I could understand how to set a model, like <code>Input =(None,None,3)</code>. However, I'm not sure how to prepare the input/output datasets.\nConcretely, now I want to combine the datasets with (100,100) and (240,360).\nHowever, I don't know how to combine these datasets.</p>\n"
  },
  {
    "tags": [
      "normalization"
    ],
    "owner": {
      "account_id": 949038,
      "reputation": 163,
      "user_id": 76485,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/0K6P5.png?s=256",
      "display_name": "vuamitom",
      "link": "https://datascience.stackexchange.com/users/76485/vuamitom"
    },
    "is_answered": true,
    "view_count": 19081,
    "accepted_answer_id": 54383,
    "answer_count": 4,
    "score": 16,
    "last_activity_date": 1561374780,
    "creation_date": 1561222126,
    "question_id": 54296,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/54296/should-input-images-be-normalized-to-1-to-1-or-0-to-1",
    "title": "Should input images be normalized to -1 to 1 or 0 to 1",
    "body": "<p>Many ML tutorials are normalizing input images to value of -1 to 1 before feeding them to ML model. The ML model is most likely a few conv 2d layers followed by a fully connected layers. Assuming activation function is ReLu. </p>\n\n<p>My question is, would normalizing images to [-1, 1] range be unfair to input pixels in negative range since through ReLu, output would be 0. Would normalizing images into [0, 1] range instead be a better idea?</p>\n\n<p>Thank you.</p>\n"
  },
  {
    "tags": [
      "time-series",
      "pandas",
      "numpy",
      "outlier",
      "seaborn"
    ],
    "owner": {
      "account_id": 16162762,
      "reputation": 362,
      "user_id": 76265,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-ntD0seAU7RU/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rdB0SgFgfeQwFrYfmcsw7B9MdZz3A/mo/s256-rj/photo.jpg",
      "display_name": "Uday T",
      "link": "https://datascience.stackexchange.com/users/76265/uday-t"
    },
    "is_answered": true,
    "view_count": 74008,
    "accepted_answer_id": 57199,
    "answer_count": 3,
    "score": 16,
    "last_activity_date": 1720840306,
    "creation_date": 1561954525,
    "last_edit_date": 1561957363,
    "question_id": 54808,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/54808/how-to-remove-outliers-using-box-plot",
    "title": "How to remove outliers using box-plot?",
    "body": "<p>I have data of a metric grouped date wise. I have plotted the data, now, how do I remove the values outside the range of the boxplot (outliers)?</p>\n\n<p>All the ['AVG'] data is in a single column,\nI need it for time series modelling.</p>\n\n<p><a href=\"https://i.sstatic.net/iDYC5.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/iDYC5.png\" alt=\"enter image description here\"></a></p>\n"
  },
  {
    "tags": [
      "python",
      "seaborn"
    ],
    "owner": {
      "account_id": 3336698,
      "reputation": 263,
      "user_id": 77817,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ad6b5315ad2814dbe4048c4d18e0b3e5?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user2804064",
      "link": "https://datascience.stackexchange.com/users/77817/user2804064"
    },
    "is_answered": true,
    "view_count": 54802,
    "accepted_answer_id": 58637,
    "answer_count": 4,
    "score": 16,
    "last_activity_date": 1615677940,
    "creation_date": 1565278419,
    "last_edit_date": 1615677940,
    "question_id": 57245,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/57245/seaborn-heatmap-not-displaying-correctly",
    "title": "seaborn heatmap not displaying correctly",
    "body": "<p>For some reason, my heatmap is not displaying correctly anymore. It was working just fine even with 6 classes. Since the last time I used it, I've installed many packages (including plotly). I don't know what exactly has caused this. How can I make the annotations and the x/y labels centered again? In both images the exact same code is used.</p>\n<pre><code>    import matplotlib.pyplot as plt\n    import seaborn\n    conf_mat = confusion_matrix(valid_y, y_hat)\n    fig, ax = plt.subplots(figsize=(8,6))\n    seaborn.heatmap(conf_mat, annot=True, fmt='d',xticklabels=classes, yticklabels=classes)\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    plt.show()\n</code></pre>\n<p><a href=\"https://i.sstatic.net/5bXBb.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/5bXBb.png\" alt=\"conf matrix not displaying correctly\" /></a></p>\n<p><a href=\"https://i.sstatic.net/s9OGF.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/s9OGF.png\" alt=\"conf matrix displayin correctly\" /></a></p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "bert",
      "finetuning"
    ],
    "owner": {
      "account_id": 6783540,
      "reputation": 279,
      "user_id": 58342,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/c12cb0974f67a5b5090cd582bfad791f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "zwlayer",
      "link": "https://datascience.stackexchange.com/users/58342/zwlayer"
    },
    "is_answered": true,
    "view_count": 22273,
    "answer_count": 2,
    "score": 16,
    "last_activity_date": 1626312176,
    "creation_date": 1576002703,
    "last_edit_date": 1580634407,
    "question_id": 64583,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/64583/what-are-the-good-parameter-ranges-for-bert-hyperparameters-while-finetuning-it",
    "title": "What are the good parameter ranges for BERT hyperparameters while finetuning it on a very small dataset?",
    "body": "<p>I need to finetune BERT model (from the huggingface repository) on a sentence classification task. However, my dataset is really small.I have 12K sentences and only 10% of them are from positive classes. Does anyone here have any experience on finetuning bert in small datasets? Do you have any suggestions for learning rate, batch size, epoch number warmup steps etc.?</p>\n"
  },
  {
    "tags": [
      "classification",
      "feature-selection"
    ],
    "owner": {
      "account_id": 18626438,
      "reputation": 163,
      "user_id": 97464,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/bd9b306164f65498d151314cb140f47e?s=256&d=identicon&r=PG",
      "display_name": "JuniorDataGuyAKL",
      "link": "https://datascience.stackexchange.com/users/97464/juniordataguyakl"
    },
    "is_answered": true,
    "view_count": 25477,
    "accepted_answer_id": 74486,
    "answer_count": 2,
    "score": 16,
    "last_activity_date": 1692197305,
    "creation_date": 1589899802,
    "question_id": 74465,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/74465/how-to-understand-anova-f-for-feature-selection-in-python-sklearn-selectkbest-w",
    "title": "How to understand ANOVA-F for feature selection in Python. Sklearn SelectKBest with f_classif",
    "body": "<p>I am trying to understand what it really means to calculate an ANOVA F value for feature selection for a binary classification problem.</p>\n\n<p>As I understand from the calculation of ANOVA from basic statistics, we should have at least 2 samples for which we can calculate the ANOVA value. So does this mean in the Sklearn implementation that these samples are taken from within each feature? What exactly do these samples represent in the case of feature selection for this problem?</p>\n\n<p>I tried to setup a very simple example which I have listed below, but I am still struggling to understand what the ANOVA value really means here? I'm also struggling to understand how to calculate this by hand, which usually helps me see what is happening on the inside. </p>\n\n<p>In this example, repayment status 0 means the loan is repaid and 1 means it defaulted. I have only supplied 5 rows of data to keep it simple. </p>\n\n<p><a href=\"https://i.sstatic.net/rNjz1.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/rNjz1.png\" alt=\"enter image description here\"></a></p>\n\n<p>The code and results are as follows:</p>\n\n<p><a href=\"https://i.sstatic.net/0AIL4.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/0AIL4.png\" alt=\"enter image description here\"></a></p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "class-imbalance",
      "kaggle",
      "smote"
    ],
    "owner": {
      "account_id": 14770916,
      "reputation": 6410,
      "user_id": 86339,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/NKKKx.jpg?s=256",
      "display_name": "Carlos Mougan",
      "link": "https://datascience.stackexchange.com/users/86339/carlos-mougan"
    },
    "is_answered": true,
    "view_count": 8515,
    "accepted_answer_id": 108363,
    "answer_count": 4,
    "score": 16,
    "last_activity_date": 1711764380,
    "creation_date": 1640623801,
    "last_edit_date": 1640682391,
    "question_id": 106461,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/106461/why-smote-is-not-used-in-prize-winning-kaggle-solutions",
    "title": "Why SMOTE is not used in prize-winning Kaggle solutions?",
    "body": "<p>Synthetic Minority Over-sampling Technique SMOTE, is a well known method to tackle imbalanced datasets. There are many papers with a lot of citations out-there claiming that it is used to boost accuracy in unbalanced data scenarios.</p>\n<p>But then, when I see Kaggle competitions, it is rarely used, to the best of my knowledge there are no prize-winning Kaggle/ML competitions where it is used to achieve the best solution. <strong>Why SMOTE is not used in Kaggle?</strong></p>\n<p>I even see applied research papers (where there are millions of $ at stake) that SMOTE is not used: <a href=\"https://dl.acm.org/doi/10.1145/2648584.2648589\" rel=\"noreferrer\">Practical Lessons from Predicting Clicks on Ads at Facebook</a></p>\n<p>Is this because it's not the best strategy possible?\nIs it a research niche with no optimal real-life application?\nIs there any ML competition with a high-reward where this was used to achieve the best solution?</p>\n<p>I guess I am just hesitant that creating synthetic data actually helps.</p>\n"
  },
  {
    "tags": [
      "nosql",
      "tools",
      "processing",
      "apache-hadoop"
    ],
    "owner": {
      "account_id": 172198,
      "reputation": 295,
      "user_id": 134,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/WbDC8.jpg?s=256",
      "display_name": "&#252;",
      "link": "https://datascience.stackexchange.com/users/134/%d1%80%c3%bc%d1%84%d1%84%d0%bf"
    },
    "is_answered": true,
    "view_count": 3693,
    "accepted_answer_id": 43,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1431952219,
    "creation_date": 1400064298,
    "last_edit_date": 1400106419,
    "question_id": 38,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/38/what-is-the-difference-between-hadoop-and-nosql",
    "title": "What is the difference between Hadoop and noSQL",
    "body": "<p>I heard about many tools / frameworks for helping people to process their data (big data environment). </p>\n\n<p>One is called Hadoop and the other is the noSQL concept. What is the difference in point of processing? </p>\n\n<p>Are they complementary?</p>\n"
  },
  {
    "tags": [
      "clustering"
    ],
    "owner": {
      "account_id": 1941231,
      "reputation": 367,
      "user_id": 116,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6624ea4be352973c3aa9f114173060c2?s=256&d=identicon&r=PG",
      "display_name": "AdrianBR",
      "link": "https://datascience.stackexchange.com/users/116/adrianbr"
    },
    "is_answered": true,
    "view_count": 3131,
    "accepted_answer_id": 101,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1400190082,
    "creation_date": 1400144649,
    "last_edit_date": 1400148366,
    "question_id": 86,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/86/clustering-unique-visitors-by-useragent-ip-session-id",
    "title": "Clustering unique visitors by useragent, ip, session_id",
    "body": "<p>Given website access data in the form <code>session_id, ip, user_agent</code>, and optionally timestamp, following the conditions below, how would you best cluster the sessions into unique visitors?</p>\n\n<p><code>session_id</code>: is an id given to every new visitor. It does not expire, however if the user doesn't accept cookies/clears cookies/changes browser/changes device, he will not be recognised anymore</p>\n\n<p><code>IP</code> can be shared between different users (Imagine a free wi-fi cafe, or your ISP reassigning IPs), and they will often have at least 2, home and work.</p>\n\n<p><code>User_agent</code> is the browser+OS version, allowing to distinguish between devices. For example a user is likely to use both phone and laptop, but is unlikely to use windows+apple laptops. It is unlikely that the same session id has multiple useragents.</p>\n\n<p>Data might look as the fiddle here:\n<a href=\"http://sqlfiddle.com/#!2/c4de40/1\">http://sqlfiddle.com/#!2/c4de40/1</a></p>\n\n<p>Of course, we are talking about assumptions, but it's about getting as close to reality as possible. For example, if we encounter the same ip and useragent in a limited time frame with a different session_id, it would be a fair assumption that it's the same user, with some edge case exceptions.</p>\n\n<p>Edit: Language in which the problem is solved is irellevant, it's mostly about logic and not implementation. Pseudocode is fine.</p>\n\n<p>Edit: due to the slow nature of the fiddle, you can alternatively read/run the mysql:</p>\n\n<pre><code>select session_id, floor(rand()*256*256*256*256) as ip_num , floor(rand()*1000) as user_agent_id\nfrom \n    (select 1+a.nr+10*b.nr as session_id, ceil(rand()*3) as nr\n    from\n        (select 1 as nr union all select 2 union all select 3   union all select 4 union all select 5\n        union all select 6 union all select 7 union all select 8 union all select 9 union all select 0)a\n    join\n        (select 1 as nr union all select 2 union all select 3   union all select 4 union all select 5\n        union all select 6 union all select 7 union all select 8 union all select 9 union all select 0)b\n        order by 1\n    )d\ninner join\n    (select 1 as nr union all select 2 union all select 3   union all select 4 union all select 5\n    union all select 6 union all select 7 union all select 8 union all select 9 )e\n    on d.nr&gt;=e.nr\n</code></pre>\n"
  },
  {
    "tags": [
      "data-mining",
      "machine-learning"
    ],
    "owner": {
      "account_id": 1450775,
      "reputation": 253,
      "user_id": 212,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/26b97bd54732698cc0679fec4d977c49?s=256&d=identicon&r=PG",
      "display_name": "Alex Gao",
      "link": "https://datascience.stackexchange.com/users/212/alex-gao"
    },
    "is_answered": true,
    "view_count": 4695,
    "accepted_answer_id": 131,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1611567782,
    "creation_date": 1400316308,
    "question_id": 115,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/115/is-there-any-apis-for-crawling-abstract-of-paper",
    "title": "Is there any APIs for crawling abstract of paper?",
    "body": "<p>If I have a very long list of paper names, how could I get abstract of these papers from internet or any database?</p>\n\n<p>The paper names are like \"Assessment of Utility in Web Mining for the Domain of Public Health\".</p>\n\n<p>Does any one know any API that can give me a solution? I tried to crawl google scholar, however, google blocked my crawler.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "statistics",
      "feature-selection"
    ],
    "owner": {
      "account_id": 2937730,
      "reputation": 503,
      "user_id": 113,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/2Tl2O.jpg?s=256",
      "display_name": "vefthym",
      "link": "https://datascience.stackexchange.com/users/113/vefthym"
    },
    "is_answered": true,
    "view_count": 5805,
    "accepted_answer_id": 170,
    "answer_count": 4,
    "score": 15,
    "last_activity_date": 1615493544,
    "creation_date": 1400514924,
    "last_edit_date": 1431955846,
    "question_id": 169,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/169/how-to-specify-important-attributes",
    "title": "How to specify important attributes?",
    "body": "<p>Assume a set of loosely structured data (e.g. Web tables/Linked Open Data), composed of many data sources. There is no common schema followed by the data and each source can use synonym attributes to describe the values (e.g. \"nationality\" vs \"bornIn\"). </p>\n\n<p>My goal is to find some \"important\" attributes that somehow \"define\" the entities that they describe. So, when I find the same value for such an attribute, I will know that the two descriptions are most likely about the same entity (e.g. the same person).</p>\n\n<p>For example, the attribute \"lastName\" is more discriminative than the attribute \"nationality\". </p>\n\n<p><strong>How could I (statistically) find such attributes that are more important than others?</strong></p>\n\n<p>A naive solution would be to take the average IDF of the values of each attribute and make this the \"importance\" factor of the attribute. A similar approach would be to count how many distinct values appear for each attribute.</p>\n\n<p>I have seen the term feature, or attribute selection in machine learning, but I don't want to discard the remaining attributes, I just want to put higher weights to the most important ones.</p>\n"
  },
  {
    "tags": [
      "dataset",
      "statistics",
      "ab-test"
    ],
    "owner": {
      "account_id": 4827725,
      "reputation": 253,
      "user_id": 2830,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/xyB4q.png?s=256",
      "display_name": "teebszet",
      "link": "https://datascience.stackexchange.com/users/2830/teebszet"
    },
    "is_answered": true,
    "view_count": 5558,
    "accepted_answer_id": 909,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1452080905,
    "creation_date": 1407191230,
    "last_edit_date": 1452080905,
    "question_id": 903,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/903/analyzing-a-b-test-results-which-are-not-normally-distributed-using-independent",
    "title": "Analyzing A/B test results which are not normally distributed, using independent t-test",
    "body": "<p>I have a set of results from an A/B test (one control group, one feature group) which do not fit a Normal Distribution. \nIn fact the distribution resembles more closely the Landau Distribution.</p>\n\n<p>I believe the independent t-test requires that the samples be at least approximately normally distributed, which discourages me using the t-test as a valid method of significance testing.</p>\n\n<p>But my question is: \n<strong>At what point can one say that the t-test is not a good method of significance testing?</strong></p>\n\n<p>Or put another way, how can one qualify how reliable the p-values of a t-test are, given only the data set?</p>\n"
  },
  {
    "tags": [
      "machine-learning"
    ],
    "migrated_from": {
      "other_site": {
        "aliases": [
          "https://statistics.stackexchange.com",
          "https://crossvalidated.com"
        ],
        "styling": {
          "tag_background_color": "#edefed",
          "tag_foreground_color": "#5D5D5D",
          "link_color": "#0077CC"
        },
        "related_sites": [
          {
            "relation": "meta",
            "api_site_parameter": "stats.meta",
            "site_url": "https://stats.meta.stackexchange.com",
            "name": "Cross Validated Meta"
          },
          {
            "relation": "chat",
            "site_url": "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com",
            "name": "Chat Stack Exchange"
          }
        ],
        "markdown_extensions": [
          "MathJax",
          "Prettify"
        ],
        "launch_date": 1288900046,
        "open_beta_date": 1280170800,
        "closed_beta_date": 1279566000,
        "site_state": "normal",
        "high_resolution_icon_url": "https://cdn.sstatic.net/Sites/stats/Img/apple-touch-icon@2.png",
        "favicon_url": "https://cdn.sstatic.net/Sites/stats/Img/favicon.ico",
        "icon_url": "https://cdn.sstatic.net/Sites/stats/Img/apple-touch-icon.png",
        "audience": "people interested in statistics, machine learning, data analysis, data mining, and data visualization",
        "site_url": "https://stats.stackexchange.com",
        "api_site_parameter": "stats",
        "logo_url": "https://cdn.sstatic.net/Sites/stats/Img/logo.png",
        "name": "Cross Validated",
        "site_type": "main_site"
      },
      "on_date": 1411136824,
      "question_id": 116034
    },
    "owner": {
      "account_id": 4889850,
      "reputation": 161,
      "user_id": 3360,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6bfd49bc1aab960f2cdceaa6a932671a?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Vinay Tiwari",
      "link": "https://datascience.stackexchange.com/users/3360/vinay-tiwari"
    },
    "is_answered": true,
    "view_count": 782,
    "answer_count": 4,
    "score": 15,
    "last_activity_date": 1610669371,
    "creation_date": 1411117735,
    "last_edit_date": 1495636300,
    "question_id": 1143,
    "link": "https://datascience.stackexchange.com/questions/1143/studying-machine-learning-algorithms-depth-of-understanding-vs-number-of-algor",
    "title": "Studying machine learning algorithms: depth of understanding vs. number of algorithms",
    "body": "<p>Recently I was introduced to the field of Data Science (its been 6 months approx), and Ii started the journey with Machine Learning Course by Andrew Ng and post that started working on the Data Science Specialization by JHU.</p>\n\n<p>On practical application front, I have been working on building a predictive model that would predict attrition. So far I have used glm, bayesglm, rf in an effort to learn and apply these methods, but I find a lot of gap in my understanding of these algorithms.</p>\n\n<p>My basic dilemma is:</p>\n\n<p><em>Whether I should focus more on learning the intricacies of a few algorithms or should I use the approach of knowing a lot of them as and when and as much as required?</em></p>\n\n<p>Please guide me in the right direction, maybe by suggesting books or articles or anything that you think would help. </p>\n\n<p>I would be grateful if you would reply with an idea of guiding someone who has just started his career in the field of Data Science, and wants to be a person who solves practical issues for the business world.</p>\n\n<p>I would read (as many as possible) resources (books,articles) suggested in this post and would provide a personal feed back on the pros and cons of the same so as to make this a helpful post for people who come across a similar question in future,and i think it would be great if people suggesting these books can do the same.</p>\n"
  },
  {
    "tags": [
      "random-forest",
      "online-learning"
    ],
    "owner": {
      "account_id": 2325017,
      "reputation": 566,
      "user_id": 4719,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/cayyb.png?s=256",
      "display_name": "tashuhka",
      "link": "https://datascience.stackexchange.com/users/4719/tashuhka"
    },
    "is_answered": true,
    "view_count": 7203,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1552795821,
    "creation_date": 1413794922,
    "question_id": 2314,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/2314/on-line-random-forests-by-adding-more-single-decisions-trees",
    "title": "On-line random forests by adding more single Decisions Trees",
    "body": "<p>A Random Forest (RF) is created by an ensemble of Decision Trees's (DT). By using bagging, each DT is trained in a different data subset. Hence, <strong>is there any way of implementing an on-line random forest by adding more decision tress on new data?</strong></p>\n\n<p>For example, we have 10K samples and train 10 DT's. Then we get 1K samples, and instead of training again the full RF, we add a new DT. The prediction is done now by the Bayesian average of 10+1 DT's.</p>\n\n<p>In addition, if we keep all the previous data, the new DT's can be trained mainly in the new data, where the probability of picking a sample is weighted depending how many times have been already picked.</p>\n"
  },
  {
    "tags": [
      "scalability",
      "scala"
    ],
    "owner": {
      "account_id": 1349569,
      "reputation": 1169,
      "user_id": 3466,
      "user_type": "registered",
      "accept_rate": 62,
      "profile_image": "https://www.gravatar.com/avatar/b6864e53f49bd4ac6fc89c4b8697497e?s=256&d=identicon&r=PG",
      "display_name": "sheldonkreger",
      "link": "https://datascience.stackexchange.com/users/3466/sheldonkreger"
    },
    "is_answered": true,
    "view_count": 615,
    "accepted_answer_id": 3696,
    "answer_count": 4,
    "score": 15,
    "last_activity_date": 1615493670,
    "creation_date": 1418193437,
    "question_id": 2668,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/2668/data-science-tools-using-scala",
    "title": "Data Science Tools Using Scala",
    "body": "<p>I know that Spark is fully integrated with Scala. It's use case is specifically for large data sets. Which other tools have good Scala support? Is Scala best suited for larger data sets? Or is it also suited for smaller data sets? </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "visualization",
      "deep-learning"
    ],
    "owner": {
      "account_id": 459625,
      "reputation": 293,
      "user_id": 5316,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/RAyTX.jpg?s=256",
      "display_name": "runDOSrun",
      "link": "https://datascience.stackexchange.com/users/5316/rundosrun"
    },
    "is_answered": true,
    "view_count": 4735,
    "accepted_answer_id": 2679,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1596639458,
    "creation_date": 1418206500,
    "question_id": 2670,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/2670/visualizing-deep-neural-network-training",
    "title": "Visualizing deep neural network training",
    "body": "<p>I'm trying to find an equivalent of Hinton Diagrams for multilayer networks to plot the weights during training. </p>\n\n<p>The trained network is somewhat similar to a Deep SRN, i.e. it has a high number of multiple weight matrices which would make the simultaneous plot of several Hinton Diagrams visually confusing. </p>\n\n<p>Does anyone know of a good way to visualize the weight update process for recurrent networks with multiple layers? </p>\n\n<p>I haven't found much papers on the topic. I was thinking to display time-related information on the weights per layer instead if I can't come up with something. E.g. the weight-delta over time for each layer (omitting the use of every single connection). PCA is another possibility, though I'd like to not produce much additional computations, since the visualization is done online during training.</p>\n"
  },
  {
    "tags": [
      "svm"
    ],
    "owner": {
      "account_id": 985917,
      "reputation": 461,
      "user_id": 5203,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/9d8b6657ed0cc7c26aaebc42cdab0323?s=256&d=identicon&r=PG",
      "display_name": "ASX",
      "link": "https://datascience.stackexchange.com/users/5203/asx"
    },
    "is_answered": true,
    "view_count": 44972,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1610297936,
    "creation_date": 1422226982,
    "question_id": 4943,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/4943/intuition-for-the-regularization-parameter-in-svm",
    "title": "Intuition for the regularization parameter in SVM",
    "body": "<p>How does varying the regularization parameter in an SVM change the decision boundary for a non-separable dataset? A visual answer and/or some commentary on the limiting behaviors (for large and small regularization) would be very helpful.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-mining",
      "classification",
      "clustering"
    ],
    "owner": {
      "account_id": 6334360,
      "reputation": 303,
      "user_id": 9724,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/02ac887f8362ef74dd4cd30912b6209f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "sylvia",
      "link": "https://datascience.stackexchange.com/users/9724/sylvia"
    },
    "is_answered": true,
    "view_count": 11337,
    "accepted_answer_id": 5857,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1559965459,
    "creation_date": 1432078465,
    "last_edit_date": 1559965459,
    "question_id": 5838,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/5838/using-attributes-to-classify-cluster-user-profiles",
    "title": "Using attributes to classify/cluster user profiles",
    "body": "<p>I have a dataset of users purchasing products from a website.</p>\n\n<p>The attributes I have are user id, region(state) of the user, the categories id of product, keywords id of product, keywords id of website, and sales amount spent of the product.</p>\n\n<p>The goal is to use the information of a product and website to identity who the users are, such as \"male young gamer\" or \"stay at home mom\".</p>\n\n<p>I attached a sample picture as below:</p>\n\n<p><img src=\"https://i.sstatic.net/S7OSa.png\" alt=\"enter image description here\"></p>\n\n<p>There are all together 1940 unique categories and 13845 unique keywords for products. For the website, there are 13063 unique keywords. The whole dataset is huge as that's the daily logging data.</p>\n\n<p>I am thinking of clustering, as those are unsupervised, but those id are ordered number having no numeric meaning. Then I don't know how to apply the algorithm. I am also thinking of classification. If I add a column of class based on the sales amount of product purchased. I think clustering is more preferred. I don't know what algorithm I should use for this case as the dimensions of the keywords id could be more than 10000 (each product could have many keywords, so does website). I need to use Spark for this project. </p>\n\n<p>Can anyone help me out with some ideas or suggestions?</p>\n\n<p>Thank you so much!</p>\n"
  },
  {
    "tags": [
      "python",
      "apache-spark",
      "categorical-data",
      "pyspark"
    ],
    "owner": {
      "account_id": 5333088,
      "reputation": 1065,
      "user_id": 5043,
      "user_type": "registered",
      "accept_rate": 27,
      "profile_image": "https://www.gravatar.com/avatar/65d37f7e419b428d315d33900d555d88?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "SRS",
      "link": "https://datascience.stackexchange.com/users/5043/srs"
    },
    "is_answered": true,
    "view_count": 32962,
    "answer_count": 3,
    "score": 15,
    "last_activity_date": 1618982290,
    "creation_date": 1435618528,
    "last_edit_date": 1462888589,
    "question_id": 6268,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/6268/how-to-convert-categorical-data-to-numerical-data-in-pyspark",
    "title": "How to convert categorical data to numerical data in Pyspark",
    "body": "<p>I am using Ipython notebook to work with pyspark applications. I have a CSV file with lots of categorical columns to determine whether the income falls under or over the 50k range. I would like to perform a classification algorithm taking all the inputs to determine the income range. I need to build a dictionary of variables to mapped variables and use a map function to map the variables to numbers for processing. Essentially, I would my dataset to be in a numerical format so that I can work on implementing the models. </p>\n\n<p>In the data set, there are categorical columns like education, marital status, working class etc. Can someone tell me how to convert them into numerical columns in pyspark?  </p>\n\n<pre><code>workclass = {'?':0,'Federal-gov':1,'Local-gov':2,'Never-  worked':3,'Private':4,'Self-emp-inc':5,'Self-emp-not-inc':6,'State-gov':7,'Without-pay':8}\n</code></pre>\n\n<p>I created a sample dictionary with key value pairs for work class. But, I don't know how to use this in a map function and replace the categorical data in the CSV file with the corresponding value. </p>\n\n<pre><code>wc = pd.read_csv('PATH', usecols = ['Workclass'])\n\ndf = pd.DataFrame(wc)\nwcdict = {' ?':0,' Federal-gov':1,' Local-gov':2,' Never-worked':3,' Private':4,' Self-emp-inc':5,' Self-emp-n-inc':6,' State-gov':7,' Without-pay':8}\ndf_new = df.applymap(lambda s: wcdict.get(s) if s in wcdict else s)\nprint(df_new)\n</code></pre>\n\n<p>This is the code I have written in normal python to convert the categorical data into numerical data. It works fine. I want to do the conversion in spark context. And, there are 9 categorical columns in the data source. Is there a way to automate the dictionary update process to have a KV pair for all 9 columns?</p>\n"
  },
  {
    "tags": [
      "apache-spark",
      "scala"
    ],
    "owner": {
      "account_id": 6490224,
      "reputation": 253,
      "user_id": 10832,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e4cb8ad5ac7461d0377b7a7b44918177?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "the3rdNotch",
      "link": "https://datascience.stackexchange.com/users/10832/the3rdnotch"
    },
    "is_answered": true,
    "view_count": 108906,
    "accepted_answer_id": 8545,
    "answer_count": 3,
    "score": 15,
    "last_activity_date": 1696688520,
    "creation_date": 1437574582,
    "last_edit_date": 1437947364,
    "question_id": 6546,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/6546/how-to-calculate-the-mean-of-a-dataframe-column-and-find-the-top-10",
    "title": "How to calculate the mean of a dataframe column and find the top 10%",
    "body": "<p>I am very new to Scala and Spark, and am working on some self-made exercises using baseball statistics. I am using a case class create a RDD and assign a schema to the data, and am then turning it into a DataFrame so I can use SparkSQL to select groups of players via their stats that meet certain criteria.</p>\n\n<p>Once I have the subset of players I am interested in looking at further, I would like to find the mean of a column; eg Batting Average or RBIs. From there I would like to break all the players into percentile groups based on their average performance compared to all players; the top 10%, bottom 10%, 40-50%</p>\n\n<p>I've been able to use the DataFrame.describe() function to return a summary of a desired column (mean, stddev, count, min, and max) all as strings though. Is there a better way to get just the mean and stddev as Doubles, and what is the best way of breaking the players into groups of 10-percentiles?</p>\n\n<p>So far my thoughts are to find the values that bookend the percentile ranges and writing a function that groups players via comparators, but that feels like it is bordering on reinventing the wheel.</p>\n\n<p>I have the following imports currently:</p>\n\n<pre><code> import org.apache.spark.rdd.RDD \n import org.apache.spark.sql.SQLContext \n import org.apache.spark.{SparkConf, SparkContext} \n import org.joda.time.format.DateTimeFormat  \n</code></pre>\n"
  },
  {
    "tags": [
      "nlp",
      "deep-learning",
      "word-embeddings",
      "unsupervised-learning"
    ],
    "owner": {
      "account_id": 1645301,
      "reputation": 351,
      "user_id": 12378,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b7783c2c66480613a4c46abafb25eae7?s=256&d=identicon&r=PG",
      "display_name": "gaurus",
      "link": "https://datascience.stackexchange.com/users/12378/gaurus"
    },
    "is_answered": true,
    "view_count": 18179,
    "answer_count": 4,
    "score": 15,
    "last_activity_date": 1613681287,
    "creation_date": 1451101668,
    "question_id": 9525,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9525/how-word2vec-can-be-used-to-identify-unseen-words-and-relate-them-to-already-tra",
    "title": "How word2vec can be used to identify unseen words and relate them to already trained data",
    "body": "<p>I was working on word2vec gensim model and found it really interesting.\nI am intersted in finding how a unknown/unseen word when checked with the model will be able to get similar terms from the trained model.</p>\n\n<p>Is this possible?\nCan word2vec be tweaked for this?\nOr the training corpus needs to have all the words of which i want to find similarity.</p>\n"
  },
  {
    "tags": [
      "bayesian-networks",
      "pgm"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 7840,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1514407401,
    "creation_date": 1453924680,
    "last_edit_date": 1453928453,
    "question_id": 10000,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10000/what-is-the-difference-between-a-dynamic-bayes-network-and-a-hmm",
    "title": "What is the difference between a (dynamic) Bayes network and a HMM?",
    "body": "<p>I have read that HMMs, Particle Filters and Kalman filters are special cases of dynamic Bayes networks. However, I only know HMMs and I don't see the difference to dynamic Bayes networks.</p>\n\n<p>Could somebody please explain?</p>\n\n<p>It would be nice if your answer could be similar to the following, but for bayes Networks:</p>\n\n<h2>Hidden Markov Models</h2>\n\n<p>A Hidden Markov Model (HMM) is a 5-tuple $\\lambda = (S, O, A, B, \\Pi)$:</p>\n\n<ul>\n<li>$S \\neq \\emptyset$: A set of states (e.g. \"beginning of phoneme\", \"middle of phoneme\", \"end of phoneme\")</li>\n<li>$O \\neq \\emptyset$: A set of possible observations (audio signals)</li>\n<li>$A \\in \\mathbb{R}^{|S| \\times |S|}$: A stochastic matrix which gives probabilites $(a_{ij})$ to get from state $i$ to state $j$.</li>\n<li>$B \\in \\mathbb{R}^{|S| \\times |O|}$: A stochastic matrix which gives probabilites $(b_{kl})$ to get in state $k$ the observation $l$.</li>\n<li>$\\Pi \\in \\mathbb{R}^{|S|}$: Initial distribution to start in one of the states.</li>\n</ul>\n\n<p>It is usually displayed as a directed graph, where each node corresponds to one state $s \\in S$ and the transition probabilities are denoted on the edges.</p>\n\n<p>Hidden Markov Models are called \"hidden\", because the current state is hidden. The algorithms have to guess it from the observations and the model itself. They are called \"Markov\", because for the next state only the current state matters.</p>\n\n<p>For HMMs, you give a fixed topology (number of states, possible edges). Then there are 3 possible tasks</p>\n\n<ul>\n<li><strong>Evaluation</strong>: given a HMM $\\lambda$, how likely is it to get observations $o_1, \\dots, o_t$ (Forward algorithm)</li>\n<li><strong>Decoding</strong>: given a HMM $\\lambda$ and a observations $o_1, \\dots, o_t$, what is the most likely sequence of states $s_1, \\dots, s_t$ (Viterbi algorithm)</li>\n<li><strong>Learning</strong>: learn $A, B, \\Pi$: <a href=\"https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm\" rel=\"noreferrer\">Baum-Welch algorithm</a>, which is a special case of Expectation maximization.</li>\n</ul>\n\n<h2>Bayes networks</h2>\n\n<p>Bayes networks are directed acyclical graphs (DAGs) $G = (\\mathcal{X}, \\mathcal{E})$. The nodes represent random variables $X \\in \\mathcal{X}$. For every $X$, there is a probability distribution which is conditioned on the parents of $X$:</p>\n\n<p>$$P(X|\\text{parents}(X))$$</p>\n\n<p>There seem to be (please clarify) two tasks:</p>\n\n<ul>\n<li><strong>Inference</strong>: Given some variables, get the most likely values of the others variables. Exact inference is NP-hard. Approximately, you can use MCMC.</li>\n<li><p><strong>Learning</strong>: How you learn those distributions depends on the exact problem (<a href=\"http://www.eng.tau.ac.il/~bengal/BN.pdf\" rel=\"noreferrer\">source</a>):</p>\n\n<ul>\n<li>known structure, fully observable:  maximum likelihood estimation (MLE)</li>\n<li>known structure, partially observable: Expectation Maximization (EM) or Markov Chain Monte Carlo (MCMC)</li>\n<li>unknown structure, fully observable: search through model space</li>\n<li>unknown structure, partially observable: EM + search through model space</li>\n</ul></li>\n</ul>\n\n<h2>Dynamic Bayes networks</h2>\n\n<p>I guess dynamic Bayes networks (DBNs) are also directed probabilistic graphical models. The variability seems to come from the network changing over time. However, it seems to me that this is equivalent to only copying the same network and connecting every node at time $t$ with every the corresponding node at time $t+1$. Is that the case?</p>\n"
  },
  {
    "tags": [
      "visualization",
      "dimensionality-reduction",
      "tsne",
      "manifold"
    ],
    "owner": {
      "account_id": 118734,
      "reputation": 1500,
      "user_id": 5143,
      "user_type": "registered",
      "accept_rate": 40,
      "profile_image": "https://www.gravatar.com/avatar/0b9f57174e3bafdf65e512aca66cdb9b?s=256&d=identicon&r=PG",
      "display_name": "Javierfdr",
      "link": "https://datascience.stackexchange.com/users/5143/javierfdr"
    },
    "is_answered": true,
    "view_count": 5654,
    "accepted_answer_id": 10820,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1474163899,
    "creation_date": 1458490305,
    "last_edit_date": 1458598377,
    "question_id": 10802,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10802/can-closer-points-be-considered-more-similar-in-t-sne-visualization",
    "title": "Can closer points be considered more similar in T-SNE visualization?",
    "body": "<p>I understand from Hinton's paper that T-SNE does a good job in keeping local similarities and a decent job in preserving global structure (clusterization).</p>\n\n<p>However I'm not clear if points appearing closer in a 2D t-sne visualization can be assumed as \"more-similar\" data-points. I'm using data with 25 features.</p>\n\n<p>As an example, observing the image below, can I assume that blue datapoints are more similar to green ones, specifically to the biggest green-points cluster?. Or, asking differently, is it ok to assume that blue points are more similar to green one in the closest cluster, than to red ones in the other cluster? (disregarding green points in the red-ish cluster)</p>\n\n<p><a href=\"https://i.sstatic.net/xAnH5.png\"><img src=\"https://i.sstatic.net/xAnH5.png\" alt=\"enter image description here\"></a></p>\n\n<p>When observing other examples, such as the ones presented at sci-kit learn Manifold learning it seems right to assume this, but I'm not sure if is correct statistically speaking.</p>\n\n<p><a href=\"https://i.sstatic.net/7L587.png\"><img src=\"https://i.sstatic.net/7L587.png\" alt=\"enter image description here\"></a></p>\n\n<p><strong>EDIT</strong></p>\n\n<p>I have calculated the distances from the original dataset manually (the mean pairwise euclidean distance) and the visualization actually represents a proportional spatial distance regarding the dataset. However, I would like to know if this is fairly acceptable to be expected from the original mathematical formulation of t-sne and not mere coincidence.</p>\n"
  },
  {
    "tags": [
      "logistic-regression",
      "scikit-learn"
    ],
    "owner": {
      "account_id": 5019090,
      "reputation": 931,
      "user_id": 12443,
      "user_type": "registered",
      "accept_rate": 0,
      "profile_image": "https://www.gravatar.com/avatar/810526126c32a812e74cd963b974ed7a?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "sebastianspiegel",
      "link": "https://datascience.stackexchange.com/users/12443/sebastianspiegel"
    },
    "is_answered": true,
    "view_count": 22443,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1458732621,
    "creation_date": 1458543077,
    "last_edit_date": 1458545199,
    "question_id": 10805,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10805/does-scikit-learn-use-regularization-by-default",
    "title": "Does scikit-learn use regularization by default?",
    "body": "<p>I just fitted a logistic curve to some fake data. I made the data essentially a step function. </p>\n\n<pre><code>data = -------------++++++++++++++\n</code></pre>\n\n<p>But when I look at the fitted curve, the slope is very small. The function that best minimizes the cost function, assuming cross entropy, is the step function. Why does it not look like a step function? Is there some regularization, L1 or L2, done by default?</p>\n\n<p><a href=\"https://i.sstatic.net/cDRXN.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/cDRXN.png\" alt=\"Logistic regression using scikit-learn\"></a></p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "cross-validation"
    ],
    "owner": {
      "account_id": 4981280,
      "reputation": 419,
      "user_id": 20635,
      "user_type": "registered",
      "accept_rate": 57,
      "profile_image": "https://graph.facebook.com/100001020739527/picture?type=large",
      "display_name": "Armon Safai",
      "link": "https://datascience.stackexchange.com/users/20635/armon-safai"
    },
    "is_answered": true,
    "view_count": 14484,
    "accepted_answer_id": 14001,
    "answer_count": 3,
    "score": 15,
    "last_activity_date": 1477320436,
    "creation_date": 1473737004,
    "last_edit_date": 1473774002,
    "question_id": 13960,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/13960/how-to-choose-a-classifier-after-cross-validation",
    "title": "How to choose a classifier after cross-validation?",
    "body": "<p>When we do k-fold cross validation, should we just use the classifier that has the highest test accuracy? What is generally the best approach in getting a classifier from cross validation?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "regression",
      "scikit-learn"
    ],
    "owner": {
      "account_id": 9231209,
      "reputation": 159,
      "user_id": 24526,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-Ret_VDiWjW8/AAAAAAAAAAI/AAAAAAAAACk/VYJdCFwHjOY/s256-rj/photo.jpg",
      "display_name": "Hamid Mahdavian",
      "link": "https://datascience.stackexchange.com/users/24526/hamid-mahdavian"
    },
    "is_answered": true,
    "view_count": 7008,
    "answer_count": 3,
    "score": 15,
    "last_activity_date": 1615572095,
    "creation_date": 1474445299,
    "last_edit_date": 1474547708,
    "question_id": 14124,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/14124/predict-the-best-time-of-call",
    "title": "Predict the best time of call",
    "body": "<p>I have a dataset including a set of customers in different cities of California, time of calling for each customer, and the status of call (True if customer answers the call and False if customer does not answer). </p>\n\n<p>I have to find an appropriate time of calling for future customers such that the probability of answering the call is high. So, what is the best strategy for this problem? Should I consider it as a classification problem which the hours (0,1,2,... 23) are the classes? Or should I consider it as a regression task which the time is a continuous variable? How can I make sure that the probability of answering the call will be high?</p>\n\n<p>Any help would be appreciated. It also would be great if you refer me to similar problems.</p>\n\n<p>Below is a snapshot of the data. <img src=\"https://i.sstatic.net/geF7x.png\" alt=\"\"></p>\n"
  },
  {
    "tags": [
      "neural-network",
      "deep-learning"
    ],
    "owner": {
      "account_id": 4279202,
      "reputation": 842,
      "user_id": 14909,
      "user_type": "registered",
      "accept_rate": 38,
      "profile_image": "https://www.gravatar.com/avatar/d8c5bd85dc73f5783c00babbec5cef63?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Make42",
      "link": "https://datascience.stackexchange.com/users/14909/make42"
    },
    "is_answered": true,
    "view_count": 891,
    "accepted_answer_id": 14387,
    "answer_count": 3,
    "score": 15,
    "last_activity_date": 1493624839,
    "creation_date": 1475586795,
    "last_edit_date": 1493624839,
    "question_id": 14352,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/14352/how-are-deep-learning-nns-different-now-2016-from-the-ones-i-studied-just-4-ye",
    "title": "How are deep-learning NNs different now (2016) from the ones I studied just 4 years ago (2012)?",
    "body": "<p>It is said in <a href=\"https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_network_architectures\">Wikipedia</a> and <a href=\"http://deeplearning4j.org/neuralnet-overview.html#concept\">deeplearning4j</a> that Deep-learning NN (DLNN) are NN that have >1 hidden layer.</p>\n\n<p>These kind of NN were standard at university for me, while DLNN are very hyped right now. <strong>Been there, done that - what's the big deal?</strong></p>\n\n<p>I heard also that stacked NN are considered deep-learning. <strong>How is deep-learning really defined?</strong></p>\n\n<p>My background of NN is mostly from university, not from jobs:</p>\n\n<ul>\n<li>studied applications of NN in industry</li>\n<li>had about 5 courses on artif. intel. &amp; mach. learn. - though maybe 2 of them on NN</li>\n<li>used NN for small, simple project on image recognition - used 3 layer feed-forward NN</li>\n<li>did not do real research (as in doctor thesis) on them</li>\n</ul>\n"
  },
  {
    "tags": [
      "classification",
      "rbm"
    ],
    "owner": {
      "account_id": 438612,
      "reputation": 263,
      "user_id": 25289,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/B9PSD.jpg?s=256",
      "display_name": "Stefan Falk",
      "link": "https://datascience.stackexchange.com/users/25289/stefan-falk"
    },
    "is_answered": true,
    "view_count": 10684,
    "accepted_answer_id": 15162,
    "answer_count": 3,
    "score": 15,
    "last_activity_date": 1539227038,
    "creation_date": 1476642996,
    "question_id": 14568,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/14568/how-to-use-rbm-for-classification",
    "title": "How to use RBM for classification?",
    "body": "<p>At the moment I'm playing with Restricted Boltzmann Machines and since I'm at it I would like try to classify handwritten digits with it.</p>\n\n<p>The model I created is now a quite fancy generative model but I don't know how to go further with it.</p>\n\n<p>In <a href=\"https://www.quora.com/How-can-the-Restricted-Boltzmann-Machine-be-used-in-classification-problems\" rel=\"noreferrer\">this article</a> the author say, that after creating a good generative model, one \"<em>then trains a discriminative classifier (i.e., linear classifier, Support Vector Machine) on top of the RBM using the labelled samples</em>\" and furtherly states \"<em>since you propagate the data vectors to the hidden units of the RBM model to get hidden unit vectors, or a higher-level representation of the data</em>\". The problem is that I'm not sure if I get that right.</p>\n\n<p>Does that mean all I have to do is propagate the input to the hidden units and there I have my RBM feature for classification?</p>\n\n<p>Can somebody explain this process to me?</p>\n"
  },
  {
    "tags": [
      "python",
      "visualization",
      "geospatial"
    ],
    "owner": {
      "account_id": 1048523,
      "reputation": 333,
      "user_id": 25611,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/771f16da1e8fc82bcc5883f2ff6afc79?s=256&d=identicon&r=PG",
      "display_name": "ScottieB",
      "link": "https://datascience.stackexchange.com/users/25611/scottieb"
    },
    "is_answered": true,
    "view_count": 39628,
    "accepted_answer_id": 14785,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1615666683,
    "creation_date": 1477527488,
    "last_edit_date": 1615666612,
    "question_id": 14774,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/14774/heatmap-on-a-map-in-python",
    "title": "Heatmap on a map in Python",
    "body": "<p><a href=\"https://community.modeanalytics.com/gallery/geographic-heat-map/\" rel=\"nofollow noreferrer\">Mode Analytics</a> has a nice heatmap feature, but it is not conducive to comparing maps (only one per report).</p>\n<p><a href=\"https://i.sstatic.net/QhA1I.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/QhA1I.png\" alt=\"enter image description here\" /></a></p>\n<p>What they do allow is data to be pulled easily into a wrapped python notebook.  And then any image in python can easily be added to a report.</p>\n<p>So my question is: how do I recreate a heatmap on an actual map in Python?  I've checked out follium and plotly, but neither seem to have similar functionality.</p>\n"
  },
  {
    "tags": [
      "keras",
      "cross-validation",
      "overfitting"
    ],
    "owner": {
      "account_id": 4955209,
      "reputation": 263,
      "user_id": 23718,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/fn04Q.jpg?s=256",
      "display_name": "DeusXMachina",
      "link": "https://datascience.stackexchange.com/users/23718/deusxmachina"
    },
    "is_answered": true,
    "view_count": 11881,
    "accepted_answer_id": 15258,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1596962847,
    "creation_date": 1479649399,
    "last_edit_date": 1479853383,
    "question_id": 15242,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/15242/can-overfitting-occur-even-with-validation-loss-still-dropping",
    "title": "Can overfitting occur even with validation loss still dropping?",
    "body": "<p>I have a convolutional + LSTM model in Keras, similar to this (ref 1), that I am using for a Kaggle contest. Architecture is shown below. I have trained it on my labeled set of 11000 samples (two classes, initial prevalence is ~9:1, so I upsampled the 1's to about a 1/1 ratio) for 50 epochs with 20% validation split.I was getting blatant overfitting for a while but I thought it got it under control with noise and dropout layers. </p>\n\n<p>Model looked like it was training wonderfully, at the end scored 91% on the entirety of the training set, but upon testing on the test data set, absolute garbage. </p>\n\n<p><a href=\"https://i.sstatic.net/UBNm6.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/UBNm6.png\" alt=\"Loss by epoch\"></a> </p>\n\n<p><a href=\"https://i.sstatic.net/j5GxE.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/j5GxE.png\" alt=\"Validation accuracy by epoch\"></a></p>\n\n<p><strong>Notice: the validation accuracy is higher than the training accuracy. This is the opposite of \"typical\" overfitting.</strong></p>\n\n<p>My intuition is, given the small-ish validation split, the model is still managing to fit too strongly to the input set and losing generalization. The other clue is that val_acc is greater than acc, that seems fishy. Is that the most likely scenario here?</p>\n\n<p>If this is overfitting, would increasing the validation split mitigate this at all, or am I going to run into the same issue, since on average, each sample will see half the total epochs still?</p>\n\n<p>The model:</p>\n\n<pre><code>Layer (type)                     Output Shape          Param #     Connected to                     \n====================================================================================================\nconvolution1d_19 (Convolution1D) (None, None, 64)      8256        convolution1d_input_16[0][0]     \n____________________________________________________________________________________________________\nmaxpooling1d_18 (MaxPooling1D)   (None, None, 64)      0           convolution1d_19[0][0]           \n____________________________________________________________________________________________________\nbatchnormalization_8 (BatchNormal(None, None, 64)      128         maxpooling1d_18[0][0]            \n____________________________________________________________________________________________________\ngaussiannoise_5 (GaussianNoise)  (None, None, 64)      0           batchnormalization_8[0][0]       \n____________________________________________________________________________________________________\nlstm_16 (LSTM)                   (None, 64)            33024       gaussiannoise_5[0][0]            \n____________________________________________________________________________________________________\ndropout_9 (Dropout)              (None, 64)            0           lstm_16[0][0]                    \n____________________________________________________________________________________________________\nbatchnormalization_9 (BatchNormal(None, 64)            128         dropout_9[0][0]                  \n____________________________________________________________________________________________________\ndense_23 (Dense)                 (None, 64)            4160        batchnormalization_9[0][0]       \n____________________________________________________________________________________________________\ndropout_10 (Dropout)             (None, 64)            0           dense_23[0][0]                   \n____________________________________________________________________________________________________\ndense_24 (Dense)                 (None, 2)             130         dropout_10[0][0]                 \n====================================================================================================\nTotal params: 45826\n</code></pre>\n\n<p>Here is the call to fit the model (class weight is typically around 1:1 since I upsampled the input): </p>\n\n<pre><code>class_weight= {0:1./(1-ones_rate), 1:1./ones_rate} # automatically balance based on class occurence\nm2.fit(X_train, y_train, nb_epoch=50, batch_size=64, shuffle=True, class_weight=class_weight, validation_split=0.2 )\n</code></pre>\n\n<p>SE has some silly rule that I can post no more than 2 links until my score is higher, so here is the example in case you are interested:\nRef 1: machinelearningmastery DOT com SLASH sequence-classification-lstm-recurrent-neural-networks-python-keras</p>\n"
  },
  {
    "tags": [
      "feature-selection",
      "performance",
      "model-selection"
    ],
    "owner": {
      "account_id": 6531920,
      "reputation": 341,
      "user_id": 25217,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0e15fb28ca8559d011e2da6c60d493c2?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "hopfk",
      "link": "https://datascience.stackexchange.com/users/25217/hopfk"
    },
    "is_answered": true,
    "view_count": 3718,
    "answer_count": 4,
    "score": 15,
    "last_activity_date": 1506510586,
    "creation_date": 1481031065,
    "question_id": 15553,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/15553/how-to-compare-the-performance-of-feature-selection-methods",
    "title": "How to compare the performance of feature selection methods?",
    "body": "<p>There are several <strong>feature selection</strong> / variable selection approaches (see for example <a href=\"http://www.jmlr.org/papers/v3/guyon03a.html\" rel=\"noreferrer\">Guyon &amp; Elisseeff, 2003</a>; <a href=\"http://www.jmlr.org/proceedings/papers/v10/liu10b/liu10b.pdf\" rel=\"noreferrer\">Liu et al., 2010</a>): </p>\n\n<ul>\n<li>filter methods (e.g., correlation-based, entropy-based, random forest importance based),</li>\n<li>wrapper methods (e.g., forward-search, hill-climbing search), and </li>\n<li>embedded methods where the feature selection is part of the model learning. </li>\n</ul>\n\n<p>Many published algorithms are also implemented in the machine learning tools like R, Python, etc. </p>\n\n<p>What would be an appropriate method to compare different feature selection algorithms and to select the best method for a given problem / dataset? \nA further question would be, whether there are any metrics known that measure the performance of feature selection algorithms?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "predictive-modeling",
      "feature-selection",
      "random-forest"
    ],
    "owner": {
      "account_id": 9093136,
      "reputation": 783,
      "user_id": 26163,
      "user_type": "registered",
      "accept_rate": 23,
      "profile_image": "https://lh3.googleusercontent.com/-K9QrmyE0F5k/AAAAAAAAAAI/AAAAAAAAAGs/QOtuR17ZGOA/s256-rj/photo.jpg",
      "display_name": "LUSAQX",
      "link": "https://datascience.stackexchange.com/users/26163/lusaqx"
    },
    "is_answered": true,
    "view_count": 17319,
    "accepted_answer_id": 16064,
    "answer_count": 3,
    "score": 15,
    "last_activity_date": 1560098276,
    "creation_date": 1483519602,
    "last_edit_date": 1560098276,
    "question_id": 16062,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/16062/is-feature-selection-necessary",
    "title": "Is feature selection necessary?",
    "body": "<p>I would like to run some machine learning model like random forest, gradient boosting, or SVM on my dataset. There are more than 200 predictor variables in my dataset and my target classes are a binary variable.</p>\n\n<p>Do I need to run feature selection before the model fitting? Does it affect the model performance significantly or is there not much difference if I directly fit the model using all predictor variables?  </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "random-forest",
      "sampling",
      "training"
    ],
    "owner": {
      "account_id": 9093136,
      "reputation": 783,
      "user_id": 26163,
      "user_type": "registered",
      "accept_rate": 23,
      "profile_image": "https://lh3.googleusercontent.com/-K9QrmyE0F5k/AAAAAAAAAAI/AAAAAAAAAGs/QOtuR17ZGOA/s256-rj/photo.jpg",
      "display_name": "LUSAQX",
      "link": "https://datascience.stackexchange.com/users/26163/lusaqx"
    },
    "is_answered": true,
    "view_count": 15624,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1484208040,
    "creation_date": 1484182707,
    "last_edit_date": 1484208040,
    "question_id": 16265,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/16265/is-stratified-sampling-necessary-random-forest-python",
    "title": "Is stratified sampling necessary (random forest, Python)?",
    "body": "<p>I use Python to run a random forest model on my imbalanced dataset (the target variable was a binary class). When splitting the training and testing dataset, I struggled whether to used stratified sampling (like the code shown) or not. So far, I observed in my project that the stratified case would lead to a higher model performance. But I think if I will use my model to predict the new cases which would highly probably differ in the distribution of target class with my current dataset. So I inclined to loosen this constrain and use the unstratified split. \nCould anyone can advice to clarify this point?   </p>\n\n<pre><code>train,test=train_test_split(myDataset, test_size=0.25, stratify=y)\n</code></pre>\n"
  },
  {
    "tags": [
      "rnn"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user27994"
    },
    "is_answered": true,
    "view_count": 26734,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1513007252,
    "creation_date": 1484589184,
    "last_edit_date": 1495543133,
    "question_id": 16350,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/16350/how-many-lstm-cells-should-i-use",
    "title": "How many LSTM cells should I use?",
    "body": "<p>Are there any rules of thumb (or actual rules) pertaining to the minimum, maximum and \"reasonable\" amount of LSTM cells I should use? Specifically I am relating to <a href=\"https://www.tensorflow.org/api_docs/python/rnn_cell/rnn_cells_for_use_with_tensorflow_s_core_rnn_methods#BasicLSTMCell.__init__\" rel=\"noreferrer\">BasicLSTMCell</a> from TensorFlow and <code>num_units</code> property.</p>\n\n<p>Please assume that I have a classification problem defined by:</p>\n\n<pre><code>t - number of time steps\nn - length of input vector in each time step\nm - length of output vector (number of classes)\ni - number of training examples\n</code></pre>\n\n<p>Is it true, for example, that the number of training examples should be larger than:</p>\n\n<p><code>4*((n+1)*m + m*m)*c</code></p>\n\n<p>where <code>c</code> is number of cells? I based this on this: <a href=\"https://stackoverflow.com/questions/38080035/how-to-calculate-the-number-of-parameters-of-an-lstm-network\">How to calculate the number of parameters of an LSTM network?</a> As I understand, this should give the total number of parameters, which should be less than number of training examples.</p>\n"
  },
  {
    "tags": [
      "visualization",
      "confusion-matrix"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 14294,
    "accepted_answer_id": 25008,
    "answer_count": 3,
    "score": 15,
    "last_activity_date": 1594715794,
    "creation_date": 1487654518,
    "last_edit_date": 1492087718,
    "question_id": 17079,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/17079/how-can-i-make-big-confusion-matrices-easier-to-read",
    "title": "How can I make big confusion matrices easier to read?",
    "body": "<p>I have recently published a dataset (<a href=\"https://arxiv.org/abs/1701.08380\" rel=\"noreferrer\">link</a>) with 369 classes. I ran a couple of experiments on them to get a feeling for how difficult the classification task is. Usually, I like it if there are confusion matrices to see the type of error being made. However, a $369 \\times 369$ matrix is not practical.</p>\n\n<p>Is there a way to give the important information of big confusion matrices? For example, usually there are a lot of 0s which are not so interesting. Is it possible to sort the classes so that most non-zero entries are around the diagonal in order to allow showing multiple matrices which are part of the complete confusion matrix?</p>\n\n<p>Here is <a href=\"https://github.com/MartinThoma/algorithms/tree/master/ML/confusion-matrix\" rel=\"noreferrer\">an example for a big confusion matrix</a>.</p>\n\n<h2>Examples in the Wild</h2>\n\n<p>Figure 6 of <a href=\"https://arxiv.org/pdf/1702.05373.pdf\" rel=\"noreferrer\">EMNIST</a> looks nice:</p>\n\n<p><a href=\"https://i.sstatic.net/NKEgU.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/NKEgU.png\" alt=\"enter image description here\"></a></p>\n\n<p>It is easy to see where many cases are. However, those are only $26$ classes. If the whole page was used instead of only one column this could probably be 3x as many, but that would still only be $3 \\cdot 26 = 78$ classes. Not even close to 369 classes of HASY or 1000 of ImageNet.</p>\n\n<h2>See also</h2>\n\n<p>My similar question on <a href=\"https://cs.stackexchange.com/q/70627/2914\">CS.stackexchange</a></p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "nlp",
      "convolution"
    ],
    "owner": {
      "account_id": 8700074,
      "reputation": 8747,
      "user_id": 21560,
      "user_type": "registered",
      "accept_rate": 36,
      "profile_image": "https://www.gravatar.com/avatar/349cff1d621bb8779315322f202fba5b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Hendrik",
      "link": "https://datascience.stackexchange.com/users/21560/hendrik"
    },
    "is_answered": true,
    "view_count": 19370,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1535374646,
    "creation_date": 1488269528,
    "last_edit_date": 1535374646,
    "question_id": 17241,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/17241/what-is-a-1d-convolutional-layer-in-deep-learning",
    "title": "What is a 1D Convolutional Layer in Deep Learning?",
    "body": "<p>I have a good general understanding of the role and mechanism of convolutional layers in Deep Learning for image processing in case of 2D or 3D implementations - they \"simply\" try to catch 2D patterns in images (in 3 channels in case of 3D).</p>\n\n<p>But recently I bumped into 1D convolutional layers in the context of Natural Language Processing, which is a kind of surprise for me, because in my understanding the 2D convolution is especially used to catch 2D patterns that are impossible to reveal in 1D (vector) form of image pixels. What is the logic behind 1D convolution?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "gan"
    ],
    "owner": {
      "account_id": 4685579,
      "reputation": 305,
      "user_id": 29771,
      "user_type": "registered",
      "accept_rate": 75,
      "profile_image": "https://i.sstatic.net/CtLTn.jpg?s=256",
      "display_name": "Alwyn Mathew",
      "link": "https://datascience.stackexchange.com/users/29771/alwyn-mathew"
    },
    "is_answered": true,
    "view_count": 8046,
    "accepted_answer_id": 18104,
    "answer_count": 4,
    "score": 15,
    "last_activity_date": 1615493780,
    "creation_date": 1491305177,
    "last_edit_date": 1491305487,
    "question_id": 18103,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/18103/can-we-generate-huge-dataset-with-generative-adversarial-networks",
    "title": "Can we generate huge dataset with Generative Adversarial Networks",
    "body": "<p>I'm dealing with a problem where I couldn't find enough dataset(images) to feed into my deep neural network for training.</p>\n\n<p>I was so inspired by the paper <a href=\"https://arxiv.org/abs/1605.05396\" rel=\"noreferrer\">Generative Adversarial Text to Image Synthesis</a> published by Scott Reed et al. on Generative Adversarial Networks. </p>\n\n<p>I was curious to know that, can I use available small dataset as an input to a GAN model and generate a much bigger dataset to deal with deeper network models?</p>\n\n<p>Will it be good enough?</p>\n"
  },
  {
    "tags": [
      "neural-network"
    ],
    "owner": {
      "account_id": 10475416,
      "reputation": 151,
      "user_id": 30056,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/48248a8939c6db9b3d90f02b72354198?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "navya",
      "link": "https://datascience.stackexchange.com/users/30056/navya"
    },
    "is_answered": true,
    "view_count": 1260,
    "answer_count": 4,
    "score": 15,
    "last_activity_date": 1642264938,
    "creation_date": 1495448961,
    "last_edit_date": 1525742852,
    "question_id": 19112,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/19112/do-neural-networks-have-explainability-like-decision-trees-do",
    "title": "Do neural networks have explainability like decision trees do?",
    "body": "<p>In Decision Trees, we can understand the output of the tree structure and we can also visualize how the Decision Tree makes decisions. So decision trees have explainability (their output can be explained easily.)</p>\n\n<p>Do we have explainability in Neural Networks like with Decision Trees?</p>\n"
  },
  {
    "tags": [
      "tensorflow",
      "keras",
      "theano",
      "parallel"
    ],
    "owner": {
      "account_id": 4817698,
      "reputation": 1974,
      "user_id": 16024,
      "user_type": "registered",
      "accept_rate": 60,
      "profile_image": "https://www.gravatar.com/avatar/eb2998bb8ee8b3572618c558e57e3d1f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "chmodsss",
      "link": "https://datascience.stackexchange.com/users/16024/chmodsss"
    },
    "is_answered": true,
    "view_count": 20644,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1525386664,
    "creation_date": 1502182932,
    "last_edit_date": 1504620567,
    "question_id": 22058,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/22058/make-keras-run-on-multi-machine-multi-core-cpu-system",
    "title": "Make Keras run on multi-machine multi-core cpu system",
    "body": "<p>I'm working on <a href=\"https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/\" rel=\"noreferrer\">Seq2Seq model</a> using LSTM from Keras (using Theano background) and I would like to parallelize the processes, because even few MBs of data need several hours for training. </p>\n\n<p>It is clear that GPUs are far much better in parallelization than CPUs. At the moment, I only have CPUs to work with. I could access 16 CPUs(2 Threads per core X 4 cores per socket X 2 sockets)</p>\n\n<p>From the doc of multi-core <a href=\"http://deeplearning.net/software/theano/tutorial/multi_cores.html\" rel=\"noreferrer\">support</a> in Theano, I managed to use all the four cores of a single socket. So, basically the CPU is at 400% usage with 4CPUs used and the remaining 12 CPUs remain unused. How do I make use of them too. Tensorflow could also be used instead of Theano background, if it works.</p>\n\n<p><a href=\"https://i.sstatic.net/aFGsp.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/aFGsp.png\" alt=\"enter image description here\"></a></p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "stanford-nlp",
      "randomized-algorithms"
    ],
    "owner": {
      "account_id": 5174557,
      "reputation": 367,
      "user_id": 26856,
      "user_type": "registered",
      "accept_rate": 67,
      "profile_image": "https://lh5.googleusercontent.com/-lr14bSsitOU/AAAAAAAAAAI/AAAAAAAATYc/yuSCjwCDkt8/s256-rj/photo.jpg",
      "display_name": "cinqS",
      "link": "https://datascience.stackexchange.com/users/26856/cinqs"
    },
    "is_answered": true,
    "view_count": 12188,
    "accepted_answer_id": 22097,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1615493822,
    "creation_date": 1502263839,
    "last_edit_date": 1502291672,
    "question_id": 22093,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/22093/why-should-the-initialization-of-weights-and-bias-be-chosen-around-0",
    "title": "Why should the initialization of weights and bias be chosen around 0?",
    "body": "<p>I read this:</p>\n\n<blockquote>\n  <p>To train our neural network, we will initialize each parameter W(l)ijWij(l) and each b(l)ibi(l) to a small random value near zero (say according to a Normal(0,2)Normal(0,2) distribution for some small , say 0.01)</p>\n</blockquote>\n\n<p>from <a href=\"http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/\" rel=\"noreferrer\">Stanford Deep learning tutorials</a>\nat the 7th paragraph in the <em>Backpropagation Algorithm</em></p>\n\n<p>What I don't understand is why the initialization of the weight or bias should be <strong>around 0</strong>?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-mining",
      "clustering",
      "text-mining"
    ],
    "owner": {
      "account_id": 11616669,
      "reputation": 685,
      "user_id": 38395,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/ac277f6f12e623b0a942cf11d31235ff?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Smith Volka",
      "link": "https://datascience.stackexchange.com/users/38395/smith-volka"
    },
    "is_answered": true,
    "view_count": 49868,
    "answer_count": 5,
    "score": 15,
    "last_activity_date": 1510183810,
    "creation_date": 1504587777,
    "question_id": 22828,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/22828/clustering-with-cosine-similarity",
    "title": "Clustering with cosine similarity",
    "body": "<p>I have a large data set and a cosine similarity between them. I would like to cluster them using cosine similarity that puts similar objects together without needing to specify beforehand the number of clusters I expect.</p>\n\n<p>I read the sklearn documentation of DBSCAN and Affinity Propagation, where both of them requires a distance matrix (not cosine similarity matrix).</p>\n\n<p>Really, I'm just looking for any algorithm that doesn't require <strong>a) a distance metric and b) a pre-specified number of clusters</strong>.</p>\n\n<p>Does anyone know of an algorithm that would do that? </p>\n"
  },
  {
    "tags": [
      "python",
      "pandas",
      "performance"
    ],
    "owner": {
      "account_id": 2671878,
      "reputation": 3950,
      "user_id": 201,
      "user_type": "registered",
      "accept_rate": 83,
      "profile_image": "https://i.sstatic.net/2ZFPD.png?s=256",
      "display_name": "Tasos",
      "link": "https://datascience.stackexchange.com/users/201/tasos"
    },
    "is_answered": true,
    "view_count": 35776,
    "accepted_answer_id": 23272,
    "answer_count": 3,
    "score": 15,
    "last_activity_date": 1645861117,
    "creation_date": 1506250217,
    "last_edit_date": 1506352519,
    "question_id": 23264,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/23264/improve-pandas-dataframe-filtering-speed",
    "title": "Improve Pandas dataframe filtering speed",
    "body": "<p>I have a dataset with 19 columns and about 250k rows. I have worked with bigger datasets, but this time, Pandas decided to play with my nerves.</p>\n\n<p>I tried to split the original dataset into 3 sub-dataframes based on some simple rules. However, it takes a long time to execute the code. About 15-20 seconds just for the filtering.</p>\n\n<p>Any alternative way that will improve the performance of the code?</p>\n\n<pre><code>import pandas as pd\n\n#read dataset\ndf = pd.read_csv('myData.csv')\n\n#create a dataframe with col1 10 and col2 &lt;= 15\ndf1 = df[(df.col1 == 10) &amp; (df.col2 &lt;= 15)]\ndf = df[~df.isin(df1)].dropna()\n\n#create a dataframe with col3 7 and col4 &gt;= 4\ndf2 = df[(df.col3 == 7) &amp; (df.col4 &gt;= 4)]\ndf = df[~df.isin(df2)].dropna()\n</code></pre>\n\n<p>In the end, I have the <code>df1, df2, df</code> dataframes with the filtered data.</p>\n"
  },
  {
    "tags": [
      "graphs"
    ],
    "owner": {
      "account_id": 11556970,
      "reputation": 731,
      "user_id": 37986,
      "user_type": "registered",
      "accept_rate": 33,
      "profile_image": "https://www.gravatar.com/avatar/3ef1322d57967f65e05d0ed8fbb0eafb?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Volka",
      "link": "https://datascience.stackexchange.com/users/37986/volka"
    },
    "is_answered": true,
    "view_count": 16282,
    "accepted_answer_id": 24083,
    "answer_count": 4,
    "score": 15,
    "last_activity_date": 1692961549,
    "creation_date": 1508972090,
    "last_edit_date": 1509019117,
    "question_id": 24081,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/24081/what-are-graph-embedding",
    "title": "What are graph embedding?",
    "body": "<p>I recently came across graph embedding such as DeepWalk and LINE. However, I still do not have a clear idea as what is meant by graph embeddings and when to use it (applications)? Any suggestions are welcome!</p>\n"
  },
  {
    "tags": [
      "classification",
      "dataset",
      "sampling",
      "class-imbalance"
    ],
    "owner": {
      "account_id": 5631107,
      "reputation": 481,
      "user_id": 32220,
      "user_type": "registered",
      "accept_rate": 60,
      "profile_image": "https://www.gravatar.com/avatar/2a03f78eff976de690746f223967bb8b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "sara",
      "link": "https://datascience.stackexchange.com/users/32220/sara"
    },
    "is_answered": true,
    "view_count": 5115,
    "accepted_answer_id": 24393,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1615666447,
    "creation_date": 1509948929,
    "last_edit_date": 1615666447,
    "question_id": 24392,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/24392/why-do-we-need-to-handle-data-imbalance",
    "title": "Why do we need to handle data imbalance?",
    "body": "<p>I would like to know <em>why</em> we need to deal with data imbalance. I know how to deal with it and different methods to solve the issue - by up sampling or down sampling or by using SMOTE.</p>\n<p>For example, if I have a rare disease 1 percent out of 100, and lets say I decided to have a balanced data set for my training set which is:  50/50 sample\nwon't that make the machine think 50% of patients will have disease? even though the ratio is 1 of 100.\nSo</p>\n<ol>\n<li>Why do we need to deal with data imbalance?</li>\n<li>What is the recommended ratio to have balance set?</li>\n</ol>\n"
  },
  {
    "tags": [
      "deep-learning",
      "tensorflow",
      "pytorch"
    ],
    "owner": {
      "account_id": 343021,
      "reputation": 28133,
      "user_id": 14675,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/L5oHj.jpg?s=256",
      "display_name": "noe",
      "link": "https://datascience.stackexchange.com/users/14675/noe"
    },
    "is_answered": true,
    "view_count": 2731,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1519023251,
    "creation_date": 1510074734,
    "question_id": 24458,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/24458/pytorch-vs-tensorflow-eager",
    "title": "PyTorch vs. Tensorflow eager",
    "body": "<p>Google recently included in tensorflow's nightly builds its <strong>Eager</strong> mode, an imperative API to access tensorflow computation capabilities.</p>\n\n<p><strong>How do tensorflow eager compare to PyTorch?</strong></p>\n\n<p>Some aspects that could affect the comparison could be: </p>\n\n<ul>\n<li>Advantages and disadvantages of eager due to its static graph legacy (e.g. names in nodes).</li>\n<li>Intrinsic limitations of either of them that the other does not have.</li>\n<li>Areas in which one of them needs improvement (e.g. feature completeness, computational optimizations).</li>\n<li>Ecosystem differences (e.g. tensorboard?).</li>\n</ul>\n\n<p>Note1: Yaroslav Bulatov wrote <a href=\"https://medium.com/@yaroslavvb/tensorflow-meets-pytorch-with-eager-mode-714cce161e6c\" rel=\"noreferrer\">a review about eager's nice features</a>.</p>\n\n<p>Note2: In <a href=\"https://datascience.stackexchange.com/questions/16835/pytorch-vs-tensorflow-fold\">a previous question</a>, I requested a comparison between PyTorch and Tensorflow Fold. At that time, it seemed to me that Fold could face PyTorch thanks to Google backing it. I was very very wrong: in the end, Google itself <a href=\"https://github.com/tensorflow/fold/issues/87\" rel=\"noreferrer\">abandoned Fold</a> in favour of Eager. I understand that this was due to intrinsic limitations in the normal tensorflow API that led Fold not to be very friendly, which constrained its adoption.</p>\n"
  },
  {
    "tags": [
      "scikit-learn",
      "model-evaluations"
    ],
    "owner": {
      "account_id": 8116438,
      "reputation": 271,
      "user_id": 42204,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7a2501156a97869de3c7dae5df15a79d?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Anderlecht",
      "link": "https://datascience.stackexchange.com/users/42204/anderlecht"
    },
    "is_answered": true,
    "view_count": 5703,
    "accepted_answer_id": 24992,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1511295654,
    "creation_date": 1511289849,
    "question_id": 24990,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/24990/irregular-precision-recall-curve",
    "title": "Irregular Precision-Recall Curve",
    "body": "<p>I'd expect that for a precision-recall curve, precision decreases while recall increases monotonically. I have a plot that is not smooth and looks funny. I used scikit learn the values for plotting the curve. Is the curve below abnormal? If yes, why and how can I correct it considering scikit learn automatically sorts the true and predicted labels. If the plot is OK, how best do I explain this behaviour?\n<a href=\"https://i.sstatic.net/4wQQy.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/4wQQy.png\" alt=\"enter image description here\"></a></p>\n"
  },
  {
    "tags": [
      "neural-network",
      "tensorflow",
      "convolutional-neural-network",
      "gan",
      "generative-models"
    ],
    "owner": {
      "account_id": 5848560,
      "reputation": 305,
      "user_id": 43141,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ff23f76651bcf5ad097b97eadb435c4a?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "snowparrot",
      "link": "https://datascience.stackexchange.com/users/43141/snowparrot"
    },
    "is_answered": true,
    "view_count": 42106,
    "accepted_answer_id": 36847,
    "answer_count": 4,
    "score": 15,
    "last_activity_date": 1613234852,
    "creation_date": 1515525258,
    "last_edit_date": 1613234852,
    "question_id": 26451,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/26451/how-to-calculate-the-output-shape-of-conv2d-transpose",
    "title": "How to calculate the output shape of conv2d_transpose?",
    "body": "<p>Currently I code a GAN to generate MNIST numbers but the generator doesnt want to work. First I choose z with shape 100 per Batch, put into a layer to get into the shape (7,7, 256). Then conv2d_transpose layer\nto into 28, 28, 1. (which is basically a mnist pic) </p>\n\n<p>I have two questions \n1.) This code doesn't work for obvious. Do you have any clue, why?\n2.) I am very aware how transpose convolution works but I can't find any resource to calculate the output size given input, strides and kernel size specific to Tensorflow. The useful information I found is <a href=\"https://arxiv.org/pdf/1603.07285v1.pdf\" rel=\"noreferrer\">https://arxiv.org/pdf/1603.07285v1.pdf</a> but e.g. padding in Tensorflow works very different. Can you help me?</p>\n\n<pre><code>mb_size = 32 #Size of image batch to apply at each iteration.\nX_dim = 784\nz_dim = 100\nh_dim = 7*7*256\ndropoutRate = 0.7\nalplr = 0.2 #leaky Relu\n\n\ndef generator(z, G_W1, G_b1, keepProb, first_shape):\n\n    G_W1 = tf.Variable(xavier_init([z_dim, h_dim]))\n    G_b1 = tf.Variable(tf.zeros(shape=[h_dim]))    \n\n\n    G_h1 = lrelu(tf.matmul(z, G_W1) + G_b1, alplr)\n    G_h1Drop = tf.nn.dropout(G_h1, keepProb)  # drop out\n\n    X = tf.reshape(G_h1Drop, shape=first_shape)\n    out = create_new_trans_conv_layer(X, 256, INPUT_CHANNEL, [3, 3], [2,2], \"transconv1\", [-1, 28, 28, 1])    \n    return out\n\n\n\n\n# new transposed cnn\ndef create_new_trans_conv_layer(input_data, num_input_channels, num_output_channels, filter_shape, stripe, name, output_shape):\n    # setup the filter input shape for tf.nn.conv_2d\n    conv_filt_shape = [filter_shape[0], filter_shape[1], num_output_channels, num_input_channels]\n\n\n    # initialise weights and bias for the filter\n    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03),\n                          name=name + '_W')\n    bias = tf.Variable(tf.truncated_normal([num_input_channels]), name=name + '_b')\n\n    # setup the convolutional layer operation\n    conv1 = tf.nn.conv2d_transpose(input_data, weights, output_shape, [1, stripe[0], stripe[1], 1], padding='SAME')\n\n    # add the bias\n    conv1 += bias\n\n    # apply a ReLU non-linear activation\n\n    conv1 = lrelu(conv1, alplr)\n\n    return conv1\n\n\n...\n\n\n    _, G_loss_curr = sess.run(\n        [G_solver, G_loss],\n        feed_dict={z: sample_z(mb_size, z_dim), keepProb: 1.0} #training generator\n</code></pre>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "keras",
      "multitask-learning"
    ],
    "owner": {
      "account_id": 405652,
      "reputation": 253,
      "user_id": 45804,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/29b2bb2b9e784b7fe19215329de18b43?s=256&d=identicon&r=PG",
      "display_name": "Aditya",
      "link": "https://datascience.stackexchange.com/users/45804/aditya"
    },
    "is_answered": true,
    "view_count": 12401,
    "accepted_answer_id": 27530,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1517932335,
    "creation_date": 1517860607,
    "question_id": 27498,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/27498/multi-task-learning-in-keras",
    "title": "Multi task learning in Keras",
    "body": "<p>I am trying to implement shared layers in Keras. I do see that Keras has <code>keras.layers.concatenate</code>, but I am unsure from documentation about its use. Can I use it to create multiple shared layers? What would be the best way to implement a simple shared neural network as shown below using Keras? <a href=\"https://dam-prod.media.mit.edu/thumb/2017/12/12/Screen%20Shot%202017-12-12%20at%2011.50.03%20AM.png.1400x1400.png\" rel=\"noreferrer\"><img src=\"https://dam-prod.media.mit.edu/thumb/2017/12/12/Screen%20Shot%202017-12-12%20at%2011.50.03%20AM.png.1400x1400.png\" alt=\"Shared Neural network\"></a></p>\n\n<p>Note that all the shape of input, output and shared layers for all 3 NNs are the same. There are multiple shared layers (and non-shared layers) in the three NNs. The coloured layers are unique to each NN, and have same shape.</p>\n\n<p>Basically, the figure represents 3 identical NNs with multiple shared hidden layers, followed by multiple non-shared hidden layers.</p>\n\n<p>I am unsure how to share multiple layers as in the Twitter example, there was just one shared layer (example in API doc).</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "dataset",
      "image-classification",
      "convolutional-neural-network"
    ],
    "owner": {
      "account_id": 1893897,
      "reputation": 399,
      "user_id": 27526,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://www.gravatar.com/avatar/593c2d4f4372384ae06efe0d991fa184?s=256&d=identicon&r=PG",
      "display_name": "DeltaIV",
      "link": "https://datascience.stackexchange.com/users/27526/deltaiv"
    },
    "is_answered": true,
    "view_count": 18473,
    "accepted_answer_id": 27695,
    "answer_count": 3,
    "score": 15,
    "last_activity_date": 1613234524,
    "creation_date": 1518337282,
    "last_edit_date": 1613234524,
    "question_id": 27694,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/27694/is-there-a-person-class-in-imagenet-are-there-any-classes-related-to-humans",
    "title": "Is there a person class in ImageNet? Are there any classes related to humans?",
    "body": "<p>If I look at one of the many sources for the Imagenet classes on the <a href=\"https://github.com/Lasagne/Recipes/blob/master/examples/resnet50/imagenet_classes.txt\" rel=\"noreferrer\">Internet</a> I cannot find a single class related to human beings (and no, <em>harvestman</em> is not someone who harvests, but it's what I knew as a daddy longlegs, a kind of spider :-). How is that possible? I would have at least expected a <code>person</code> class, and even something more specific such as <code>man</code>, <code>woman</code>, <code>toddler</code>, etc. Nothing of the sort. Why? Did <em>Fei-Fei Li</em> and her team make a conscious choice not to have people images in the database? Am I looking at the wrong file? For the sake of the question, we can consider the <code>ImageNet</code> versions from 2014 onwards.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "neural-network",
      "deep-learning",
      "tensorflow"
    ],
    "owner": {
      "account_id": 12563799,
      "reputation": 2485,
      "user_id": 50406,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Eq7j1.jpg?s=256",
      "display_name": "thanatoz",
      "link": "https://datascience.stackexchange.com/users/50406/thanatoz"
    },
    "is_answered": true,
    "view_count": 22516,
    "accepted_answer_id": 30217,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1554616677,
    "creation_date": 1523526152,
    "last_edit_date": 1554616677,
    "question_id": 30215,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/30215/what-is-one-hot-encoding-in-tensorflow",
    "title": "What is one hot encoding in tensorflow?",
    "body": "<p>I am currently doing a course in tensorflow in which they used tf.one_hot(indices, depth). Now I don't understand how these indices change into that binary sequence. </p>\n\n<p>Can somebody please explain to me the exact process???</p>\n"
  },
  {
    "tags": [
      "decision-trees"
    ],
    "owner": {
      "account_id": 23479,
      "reputation": 2381,
      "user_id": 15828,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/6OsC0.jpg?s=256",
      "display_name": "Imran",
      "link": "https://datascience.stackexchange.com/users/15828/imran"
    },
    "is_answered": true,
    "view_count": 747,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1603127073,
    "creation_date": 1528389010,
    "last_edit_date": 1528397102,
    "question_id": 32796,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/32796/can-gradient-boosted-trees-fit-any-function",
    "title": "Can gradient boosted trees fit any function?",
    "body": "<p>For neural networks we have the <a href=\"https://en.wikipedia.org/wiki/Universal_approximation_theorem\" rel=\"noreferrer\">universal approximation theorem</a> which states that neural networks can approximate any continuous function on a compact subset of $R^n$.</p>\n\n<p>Is there a similar result for gradient boosted trees? It seems reasonable since you can keep adding more branches, but I cannot find any formal discussion of the subject.</p>\n\n<p>EDIT: My question seems very similar to <a href=\"https://datascience.stackexchange.com/questions/9406/can-regression-trees-predict-continuously?rq=1\">\nCan regression trees predict continuously?</a>, though maybe not asking exactly the same thing. But see that question for relevant discussion.</p>\n"
  },
  {
    "tags": [
      "classification",
      "data-mining",
      "data-cleaning",
      "class-imbalance"
    ],
    "owner": {
      "account_id": 3908463,
      "reputation": 587,
      "user_id": 42498,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/3e71ffde6b6e8792aad31e05bda7f72e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Divyanshu Shekhar",
      "link": "https://datascience.stackexchange.com/users/42498/divyanshu-shekhar"
    },
    "is_answered": true,
    "view_count": 25463,
    "answer_count": 3,
    "score": 15,
    "last_activity_date": 1597636499,
    "creation_date": 1528888692,
    "question_id": 33076,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/33076/how-can-i-perform-stratified-sampling-for-multi-label-multi-class-classification",
    "title": "How can I perform stratified sampling for multi-label multi-class classification?",
    "body": "<p>I am asking this question for few reasons:</p>\n\n<ul>\n<li>The dataset in hand is imbalanced</li>\n<li><p>I used below code </p>\n\n<pre><code>x = dataset[['Message']] \ny = dataset[['Label1', 'Label2']]\ntrain_data, test_data = train_test_split(x, test_size = 0.1, stratify=y, random_state = 73)\n</code></pre>\n\n<p>but the error message that I am getting is <code>The least populated class in y has only 1 member, which is too few. The minimum number of labels for any class cannot be less than 2.</code> I removed those classes where the class count is &lt; 2 in each individual label. </p></li>\n</ul>\n\n<p>I am not sure why this error is popping. </p>\n\n<p>So, I am thinking to go to implement the stratified sampling myself. Here,  I need help in deciphering the cause of the problem here and the implementation of the stratified sampling in multi-label classification so that it works well for the individual batches too while training.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "regression",
      "cross-validation"
    ],
    "owner": {
      "account_id": 11245218,
      "reputation": 6136,
      "user_id": 50727,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/9Y5b3.jpg?s=256",
      "display_name": "David Masip",
      "link": "https://datascience.stackexchange.com/users/50727/david-masip"
    },
    "is_answered": true,
    "view_count": 7444,
    "accepted_answer_id": 33149,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1685465958,
    "creation_date": 1528966099,
    "last_edit_date": 1685465958,
    "question_id": 33140,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/33140/stratify-on-regression",
    "title": "Stratify on regression",
    "body": "<p>I have worked in classification problems, and stratified cross-validation is one of the most useful and simple techniques I've found. In that case, what it means is to build a training and validation set that have the same prorportions of classes of the target variable.</p>\n\n<p>I am wondering if such an strategy exists in regression. A simple approach would be to split the data in quartiles or deciles and make sure that the proportions of training and validation instances in the respective quartiles and deciles are the same. </p>\n\n<p>The question is: is there a standard way to do this? if so, is there an implementation in sklearn?</p>\n"
  },
  {
    "tags": [
      "gradient-descent",
      "mini-batch-gradient-descent"
    ],
    "owner": {
      "account_id": 4410342,
      "reputation": 2756,
      "user_id": 43077,
      "user_type": "registered",
      "accept_rate": 72,
      "profile_image": "https://i.sstatic.net/4IA0y.jpg?s=256",
      "display_name": "Kari",
      "link": "https://datascience.stackexchange.com/users/43077/kari"
    },
    "is_answered": true,
    "view_count": 3945,
    "accepted_answer_id": 33508,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1529661086,
    "creation_date": 1529646543,
    "last_edit_date": 1529647434,
    "question_id": 33489,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/33489/why-averaging-the-gradient-works-in-gradient-descent",
    "title": "Why averaging the gradient works in Gradient Descent?",
    "body": "<p>In Full-batch Gradient descent or Minibatch-GD we are getting gradient from several training examples. We then average them out to get a \"high-quality\" gradient, from several estimations and finally use it to correct the network, at once.</p>\n\n<p>But why does averaging the gathered gradient work? <strong>Each training sample ends up in a distant, completely separate location on the error-surface</strong></p>\n\n<p>Each sample would thus have its direction of a steepest descent pointing in different directions compared to other training examples. <strong>Averaging</strong> those directions should not make sense? Yet it works so well. In fact, the more examples we average, the more precise the correction will be: full-batch vs mini-batch approach</p>\n\n<p><a href=\"https://i.sstatic.net/3RPxn.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/3RPxn.jpg\" alt=\"enter image description here\"></a></p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "regression",
      "loss-function"
    ],
    "owner": {
      "account_id": 9899824,
      "reputation": 4429,
      "user_id": 44456,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/3Gtby.jpg?s=256",
      "display_name": "TwinPenguins",
      "link": "https://datascience.stackexchange.com/users/44456/twinpenguins"
    },
    "is_answered": true,
    "view_count": 42893,
    "accepted_answer_id": 36946,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1638689111,
    "creation_date": 1534285717,
    "last_edit_date": 1546030356,
    "question_id": 36945,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/36945/interpreting-the-root-mean-squared-error-rmse",
    "title": "Interpreting the Root Mean Squared Error (RMSE)!",
    "body": "<p>I read all about pros and cons of RMSE vs. other absolute errors namely mean absolute error (MAE). See the the following references:</p>\n\n<ul>\n<li><a href=\"https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d\" rel=\"noreferrer\">MAE and RMSEWhich Metric is Better?</a></li>\n<li><a href=\"https://people.duke.edu/~rnau/compare.htm\" rel=\"noreferrer\">What's the bottom line? How to compare models</a></li>\n<li>Or this nice <a href=\"http://www.eumetrain.org/data/4/451/english/msg/ver_cont_var/uos3/uos3_ko1.htm\" rel=\"noreferrer\">blogpost</a>, or this question in <a href=\"https://stats.stackexchange.com/questions/48267/mean-absolute-error-or-root-mean-squared-error\">stats.stackexchange</a> containing interesting responses, and this one in <a href=\"https://datascience.stackexchange.com/questions/6255/root-mean-square-error-significance-of-square-root\">datascience.stackexchange</a></li>\n</ul>\n\n<p><strong>Still I can not get my head around something about RMSE:</strong></p>\n\n<p>Scenario: Let's say we have a regressor for predicting house prices with a MAE of 20.5\\$ and a RMSE of 24.5\\$. Based on MAE, I can certainly interpret that the average difference between the predicted and the actual price is 20.5\\$. How can I interpret RMSE? Can we still safely say the predicted and the actual price are off by 24.5\\$ at the same time base on RMSE (upper-bound of prediction error)?</p>\n\n<p>In the first medium post, it says: </p>\n\n<blockquote>\n  <p>RMSE does not describe average error alone and has other implications\n  that are more difficult to tease out and understand.</p>\n</blockquote>\n\n<p>It confuses me a little. And I could not find any reliable reference to also clearly state that one can safely interpret RSME as one does MAE. Is RMSE is simply a only mathematically more convenient for optimization etc., and we are better off with MAE for the interpretation?</p>\n\n<p>Any detailed explanation is highly appreciated.</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "optimization",
      "genetic-algorithms"
    ],
    "owner": {
      "account_id": 14354942,
      "reputation": 413,
      "user_id": 59160,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/80af13069659d260829a9277ff4dd801?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "cat91",
      "link": "https://datascience.stackexchange.com/users/59160/cat91"
    },
    "is_answered": true,
    "view_count": 4043,
    "accepted_answer_id": 38325,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1537540979,
    "creation_date": 1537086889,
    "question_id": 38321,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/38321/why-arent-genetic-algorithms-used-for-optimizing-neural-networks",
    "title": "Why aren&#39;t Genetic Algorithms used for optimizing neural networks?",
    "body": "<p>From my understanding, Genetic Algorithms are powerful tools for multi-objective optimization. </p>\n\n<p>Furthermore, training Neural Networks (especially deep ones) is hard and has many issues (non-convex cost functions - local minima, vanishing and exploding gradients etc.).</p>\n\n<p>Also I'm that conceptually training a NN with GA is feasible. I was wondering, why aren't they used in practice? Is it a matter of performance?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "scikit-learn",
      "classifier",
      "mlp"
    ],
    "owner": {
      "account_id": 14506410,
      "reputation": 611,
      "user_id": 62528,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/674e3f3f17f0061a3c8774eb2b593dd4?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "jochen6677",
      "link": "https://datascience.stackexchange.com/users/62528/jochen6677"
    },
    "is_answered": true,
    "view_count": 28133,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1548706306,
    "creation_date": 1548681431,
    "question_id": 44700,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/44700/how-do-i-get-the-feature-importace-for-a-mlpclassifier",
    "title": "How do I get the feature importace for a MLPClassifier?",
    "body": "<p>I use the MLPClassifier from scikit learn. I have about 20 features. Is there a scikit method to get the feature importance? I found </p>\n\n<p>clf.feature_importances_</p>\n\n<p>but it seems that it only exists for decision trees.</p>\n"
  },
  {
    "tags": [
      "seaborn"
    ],
    "owner": {
      "account_id": 3873970,
      "reputation": 829,
      "user_id": 68313,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2c965e9fbb680e2527285a75b1712497?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "mon",
      "link": "https://datascience.stackexchange.com/users/68313/mon"
    },
    "is_answered": true,
    "view_count": 79642,
    "accepted_answer_id": 46119,
    "answer_count": 4,
    "score": 15,
    "last_activity_date": 1653029722,
    "creation_date": 1550976627,
    "question_id": 46117,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/46117/meaning-of-hue-in-seaborn-barplot",
    "title": "Meaning of &#39;hue&quot; in seaborn barplot",
    "body": "<p>Seaborn <a href=\"https://seaborn.pydata.org/generated/seaborn.barplot.html\" rel=\"noreferrer\">barplot</a> has three parameters.</p>\n\n<blockquote>\n  <p>x, y, hue : names of variables in data or vector data, optional</p>\n</blockquote>\n\n<h2>Question</h2>\n\n<p>What is hue? It seems the attribute to plot but why it is called \"hue\" because when I googled, the result is about color?</p>\n\n<h2>Google</h2>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Hue\" rel=\"noreferrer\">Hue - Wikipedia</a></p>\n\n<blockquote>\n  <p>Hue is one of the main properties (called color appearance parameters) of a color, defined technically (in the CIECAM02 model)</p>\n</blockquote>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "optimization",
      "gradient-descent"
    ],
    "owner": {
      "account_id": 12190501,
      "reputation": 871,
      "user_id": 41591,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/82CEl.jpg?s=256",
      "display_name": "rawwar",
      "link": "https://datascience.stackexchange.com/users/41591/rawwar"
    },
    "is_answered": true,
    "view_count": 4508,
    "accepted_answer_id": 47160,
    "answer_count": 4,
    "score": 15,
    "last_activity_date": 1552410633,
    "creation_date": 1552385055,
    "last_edit_date": 1552399426,
    "question_id": 47142,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/47142/is-gradient-descent-central-to-every-optimizer",
    "title": "Is Gradient Descent central to every optimizer?",
    "body": "<p>I want to know whether Gradient descent is the main algorithm used in optimizers like Adam, Adagrad, RMSProp and several other optimizers. </p>\n"
  },
  {
    "tags": [
      "gan",
      "databases"
    ],
    "owner": {
      "account_id": 4060371,
      "reputation": 724,
      "user_id": 62938,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/84cb37913d86678e88cb670951f29b31?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Nagabhushan S N",
      "link": "https://datascience.stackexchange.com/users/62938/nagabhushan-s-n"
    },
    "is_answered": true,
    "view_count": 17756,
    "accepted_answer_id": 47462,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1552814987,
    "creation_date": 1552809530,
    "question_id": 47458,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/47458/what-is-the-difference-between-imagenet-and-imagenet1k-how-to-download-it",
    "title": "What is the difference between ImageNet and ImageNet1k? How to download it?",
    "body": "<p>Some papers mention just ImageNet and some papers mention ImageNet 1k database?\nWhat is the difference between these 2? Are they same or is the latter one subset of the former one?</p>\n\n<p>I'm working on Generative Adversarial Nets. I wanted to train it on ImageNet Database. How to download ImageNet 1k? I went to <a href=\"http://www.image-net.org/download-images\" rel=\"noreferrer\">ImageNet</a> site and created &amp; verified my account. Then there were several links. Which one to select?</p>\n\n<p>Thanks!</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "lightgbm"
    ],
    "owner": {
      "account_id": 5149646,
      "reputation": 261,
      "user_id": 67240,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/9r8R0.png?s=256",
      "display_name": "Duy Bui",
      "link": "https://datascience.stackexchange.com/users/67240/duy-bui"
    },
    "is_answered": true,
    "view_count": 9050,
    "answer_count": 4,
    "score": 15,
    "last_activity_date": 1630814797,
    "creation_date": 1556644141,
    "last_edit_date": 1630814797,
    "question_id": 51188,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/51188/lightgbm-gives-different-results-metrics-depending-on-the-columns-order",
    "title": "LightGBM gives different results (metrics) depending on the columns order",
    "body": "<p>I have two nearly identical datasets A and B which differ only in terms of columns ordering. I then train a LightGBM model on each of the two datasets with the following steps:</p>\n<ol>\n<li>Divide each dataset into training and testing (use the same random seed and ratio for both A and B)</li>\n<li>Leave the hyperparameters as pretty much defaults</li>\n<li>Set <code>random_state</code> to a fixed number (for reproduction)</li>\n<li>Tune the <code>learning_rate</code> using a Grid Search</li>\n<li>Train a LightGBM model on the training set and test it on the\ntesting set</li>\n<li>Learning rate with the best performance on the testing set will be\nchosen</li>\n</ol>\n<p>The output of the two models based on these two datasets is very different, which makes me think that the ordering of columns affects the performance of LightGBM models.</p>\n<p>Do you know why this can be the case?</p>\n"
  },
  {
    "tags": [
      "python",
      "scikit-learn",
      "pandas",
      "missing-data"
    ],
    "owner": {
      "account_id": 12244890,
      "reputation": 195,
      "user_id": 74140,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/95aad2d23fedae64a493a244ea503bbd?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Rupali Singh",
      "link": "https://datascience.stackexchange.com/users/74140/rupali-singh"
    },
    "is_answered": true,
    "view_count": 49038,
    "protected_date": 1594322433,
    "accepted_answer_id": 51891,
    "answer_count": 4,
    "score": 15,
    "last_activity_date": 1594316289,
    "creation_date": 1557756112,
    "question_id": 51890,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/51890/how-to-use-simpleimputer-class-to-replace-missing-values-with-mean-values-using",
    "title": "How to use SimpleImputer Class to replace missing values with mean values using Python?",
    "body": "<p>This is my code</p>\n\n<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n#Importing Dataset\ndataset = pd.read_csv('C:/Users/Rupali Singh/Desktop/ML A-Z/Machine Learning A-Z Template Folder/Part 1 - Data Preprocessing/Data.csv')\nprint(dataset)\nX = dataset.iloc[:, :-1].values\nY = dataset.iloc[:, 3].values\n#Missing Data\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values= np.nan, strategy='mean')\nX.fit[:, 1:3] = imputer.fit_transform(X[:, 1:3])\nprint(X)\n</code></pre>\n\n<p>My data set:</p>\n\n<pre><code>Country   Age   Salary Purchased\n0   France  44.0  72000.0        No\n1    Spain  27.0  48000.0       Yes\n2  Germany  30.0  54000.0        No\n3    Spain  38.0  61000.0        No\n4  Germany  40.0      NaN       Yes\n5   France  35.0  58000.0       Yes\n6    Spain   NaN  52000.0        No\n7   France  48.0  79000.0       Yes\n8  Germany  50.0  83000.0        No\n9   France  37.0  67000.0       Yes\n</code></pre>\n\n<p>Error Message:</p>\n\n<pre><code>File \"C:/Users/Rupali Singh/PycharmProjects/Machine_Learning/data_preprocessing_Template.py\", line 15, in &lt;module&gt;\n    X.fit[:, 1:3] = imputer.fit_transform(X[:, 1:3])\nAttributeError: 'numpy.ndarray' object has no attribute 'fit'\n</code></pre>\n"
  },
  {
    "tags": [
      "python",
      "nlp",
      "preprocessing",
      "bert",
      "transformer"
    ],
    "owner": {
      "account_id": 9899824,
      "reputation": 4429,
      "user_id": 44456,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/3Gtby.jpg?s=256",
      "display_name": "TwinPenguins",
      "link": "https://datascience.stackexchange.com/users/44456/twinpenguins"
    },
    "is_answered": true,
    "view_count": 8932,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1630715503,
    "creation_date": 1573194528,
    "last_edit_date": 1602586412,
    "question_id": 62862,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/62862/preprocessing-for-text-classification-in-transformer-models-bert-variants",
    "title": "Preprocessing for Text Classification in Transformer Models (BERT variants)",
    "body": "<p>This might be silly to ask, but I am wondering if one should carry out the conventional text preprocessing steps for training one of the transformer models?</p>\n<p>I remember for training a Word2Vec or Glove, we needed to perform an extensive text cleaning like: tokenize, remove stopwords, remove punctuations, stemming or lemmatization and more. However, during last few days I have had a quick jump into transformer models (fascinated btw), and what I have noticed that most of these models have a built-in tokenizer (cool), but none of the demos, examples, or tutorials are performing any of the these text preprocessing steps. You may take <a href=\"https://github.com/kaushaltrivedi/fast-bert\" rel=\"noreferrer\">fast-bert</a> for instance, there are no text preprocessing involved for the demos (maybe it is just a demo), but at <a href=\"https://github.com/kaushaltrivedi/fast-bert#5-model-inference\" rel=\"noreferrer\">inference</a> the whole sentences are passed without any cleaning:</p>\n<pre><code>texts = ['I really love the Netflix original movies',\n         'this movie is not worth watching']\npredictions = learner.predict_batch(texts)\n</code></pre>\n<p>The same is true for the <a href=\"https://github.com/huggingface/transformers\" rel=\"noreferrer\">original transformer</a> by HuggingFace. Or many tutorials that I have looked at (take <a href=\"https://github.com/kaushaltrivedi/bert-toxic-comments-multilabel/blob/master/toxic-bert-multilabel-classification.ipynb\" rel=\"noreferrer\">this</a> or <a href=\"https://towardsdatascience.com/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d\" rel=\"noreferrer\">another one</a>). I can imagine that depending on the task this might not be required, e.g. next work prediction or machine translation and more. More importantly I think this is part of the contextual-based approach that these models offer (that is the innovation so to say) that are meant to keep most of the text and we may obtain a minimum but still good representation of the each token (out of vocabulary word). Borrowed from <a href=\"https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d\" rel=\"noreferrer\">medium article</a> by HuggingFace:</p>\n<blockquote>\n<p><strong>Tokenisation</strong>\nBERT-Base, uncased uses a vocabulary of 30,522 words. The\nprocesses of tokenisation involves splitting the input text into list\nof tokens that are available in the vocabulary. In order to deal with\nthe words not available in the vocabulary, BERT uses a technique\ncalled BPE based WordPiece tokenisation. In this approach an out of\nvocabulary word is progressively split into subwords and the word is\nthen represented by a group of subwords. Since the subwords are part\nof the vocabulary, we have learned representations an context for\nthese subwords and the context of the word is simply the combination\nof the context of the subwords.</p>\n</blockquote>\n<p>But does that hold true for tasks like multi-label text classification? In my use case the text is full of not useful stopwords, punctuation, characters and abbreviations and it is multi-label text classification as mentioned earlier. And in fact the prediction accuracy is not good (after a few rounds of training using <a href=\"https://github.com/kaushaltrivedi/fast-bert\" rel=\"noreferrer\">fast-bert</a>). What do I miss here?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "feature-engineering",
      "categorical-data",
      "encoding"
    ],
    "owner": {
      "account_id": 11245218,
      "reputation": 6136,
      "user_id": 50727,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/9Y5b3.jpg?s=256",
      "display_name": "David Masip",
      "link": "https://datascience.stackexchange.com/users/50727/david-masip"
    },
    "is_answered": true,
    "view_count": 8650,
    "answer_count": 3,
    "score": 15,
    "last_activity_date": 1710586112,
    "creation_date": 1574696196,
    "last_edit_date": 1641651088,
    "question_id": 63749,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/63749/why-does-frequency-encoding-work",
    "title": "Why does frequency encoding work?",
    "body": "<p>Frequency encoding is a widely used technique in Kaggle competitions, and many times proves to be a very reasonable way of dealing with categorical features with high cardinality. I really don't understand why it works. </p>\n\n<p>Does it work in very specific cases where frequencies are correlated with the target or is it more general? What's the rationale behind it?</p>\n"
  },
  {
    "tags": [
      "google",
      "colab"
    ],
    "owner": {
      "account_id": 10170447,
      "reputation": 253,
      "user_id": 82767,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/358f4c00579f28902be76a696499c4b2?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Georgi Stoyanov",
      "link": "https://datascience.stackexchange.com/users/82767/georgi-stoyanov"
    },
    "is_answered": true,
    "view_count": 25617,
    "protected_date": 1674419281,
    "accepted_answer_id": 104093,
    "answer_count": 3,
    "score": 15,
    "last_activity_date": 1674400448,
    "creation_date": 1575903593,
    "question_id": 64486,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/64486/how-to-automatically-mount-my-google-drive-to-google-colab",
    "title": "How to automatically mount my Google Drive to Google Colab",
    "body": "<p>I have recently discovered Google Colab and I am wondering if there is an option to permanently authorize Google Colab to access and mount my Google Drive. </p>\n\n<pre><code>from google.colab import drive\ndrive.mount('/content/drive')\nGo to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=xxx\nEnter your authorization code:\n\nMounted at /content/drive\n</code></pre>\n\n<p>Right now, I need to all the time insert manually the authorization code and I want when I start a new Google Colab to have my drive mounted. </p>\n\n<p>Is there an option to do that or I should execute this all the time manually? </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "gradient-descent",
      "gradient"
    ],
    "owner": {
      "account_id": 18216844,
      "reputation": 189,
      "user_id": 94320,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/8965570f3f76135787e0ad8df043750b?s=256&d=identicon&r=PG",
      "display_name": "yashdk",
      "link": "https://datascience.stackexchange.com/users/94320/yashdk"
    },
    "is_answered": true,
    "view_count": 26959,
    "answer_count": 4,
    "score": 15,
    "last_activity_date": 1723039707,
    "creation_date": 1586926824,
    "last_edit_date": 1723039707,
    "question_id": 72351,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/72351/how-to-prevent-vanishing-gradient-or-exploding-gradient",
    "title": "How to prevent vanishing gradient or exploding gradient?",
    "body": "<p>What's causing the vanishing gradient or exploding gradient, and what are the measures to be taken to prevent it?</p>\n"
  },
  {
    "tags": [
      "classification",
      "accuracy",
      "metric",
      "f1score"
    ],
    "owner": {
      "account_id": 9954432,
      "reputation": 287,
      "user_id": 64471,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/GjL5q.jpg?s=256",
      "display_name": "Ric S",
      "link": "https://datascience.stackexchange.com/users/64471/ric-s"
    },
    "is_answered": true,
    "view_count": 10969,
    "accepted_answer_id": 73992,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1644451108,
    "creation_date": 1589210962,
    "question_id": 73974,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/73974/balanced-accuracy-vs-f1-score",
    "title": "Balanced Accuracy vs. F1 Score",
    "body": "<p>I've read plenty of online posts with clear explanations about the difference between accuracy and F1 score in a binary classification context. However, when I came across the concept of <em>balanced accuracy</em>, explained e.g. in the following image (<a href=\"http://mvpa.blogspot.com/2015/12/balanced-accuracy-what-and-why.html\" rel=\"noreferrer\">source</a>) or in this scikit-learn <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html?highlight=balanced_accuracy_score\" rel=\"noreferrer\">page</a>, I was a bit puzzled as I was trying to compare it with F1 score.</p>\n\n<p><a href=\"https://i.sstatic.net/7PfAE.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/7PfAE.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>I know that it is probably impossible to establish which is better between balanced accuracy and F1 score as it could be situation-dependent, but I would like to understand some pros/cons of the two performance metrics, as well as some applications in which one could be more suitable and useful than the other (especially in an imbalanced binary classification context).</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "nlp",
      "computer-vision",
      "attention-mechanism"
    ],
    "owner": {
      "account_id": 20866769,
      "reputation": 251,
      "user_id": 113319,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ffd20559eda003962bd55e47378dea51?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Shir",
      "link": "https://datascience.stackexchange.com/users/113319/shir"
    },
    "is_answered": true,
    "view_count": 15596,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1654354965,
    "creation_date": 1615744860,
    "question_id": 90649,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/90649/class-token-in-vit-and-bert",
    "title": "Class token in ViT and BERT",
    "body": "<p>I'm trying to understand the architecture of the ViT Paper, and noticed they use a CLASS token like in BERT.</p>\n<p>To the best of my understanding this token is used to gather knowledge of the entire class, and is then solely used to predict the class of the image. My question is  why does this token exist as input in all the transformer blocks and is treated the same as the word / patches tokens?</p>\n<p>Treating the class token like the rest of the tokens means other tokens can attend to it. I'd expect that the class token will be able to attend other tokens while they could not attend it.</p>\n<p>Also, specifically in ViT, why does the class token receive positional encodings? It represents the entire class and thus doesn't have any specific location.</p>\n<p>Thanks!</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "machine-learning-model",
      "accuracy",
      "model-evaluations"
    ],
    "owner": {
      "account_id": 16707460,
      "reputation": 334,
      "user_id": 119136,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/743571ff47ba100670d94ce757b72013?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "PicaR",
      "link": "https://datascience.stackexchange.com/users/119136/picar"
    },
    "is_answered": true,
    "view_count": 8287,
    "accepted_answer_id": 110131,
    "answer_count": 3,
    "score": 15,
    "last_activity_date": 1717515082,
    "creation_date": 1650270901,
    "last_edit_date": 1650900540,
    "question_id": 110124,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/110124/what-are-the-disadvantages-of-accuracy",
    "title": "What are the disadvantages of accuracy?",
    "body": "<p>I have been reading about evaluating a model with accuracy only and I have found some disadvantages. Among them, I read that it equates all errors. How could this problem be solved? Maybe assigning costs to each type of failure? Thank you very much for your help.</p>\n"
  },
  {
    "tags": [
      "descriptive-statistics",
      "search",
      "anonymization",
      "counts",
      "privacy"
    ],
    "migrated_from": {
      "other_site": {
        "aliases": [
          "https://itsecurity.stackexchange.com"
        ],
        "styling": {
          "tag_background_color": "#FFF",
          "tag_foreground_color": "#000",
          "link_color": "#0077CC"
        },
        "related_sites": [
          {
            "relation": "meta",
            "api_site_parameter": "security.meta",
            "site_url": "https://security.meta.stackexchange.com",
            "name": "Information Security Meta Stack Exchange"
          },
          {
            "relation": "chat",
            "site_url": "https://chat.stackexchange.com?tab=site&host=security.stackexchange.com",
            "name": "Chat Stack Exchange"
          }
        ],
        "launch_date": 1310490000,
        "open_beta_date": 1290106800,
        "closed_beta_date": 1289502000,
        "site_state": "normal",
        "high_resolution_icon_url": "https://cdn.sstatic.net/Sites/security/Img/apple-touch-icon@2.png",
        "favicon_url": "https://cdn.sstatic.net/Sites/security/Img/favicon.ico",
        "icon_url": "https://cdn.sstatic.net/Sites/security/Img/apple-touch-icon.png",
        "audience": "information security professionals",
        "site_url": "https://security.stackexchange.com",
        "api_site_parameter": "security",
        "logo_url": "https://cdn.sstatic.net/Sites/security/Img/logo.png",
        "name": "Information Security",
        "site_type": "main_site"
      },
      "on_date": 1679565451,
      "question_id": 269288
    },
    "owner": {
      "account_id": 277724,
      "reputation": 253,
      "user_id": 148239,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/88466b7d207f49f462681dd105891a27?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "mal",
      "link": "https://datascience.stackexchange.com/users/148239/mal"
    },
    "is_answered": true,
    "view_count": 1922,
    "accepted_answer_id": 120432,
    "answer_count": 5,
    "score": 15,
    "last_activity_date": 1687803021,
    "creation_date": 1679561866,
    "last_edit_date": 1687803021,
    "question_id": 120426,
    "link": "https://datascience.stackexchange.com/questions/120426/how-can-i-ensure-anonymity-with-queries-to-small-datasets",
    "title": "How can I ensure anonymity with queries to small datasets?",
    "body": "<p>I'm building a service that will contain personal data relating to real people.\nInitially the dataset will be quite small, and as such it may be possible to identify individuals if the search parameters are narrowed sufficiently.\nAn example of a filter that can be applied is app version - android, ios, etc... This data will be displayed as sums and percentages.\nFor example:</p>\n<p>Android users:</p>\n<pre><code>{\n  &quot;users&quot; : 124,\n  &quot;percentage&quot; : 23\n}\n</code></pre>\n<p>There will be many filter types, including date ranges, gender, age, etc....<br />\nHow can I ensure reasonable anonymity?<br />\nI am considering requiring a minimum size result set in order help with this, for example if the query returns less than 25 results the service won't return the data, but will return some sort of notification about minimum dataset size.<br />\nWhat is a good way to approach this problem?</p>\n"
  },
  {
    "tags": [
      "bigdata",
      "statistics"
    ],
    "owner": {
      "account_id": 4070243,
      "reputation": 143,
      "user_id": 179,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/5c618093995016eb743df1564ba02546?s=256&d=identicon&r=PG",
      "display_name": "user179",
      "link": "https://datascience.stackexchange.com/users/179/user179"
    },
    "is_answered": true,
    "view_count": 796,
    "accepted_answer_id": 84,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1400142347,
    "creation_date": 1400105557,
    "question_id": 71,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/71/when-are-p-values-deceptive",
    "title": "When are p-values deceptive?",
    "body": "<p>What are the data conditions that we should watch out for, where p-values may not be the best way of deciding statistical significance?  Are there specific problem types that fall into this category?</p>\n"
  },
  {
    "tags": [
      "data-mining",
      "bigdata",
      "usecase"
    ],
    "owner": {
      "account_id": 1427311,
      "reputation": 241,
      "user_id": 496,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/1fdc7b4b3da278b1c1c5b4c4b250bafb?s=256&d=identicon&r=PG",
      "display_name": "Brown_Dynamite",
      "link": "https://datascience.stackexchange.com/users/496/brown-dynamite"
    },
    "is_answered": true,
    "view_count": 3611,
    "protected_date": 1471332183,
    "answer_count": 4,
    "score": 14,
    "last_activity_date": 1597596872,
    "creation_date": 1402466865,
    "last_edit_date": 1471430501,
    "question_id": 307,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/307/big-data-case-study-or-use-case-example",
    "title": "Big data case study or use case example",
    "body": "<p>I have read lot of blogs\\article on how different type of industries are using Big Data Analytic. But most of these article fails to mention</p>\n\n<ol>\n<li>What kinda data these companies used. What was the size of the data</li>\n<li>What kinda of tools technologies they used to process the data</li>\n<li>What was the problem they were facing and how the insight they got the data helped them to resolve the issue.</li>\n<li>How they selected the tool\\technology to suit their need.</li>\n<li>What kinda pattern they identified from the data &amp; what kind of patterns they were looking from the data.</li>\n</ol>\n\n<p>I wonder if someone can provide me answer to all these questions or a link which at-least answer some of the the questions. I am looking for real world example. </p>\n\n<p>It would be great if someone share how finance industry is making use of Big Data Analytic.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "bigdata",
      "efficiency",
      "scalability",
      "distributed"
    ],
    "owner": {
      "account_id": 4507412,
      "reputation": 143,
      "user_id": 913,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/735d5d5563f7fb3e2adabfb3834c46e8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "chrshmmmr",
      "link": "https://datascience.stackexchange.com/users/913/chrshmmmr"
    },
    "is_answered": true,
    "view_count": 1624,
    "accepted_answer_id": 525,
    "answer_count": 4,
    "score": 14,
    "last_activity_date": 1403530611,
    "creation_date": 1403001442,
    "last_edit_date": 1403012267,
    "question_id": 430,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/430/looking-for-example-infrastructure-stacks-workflows-pipelines",
    "title": "Looking for example infrastructure stacks/workflows/pipelines",
    "body": "<p>I'm trying to understand how all the \"big data\" components play together in a real world use case, e.g. hadoop, monogodb/nosql, storm, kafka, ... I know that this is quite a wide range of tools used for different types, but I'd like to get to know more about their interaction in applications, e.g. thinking machine learning for an app, webapp, online shop.</p>\n\n<p>I have vistors/session, transaction data etc and store that; but if I want to make recommendations on the fly, I can't run slow map/reduce jobs for that on some big database of logs I have. Where can I learn more about the infrastructure aspects? I think I can use most of the tools on their own, but plugging them into each other seems to be an art of its own.</p>\n\n<p>Are there any public examples/use cases etc available? I understand that the individual pipelines strongly depend on the use case and the user, but just examples will probably be very useful to me.</p>\n"
  },
  {
    "tags": [
      "statistics",
      "dataset"
    ],
    "owner": {
      "account_id": 2871380,
      "reputation": 739,
      "user_id": 728,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/37fab8969beac8475edbef85f84367c9?s=256&d=identicon&r=PG",
      "display_name": "Jack Twain",
      "link": "https://datascience.stackexchange.com/users/728/jack-twain"
    },
    "is_answered": true,
    "view_count": 893,
    "answer_count": 6,
    "score": 14,
    "last_activity_date": 1615677824,
    "creation_date": 1403594997,
    "question_id": 554,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/554/datasets-understanding-best-practices",
    "title": "Datasets understanding best practices",
    "body": "<p>I am a CS master student in data mining. My supervisor once told me that before I run any classifier or do anything with a dataset I must fully understand the data and make sure that the data is clean and correct.</p>\n\n<p>My questions:</p>\n\n<ul>\n<li><p>What are the best practices to understand a dataset (high dimensional with numerical and nominal attributes)?</p></li>\n<li><p>Practices to make sure the dataset is clean?</p></li>\n<li><p>Practices to make sure the dataset doesn't have wrong values or so?</p></li>\n</ul>\n"
  },
  {
    "tags": [
      "dataset"
    ],
    "owner": {
      "account_id": 2901835,
      "reputation": 4125,
      "user_id": 793,
      "user_type": "registered",
      "accept_rate": 60,
      "profile_image": "https://i.sstatic.net/N5kqP.jpg?s=256",
      "display_name": "tejaskhot",
      "link": "https://datascience.stackexchange.com/users/793/tejaskhot"
    },
    "is_answered": true,
    "view_count": 32036,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1617540197,
    "creation_date": 1403719574,
    "last_edit_date": 1437568471,
    "question_id": 587,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/587/where-can-i-download-historical-market-capitalization-and-daily-turnover-data-fo",
    "title": "Where can I download historical market capitalization and daily turnover data for stocks?",
    "body": "<p>There are plenty of sources which provide the historical stock data but they only provide the OHLC fields along with volume and adjusted close. Also a couple of sources I found provide market cap data sets but they're restricted to US stocks. Yahoo Finance provides this data online but there's no option to download it ( or none I am aware of ). </p>\n\n<ul>\n<li>Where can I download this data for stocks belonging to various top stock exchanges across countries by using their ticker name ?</li>\n<li>Is there some way to download it via Yahoo Finance or Google Finance ?</li>\n</ul>\n\n<p>I need data for the last decade or so and hence need some script or API which would do this.</p>\n"
  },
  {
    "tags": [
      "bigdata",
      "python"
    ],
    "owner": {
      "account_id": 2360984,
      "reputation": 1854,
      "user_id": 890,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0c86e226759ee03937af84103fd1b1ab?s=256&d=identicon&r=PG",
      "display_name": "ragingSloth",
      "link": "https://datascience.stackexchange.com/users/890/ragingsloth"
    },
    "is_answered": true,
    "view_count": 17357,
    "accepted_answer_id": 781,
    "answer_count": 9,
    "score": 14,
    "last_activity_date": 1660816772,
    "creation_date": 1405722888,
    "question_id": 778,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/778/is-python-suitable-for-big-data",
    "title": "Is Python suitable for big data",
    "body": "<p>I read in this post <a href=\"https://datascience.stackexchange.com/questions/41/is-the-r-language-suitable-for-big-data\">Is the R language suitable for Big Data</a> that big data constitutes <code>5TB</code>, and while it does a good job of providing information about the feasibility of working with this type of data in <code>R</code> it provides very little information about <code>Python</code>. I was wondering if <code>Python</code> can work with this much data as well. </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "parsing"
    ],
    "owner": {
      "account_id": 2193343,
      "reputation": 341,
      "user_id": 2920,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4b0c28eb357bb2ad8dee40d974871340?s=256&d=identicon&r=PG",
      "display_name": "Erich Schubert",
      "link": "https://datascience.stackexchange.com/users/2920/erich-schubert"
    },
    "is_answered": true,
    "view_count": 5473,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1596711830,
    "creation_date": 1407830223,
    "last_edit_date": 1407859584,
    "question_id": 963,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/963/sentiment-data-for-emoji",
    "title": "Sentiment data for Emoji",
    "body": "<p>For experimenting we'd like to use the <a href=\"https://en.wikipedia.org/wiki/Emoji\">Emoji</a> embedded in many Tweets as a ground truth/training data for simple quantitative senitment analysis. Tweets are usually too unstructured for NLP to work well.</p>\n\n<p>Anyway, there are 722 Emoji in Unicode 6.0, and probably another 250 will be added in Unicode 7.0.</p>\n\n<p>Is there a database (like e.g. SentiWordNet) that contains sentiment annotations for them?</p>\n\n<p>(Note that SentiWordNet does allow for <em>ambiguous</em> meanings, too. Consider e.g. <a href=\"http://sentiwordnet.isti.cnr.it/search.php?q=funny\">funny</a>, which is not just positive: \"this tastes funny\" is probably not positive... same will hold for <code>;-)</code> for example. But I don't think this is harder for Emoji than it is for regular words...)</p>\n\n<p>Also, if you have experience with using them for sentiment analysis, I'd be interested to hear.</p>\n"
  },
  {
    "tags": [
      "machine-learning"
    ],
    "owner": {
      "account_id": 1541177,
      "reputation": 321,
      "user_id": 2487,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/fc664f117f44591e9ea4bd7e7ab56127?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "the911s",
      "link": "https://datascience.stackexchange.com/users/2487/the911s"
    },
    "is_answered": true,
    "view_count": 2068,
    "accepted_answer_id": 1097,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1546050342,
    "creation_date": 1410211526,
    "last_edit_date": 1546050342,
    "question_id": 1092,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/1092/machine-learning-libraries-for-ruby",
    "title": "Machine learning libraries for Ruby",
    "body": "<p>Are there any machine learning libraries for Ruby that are relatively complete (including a wide variety of algorithms for supervised and unsupervised learning), robustly tested, and well-documented? I love Python's <a href=\"http://scikit-learn.org/\" rel=\"nofollow noreferrer\">scikit-learn</a> for its incredible documentation, but a client would prefer to write the code in Ruby since that's what they're familiar with.</p>\n\n<p>Ideally I am looking for a library or set of libraries which, like <code>scikit</code> and <code>numpy</code>, can implement a wide variety of data structures like sparse matrices, as well as learners.</p>\n\n<p>Some examples of things we'll need to do are binary classification using SVMs, and  implementing bag of words models which we hope to concatenate with arbitrary numeric data, as described in <a href=\"https://stackoverflow.com/q/20106940/1435804\">this StackOverflow post</a>.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-mining",
      "classification",
      "naive-bayes-classifier"
    ],
    "owner": {
      "account_id": 3115002,
      "reputation": 143,
      "user_id": 6451,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e72972e0968252435cfb2c03649e17af?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "matsair",
      "link": "https://datascience.stackexchange.com/users/6451/matsair"
    },
    "is_answered": true,
    "view_count": 25469,
    "accepted_answer_id": 3717,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1670069976,
    "creation_date": 1418735275,
    "last_edit_date": 1597679705,
    "question_id": 3711,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/3711/how-does-the-naive-bayes-classifier-handle-missing-data-in-training",
    "title": "How does the naive Bayes classifier handle missing data in training?",
    "body": "<p>Naive Bayes apparently handles missing data differently, depending on whether they exist in training or testing/classification instances.</p>\n<p>When classifying instances, the attribute with the missing value is simply not included in the probability calculation (<a href=\"http://www.inf.ed.ac.uk/teaching/courses/iaml/slides/naive-2x2.pdf\" rel=\"noreferrer\">reference</a>)</p>\n<p>In training, <em>the instance</em> [with the missing data] <em>is not included in frequency count for attribute value-class combination</em>. (<a href=\"http://www.csee.wvu.edu/%7Etimm/cs591o/old/BasicMethods.html\" rel=\"noreferrer\">reference</a>)</p>\n<p>Does that mean that particular training record simply isn't included in the training phase? Or does it mean something else?</p>\n"
  },
  {
    "tags": [
      "clustering",
      "k-means"
    ],
    "owner": {
      "account_id": 2048845,
      "reputation": 3152,
      "user_id": 26,
      "user_type": "registered",
      "accept_rate": 60,
      "profile_image": "https://i.sstatic.net/S6fiE.png?s=256",
      "display_name": "Alex I",
      "link": "https://datascience.stackexchange.com/users/26/alex-i"
    },
    "is_answered": true,
    "view_count": 2733,
    "accepted_answer_id": 5780,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1615665897,
    "creation_date": 1431327232,
    "last_edit_date": 1615665897,
    "question_id": 5746,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/5746/fast-k-means-like-algorithm-for-1010-points",
    "title": "Fast k-means like algorithm for $10^{10}$ points?",
    "body": "<p>I am looking to do k-means clustering on a set of 10-dimensional points.  The catch: <strong>there are <span class=\"math-container\">$10^{10}$</span> points</strong>.</p>\n<p>I am looking for just the center and size of the largest clusters (let's say 10 to 100 clusters); I don't care about what cluster each point ends up in.  Using k-means specifically is not important; I am just looking for a similar effect, any approximate k-means or related algorithm would be great (minibatch-SGD means, ...).  Since GMM is in a sense the same problem as k-means, doing GMM on the same size data is also interesting.</p>\n<p>At this scale, subsampling the data probably doesn't change the result significantly: the odds of finding the same top 10 clusters using a 1/10000th sample of the data are very good.  But even then, that is a <span class=\"math-container\">$10^6$</span> point problem which is on/beyond the edge of tractable.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "regression",
      "feature-selection",
      "feature-construction",
      "missing-data"
    ],
    "owner": {
      "account_id": 302279,
      "reputation": 2500,
      "user_id": 122,
      "user_type": "registered",
      "accept_rate": 62,
      "profile_image": "https://www.gravatar.com/avatar/0e9087f2672b0e4f28d91266acf9ce57?s=256&d=identicon&r=PG",
      "display_name": "alvas",
      "link": "https://datascience.stackexchange.com/users/122/alvas"
    },
    "is_answered": true,
    "view_count": 6646,
    "accepted_answer_id": 9933,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1536179108,
    "creation_date": 1447837947,
    "question_id": 8883,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/8883/what-to-do-when-testing-data-has-less-features-than-training-data",
    "title": "What to do when testing data has less features than training data?",
    "body": "<p>Let's say we are predicting the sales of a shop and my training data has two sets of features:</p>\n\n<ul>\n<li>One about the store sales with the dates (the field \"Store\" is not unique)</li>\n<li>One about the store types (the field \"Store\" is unique here)</li>\n</ul>\n\n<p>So the matrix would look something like this:</p>\n\n<pre><code>+-------+-----------+------------+---------+-----------+------+-------+--------------+\n| Store | DayOfWeek |    Date    |  Sales  | Customers | Open | Promo | StateHoliday |\n+-------+-----------+------------+---------+-----------+------+-------+--------------+\n|   1   |     5     | 2015-07-31 |  5263.0 |   555.0   |  1   |   1   |      0       |\n|   2   |     5     | 2015-07-31 |  6064.0 |   625.0   |  1   |   1   |      0       |\n|   3   |     5     | 2015-07-31 |  8314.0 |   821.0   |  1   |   1   |      0       |\n|   4   |     5     | 2015-07-31 | 13995.0 |   1498.0  |  1   |   1   |      0       |\n|   5   |     5     | 2015-07-31 |  4822.0 |   559.0   |  1   |   1   |      0       |\n|   6   |     5     | 2015-07-31 |  5651.0 |   589.0   |  1   |   1   |      0       |\n|   7   |     5     | 2015-07-31 | 15344.0 |   1414.0  |  1   |   1   |      0       |\n|   8   |     5     | 2015-07-31 |  8492.0 |   833.0   |  1   |   1   |      0       |\n|   9   |     5     | 2015-07-31 |  8565.0 |   687.0   |  1   |   1   |      0       |\n|   10  |     5     | 2015-07-31 |  7185.0 |   681.0   |  1   |   1   |      0       |\n+-------+-----------+------------+---------+-----------+------+-------+--------------+\n[986159 rows x 4 columns]\n</code></pre>\n\n<p>and</p>\n\n<pre><code>+-------+-----------+------------+---------------------+\n| Store | StoreType | Assortment | CompetitionDistance |\n+-------+-----------+------------+---------------------+\n|   1   |     c     |     a      |         1270        |\n|   2   |     a     |     a      |         570         |\n|   3   |     a     |     a      |        14130        |\n|   4   |     c     |     c      |         620         |\n|   5   |     a     |     a      |        29910        |\n|   6   |     a     |     a      |         310         |\n|   7   |     a     |     c      |        24000        |\n|   8   |     a     |     a      |         7520        |\n|   9   |     a     |     c      |         2030        |\n|   10  |     a     |     a      |         3160        |\n+-------+-----------+------------+---------------------+\n[1115 rows x 4 columns]\n</code></pre>\n\n<p>The second matrix describes the store type, the assortment groups of item each of them sell and the distance from the nearest competitor store.</p>\n\n<p>But in my test data, I only have information in the first matrix without the <code>Customers</code> and <code>Sales</code> fields. The aim is to predict the sales field given the </p>\n\n<ul>\n<li>Store</li>\n<li>DayofWeek</li>\n<li>Date</li>\n<li>Open (whether the store is open)</li>\n<li>Promo (whether the store is having a promotion)</li>\n<li>StateHoliday (whether it is a state holiday)</li>\n</ul>\n\n<p>I can easily train a classifier based on the bulleted fields above to predict <code>Sales</code> but <strong>how can I make use of the second matrix in my training data that I would not get in test data?</strong></p>\n\n<p><strong>Is it logical to assume that the second matrix about the Store types is static and I can easily join it to the test data?</strong></p>\n\n<p>What happens if there are holes in my test data feature set, let's say for some rows in the test data, I don't have the \"Promo\" values.</p>\n"
  },
  {
    "tags": [
      "nlp",
      "text-mining"
    ],
    "owner": {
      "account_id": 6444670,
      "reputation": 8476,
      "user_id": 11097,
      "user_type": "registered",
      "accept_rate": 75,
      "profile_image": "https://i.sstatic.net/z0jss.png?s=256",
      "display_name": "Dawny33",
      "link": "https://datascience.stackexchange.com/users/11097/dawny33"
    },
    "is_answered": true,
    "view_count": 29931,
    "accepted_answer_id": 9867,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1583835084,
    "creation_date": 1453271634,
    "last_edit_date": 1489682523,
    "question_id": 9865,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9865/what-is-the-difference-between-nlp-and-text-mining",
    "title": "What is the difference between NLP and text mining?",
    "body": "<p>As discussed with Sean in <a href=\"https://datascience.meta.stackexchange.com/q/2200/11097\">this Meta post</a>, I thought it would be nice to have a question which can help people who were confused like me, to know about the differences between text mining and NLP!</p>\n\n<p>So, what are the differences between <a href=\"/questions/tagged/nlp\" class=\"post-tag\" title=\"show questions tagged &#39;nlp&#39;\" rel=\"tag\">nlp</a> and <a href=\"/questions/tagged/text-mining\" class=\"post-tag\" title=\"show questions tagged &#39;text-mining&#39;\" rel=\"tag\">text-mining</a>?</p>\n\n<hr>\n\n<p>I have included my understanding as an answer. If possible, please explain your answer with a brief example!</p>\n"
  },
  {
    "tags": [
      "categorical-data",
      "numerical"
    ],
    "owner": {
      "account_id": 5686058,
      "reputation": 243,
      "user_id": 15617,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/8j8sc.jpg?s=256",
      "display_name": "Poisson Fish",
      "link": "https://datascience.stackexchange.com/users/15617/poisson-fish"
    },
    "is_answered": true,
    "view_count": 9917,
    "accepted_answer_id": 9893,
    "answer_count": 4,
    "score": 14,
    "last_activity_date": 1721882270,
    "creation_date": 1453407304,
    "question_id": 9892,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9892/how-can-i-dynamically-distinguish-between-categorical-data-and-numerical-data",
    "title": "How can I dynamically distinguish between categorical data and numerical data?",
    "body": "<p>I know someone who is working on a project that involves ingesting files of data without regard to the columns or data types. The task is to take a file with any number of columns and various data types and output summary statistics on the numerical data.</p>\n\n<p>However, he is unsure of how to go about dynamically assigning data types for certain number-based data. For example:</p>\n\n<pre><code>    CITY\n    Albuquerque\n    Boston\n    Chicago\n</code></pre>\n\n<p>This is obviously not numerical data and will be stored as text. However,</p>\n\n<pre><code>    ZIP\n    80221\n    60653\n    25525\n</code></pre>\n\n<p>are not clearly marked as categorical. His software would assign the ZIP code as numerical and output summary statistics for it, which does not make sense for that sort of data.</p>\n\n<hr>\n\n<p>A couple ideas we had were:</p>\n\n<ol>\n<li>If a column is all integers, label it as categorical. This clearly wouldn't work, but it was an idea.</li>\n<li>If a column has fewer than <em>n</em> unique values and is numeric, label it categorical. This might be closer, but there could still be issues with numerical data falling through.</li>\n<li>Maintain a list of common numeric data that should actually be categorical and compare the column headers to this list for matches. For example, anything with \"ZIP\" in it would be categorical.</li>\n</ol>\n\n<hr>\n\n<p>My gut tells me that there is no way to accurately assign numeric data as categorical or numerical, but was hoping for a suggestion. Any insight you have is greatly appreciated.</p>\n"
  },
  {
    "tags": [
      "pandas"
    ],
    "owner": {
      "account_id": 5019090,
      "reputation": 931,
      "user_id": 12443,
      "user_type": "registered",
      "accept_rate": 0,
      "profile_image": "https://www.gravatar.com/avatar/810526126c32a812e74cd963b974ed7a?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "sebastianspiegel",
      "link": "https://datascience.stackexchange.com/users/12443/sebastianspiegel"
    },
    "is_answered": true,
    "view_count": 69118,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1600881583,
    "creation_date": 1458380881,
    "question_id": 10783,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10783/how-do-i-merge-two-data-frames-in-python-pandas",
    "title": "How do I merge two data frames in Python Pandas?",
    "body": "<p>I have two data frames df1 and df2 and I would like to merge them into a single data frame. It is as if df1 and df2 were created by splitting a single data frame down the center vertically, like tearing a piece of paper that contains a list in half so that half the columns go on one paper and half the columns go on the other. I would like to merge them back together. How do I do it?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "backpropagation"
    ],
    "owner": {
      "account_id": 2811843,
      "reputation": 2023,
      "user_id": 13736,
      "user_type": "registered",
      "accept_rate": 45,
      "profile_image": "https://www.gravatar.com/avatar/9fe7def9462f2e9e696d8d5c367af484?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user",
      "link": "https://datascience.stackexchange.com/users/13736/user"
    },
    "is_answered": true,
    "view_count": 40884,
    "accepted_answer_id": 21792,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1682179388,
    "creation_date": 1462373736,
    "question_id": 11589,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/11589/creating-neural-net-for-xor-function",
    "title": "Creating neural net for xor function",
    "body": "<p>It is a well known fact that a 1-layer network cannot predict the xor function, since it is not linearly separable. I attempted to create a 2-layer network, using the logistic sigmoid function and backprop, to predict xor. My network has 2 neurons (and one bias) on the input layer, 2 neurons and 1 bias in the hidden layer, and 1 output neuron.\nTo my surprise, this will not converge. if I add a new layer, so I have a 3-layer network with input (2+1), hidden1 (2+1), hidden2 (2+1), and output, it works. Also, if I keep a 2-layer network, but I increase the hidden layer size to 4 neurons + 1 bias, it also converges. Is there a reason why a 2-layer network with 3 or less hidden neurons won't be able to model the xor function?</p>\n"
  },
  {
    "tags": [
      "python",
      "neural-network",
      "deep-learning",
      "keras"
    ],
    "owner": {
      "account_id": 6871697,
      "reputation": 3091,
      "user_id": 15412,
      "user_type": "registered",
      "accept_rate": 86,
      "profile_image": "https://www.gravatar.com/avatar/2e213ca2cb638ccd192024a844956b15?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "enterML",
      "link": "https://datascience.stackexchange.com/users/15412/enterml"
    },
    "is_answered": true,
    "view_count": 35309,
    "accepted_answer_id": 11705,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1626237270,
    "creation_date": 1463060471,
    "last_edit_date": 1626237270,
    "question_id": 11704,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/11704/reshaping-of-data-for-deep-learning-using-keras",
    "title": "Reshaping of data for deep learning using Keras",
    "body": "<p>I am a beginner to Keras and I have started with the MNIST example to understand how the library actually works. The code snippet of the MNIST problem in the Keras example folder is given as :</p>\n<pre><code>import numpy as np\nnp.random.seed(1337)  # for reproducibility\n\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten  \nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.utils import np_utils\n\nbatch_size = 128\nnb_classes = 10\nnb_epoch = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n# number of convolutional filters to use\nnb_filters = 32\n# size of pooling area for max pooling\nnb_pool = 2\n# convolution kernel size\nnb_conv = 3\n\n# the data, shuffled and split between train and test sets\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\nX_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\n..........\n</code></pre>\n<p>I am unable to understand the reshape function here. What is it doing and why we have applied it?</p>\n"
  },
  {
    "tags": [
      "pyspark"
    ],
    "owner": {
      "account_id": 8906697,
      "reputation": 141,
      "user_id": 21998,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/3227c6bed814f7472a9171a7600d5afd?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "neha",
      "link": "https://datascience.stackexchange.com/users/21998/neha"
    },
    "is_answered": true,
    "view_count": 111528,
    "answer_count": 4,
    "score": 14,
    "last_activity_date": 1559776302,
    "creation_date": 1470050511,
    "last_edit_date": 1559776302,
    "question_id": 13123,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/13123/import-csv-file-contents-into-pyspark-dataframes",
    "title": "Import csv file contents into pyspark dataframes",
    "body": "<p>How can I import a .csv file into pyspark dataframes? I even tried to read csv file in Pandas and then convert it to a spark dataframe using createDataFrame, but it is still showing some error. Can someone guide me through this? Also, please tell me how can I import an xlsx file?\nI'm trying to import csv content into pandas dataframes and then convert it into spark data frames, but it is showing the error:</p>\n\n<pre><code>\"Py4JJavaError\" An error occurred while calling o28.applySchemaToPythonRDD. : java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient \n</code></pre>\n\n<p>My code is:</p>\n\n<pre><code>from pyspark import SparkContext \nfrom pyspark.sql import SQLContext \nimport pandas as pd \nsqlc=SQLContext(sc) \ndf=pd.read_csv(r'D:\\BestBuy\\train.csv') \nsdf=sqlc.createDataFrame(df) \n</code></pre>\n"
  },
  {
    "tags": [
      "python",
      "random-forest"
    ],
    "migrated_from": {
      "other_site": {
        "aliases": [
          "https://www.stackoverflow.com",
          "https://facebook.stackoverflow.com"
        ],
        "styling": {
          "tag_background_color": "#E0EAF1",
          "tag_foreground_color": "#3E6D8E",
          "link_color": "#0077CC"
        },
        "related_sites": [
          {
            "relation": "meta",
            "api_site_parameter": "meta.stackoverflow",
            "site_url": "https://meta.stackoverflow.com",
            "name": "Meta Stack Overflow"
          },
          {
            "relation": "chat",
            "site_url": "https://chat.stackoverflow.com/",
            "name": "Stack Overflow Chat"
          }
        ],
        "markdown_extensions": [
          "Prettify"
        ],
        "launch_date": 1221436800,
        "open_beta_date": 1217462400,
        "site_state": "normal",
        "high_resolution_icon_url": "https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon@2.png",
        "favicon_url": "https://cdn.sstatic.net/Sites/stackoverflow/Img/favicon.ico",
        "icon_url": "https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon.png",
        "audience": "professional and enthusiast programmers",
        "site_url": "https://stackoverflow.com",
        "api_site_parameter": "stackoverflow",
        "logo_url": "https://cdn.sstatic.net/Sites/stackoverflow/Img/logo.png",
        "name": "Stack Overflow",
        "site_type": "main_site"
      },
      "on_date": 1472558127,
      "question_id": 38783745
    },
    "owner": {
      "account_id": 18348,
      "reputation": 879,
      "user_id": 133,
      "user_type": "registered",
      "accept_rate": 75,
      "profile_image": "https://www.gravatar.com/avatar/4480675ad0ede0043e7caba04750d974?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "gc5",
      "link": "https://datascience.stackexchange.com/users/133/gc5"
    },
    "is_answered": true,
    "view_count": 47773,
    "answer_count": 5,
    "score": 14,
    "last_activity_date": 1744798665,
    "creation_date": 1470382792,
    "question_id": 13754,
    "link": "https://datascience.stackexchange.com/questions/13754/feature-importance-with-scikit-learn-random-forest-shows-very-high-standard-devi",
    "title": "Feature importance with scikit-learn Random Forest shows very high Standard Deviation",
    "body": "<p>I am using <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" rel=\"noreferrer\">scikit-learn Random Forest Classifier</a> and I want to plot the feature importance such as in <a href=\"http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\" rel=\"noreferrer\">this example</a>.</p>\n\n<p>However my result is completely different, in the sense that feature importance standard deviation is almost always bigger than feature importance itself (see attached image).</p>\n\n<p><a href=\"https://i.sstatic.net/xjhXa.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/xjhXa.png\" alt=\"feature importance\"></a></p>\n\n<p>Is it possible to have such kind of behaviour, or am I doing some mistakes when plotting it?</p>\n\n<p>My code is the following:</p>\n\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\nclf.fit(predictors.values, outcome.values.ravel())\n\nimportance = clf.feature_importances_\nimportance = pd.DataFrame(importance, index=predictors.columns, \n                          columns=[\"Importance\"])\n\nimportance[\"Std\"] = np.std([tree.feature_importances_\n                            for tree in clf.estimators_], axis=0)\n\nx = range(importance.shape[0])\ny = importance.ix[:, 0]\nyerr = importance.ix[:, 1]\n\nplt.bar(x, y, yerr=yerr, align=\"center\")\n\nplt.show()\n</code></pre>\n"
  },
  {
    "tags": [
      "data-mining",
      "clustering",
      "text-mining",
      "time-series",
      "correlation"
    ],
    "owner": {
      "account_id": 1675476,
      "reputation": 241,
      "user_id": 23213,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/c9020f250805f84778dbafedb2ce85e6?s=256&d=identicon&r=PG",
      "display_name": "OoDeLally",
      "link": "https://datascience.stackexchange.com/users/23213/oodelally"
    },
    "is_answered": true,
    "view_count": 319,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1552685798,
    "creation_date": 1470661279,
    "last_edit_date": 1552685798,
    "question_id": 13295,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/13295/recognize-a-grammar-in-a-sequence-of-fuzzy-tokens",
    "title": "Recognize a grammar in a sequence of fuzzy tokens",
    "body": "<p>I have text documents which contain mainly lists of Items.</p>\n\n<p>Each Item is a group of several token from different types: FirstName, LastName, BirthDate, PhoneNumber, City, Occupation, etc.\nA token is a group of words.</p>\n\n<p>Items can lie on several lines.</p>\n\n<p>Items from a document have about the same token syntax, but they don't necessarily have to be exactly the same.</p>\n\n<p>They may be some more/less tokens between Items, as well as within Items.</p>\n\n<pre><code>FirstName LastName BirthDate PhoneNumber\nOccupation City\nFirstName LastName BirthDate PhoneNumber PhoneNumber\nOccupation City\nFirstName LastName BirthDate PhoneNumber\nOccupation UnrecognizedToken\nFirstName LastName PhoneNumber\nOccupation City\nFirstName LastName BirthDate PhoneNumber\nCity Occupation\n</code></pre>\n\n<p>The goal is to identify the used grammar, e.g.</p>\n\n<pre><code>Occupation City\n</code></pre>\n\n<p>and in the end identify all the Items, even thought they don't exactly match.</p>\n\n<p>In order to stay short and readable, let's use instead some aliases A, B, C, D, ... to designate those token types.</p>\n\n<p>e.g.</p>\n\n<pre><code>A B C\nD F\nA B C\nD E F\nF\nA B C\nD E E F\nA C B\nD E F\nA B D C\nD E F\nA B C\nD E F G\n</code></pre>\n\n<p>Here we can see that the Item syntax is</p>\n\n<pre><code>A B C\nD E F\n</code></pre>\n\n<p>because it is the one that matches the best the sequence.</p>\n\n<p>The syntax (token types and orders) can vary a lot from one document to another.\ne.g. another document may have that list</p>\n\n<pre><code>D A\nD A\nD\nD A\nB\nD A\n</code></pre>\n\n<p>The goal is to figure out that syntax <em>without prior knowledge of it</em>.</p>\n\n<p>From now, a new line is considered as a token as well.\nA document can then be represented as a 1-dimension sequence of tokens:</p>\n\n<pre><code>\n</code></pre>\n\n<p>Here the repeated sequence would be <code>A B C B</code> because it is the token that creates the least conflicts.</p>\n\n<p>Let's complexify it a bit.\nFrom now each token has no determined type.\nIn the real world, we are not always 100% sure of some token's type. Instead, we give it a probability of having a certain type.</p>\n\n<pre><code>  A 0.2    A 0.0    A 0.1\n  B 0.5    B 0.5    B 0.9     etc.\n  C 0.0    C 0.0    C 0.0\n  D 0.3    D 0.5    D 0.0\n</code></pre>\n\n<p>Here is an abstract graphic of what I want to achieve:</p>\n\n<p><img src=\"https://i.sstatic.net/e2njr.png\" alt=\"\"></p>\n\n<h3>Solution considered A: Convolution of a patch of tokens</h3>\n\n<p>This solution consists in applying a convolution with several patches of tokens, and take the one that creates the least conflicts.</p>\n\n<p>The hard part here is to find potential patches to roll along the observe sequence.\nFew ideas for this one, but nothing very satisfying:</p>\n\nBuild a Markov model of the transition between tokens\n\n<p>Drawback: since a Markov model has no memory, we will lose the orders of transition.\ne.g. If the repeated sequence is <code>A B C B D</code>, we lose the fact that A->B occurs before C->B.</p>\n\nBuild a suffix tree\n\n<p>This seems to be used extensively in biology in order to analyse nucleobases (GTAC) in DNA/RNA.\nDrawback: Suffix trees are good for exact matching of exact tokens (e.g. characters). We have neither exact sequences, nor exact tokens.</p>\n\nBrute force\n\n<p>Try every combination of every size. Could actually work, but would take some (long (long)) time.</p>\n\n<h3>Solution considered B: Build a table of Levenshtein distances of suffixes</h3>\n\n<p>The intuition is that there may exist some local minima of distance when computing the distance from every suffix to every suffix.</p>\n\n<p>The distance function is the Levenshtein distance, but we will be able to customize it in the future in order to take in account the probability of being of a certain types, instead of having a fixed type for each token.</p>\n\n<p>In order to stay simple in that demonstration, we will use fixed-type tokens, and use the classic Levenshtein to compute the distance between tokens.</p>\n\n<p>e.g. Let's have the input sequence <code>ABCGDEFGH ABCDEFGH ABCDNEFGH</code>.</p>\n\n<p>We compute the distance of every suffix with every suffix (cropped to be of equal size):</p>\n\n<pre><code>for i = 0 to sequence.lengh\n  for j = i to sequence.lengh\n    # Create the suffixes\n    suffixA = sequence.substr(i)\n    suffixB = sequence.substr(j)\n    # Make the suffixes the same size\n    chunkLen = Math.min(suffixA.length, suffixB.length)\n    suffixA = suffixA.substr(0, chunkLen)\n    suffixB = suffixB.substr(0, chunkLen)\n    # Compute the distance\n    distance[i][j] = LevenshteinDistance(suffixA, suffixB)\n</code></pre>\n\n<p>We get e.g. the following result (white is small distance, black is big):</p>\n\n<p><img src=\"https://i.sstatic.net/VFTBZ.png\" alt=\"\"></p>\n\n<p>Now, it's obvious that any suffix compared to itself will have a null distance. But we are not interested by suffix (exactly or partially) matching itself, so we crop that part.</p>\n\n<p><img src=\"https://i.sstatic.net/ypYa8.png\" alt=\"\"></p>\n\n<p>Since the suffixes are cropped to the same size, comparing long string will always yield a bigger distance than comparing smaller strings.</p>\n\n<p>We need to compensate that by a smooth penalty starting from the right (+P), fading out linearly to the left.</p>\n\n<p>I am not sure yet how to choose a good penalty function that would fit all cases.</p>\n\n<p>Here we apply a (+P=6) penalty on the extreme right, fading out to 0 to the left.</p>\n\n<p><img src=\"https://i.sstatic.net/ccFge.png\" alt=\"\"></p>\n\n<p>Now we can clearly see 2 clear diagonal lines emerge. There are 3 Items (Item1, Item2, Item3) in that sequence. The longest line represents the matching between Item1 vs Item2 and Item2 vs Item3. The second longest represents the matching between Item1 vs Item3.</p>\n\n<p>Now I am not sure on the best way to exploit that data. Is it as simple as taking the highest diagonal lines?</p>\n\n<p>Let's assume it is.</p>\n\n<p>Let's compute the average value of the diagonal line that start from each token.\nWe can see the result on the following picture (the vector below the matrix) :</p>\n\n<p><img src=\"https://i.sstatic.net/PXoP7.png\" alt=\"\"></p>\n\n<p>There are clearly 3 local minima, that match the beginning of each Item.\nLooks fantastic!</p>\n\n<p>Now let's add some more imperfections in the sequence:\n<code>ABCGDEFGH ABCDEFGH TROLL ABCDEFGH</code></p>\n\n<p><img src=\"https://i.sstatic.net/D2oYi.png\" alt=\"\"></p>\n\n<p>Clearly now, our vector of diagonal averages is messed up, and we cannot exploit it anymore...</p>\n\n<p>My assumption is that this could be solved by a customized distance function (instead of Levenshtein), where the insertion of a whole block may not be so much penalized. That is what I am not sure of.</p>\n\n<h2>Conclusion</h2>\n\n<p>None of the explored convolution-based solutions seem to fit our problem.</p>\n\n<p>The Levenshtein-distance-based solution seems promising, especially because it is compatible with probability-based-type tokens. But I am not sure yet about how to exploit the results of it.</p>\n\n<p>I would be very grateful if you have experience in a related field, and a couple of good hints to give us, or other techniques to explore.\nThank you very much in advance.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "graphs",
      "distributed"
    ],
    "owner": {
      "account_id": 7135084,
      "reputation": 698,
      "user_id": 14560,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/xoTmU.jpg?s=256",
      "display_name": "20-roso",
      "link": "https://datascience.stackexchange.com/users/14560/20-roso"
    },
    "is_answered": true,
    "view_count": 17150,
    "accepted_answer_id": 13467,
    "answer_count": 4,
    "score": 14,
    "last_activity_date": 1684139296,
    "creation_date": 1471349924,
    "question_id": 13455,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/13455/large-graphs-networkx-distributed-alternative",
    "title": "Large Graphs: NetworkX distributed alternative",
    "body": "<p>I have built some implementations using <code>NetworkX</code>(graph Python module) native algorithms in which I output some attributes which I use them for classification purposes.</p>\n\n<p>I want to scale it to a distributed environment. I have seen many approaches like <code>neo4j</code>, <code>Graphx</code>, <code>GraphLab</code>. However, I am quite new to this, thus I want to ask, which of them would be easy to locally apply graph algorithms (ex. node centrality measures), preferably using Python. To be more specific, which available option is closer related to <code>NetworkX</code> (easy installation, premade functions/algorithms, ML wise)?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "deep-learning",
      "keras"
    ],
    "owner": {
      "account_id": 1695359,
      "reputation": 1042,
      "user_id": 14360,
      "user_type": "registered",
      "accept_rate": 56,
      "profile_image": "https://www.gravatar.com/avatar/ae52e7465a397c7740bc8de66af2b9ee?s=256&d=identicon&r=PG",
      "display_name": "pseudomonas",
      "link": "https://datascience.stackexchange.com/users/14360/pseudomonas"
    },
    "is_answered": true,
    "view_count": 40692,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1559963843,
    "creation_date": 1471933199,
    "last_edit_date": 1559963843,
    "question_id": 13607,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/13607/validation-loss-and-accuracy-remain-constant",
    "title": "Validation loss and accuracy remain constant",
    "body": "<p>I am trying to implement <a href=\"http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0137036\" rel=\"noreferrer\">this</a> paper on a set of medical images. I am doing it in Keras. The network essentially consists of 4 conv and max-pool layers followed by a fully connected layer and soft max classifier.</p>\n\n<p>As far as I know, I have followed the architecture mentioned in the paper. However, the validation loss and accuracy just remain flat throughout. The accuracy seems to be fixed at ~57.5%.</p>\n\n<p>Any help on where I might be going wrong would be greatly appreciated.</p>\n\n<p>My code:</p>\n\n<pre><code>from keras.models import Sequential\nfrom keras.layers import Activation, Dropout, Dense, Flatten  \nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.optimizers import SGD\nfrom keras.utils import np_utils\nfrom PIL import Image\nimport numpy as np\nfrom sklearn.utils import shuffle\nfrom sklearn.cross_validation import train_test_split\nimport theano\nimport os\nimport glob as glob\nimport cv2\nfrom matplotlib import pyplot as plt\n\nnb_classes = 2\nimg_rows, img_cols = 100,100\nimg_channels = 3\n\n\n#################### DATA DIRECTORY SETTING######################\n\ndata = '/home/raghuram/Desktop/data'\nos.chdir(data)\nfile_list = os.listdir(data)\n##################################################################\n\n## Test lines\n#I = cv2.imread(file_list[1000])\n#print np.shape(I)\n####\nnon_responder_file_list = glob.glob('0_*FLAIR_*.png')\nresponder_file_list = glob.glob('1_*FLAIR_*.png')\nprint len(non_responder_file_list),len(responder_file_list)\n\nlabels = np.ones((len(file_list)),dtype = int)\nlabels[0:len(non_responder_file_list)] = 0\nimmatrix = np.array([np.array(cv2.imread(data+'/'+image)).flatten() for image in file_list])\n#img = immatrix[1000].reshape(100,100,3)\n#plt.imshow(img,cmap = 'gray')\n\n\ndata,Label = shuffle(immatrix,labels, random_state=2)\ntrain_data = [data,Label]\nX,y = (train_data[0],train_data[1])\n# Also need to look at how to preserve spatial extent in the conv network\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\nX_train = X_train.reshape(X_train.shape[0], 3, img_rows, img_cols)\nX_test = X_test.reshape(X_test.shape[0], 3, img_rows, img_cols)\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\n\nX_train /= 255\nX_test /= 255\n\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\nmodel = Sequential()\n\n## First conv layer and its activation followed by the max-pool layer#\nmodel.add(Convolution2D(16,5,5, border_mode = 'valid', subsample = (1,1), init = 'glorot_normal',input_shape = (3,100,100))) # Glorot normal is similar to Xavier initialization\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2),strides = None))\n# Output is 48x48\n\nprint 'First layer setup'\n###########################Second conv layer#################################\nmodel.add(Convolution2D(32,3,3,border_mode = 'same', subsample = (1,1),init = 'glorot_normal'))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.6))\nmodel.add(MaxPooling2D(pool_size = (2,2),strides = None))\n#############################################################################\n\nprint ' Second layer setup'\n# Output is 2x24\n\n##########################Third conv layer###################################\nmodel.add(Convolution2D(64,3,3, border_mode = 'same', subsample = (1,1), init = 'glorot_normal'))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.6))\nmodel.add(MaxPooling2D(pool_size = (2,2),strides = None))\n#############################################################################\n# Output is 12x12\n\nprint ' Third layer setup'\n###############################Fourth conv layer#############################\nmodel.add(Convolution2D(128,3,3, border_mode = 'same', subsample = (1,1), init = 'glorot_normal'))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.6))\nmodel.add(MaxPooling2D(pool_size = (2,2),strides = None))\n############################################################################# \n\nprint 'Fourth layer setup'\n\n# Output is 6x6x128\n# Create the FC layer of size 128x6x6#\nmodel.add(Flatten()) \nmodel.add(Dense(2,init = 'glorot_normal',input_dim = 128*6*6))\nmodel.add(Dropout(0.6))\nmodel.add(Activation('softmax'))\n\nprint 'Setting up fully connected layer'\nprint 'Now compiling the network'\nsgd = SGD(lr=0.01, decay=1e-4, momentum=0.6, nesterov=True)\nmodel.compile(loss = 'mse',optimizer = 'sgd', metrics=['accuracy'])\n\n# Fit the network to the data#\nprint 'Network setup successfully. Now fitting the network to the data'\nmodel. fit(X_train,Y_train,batch_size = 100, nb_epoch = 20, validation_split = None,verbose = 1)\nprint 'Testing'\nloss,accuracy = model.evaluate(X_test,Y_test,batch_size = 32,verbose = 1)\nprint \"Test fraction correct (Accuracy) = {:.2f}\".format(accuracy)\n</code></pre>\n"
  },
  {
    "tags": [
      "neural-network"
    ],
    "owner": {
      "account_id": 6787088,
      "reputation": 477,
      "user_id": 15533,
      "user_type": "registered",
      "accept_rate": 36,
      "profile_image": "https://www.gravatar.com/avatar/b5cb9fe975f6d5a7417f9ab0efd87ba0?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Monica Heddneck",
      "link": "https://datascience.stackexchange.com/users/15533/monica-heddneck"
    },
    "is_answered": true,
    "view_count": 13305,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1615665698,
    "creation_date": 1474444538,
    "last_edit_date": 1474446039,
    "question_id": 14122,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/14122/why-convolute-if-max-pooling-is-just-going-to-downsample-the-image-anyway",
    "title": "Why convolute if Max Pooling is just going to downsample the image anyway?",
    "body": "<p>The idea of applying filters to do something like identify edges, is a pretty cool idea.  </p>\n\n<p>For example, you can take an image of a 7. With some filters, you can end up with transformed images that emphasize different characteristics of the original image. The original 7:</p>\n\n<p><a href=\"https://i.sstatic.net/K4WkJ.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/K4WkJ.png\" alt=\"enter image description here\"></a></p>\n\n<p>can be experienced by the network as:  </p>\n\n<p><a href=\"https://i.sstatic.net/L02xC.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/L02xC.png\" alt=\"enter image description here\"></a>  </p>\n\n<p>Notice how each image has extracted a different edge of the original 7.  </p>\n\n<p>This is all great, but then, say the next layer in your network is a Max Pooling layer.  </p>\n\n<p>My question is, generally, doesn't this seem a little bit like overkill? We just were very careful and deliberate with identifying edges using filters -- now, we no longer care about any of that, since we've blasted the hell out of the pixel values! Please correct me if I'm wrong, but we went from 25 X 25 to 2 X 2! Why not just go straight to Max Pooling then, won't we end up with basically the same thing?</p>\n\n<p>As an extension the my question, I can't help but wonder what would happen if, coincidentally, each of the 4 squares all just happen to have a pixel with the same max value. Surely this isn't a rare case, right? Suddenly all your training images look the exact same.</p>\n"
  },
  {
    "tags": [
      "python",
      "apache-spark"
    ],
    "owner": {
      "account_id": 7820864,
      "reputation": 141,
      "user_id": 25406,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/5e6b71b3edbf4139985a5eeb58add969?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "justus",
      "link": "https://datascience.stackexchange.com/users/25406/justus"
    },
    "is_answered": true,
    "view_count": 57977,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1523929741,
    "creation_date": 1476919342,
    "last_edit_date": 1476950552,
    "question_id": 14648,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/14648/replace-all-numeric-values-in-a-pyspark-dataframe-by-a-constant-value",
    "title": "Replace all numeric values in a pyspark dataframe by a constant value",
    "body": "<p>Consider a pyspark dataframe consisting of 'null' elements and numeric elements. In general, the numeric elements have different values. How is it possible to replace all the numeric values of the dataframe by a constant numeric value (for example by the value 1)? Thanks in advance! </p>\n\n<p>Example for the pyspark dataframe:\n$$\n\\begin{array}{c|lcr}\n&amp; \\text{c1} &amp; \\text{c2} &amp; \\text{c3} \\\\\n\\hline\n1 &amp; 0.04 &amp; 1 &amp; 1.35 \\\\\n2 &amp; -1 &amp; null &amp; -1.2 \\\\\n3 &amp; null &amp; 1.2 &amp; null\n\\end{array}\n$$</p>\n\n<p>The result should be:</p>\n\n<p>$$\n\\begin{array}{c|lcr}\n&amp; \\text{c1} &amp; \\text{c2} &amp; \\text{c3} \\\\\n\\hline\n1 &amp; 1 &amp; 1 &amp; 1 \\\\\n2 &amp; 1 &amp; null &amp; 1 \\\\\n3 &amp; null &amp; 1 &amp; null\n\\end{array}\n$$</p>\n"
  },
  {
    "tags": [
      "random-forest",
      "xgboost",
      "normalization"
    ],
    "owner": {
      "account_id": 7130641,
      "reputation": 744,
      "user_id": 26202,
      "user_type": "registered",
      "accept_rate": 0,
      "profile_image": "https://lh3.googleusercontent.com/-305SHYPJuxE/AAAAAAAAAAI/AAAAAAAAABE/re8EZA9WuXs/s256-rj/photo.jpg",
      "display_name": "Soerendip",
      "link": "https://datascience.stackexchange.com/users/26202/soerendip"
    },
    "is_answered": true,
    "view_count": 29983,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1698888203,
    "creation_date": 1484041740,
    "question_id": 16225,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/16225/would-you-recommend-feature-normalization-when-using-boosting-trees",
    "title": "Would you recommend feature normalization when using boosting trees?",
    "body": "<p>For some machine learning methods it is recommended to use feature normalization to use features that are on the same scale, especially for distance based methods like k-means or when using regularization. However, in my experience, boosting tree regression works less well when I use normalized features, for some strange reason. How is your experience using feature normalization with boosted trees does it in general improve our models?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "predictive-modeling",
      "time-series",
      "scikit-learn"
    ],
    "owner": {
      "account_id": 8667979,
      "reputation": 143,
      "user_id": 31260,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/10153383747881307/picture?type=large",
      "display_name": "William D",
      "link": "https://datascience.stackexchange.com/users/31260/william-d"
    },
    "is_answered": true,
    "view_count": 8619,
    "accepted_answer_id": 18483,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1596287554,
    "creation_date": 1492694686,
    "question_id": 18481,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/18481/how-to-train-model-to-predict-events-30-minutes-prior-from-multi-dimensionnal-t",
    "title": "How to train model to predict events 30 minutes prior, from multi-dimensionnal timeseries",
    "body": "<p>Experts in my field are capable of <strong>predicting the likelyhood an event (binary spike in yellow)</strong>  30 minutes <strong>before it occurs</strong>. Frequency here is 1 sec, this view represents a few hours worth of data, <strong>i have circled in black where \"malicious\" pattern should be</strong>.\nInteractions between the dimensions exist, therefore dimensions cannot be studied individually (or can they?)</p>\n\n<p><a href=\"https://i.sstatic.net/bhiom.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/bhiom.png\" alt=\"enter image description here\"></a></p>\n\n<p>I'm trying to build a <strong>supervised</strong> ML model using Scikit Learn <strong>which learns a normal rythm, and detects when symptoms might lead to a spike</strong>. I am lost for which direction to take. I have tried Anomaly detection, but it only works for on the spot detection, not prior.</p>\n\n<p><strong>How could I detect \"malicious\" patterns prior to those events (taking them as target variables) ?</strong></p>\n\n<p>I welcome any advice on which algorithms or data processing pipeline might help, thank you :)</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "rnn"
    ],
    "owner": {
      "account_id": 1183320,
      "reputation": 709,
      "user_id": 24119,
      "user_type": "registered",
      "accept_rate": 88,
      "profile_image": "https://www.gravatar.com/avatar/9be49b9db792033fcc86542bd36e8818?s=256&d=identicon&r=PG",
      "display_name": "user1157751",
      "link": "https://datascience.stackexchange.com/users/24119/user1157751"
    },
    "is_answered": true,
    "view_count": 7639,
    "accepted_answer_id": 19269,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1587958660,
    "creation_date": 1495702273,
    "last_edit_date": 1495715028,
    "question_id": 19196,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/19196/forget-layer-in-a-recurrent-neural-network-rnn",
    "title": "Forget Layer in a Recurrent Neural Network (RNN) -",
    "body": "<p>I'm trying to figure out the dimensions of each variables in an RNN in the forget layer, however, I'm not sure if I'm on the right track. The next picture and equation is from <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" rel=\"noreferrer\">Colah's blog post \"Understanding LSTM Networks\"</a>:</p>\n\n<p><a href=\"https://i.sstatic.net/bdKbh.pnghttps://www.tensorflow.org/versions/r0.11/api_docs/python/rnn_cell/rnn_cells_for_use_with_tensorflow_s_core_rnn_methods#BasicRNNCell\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/bdKbh.pnghttps://www.tensorflow.org/versions/r0.11/api_docs/python/rnn_cell/rnn_cells_for_use_with_tensorflow_s_core_rnn_methods#BasicRNNCell\" alt=\"enter image description here\"></a></p>\n\n<p>where:</p>\n\n<ul>\n<li>$x_t$ is input of size $m*1$ vector</li>\n<li>$h_{t-1}$ is hidden state of size $n*1$ vector</li>\n<li>$[x_t, h_{t-1}]$ is a concatenation (for example, if $x_t=[1, 2, 3], h_{t-1}=[4,\n   5, 6]$, then $[x_t, h_{t-1}]=[1, 2, 3, 4, 5, 6]$)</li>\n<li>$w_f$ is weights of size $k*(m+n)$ matrix, where $k$ is the number of\ncell states (if $m=3$, and $n=3$ in the above example, and if we have\n3 cell states, then $w_f=3*3 $ matrix)</li>\n<li>$b_f$ is bias of size $k*1$ vector, where $k$ is the number of cell\nstates (since $k=3$ as the above example, then $b_f$ is a $3*1$\nvector).</li>\n</ul>\n\n<p>If we set $w_f$ to be:\n        \\begin{bmatrix}\n    1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 \\\\\n    5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 \\\\\n    3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 \\\\\n    \\end{bmatrix}</p>\n\n<p>And $b_f$ to be: $[1, 2, 3]$</p>\n\n<p>Then $W_f . [h_{t-1}, x_t] =$</p>\n\n<p>$$\n    \\begin{bmatrix}\n    1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 \\\\\n    5 &amp; 6 &amp; 7 &amp; 8 &amp; 9 &amp; 10 \\\\\n    3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 &amp; 8 \\\\\n    \\end{bmatrix}\n.        \\begin{bmatrix}\n    1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\\n    \\end{bmatrix}\n=\\begin{bmatrix} 91 &amp; 175 &amp; 133\\end{bmatrix}$$</p>\n\n<p>Then we can add the bias, $W_f . [h_{t-1}, x_t] + b_f=$</p>\n\n<p>$$\\begin{bmatrix} 91 &amp; 175 &amp; 133\\end{bmatrix} + \\begin{bmatrix} 1 &amp; 2 &amp; 3\\end{bmatrix}=\\begin{bmatrix} 92 &amp; 177 &amp; 136\\end{bmatrix}$$</p>\n\n<p>Then we feed them into a sigmoid function: $\\frac{1}{1+e^{-x}}$, where $x=\\begin{bmatrix} 92 &amp; 177 &amp; 136\\end{bmatrix}$, hence we perform this function element wise, and get \\begin{bmatrix} 1 &amp; 1 &amp; 1\\end{bmatrix}. </p>\n\n<p>Which means for each cell state, $C_{t-1}$, (there are $k=3$ cell states), we allow it to pass to the next layer.</p>\n\n<p>Is the above assumption correct?</p>\n\n<p>This also means that the number of cell state and hidden state is the same?</p>\n"
  },
  {
    "tags": [
      "tensorflow",
      "keras",
      "anomaly-detection"
    ],
    "owner": {
      "account_id": 6721244,
      "reputation": 661,
      "user_id": 32860,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://i.sstatic.net/j8M6A.jpg?s=256",
      "display_name": "Nickpick",
      "link": "https://datascience.stackexchange.com/users/32860/nickpick"
    },
    "is_answered": true,
    "view_count": 13838,
    "accepted_answer_id": 19358,
    "answer_count": 4,
    "score": 14,
    "last_activity_date": 1615666103,
    "creation_date": 1496243260,
    "last_edit_date": 1615666103,
    "question_id": 19357,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/19357/detecting-anomalies-with-neural-network",
    "title": "Detecting anomalies with neural network",
    "body": "<p>I have a large multi dimensional dataset that is generated each day.</p>\n<p>What would be a good approach to detect any kind of 'anomaly' as compared with previous days? Is this a suitable problem that could be addressed with neural networks?</p>\n<p>Any suggestions are appreciated.</p>\n<p>Additional information: there are no examples, so the method should detect the anomalies itself</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "batch-normalization"
    ],
    "owner": {
      "account_id": 4214907,
      "reputation": 543,
      "user_id": 33922,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/686df1f4367033782c57aa8131fc3ab6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "bnorm",
      "link": "https://datascience.stackexchange.com/users/33922/bnorm"
    },
    "is_answered": true,
    "view_count": 12697,
    "accepted_answer_id": 22511,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1625525646,
    "creation_date": 1498612265,
    "last_edit_date": 1503425180,
    "question_id": 20012,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/20012/does-batch-normalization-make-sense-for-a-relu-activation-function",
    "title": "Does Batch Normalization make sense for a ReLU activation function?",
    "body": "<p>Batch Normalization is described in <a href=\"https://arxiv.org/abs/1502.03167\" rel=\"noreferrer\">this</a> paper as a normalization of the input to an activation function with scale and shift variables $\\gamma$ and $\\beta$. This paper mainly describes using the sigmoid activation function, which makes sense. However, it seems to me that feeding an input from the normalized distribution produced by the batch normalization into a ReLU activation function of $max(0,x)$ is risky if $\\beta$ does not learn to shift most of the inputs past 0 such that the ReLU isn't losing input information. I.e. if the input to the ReLU were just standard normalized, we would lose a lot of our information below 0. Is there any guarantee or initialization of $\\beta$ that will guarantee that we don't lose this information? Am I missing something with how the operation of BN and ReLU work?</p>\n"
  },
  {
    "tags": [
      "pandas",
      "databases"
    ],
    "owner": {
      "account_id": 11114168,
      "reputation": 371,
      "user_id": 34116,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/GDKjG.png?s=256",
      "display_name": "Simon Boehm",
      "link": "https://datascience.stackexchange.com/users/34116/simon-boehm"
    },
    "is_answered": true,
    "view_count": 20534,
    "accepted_answer_id": 20141,
    "answer_count": 5,
    "score": 14,
    "last_activity_date": 1582198861,
    "creation_date": 1499025743,
    "last_edit_date": 1499061909,
    "question_id": 20118,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/20118/advantages-of-pandas-dataframe-to-regular-relational-database",
    "title": "Advantages of pandas dataframe to regular relational database",
    "body": "<p>In Data Science, many seem to be using <a href=\"http://pandas.pydata.org/index.html\" rel=\"noreferrer\"><strong>pandas</strong></a> dataframes as the datastore. What are the features of pandas that make it a superior datastore compared to <strong>regular relational databases</strong> like <a href=\"https://www.mysql.com/\" rel=\"noreferrer\">MySQL</a>, which are used to store data in many other fields of programming?</p>\n\n<p>While pandas does provide some useful functions for data exploration, you can't use SQL and you lose features like query optimization or access restriction.</p>\n"
  },
  {
    "tags": [
      "python",
      "dataset"
    ],
    "owner": {
      "account_id": 10411714,
      "reputation": 317,
      "user_id": 29806,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/KUPnC.png?s=256",
      "display_name": "basse",
      "link": "https://datascience.stackexchange.com/users/29806/basse"
    },
    "is_answered": true,
    "view_count": 4216,
    "accepted_answer_id": 22366,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1503035013,
    "creation_date": 1502971197,
    "last_edit_date": 1503035013,
    "question_id": 22337,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/22337/can-hdf5-be-reliably-written-to-and-read-from-simultaneously-by-separate-python",
    "title": "Can HDF5 be reliably written to and read from simultaneously by separate python processes?",
    "body": "<p>I'm writing a script to record live data over time into a single HDF5 file which includes my whole dataset for this project. I'm working with Python 3.6 and decided to create a command line tool using <code>click</code> to gather the data.</p>\n\n<p>My concern is what will happen if the data gathering script is writing to the HDF5 file and the yet-to-be ML application tries to read data from the same file?</p>\n\n<p>I took a look at The HDF Group's documentation about <a href=\"https://support.hdfgroup.org/HDF5/PHDF5/\" rel=\"noreferrer\">HDF5 parallel I/O</a>, but that didn't really clear things up for me.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "computer-vision",
      "convolutional-neural-network",
      "convolution"
    ],
    "owner": {
      "account_id": 7822595,
      "reputation": 827,
      "user_id": 33338,
      "user_type": "registered",
      "accept_rate": 88,
      "profile_image": "https://graph.facebook.com/960481744028994/picture?type=large",
      "display_name": "Shamane Siriwardhana",
      "link": "https://datascience.stackexchange.com/users/33338/shamane-siriwardhana"
    },
    "is_answered": true,
    "view_count": 18795,
    "accepted_answer_id": 23578,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1613436190,
    "creation_date": 1503065382,
    "last_edit_date": 1613436190,
    "question_id": 22387,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/22387/what-is-the-difference-between-dilated-convolution-and-deconvolution",
    "title": "What is the difference between Dilated Convolution and Deconvolution?",
    "body": "<p>These two convolution operations are very common in deep learning right now. </p>\n\n<p>I read about dilated convolutional layer in this paper : <a href=\"https://arxiv.org/pdf/1609.03499.pdf\" rel=\"nofollow noreferrer\">WAVENET: A GENERATIVE MODEL FOR RAW AUDIO</a></p>\n\n<p>and De-convolution is in this paper : <a href=\"https://arxiv.org/pdf/1411.4038.pdf\" rel=\"nofollow noreferrer\">Fully Convolutional Networks for Semantic Segmentation</a></p>\n\n<p>Both seem to up-sample the image but <strong>what is the difference?</strong> </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "keras",
      "multiclass-classification"
    ],
    "owner": {
      "account_id": 11930101,
      "reputation": 249,
      "user_id": 40180,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/i2cdO.jpg?s=256",
      "display_name": "cgn.dev",
      "link": "https://datascience.stackexchange.com/users/40180/cgn-dev"
    },
    "is_answered": true,
    "view_count": 10404,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1507707304,
    "creation_date": 1507475344,
    "question_id": 23614,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/23614/keras-multiple-softmax-in-last-layer-possible",
    "title": "Keras Multiple Softmax in last layer possible?",
    "body": "<p><a href=\"https://i.sstatic.net/EOQM4.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/EOQM4.png\" alt=\"multiple softmax in last layer\"></a></p>\n\n<p>Is it possible to implement mutiple softmaxes in the last layer in Keras? So the sum of Nodes 1-4 = 1; 5-8 = 1; etc.</p>\n\n<p>Should I go for a different network design?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "activation-function"
    ],
    "owner": {
      "account_id": 334941,
      "reputation": 243,
      "user_id": 43532,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/1c517d9017c3f3bce641d7eb299e1d3d?s=256&d=identicon&r=PG",
      "display_name": "Taiko",
      "link": "https://datascience.stackexchange.com/users/43532/taiko"
    },
    "is_answered": true,
    "view_count": 16354,
    "accepted_answer_id": 25851,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1513800305,
    "creation_date": 1513741185,
    "last_edit_date": 1513800305,
    "question_id": 25832,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/25832/input-normalization-for-relu",
    "title": "Input normalization for ReLu?",
    "body": "<p>Let's assume a vanilla MLP for classification with a given activation function for hidden layers.</p>\n\n<p>I know it is a known best practice to normalize the input of the network between 0 and 1 if sigmoid is the activation function and -0.5 and 0.5 if tanh is the activation function.</p>\n\n<p>What about ReLu ?</p>\n\n<p>Should I normalise the network input between 0 and 1, -0.5 and 0.5, or -1 and 1</p>\n\n<p>Any known best practices there?</p>\n\n<p>I am not talking about normalisation of the input of the ReLu like using Batch Normalisation just before or just after the ReLu : <a href=\"https://arxiv.org/pdf/1508.00330\" rel=\"noreferrer\">https://arxiv.org/pdf/1508.00330</a></p>\n\n<p>But I am talking about normalising the input of the whole network.</p>\n"
  },
  {
    "tags": [
      "python",
      "scikit-learn",
      "data-mining",
      "random-forest"
    ],
    "owner": {
      "account_id": 5641261,
      "reputation": 295,
      "user_id": 43455,
      "user_type": "registered",
      "accept_rate": 33,
      "profile_image": "https://i.sstatic.net/hZP2F.jpg?s=256",
      "display_name": "IS2057",
      "link": "https://datascience.stackexchange.com/users/43455/is2057"
    },
    "is_answered": true,
    "view_count": 88417,
    "accepted_answer_id": 26284,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1592344226,
    "creation_date": 1515071008,
    "last_edit_date": 1588022783,
    "question_id": 26283,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/26283/how-can-i-fit-categorical-data-types-for-random-forest-classification",
    "title": "How can I fit categorical data types for random forest classification?",
    "body": "<p>I need to find the accuracy of a training dataset by applying Random Forest Algorithm. But my the type of my data set are both categorical and numeric. When I tried to fit those data, I get an error.</p>\n\n<blockquote>\n  <p><em>'Input contains NaN, infinity or a value too large for dtype('float32')'.</em></p>\n</blockquote>\n\n<p>May be the problem is for object data types. How can I fit categorical data without transforming for applying RF?</p>\n\n<p>Here's my code.  </p>\n\n<p><a href=\"https://i.sstatic.net/V6sHp.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/V6sHp.png\" alt=\"screenshot\"></a></p>\n\n<p><a href=\"https://i.sstatic.net/k7nOE.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/k7nOE.png\" alt=\"screenshot\"></a></p>\n\n<p><a href=\"https://i.sstatic.net/3NXtO.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/3NXtO.png\" alt=\"screenshot\"></a></p>\n"
  },
  {
    "tags": [
      "python",
      "pandas",
      "dataframe"
    ],
    "owner": {
      "account_id": 12552060,
      "reputation": 173,
      "user_id": 43680,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7f03432ca6b4725d99e1d94d798a302f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Sam Joe",
      "link": "https://datascience.stackexchange.com/users/43680/sam-joe"
    },
    "is_answered": true,
    "view_count": 66029,
    "accepted_answer_id": 26312,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1551210343,
    "creation_date": 1515126439,
    "last_edit_date": 1535220401,
    "question_id": 26308,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/26308/after-grouping-to-minimum-value-in-pandas-how-to-display-the-matching-row-resul",
    "title": "after grouping to minimum value in pandas, how to display the matching row result entirely along min() value",
    "body": "<p>The dataframe contains</p>\n\n<pre><code>&gt;&gt; df\n        A          B            C\nA\n196512  196512    1325  12.9010511000000\n196512  196512  114569  12.9267705000000\n196512  196512  118910  12.8983353775637\n196512  196512  100688  12.9505091000000\n196795  196795   28978  12.7805170314276\n196795  196795   34591  12.8994111000000\n196795  196795   13078  12.9135746000000\n196795  196795   24173  12.8769653100000\n196341  196341  118910  12.8983353775637\n196341  196341  100688  12.9505091000000\n196641  196641   28972  12.7805170314276\n196641  196641   34591  12.8994111000000\n196346  196341  118910  12.8983353775637\n196346  196341  100688  12.9505091000000\n196646  196641   28980  12.7805170314276\n196646  196641   34591  12.8994111000000\n</code></pre>\n\n<p>I tried to get minimum value for each group and display using the following code,</p>\n\n<pre><code>df.columns = ['a','b','c']\ndf.index = df.a.astype(str)\ndd=df.groupby('a').min()['c']\n</code></pre>\n\n<p>it gives the result</p>\n\n<pre><code>196512    12.7805170314276\n196795    12.7805170314276\n196341    12.7805170314276\n196346    12.7805170314276\n</code></pre>\n\n<p>but after grouping, I want to get the row with the minimum 'c' value, grouped by column 'a' and display that full matching row in result \nlike,</p>\n\n<pre><code>196512    118910      12.8983353775637  \n196795     28978      12.7805170314276\n196341     28972      12.7805170314276\n196346     28980      12.7805170314276\n</code></pre>\n"
  },
  {
    "tags": [
      "keras",
      "rnn",
      "lstm",
      "sequence"
    ],
    "owner": {
      "account_id": 8700074,
      "reputation": 8747,
      "user_id": 21560,
      "user_type": "registered",
      "accept_rate": 36,
      "profile_image": "https://www.gravatar.com/avatar/349cff1d621bb8779315322f202fba5b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Hendrik",
      "link": "https://datascience.stackexchange.com/users/21560/hendrik"
    },
    "is_answered": true,
    "view_count": 14682,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1532958342,
    "creation_date": 1515401719,
    "question_id": 26401,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/26401/how-to-implement-one-to-many-and-many-to-many-sequence-prediction-in-keras",
    "title": "How to implement &quot;one-to-many&quot; and &quot;many-to-many&quot; sequence prediction in Keras?",
    "body": "<p>I struggle to interpret the Keras coding difference for one-to-many (e. g. classification of single images) and many-to-many (e. g. classification of image sequences) sequence labeling. I frequently see two different kind of codes:</p>\n\n<p>Type 1 is where no TimeDistributed applied like this:</p>\n\n<pre><code>model=Sequential()\n\nmodel.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1], border_mode=\"valid\", input_shape=[1, 56,14]))\nmodel.add(Activation(\"relu\"))\nmodel.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(pool_size=pool_size))\n\nmodel.add(Reshape((56*14,)))\nmodel.add(Dropout(0.25))\nmodel.add(LSTM(5))\nmodel.add(Dense(50))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation(\"softmax\"))\n</code></pre>\n\n<p>Type 2 is where TimeDistributed is applied like this:</p>\n\n<pre><code>model = Sequential()\n\nmodel.add(InputLayer(input_shape=(5, 224, 224, 3)))\nmodel.add(TimeDistributed(Convolution2D(64, (3, 3))))\nmodel.add(TimeDistributed(MaxPooling2D((2,2), strides=(2,2))))\nmodel.add(LSTM(10))\nmodel.add(Dense(3))\n</code></pre>\n\n<p>My questions are:</p>\n\n<ul>\n<li><p>Is my assumption correct that Type 1 is the one-to-many kind and Type\n2 is the many-to-many kind? Or <code>TimeDistributed</code> has no relevance in\nthis aspect?</p></li>\n<li><p>In either case of one-to-many or many-to-many is the last dense layer\nsupposed to be 1 node \"long\" (emitting only one value in turn) and<br>\nthe previous recurrent layer is responsible to determine how many<br>\n1-long value to emit? Or the last dense layer is supposed to consist \nof N nodes where <code>N=max sequence length</code>? If so, what is the point of<br>\nusing RNN here when we could produce a similar input with multiple<br>\noutputs with N parallel \"vanilla\" estimators?</p></li>\n<li><p>How to define the number of timesteps in RNNs? Is it somehow<br>\ncorrelated with the output sequence length or is it just a<br>\nhyperparameter to tune?</p></li>\n<li><p>Inn case of my Type 1 example above what is the point of applying<br>\nLSTM when the model emits only one class prediction (of possible<br>\n<code>nb_classes</code>)? What if one omits the LSTM layer?</p></li>\n</ul>\n"
  },
  {
    "tags": [
      "machine-learning",
      "pca"
    ],
    "owner": {
      "account_id": 205758,
      "reputation": 651,
      "user_id": 10522,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://www.gravatar.com/avatar/0c6aabd6b849d1f50d89fd1292a3fef0?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Victor",
      "link": "https://datascience.stackexchange.com/users/10522/victor"
    },
    "is_answered": true,
    "view_count": 13090,
    "accepted_answer_id": 26716,
    "answer_count": 4,
    "score": 14,
    "last_activity_date": 1516642090,
    "creation_date": 1516135369,
    "last_edit_date": 1516139030,
    "question_id": 26714,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/26714/is-pca-considered-a-machine-learning-algorithm",
    "title": "Is PCA considered a machine learning algorithm",
    "body": "<p>I've understood that principal component analysis is a dimensionality reduction technique i.e. given 10 input features, it will produce a smaller number of independent features that are orthogonal and linear transformation of original features.</p>\n\n<p>Is <code>PCA</code> by itself considered as a learning algorithm or it is a data pre-processing step.</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "cnn"
    ],
    "owner": {
      "account_id": 6576344,
      "reputation": 453,
      "user_id": 44768,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/62b06b27bd61151ec854905e153fd5a6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Andy R",
      "link": "https://datascience.stackexchange.com/users/44768/andy-r"
    },
    "is_answered": true,
    "view_count": 12749,
    "accepted_answer_id": 26761,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1524818790,
    "creation_date": 1516203906,
    "last_edit_date": 1516271438,
    "question_id": 26755,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/26755/cnn-how-does-backpropagation-with-weight-sharing-work-exactly",
    "title": "CNN - How does backpropagation with weight-sharing work exactly?",
    "body": "<p>Consider a Convolutional Neural Network (CNN) for image classification. In order to detect local features, weight-sharing is used among units in the same convolutional layer. \nIn such a network, the kernel weights are updated via the backpropagation algorithm. </p>\n\n<p>An update for the kernel weight $h_j$ in layer $l$ would be as follows:</p>\n\n<p>$h_j^l = h_j^l - \\eta \\cdot \\frac{\\delta R}{\\delta h_j^l} = h_j^l - \\eta \\cdot \\frac{\\delta R}{\\delta x_j^{L}} \\cdot \\frac{\\delta x_j^{L}}{\\delta x_j^{L - 1}} \\cdot ... \\cdot \\frac{\\delta x_j^{l}}{\\delta h_j^l}$</p>\n\n<p>How can the kernel weights be updated and still be the same (=shared)?</p>\n\n<p>I have 2 possible explanations: </p>\n\n<ol>\n<li><p>Weights of the same layer, which are initialized to the same value, will stay the same (independently of the input). This would imply that the expression $\\frac{\\delta R}{\\delta h_j^l}$ is the same for all of these weights $h_1^l$ to $h_J^l$. This does not make sense, since $x_j^l$ is different for different j's. Or am I missing something here?</p></li>\n<li><p>There is a trick, e.g. after the back-propagation update, the shared weights are set to their mean.</p></li>\n</ol>\n\n<p><strong>EDIT</strong>\nThe confusion I had was that I didn't consider that if a weight is shared, its parameter $h_j^l$ appears several times in the loss function. When differentiating for $h_j^l$, several terms (considering the according inputs) will \"survive\". Therefore the updates will be the same.</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "loss-function"
    ],
    "owner": {
      "account_id": 12469211,
      "reputation": 161,
      "user_id": 47561,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0fcd1fd6233e67a0f74e12648028dcea?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Silas Berger",
      "link": "https://datascience.stackexchange.com/users/47561/silas-berger"
    },
    "is_answered": true,
    "view_count": 4829,
    "accepted_answer_id": 29538,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1586765769,
    "creation_date": 1522009940,
    "last_edit_date": 1554471329,
    "question_id": 29526,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/29526/why-is-there-a-2-at-the-denominator-of-the-mean-squared-error-function",
    "title": "Why is there a $2$ at the denominator of the mean squared error function?",
    "body": "<p>In the famous <a href=\"http://neuralnetworksanddeeplearning.com/chap1.html\" rel=\"noreferrer\">Deep Learning Book</a>, in chapter 1, equation 6, the Quadratic Cost (or Mean Squared Error) in a neural network is defined as</p>\n\n<p><span class=\"math-container\">$ C(w, b) = \\frac{1}{2n}\\sum_{x}||y(x)-a||^2 $</span></p>\n\n<p>where <span class=\"math-container\">$w$</span> is the set of all weights and <span class=\"math-container\">$b$</span> the set of all biases, <span class=\"math-container\">$n$</span> is the number of training inputs, x is the set of all training inputs, y(x) is the expected output of the network for input x, and <span class=\"math-container\">$a$</span> is the actual output of the network for input <span class=\"math-container\">$x$</span>, with respect to <span class=\"math-container\">$w$</span> and <span class=\"math-container\">$b$</span>.</p>\n\n<p>Most of this formula seems very clear to be, except the <span class=\"math-container\">$2$</span> in the denominator. If I understand it correctly, we are summing up the squared vector length of (the actual output minus its expected output), for each training input (giving us the total squared error for the training set) and then divide this by the number of training samples, to get the mean squared error of all training samples. Why do we divide this by <span class=\"math-container\">$2$</span> as well then?</p>\n\n<p>In other places I've seen that Andrew Ng's lecture defines the Mean Square cost in a similar way, also with the <span class=\"math-container\">$2$</span> in the denominator, so this seems to be a common definition.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "keras",
      "pandas",
      "prediction"
    ],
    "owner": {
      "account_id": 6318649,
      "reputation": 283,
      "user_id": 50622,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/gcNKD.jpg?s=256",
      "display_name": "Nbenz",
      "link": "https://datascience.stackexchange.com/users/50622/nbenz"
    },
    "is_answered": true,
    "view_count": 15076,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1685283420,
    "creation_date": 1524583455,
    "last_edit_date": 1685283420,
    "question_id": 30762,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/30762/how-to-predict-the-future-values-of-time-horizon-with-keras",
    "title": "How to Predict the future values of time horizon with Keras?",
    "body": "<p>I just built this <strong>LSTM neural network</strong> with Keras </p>\n\n<pre><code>    import numpy as np\n    import pandas as pd \n    from sklearn import preprocessing\n    from keras.layers.core import Dense, Dropout, Activation\n    from keras.activations import linear\n    from keras.layers.recurrent import LSTM\n    from keras.models import Sequential\n    from matplotlib import pyplot\n\n    #read and prepare data from datafile\n    data_file_name = \"DailyDemand.csv\"\n    data_csv = pd.read_csv(data_file_name, delimiter = ';',header=None, usecols=[1,2,3,4,5])\n    yt = data_csv[1:]\n    data = yt\n    data.columns = ['MoyenneTransactHier', 'MaxTransaction', 'MinTransaction','CountTransaction','Demand']\n    # print (data.head(10))\n    pd.options.display.float_format = '{:,.0f}'.format\n    data = data.dropna ()\n    y=data['Demand'].astype(int)\n    cols=['MoyenneTransactHier', 'MaxTransaction', 'MinTransaction','CountTransaction']\n    x=data[cols].astype(int)\n\n    #scaling data\n    scaler_x = preprocessing.MinMaxScaler(feature_range =(-1, 1))\n    x = np.array(x).reshape ((len(x),4 ))\n    x = scaler_x.fit_transform(x)\n    scaler_y = preprocessing.MinMaxScaler(feature_range =(-1, 1))\n    y = np.array(y).reshape ((len(y), 1))\n    y = scaler_y.fit_transform(y)\n    print(\"longeur de y\",len(y))\n    # Split train and test data\n    train_end = 80\n    x_train=x[0: train_end ,]\n    x_test=x[train_end +1: ,]\n    y_train=y[0: train_end]\n    y_test=y[train_end +1:] \n    x_train=x_train.reshape(x_train.shape +(1,))\n    x_test=x_test.reshape(x_test.shape + (1,))\n\n    print(\"Data well prepared\")\n    print ('x_train shape ', x_train.shape)\n    print ('y_train', y_train.shape)\n\n    #Design the model - LSTM Network\n    seed = 2016\n    np.random.seed(seed)\n    fit1 = Sequential ()\n    fit1.add(LSTM(\n        output_dim = 4,\n        activation='tanh',\n        input_shape =(4, 1)))\n    fit1.add(Dense(output_dim =1))\n    fit1.add(Activation(linear))\n    #rmsprop or sgd\n    batchsize = 1\n    fit1.compile(loss=\"mean_squared_error\",optimizer=\"rmsprop\")\n    #train the model\n    fit1.fit(x_train , y_train , batch_size = batchsize, nb_epoch =20, shuffle=True)\n\n    print(fit1.summary ())\n\n    #Model error\n    score_train = fit1.evaluate(x_train ,y_train ,batch_size =batchsize)\n    score_test = fit1.evaluate(x_test , y_test ,batch_size =batchsize)\n    print(\"in  train  MSE = \",round(score_train,4))\n    print(\"in test  MSE = \",round(score_test ,4))\n\n    #Make prediction\n    pred1=fit1.predict(x_test)\n    pred1 = scaler_y.inverse_transform(np.array(pred1).reshape ((len(pred1), 1)))\n    real_test = scaler_y.inverse_transform(np.array(y_test).reshape ((len(y_test), 1))).astype(int)\n\n    #save prediction\n    testData = pd.DataFrame(real_test)\n    preddData = pd.DataFrame(pred1)\n    dataF = pd.concat([testData,preddData], axis=1)\n    dataF.columns =['Real demand','Predicted Demand']\n    dataF.to_csv('Demandprediction.csv')\n\n    pyplot.plot(pred1, label='Forecast')\n    pyplot.plot(real_test,label='Actual')\n    pyplot.legend()\n    pyplot.show()\n</code></pre>\n\n<p>then it generates this result: \n<a href=\"https://i.sstatic.net/la8gj.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/la8gj.png\" alt=\"Prediction on the test data\"></a></p>\n\n<p>After building and training a good model on the historical data, I don't know how I can generate the prediction for future values? For example the demand of the next 10 days. Data are daily.</p>\n\n<p><a href=\"https://i.sstatic.net/ZTKMA.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/ZTKMA.png\" alt=\"this is an example of how the data is shaped\"></a></p>\n\n<p>NB: this is an example of how the data is shaped, the green is the label and the yellow one are the features.<br>\nafter <code>dropna()</code> (delete null values) it remains 100 data rows, I've used 80 in the training and the 20 in the test.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "reinforcement-learning"
    ],
    "owner": {
      "account_id": 7809129,
      "reputation": 243,
      "user_id": 53006,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f4022daa1c2066b0a2ac23658f573a60?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Omegastick",
      "link": "https://datascience.stackexchange.com/users/53006/omegastick"
    },
    "is_answered": true,
    "view_count": 17373,
    "accepted_answer_id": 32577,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1551134982,
    "creation_date": 1527820437,
    "question_id": 32480,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/32480/how-does-generalised-advantage-estimation-work",
    "title": "How does generalised advantage estimation work?",
    "body": "<p>I've been trying to add <a href=\"https://arxiv.org/abs/1506.02438\" rel=\"noreferrer\">GAE</a> to my A2C implementation for a while now, but I' can't quite seem to grok how it works.</p>\n\n<p>My understanding of it, is that it reduces the variance of the advantage estimation function by kind of 'averaging out' (or generalising) the advantages based off the values in the rollout. </p>\n\n<p>I tried to run through the maths on my own, and in the end I just had one advantage for the whole rollout, is this right? Normally, we'd have one advantage for each timestep in the rollout.</p>\n\n<p>Can anyone provide an explanation on the intuition of GAE?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning"
    ],
    "owner": {
      "account_id": 4410342,
      "reputation": 2756,
      "user_id": 43077,
      "user_type": "registered",
      "accept_rate": 72,
      "profile_image": "https://i.sstatic.net/4IA0y.jpg?s=256",
      "display_name": "Kari",
      "link": "https://datascience.stackexchange.com/users/43077/kari"
    },
    "is_answered": true,
    "view_count": 5346,
    "accepted_answer_id": 33431,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1605238617,
    "creation_date": 1528546225,
    "last_edit_date": 1605238617,
    "question_id": 32873,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/32873/prioritized-replay-what-does-importance-sampling-really-do",
    "title": "Prioritized Replay, what does Importance Sampling really do?",
    "body": "<p>I can't understand the purpose of importance-sampling weights (IS) in <a href=\"https://arxiv.org/pdf/1511.05952.pdf\" rel=\"nofollow noreferrer\">Prioritized Replay (page 5)</a>.</p>\n<p>A transition is more likely to be sampled from experience replay the larger its &quot;cost&quot; is.  My understanding is that 'IS' helps with smoothely abandoning the use of prioritized replay after we've trained for long enough.  <strong>But what do we use instead, uniform sampling?</strong></p>\n<p><strong>I guess I can't realize <em>how</em> each component in such a coefficient is affecting the outcome. Could someone explain it in words?</strong></p>\n<p><span class=\"math-container\">$$w_i = \\left( \\frac{1}{N}\\cdot \\frac{1}{P(i)} \\right) ^\\beta$$</span></p>\n<p>It's then used to dampen the gradient, which we try to get from transitions.</p>\n<p>Where:</p>\n<ul>\n<li><span class=\"math-container\">$w_i$</span> is &quot;IS&quot;</li>\n<li>N is the size of Experience Replay buffer</li>\n<li>P(i) is the chance to select transition <span class=\"math-container\">$i$</span>, depending on &quot;how fat its cost is&quot;.</li>\n<li><span class=\"math-container\">$\\beta$</span> starts from 0.4 and is dragged closer and closer to 1 with each new epoch.</li>\n</ul>\n<p><strong>Is my understanding of these parameters also correct?</strong></p>\n<p><strong>Edit</strong> Sometime after the answer was accepted I found an additional source, a video which might be helpful for beginners - <a href=\"https://www.youtube.com/watch?v=ymeSc0SPFmE\" rel=\"nofollow noreferrer\">MC Simmulations: 3.5 Importance Sampling</a></p>\n<hr />\n<p><strong>Edit</strong> As @avejidah said in the comment to his answer &quot;<span class=\"math-container\">$1/N$</span> <em>is used to average the samples by the probability that they will be sampled&quot;</em>.</p>\n<p>To realise why it's important, assume <span class=\"math-container\">$\\beta$</span> is fixed to 1, we have <strong>4</strong> samples, each has <span class=\"math-container\">$P(i)$</span> as follows:</p>\n<pre><code>0.1  0.2   0.3     0.4\n</code></pre>\n<p>That is, first entry has 10% of being chosen, second is 20% etc.\nNow, inverting them, we get:</p>\n<pre><code> 10   5    3.333   2.5\n</code></pre>\n<p>Averaging via <span class=\"math-container\">$1/N$</span> (which in our case is <span class=\"math-container\">$1/4$</span>) we get:</p>\n<pre><code>2.5  1.25  0.8325  0.625     ...which would add up to '5.21'\n</code></pre>\n<p>As we can see they are much closer to zero than the simply inverted versions (<span class=\"math-container\">$10, 5, 3.333, 2.5$</span>). This means the gradient for our network won't be magnified as much, resulting in a lot less variance as we train our network.</p>\n<p>So, without this <span class=\"math-container\">$\\frac{1}{N}$</span>were we lucky to select the least likely sample (<span class=\"math-container\">$0.1$</span>), the gradient would be scaled 10 times.  It would be even worse with smaller values, say <span class=\"math-container\">$0.00001$</span> chance, if our experience replay has many thousands entries, which is quite usual.</p>\n<p>In other words, <span class=\"math-container\">$\\frac{1}{N}$</span> is just to make your hyperparameters (such as learning-rate) not require adjustment, when there you change the size of your experience replay buffer.</p>\n"
  },
  {
    "tags": [
      "pandas",
      "numpy",
      "seaborn"
    ],
    "owner": {
      "account_id": 9259086,
      "reputation": 284,
      "user_id": 44074,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-Su4LgQcW-o8/AAAAAAAAAAI/AAAAAAAAAAA/ZkY14Iuctmo/s256-rj/photo.jpg",
      "display_name": "Jodh Singh",
      "link": "https://datascience.stackexchange.com/users/44074/jodh-singh"
    },
    "is_answered": true,
    "view_count": 91326,
    "closed_date": 1530971501,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1530826330,
    "creation_date": 1528776699,
    "question_id": 32981,
    "link": "https://datascience.stackexchange.com/questions/32981/how-to-deal-with-typeerror-ufunc-isnan-not-supported-for-the-input-types",
    "closed_reason": "Not suitable for this site",
    "title": "How to deal with TypeError: ufunc &#39;isnan&#39; not supported for the input types",
    "body": "<pre><code>I have dealt with all the Nan values in the features dataframe, then why I am still getting this error? \n</code></pre>\n\n<hr>\n\n<pre><code>sns.heatmap(features, annot=True, annot_kws={\"size\": 7})\nsns.plt.show()\n</code></pre>\n\n<hr>\n\n<pre><code>TypeError                                 Traceback (most recent call last)\n    &lt;ipython-input-11-534a699b432d&gt; in &lt;module&gt;()\n    ----&gt; 1 sns.heatmap(features, annot=True, annot_kws={\"size\": 7})\n          2 sns.plt.show()\n\n    c:\\users\\jetjo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\seaborn\\matrix.py in heatmap(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\n        515     plotter = _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt,\n        516                           annot_kws, cbar, cbar_kws, xticklabels,\n    --&gt; 517                           yticklabels, mask)\n        518 \n        519     # Add the pcolormesh kwargs here\n\n    c:\\users\\jetjo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\seaborn\\matrix.py in __init__(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\n        166         # Determine good default values for the colormapping\n        167         self._determine_cmap_params(plot_data, vmin, vmax,\n    --&gt; 168                                     cmap, center, robust)\n        169 \n        170         # Sort out the annotations\n\n    c:\\users\\jetjo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\seaborn\\matrix.py in _determine_cmap_params(self, plot_data, vmin, vmax, cmap, center, robust)\n        203                                cmap, center, robust):\n        204         \"\"\"Use some heuristics to set good defaults for colorbar and range.\"\"\"\n    --&gt; 205         calc_data = plot_data.data[~np.isnan(plot_data.data)]\n        206         if vmin is None:\n        207             vmin = np.percentile(calc_data, 2) if robust else calc_data.min()\n\n    TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n</code></pre>\n"
  },
  {
    "tags": [
      "machine-learning"
    ],
    "owner": {
      "account_id": 11348737,
      "reputation": 273,
      "user_id": 52546,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/c2fe87ceb8a174942cede67bbe37c732?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Pierre Cattin",
      "link": "https://datascience.stackexchange.com/users/52546/pierre-cattin"
    },
    "is_answered": true,
    "view_count": 20127,
    "accepted_answer_id": 46875,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1551978367,
    "creation_date": 1534498211,
    "question_id": 37078,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/37078/source-of-arthur-samuels-definition-of-machine-learning",
    "title": "Source of Arthur Samuel&#39;s definition of machine learning",
    "body": "<p>Many people seem to agree that Arthur Samuel wrote or said in 1959 that machine learning is the \"<strong>Field of study that gives computers the ability to learn without being explicitly programmed</strong>\". </p>\n\n<p>For example the quote is contained in <a href=\"https://www.ibm.com/developerworks/community/blogs/jfp/entry/What_Is_Machine_Learning?lang=en_us\" rel=\"noreferrer\">this page</a>, <a href=\"https://theconversation.com/what-is-machine-learning-76759\" rel=\"noreferrer\">that one</a> and in <a href=\"https://www.coursera.org/lecture/machine-learning/what-is-machine-learning-Ujm7v\" rel=\"noreferrer\">Andrew Ng's ML course</a>. Several articles also contain this quote, and the reference is always the following article, which doesn't actually contain the quote.</p>\n\n<p><a href=\"https://pdfs.semanticscholar.org/e9e6/bb5f2a04ae30d8ecc9287f8b702eedd7b772.pdf\" rel=\"noreferrer\">Samuel, A. L. (1959). Some studies in machine learning using the game of checkers. IBM Journal of research and development, 3(3), 210-229.</a></p>\n\n<p>Is there a reliable source? Or is this actually not a quote, but rather an interpretation of Samuel's article?</p>\n"
  },
  {
    "tags": [
      "python",
      "scikit-learn",
      "dimensionality-reduction"
    ],
    "owner": {
      "account_id": 5645975,
      "reputation": 3960,
      "user_id": 26562,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-XX7E-4B44as/AAAAAAAAAAI/AAAAAAAAAFA/7Lxgn3LNwSQ/s256-rj/photo.jpg",
      "display_name": "timleathart",
      "link": "https://datascience.stackexchange.com/users/26562/timleathart"
    },
    "is_answered": true,
    "view_count": 4698,
    "accepted_answer_id": 37563,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1536879021,
    "creation_date": 1535542546,
    "question_id": 37561,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/37561/efficient-dimensionality-reduction-for-large-dataset",
    "title": "Efficient dimensionality reduction for large dataset",
    "body": "<p>I have a dataset with ~1M rows and ~500K sparse features. I want to reduce the dimensionality to somewhere in the order of 1K-5K dense features. </p>\n\n<p><code>sklearn.decomposition.PCA</code> doesn't work on sparse data, and I've tried using <code>sklearn.decomposition.TruncatedSVD</code> but get a memory error pretty quickly. What are my options for efficient dimensionality reduction on this scale?</p>\n"
  },
  {
    "tags": [
      "tensorflow",
      "linear-algebra"
    ],
    "owner": {
      "account_id": 13639761,
      "reputation": 159,
      "user_id": 59139,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/08562d91ca87bb186d549573ae10c57d?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "frt132",
      "link": "https://datascience.stackexchange.com/users/59139/frt132"
    },
    "is_answered": true,
    "view_count": 8659,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1588530605,
    "creation_date": 1537045931,
    "last_edit_date": 1588530605,
    "question_id": 38303,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/38303/how-does-tensor-product-multiplication-work-in-tensorflow",
    "title": "How does tensor product/multiplication work in TensorFlow?",
    "body": "<p>In Tensorflow, I saw the following example:</p>\n\n<pre><code>import tensorflow as tf \nimport numpy as np \n\nmat_a = tf.constant(np.arange(1,13, dtype=np.int32), shape=[2,2,3])  \nmat_b = tf.constant(np.arange(12,24, dtype=np.int32), shape=[2,3,2])  \nmul_c = tf.matmul(mat_a, mat_b)\n\nwith tf.Session() as sess:  \n   runop = sess.run(mul_c)  \n   print(runop) \n\n[[[ 88  94]  \n  [214 229]]  \n [[484 508]  \n  [642 674]]]\n</code></pre>\n\n<p>How does the tensor multiplication work?</p>\n"
  },
  {
    "tags": [
      "python",
      "nlp"
    ],
    "owner": {
      "account_id": 11234991,
      "reputation": 241,
      "user_id": 59586,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/5cf5a8fb827e2e4d5e38fe1a99be1ead?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "D500",
      "link": "https://datascience.stackexchange.com/users/59586/d500"
    },
    "is_answered": true,
    "view_count": 22556,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1676470260,
    "creation_date": 1537832007,
    "question_id": 38745,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/38745/increasing-spacy-max-nlp-limit",
    "title": "Increasing SpaCy max NLP limit",
    "body": "<p>I'm getting this error:</p>\n\n<pre><code>[E088] Text of length 1029371 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\n</code></pre>\n\n<p>The weird thing is that if I reduce the amount of documents being lemmatized, it still says the length exceeds 1 million. Is there a way of increasing the limit past 1 million? The error seems to suggest there is but I'm unable to do so.</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "deep-learning",
      "keras"
    ],
    "owner": {
      "account_id": 8828121,
      "reputation": 293,
      "user_id": 56655,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7fcf82107b730b2254bb4067cc9bf1c4?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "BhanuKiran",
      "link": "https://datascience.stackexchange.com/users/56655/bhanukiran"
    },
    "is_answered": true,
    "view_count": 35581,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1596962834,
    "creation_date": 1539624232,
    "last_edit_date": 1559135210,
    "question_id": 39718,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/39718/cant-understand-output-shape-of-a-dense-layer-keras",
    "title": "Can&#39;t understand Output shape of a Dense layer - keras",
    "body": "<p>I am following an online tutorial to classify images and started off with dense layers as a starting point to classify cifar10 data.</p>\n\n<pre><code># Create a model and add layers\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(32, 32, 3)))\nmodel.add(Dense(10, activation='softmax'))\n\n# Print summary\nmodel.summary()\n</code></pre>\n\n<p><a href=\"https://i.sstatic.net/Sh0F5.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/Sh0F5.png\" alt=\"enter image description here\"></a></p>\n\n<p><code>dense_1 output shape (None, 32, 32, 512)</code>. 'None' represents the batch size, but what does '32,32' represent? Why isn't the shape (None, 512)? Same happens with the dense_2 layer. </p>\n\n<p>Can someone explain it here or point me to a resource that explains this?</p>\n"
  },
  {
    "tags": [
      "classification",
      "class-imbalance",
      "weighted-data"
    ],
    "owner": {
      "account_id": 4227824,
      "reputation": 389,
      "user_id": 8240,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/28ee736df1c6de2d9b5d45fb6623d4b9?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user137927",
      "link": "https://datascience.stackexchange.com/users/8240/user137927"
    },
    "is_answered": true,
    "view_count": 7231,
    "accepted_answer_id": 44760,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1548753323,
    "creation_date": 1548746500,
    "question_id": 44755,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/44755/why-doesnt-class-weight-resolve-the-imbalanced-classification-problem",
    "title": "Why doesn&#39;t class weight resolve the imbalanced classification problem?",
    "body": "<p>I know that in imbalanced classification, the classifier tends to predict all the test labels as larger class label, but if we use class weight in loss function, it would be reasonable to expect the problem to be solved. So why we need some approaches like down sampling or up sampling for imbalanced classification problem? </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "scikit-learn",
      "multilabel-classification"
    ],
    "owner": {
      "account_id": 13921333,
      "reputation": 141,
      "user_id": 66707,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-zfXvrRS-kxA/AAAAAAAAAAI/AAAAAAAAAA0/kD2RP3PO6EU/s256-rj/photo.jpg",
      "display_name": "Michael Joy",
      "link": "https://datascience.stackexchange.com/users/66707/michael-joy"
    },
    "is_answered": true,
    "view_count": 38215,
    "answer_count": 6,
    "score": 14,
    "last_activity_date": 1708832602,
    "creation_date": 1549468313,
    "last_edit_date": 1549478772,
    "question_id": 45174,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/45174/how-to-use-sklearn-train-test-split-to-stratify-data-for-multi-label-classificat",
    "title": "How to use sklearn train_test_split to stratify data for multi-label classification?",
    "body": "<p>I am attempting to mirror a machine learning program by <a href=\"https://ahmedbesbes.com/overview-and-benchmark-of-traditional-and-deep-learning-models-in-text-classification.html\" rel=\"noreferrer\">Ahmed Besbes</a>, but scaled up for multi-label classification. It seems that any attempt to stratify the data returns the following error: <code>The least populated class in y has only 1 member, which is too few. The minimum number of labels for any class cannot be less than 2.</code></p>\n\n<p>In my data set, I have 1 column which contains clean, tokenized text. The other 8 columns are for the classifications based on the content of that text. Just to note, column 1 - 4 have significantly more samples than 5 - 8 (more obscure classifications derived from the text). </p>\n\n<p><a href=\"https://i.sstatic.net/wI3qe.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/wI3qe.png\" alt=\"Generic sample of the data I am working with\"></a></p>\n\n<p>Here is a generic sample from my code: </p>\n\n<pre><code>x = data['cleaned_text']\ny = data[['car','truck','ford','chevy','black','white','parked', 'driving']]\n\nx_train, x_test, y_train, y_test = train_test_split(x,\n                                                    y,\n                                                    test_size=0.1,\n                                                    random_state=42)\n\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n</code></pre>\n\n<p>Output: <code>(6293,) (700,) (6293, 8) (700, 8)</code></p>\n\n<p>Adding <code>stratify=y</code> to <code>train_test_split</code> returns the error previously mentioned. Even when I limit y to just one column, I still get the error.</p>\n\n<p>How can I stratify the data so that I give the program a fair look in the training set? </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "cross-validation",
      "accuracy",
      "overfitting"
    ],
    "owner": {
      "account_id": 4530077,
      "reputation": 336,
      "user_id": 14582,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/ZLJBF.jpg?s=256",
      "display_name": "A.B",
      "link": "https://datascience.stackexchange.com/users/14582/a-b"
    },
    "is_answered": true,
    "view_count": 20427,
    "accepted_answer_id": 47268,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1553712413,
    "creation_date": 1552504491,
    "last_edit_date": 1552566773,
    "question_id": 47263,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/47263/validation-vs-test-vs-training-accuracy-which-one-should-i-compare-for-claimi",
    "title": "Validation vs. test vs. training accuracy. Which one should I compare for claiming overfit?",
    "body": "<p>I have read on the several answers here and on the Internet that cross-validation helps to indicate that if the model will generalize well or not and about overfitting.</p>\n\n<p>But I am confused that which two accuracies/errors amoung test/training/validation should I compare to be able to see if the model is overfitting or not?</p>\n\n<p>For example:</p>\n\n<p>I divide my data for 70% training and 30% test.</p>\n\n<p>When I get to run 10 fold cross-validation, I get 10 accuracies that I can take the average/mean of. should I call this mean as <code>validation accuracy</code>?</p>\n\n<p>Afterward, I test the model on 30% test data and get <code>Test Accuracy</code>.</p>\n\n<p>In this case, what will be <code>training accuracy</code>? And which \ntwo accuracies should I compare to see if the model is overfitting or not?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "k-means",
      "similarity",
      "image"
    ],
    "owner": {
      "account_id": 7748979,
      "reputation": 329,
      "user_id": 68742,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/796286768a15a46bc0135ce800ce4ea7?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "jason",
      "link": "https://datascience.stackexchange.com/users/68742/jason"
    },
    "is_answered": true,
    "view_count": 54777,
    "accepted_answer_id": 48646,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1554437504,
    "creation_date": 1554424609,
    "question_id": 48642,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/48642/how-to-measure-the-similarity-between-two-images",
    "title": "How to measure the similarity between two images?",
    "body": "<p>I have two group images for cat and dog. And each group contain 2000 images for cat and dog respectively. </p>\n\n<p>My goal is try to cluster the images by using k-means.</p>\n\n<p>Assume image1 is <code>x</code>, and image2 is <code>y</code>.Here we need to measure the similarity between any two images. what is the common way to measure between two images?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "pytorch",
      "backpropagation"
    ],
    "owner": {
      "account_id": 16212950,
      "reputation": 141,
      "user_id": 76741,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-V1dVWYbepDg/AAAAAAAAAAI/AAAAAAAAACs/LvSKR6N633M/s256-rj/photo.jpg",
      "display_name": "Zhuoran Liu",
      "link": "https://datascience.stackexchange.com/users/76741/zhuoran-liu"
    },
    "is_answered": true,
    "view_count": 6462,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1645661373,
    "creation_date": 1562350365,
    "last_edit_date": 1573837938,
    "question_id": 55151,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/55151/differences-between-gradient-calculated-by-different-reduction-methods-in-pytorc",
    "title": "Differences between gradient calculated by different reduction methods in PyTorch",
    "body": "<p>I'm playing with different reduction methods provided in built-in loss functions. In particular, I would like to compare the following.</p>\n\n<ol>\n<li><p>The averaged gradient by performing backward pass for each loss value calculated with <code>reduction=\"none\"</code></p></li>\n<li><p>The gradient averaged by dividing the batch size with <code>reduction=\"sum\"</code></p></li>\n<li><p>The average gradient yielded by <code>reduction=\"mean\"</code></p></li>\n<li>The average gradient calculated by <code>reduction=\"mean\"</code>, with the data points fed into the model one at a time.</li>\n</ol>\n\n<p>My code for producing the experiment is as follows:</p>\n\n<pre><code>def estimate_gradient(model, optimizer, batch):\n    criterion_no_reduction = nn.CrossEntropyLoss(reduction=\"none\").cuda()\n    criterion_sum = nn.CrossEntropyLoss(reduction=\"sum\").cuda()\n    criterion_avg = nn.CrossEntropyLoss().cuda()\n\n    input, target = batch\n    input, target = input.cuda(), target.cuda()\n    output = model(input)\n    n = len(output)\n\n    loss_no_reudction = criterion_no_reduction(output, target)\n    grad_list_no_reduction = []\n    for i in range(n):\n        optimizer.zero_grad()\n        loss_no_reudction[i].backward(retain_graph=True)\n        for j, param in enumerate(model.parameters()):\n            if param.requires_grad:\n                grad = param.grad.view(-1, 1)\n                if i == 0:\n                    grad_list_no_reduction.append(grad)\n                else:\n                    grad_list_no_reduction[j] = torch.cat((grad_list_no_reduction[j], grad), dim=1)\n    grad_out_no_reduction = torch.cat(grad_list_no_reduction, dim=0)\n    grad_out_no_reduction = (torch.sum(grad_out_no_reduction, dim=1) / n).cpu().detach().numpy().flatten()\n\n    loss_sum = criterion_sum(output, target)\n    optimizer.zero_grad()\n    loss_sum.backward(retain_graph=True)\n    for j, param in enumerate(model.parameters()):\n        if param.requires_grad:\n            if j == 0:\n                grad_list_sum = param.grad.view(-1)\n            else:\n                grad_list_sum = torch.cat((grad_list_sum, param.grad.view(-1)))\n    grad_out_sum = (grad_list_sum / n).cpu().detach().numpy().flatten()\n\n    loss_avg = criterion_avg(output, target)\n    optimizer.zero_grad()\n    loss_avg.backward(retain_graph=True)\n    for j, param in enumerate(model.parameters()):\n        if param.requires_grad:\n            if j == 0:\n                grad_list_avg = param.grad.view(-1)\n            else:\n                grad_list_avg = torch.cat((grad_list_avg, param.grad.view(-1)))\n    grad_out_avg = grad_list_avg.cpu().detach().numpy().flatten()\n\n    target = target.view(-1, 1)\n    grad_list_one_by_one = []\n    for i in range(n):\n        optimizer.zero_grad()\n        curr_output = output[i].view(1, -1)\n        loss = criterion_avg(curr_output, target[i])\n        loss.backward(retain_graph=True)\n        for j, param in enumerate(model.parameters()):\n            if param.requires_grad:\n                grad = param.grad.view(-1, 1)\n                if i == 0:\n                    grad_list_one_by_one.append(grad)\n                else:\n                    grad_list_one_by_one[j] = torch.cat((grad_list_one_by_one[j], grad), dim=1)\n    grad_out_one_by_one = torch.cat(grad_list_one_by_one, dim=0)\n    grad_out_one_by_one = (torch.sum(grad_out_one_by_one, dim=1) / n).cpu().detach().numpy().flatten()\n\n    assert grad_out_no_reduction.shape == grad_out_sum.shape == grad_out_avg.shape == grad_out_one_by_one.shape\n    print(\"Maximum discrepancy between reduction = none and sum: {}\".format(np.max(np.abs(grad_out_no_reduction - grad_out_sum))))\n    print(\"Maximum discrepancy between reduction = none and avg: {}\".format(np.max(np.abs(grad_out_no_reduction - grad_out_avg))))\n    print(\"Maximum discrepancy between reduction = none and one-by-one: {}\".format(np.max(np.abs(grad_out_no_reduction - grad_out_one_by_one))))\n    print(\"Maximum discrepancy between reduction = sum and avg: {}\".format(np.max(np.abs(grad_out_sum - grad_out_avg))))\n    print(\"Maximum discrepancy between reduction = sum and one-by-one: {}\".format(np.max(np.abs(grad_out_sum - grad_out_one_by_one))))\n    print(\"Maximum discrepancy between reduction = avg and one-by-one: {}\".format(np.max(np.abs(grad_out_avg- grad_out_one_by_one))))\n</code></pre>\n\n<p>The results are as follows:</p>\n\n<pre><code>Maximum discrepancy between reduction = none and sum: 0.0316\nMaximum discrepancy between reduction = none and avg: 0.0316\nMaximum discrepancy between reduction = none and one-by-one: 0.0\nMaximum discrepancy between reduction = sum and avg: 0.0\nMaximum discrepancy between reduction = sum and one-by-one: 0.0316\nMaximum discrepancy between reduction = avg and one-by-one: 0.0316\n</code></pre>\n\n<p>That is, the result produced by <code>reduction=none</code> and one-by-one backward pass appear to be identical, while <code>reduciton=sum</code> and <code>reduction=mean</code> yields different results from the previous pair. It would be really helpful to explain the discrepancy (maybe due to <code>retain_graph=True</code>?) and thanks in advance for any help!</p>\n"
  },
  {
    "tags": [
      "data-mining",
      "pandas"
    ],
    "owner": {
      "account_id": 16422323,
      "reputation": 223,
      "user_id": 78730,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/ee90f31a0da726bc238372118d28bb77?s=256&d=identicon&r=PG",
      "display_name": "Koko",
      "link": "https://datascience.stackexchange.com/users/78730/koko"
    },
    "is_answered": true,
    "view_count": 122878,
    "accepted_answer_id": 56681,
    "answer_count": 4,
    "score": 14,
    "last_activity_date": 1698360011,
    "creation_date": 1564567731,
    "last_edit_date": 1564568660,
    "question_id": 56668,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/56668/pandas-change-value-of-a-column-based-another-column-condition",
    "title": "Pandas change value of a column based another column condition",
    "body": "<p>I have values in column1, I have columns in column2.\nWhat I want to achieve: Condition: where column2 == 2 leave to be 2 if\n column1 &lt; 30 elsif change to 3 if column1 > 90.</p>\n\n<p>Here is what i did so far, the problem is 2 does not change to 3 where column1 > 90.</p>\n\n<pre><code>filter1 = data['column1']\nfor x in filter1:\n    if x &lt; 30:\n        data['column2'] = data['column2'].replace([2], [2])\n    else:\n        data['column2'] = data['Output'].replace([2], [3])\n</code></pre>\n"
  },
  {
    "tags": [
      "features",
      "lightgbm",
      "predictor-importance",
      "shap"
    ],
    "owner": {
      "account_id": 16748752,
      "reputation": 143,
      "user_id": 82640,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-Sra6-tOB4Os/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rfKbfMIxuHaY0cEWdeo79opdOKfKg/s256-rj/photo.jpg",
      "display_name": "pbk",
      "link": "https://datascience.stackexchange.com/users/82640/pbk"
    },
    "is_answered": true,
    "view_count": 9283,
    "accepted_answer_id": 67007,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1725195170,
    "creation_date": 1570475429,
    "last_edit_date": 1579887695,
    "question_id": 61395,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/61395/shap-value-analysis-gives-different-feature-importance-on-train-and-test-set",
    "title": "SHAP value analysis gives different feature importance on train and test set",
    "body": "<p>Should SHAP value analysis be done on the train or test set?</p>\n\n<p>What does it mean if the feature importance based on mean |SHAP value|  is different between the train and test set of my lightgbm model?</p>\n\n<p>I intend to use SHAP analysis to identify how each feature contributes to each individual prediction and possibly identify individual predictions that are anomalous. For instance, if the individual prediction's top (+/-) contributing features are vastly different from that of the model's feature importance, then this prediction is less trustworthy. Does this approach make sense?</p>\n"
  },
  {
    "tags": [
      "cnn",
      "rnn",
      "object-detection",
      "yolo"
    ],
    "owner": {
      "account_id": 7939178,
      "reputation": 1511,
      "user_id": 83473,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/feab58f314adf1beb699a3548726a73c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "star",
      "link": "https://datascience.stackexchange.com/users/83473/star"
    },
    "is_answered": true,
    "view_count": 36289,
    "protected_date": 1737375997,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1737375853,
    "creation_date": 1578304778,
    "question_id": 65945,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/65945/what-is-darknet-and-why-is-it-needed-for-yolo-object-detection",
    "title": "what is darknet and why is it needed for YOLO object detection?",
    "body": "<p>what is darknet and why is it needed for YOLO object detection ? I read that its a neural network written in C , but why is it needed for YOLO object detection  when we  have lot of machine learning framework,api like tensorflow,keras,pytorch .</p>\n\n<p>Im trying to train yolo from git code and i could see they are using tensorflow/keras as well but not sure why darkenet is used initially for traning yolo  .</p>\n\n<p><a href=\"https://github.com/AlexeyAB/darknet\" rel=\"noreferrer\">darknet/yolo algorithm </a></p>\n"
  },
  {
    "tags": [
      "neural-network",
      "object-recognition"
    ],
    "owner": {
      "account_id": 3250408,
      "reputation": 143,
      "user_id": 90451,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/eb5284921c3b152a60e8a0394bf6743a?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "seb",
      "link": "https://datascience.stackexchange.com/users/90451/seb"
    },
    "is_answered": true,
    "view_count": 3216,
    "accepted_answer_id": 68461,
    "answer_count": 5,
    "score": 14,
    "last_activity_date": 1615496935,
    "creation_date": 1582276102,
    "last_edit_date": 1583323058,
    "question_id": 68450,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/68450/how-can-you-include-information-not-present-in-an-image-for-neural-networks",
    "title": "How can you include information not present in an image for neural networks?",
    "body": "<p>I am training a CNN to identify objects in images (one label per image). However, I have additional information about these images that cannot be retrieved by looking at the image itself. In more detail, I'm talking about the physical location of this object. This information proved to be important when classifying these objects.</p>\n\n<p>However, I can't think of a good solution to include this information in a image recognition model, since the CNN is classifying the object based on the pixel values and not on ordered feature data.</p>\n\n<p>One possible solution I was thinking of was to have an additional simple ML model on tabular data (including mainly the location data), such as an SVM, to give give a certain additional weight to the output of the CNN. Would this be a good strategy? I can't seem to find anything in the literature about this.</p>\n\n<p>Thanks in advance!</p>\n\n<p>edit: Someone asked what I meant by 'location'. With the location I meant the physical location of where the image was taken, in context of a large 2d space. I don't want to go too deep into the domain, but it's basically an (x,y) vector on a surface area, and obviously this meta-data cannot be extracted by looking at the pixel values.</p>\n\n<p>edit2: I want to propose an additional way I found that was useful, but was not mentioned in any answer. \nInstead of using the neural network to predict the classes, I use the neural network to produce features.</p>\n\n<p>I removed the final layer, which resulted in an output of shape 1024x1. This obviously depends on the design of your network. Then, I can use these features <em>together</em> with the meta-data (in my case location data) in an additional model to make predictions, such an SVM or another NN.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "data-cleaning",
      "categorical-data"
    ],
    "owner": {
      "account_id": 11998458,
      "reputation": 263,
      "user_id": 88539,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/765dced4f68700e37c91bf8be68d0bcc?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "nick012000",
      "link": "https://datascience.stackexchange.com/users/88539/nick012000"
    },
    "is_answered": true,
    "view_count": 5186,
    "answer_count": 10,
    "score": 14,
    "last_activity_date": 1700502782,
    "creation_date": 1584678231,
    "last_edit_date": 1606315368,
    "question_id": 69978,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/69978/how-can-i-appropriately-handle-cleaning-of-gender-data",
    "title": "How can I appropriately handle cleaning of gender data?",
    "body": "<p>Im a data science student and Ive begun working with an open mental health dataset. As part of this, I need to clean the data so that I can perform an analysis of it.</p>\n<p>In this dataset, the gender field is a string that could have had anything entered into it. While cleaning most entries are fairly straightforward (f, F, female, cis female and woman can all be coded to F), what I was wondering about was how to properly handle trans or queer identities (e.g. an entry that says something like trans female or queer/she/they).</p>\n<p>Should I create a new code for trans entries for each gender, or should I just code them as though they were members of the gender they identify as?</p>\n<p>Should I just drop them from the dataset entirely, because they might distort it? I remember reading that trans individuals suffer from much higher rates of mental illness than cis individuals.</p>\n<p>Are there any best practices that I should follow in this regard?</p>\n"
  },
  {
    "tags": [
      "pandas",
      "jupyter",
      "ipython"
    ],
    "owner": {
      "account_id": 9096094,
      "reputation": 263,
      "user_id": 85635,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-ZOOf4wwDA4E/AAAAAAAAAAI/AAAAAAAAAHQ/aYLr1lm7f1A/s256-rj/photo.jpg",
      "display_name": "GIRISH kuniyal",
      "link": "https://datascience.stackexchange.com/users/85635/girish-kuniyal"
    },
    "is_answered": true,
    "view_count": 25934,
    "answer_count": 5,
    "score": 14,
    "last_activity_date": 1672944923,
    "creation_date": 1594200699,
    "last_edit_date": 1623258899,
    "question_id": 77352,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/77352/generate-pdf-from-jupyter-notebook-without-code",
    "title": "Generate pdf from jupyter notebook without code",
    "body": "<p>I have a Jupyter notebook that contains markdown, code, and outputs (graphs). I would like to generate PDF from this notebook.</p>\n<p>I tried to hide code using HTML code which I get from <a href=\"https://stackoverflow.com/questions/27934885/how-to-hide-code-from-cells-in-ipython-notebook-visualized-with-nbviewer\">here</a> then I tried to download it as pdf but again code shows up. But when I download it as HTML it don't show any code but again when I tried to convert HTML to pdf it again shows code.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "mathematics"
    ],
    "owner": {
      "account_id": 22588547,
      "reputation": 141,
      "user_id": 123598,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/a8e71ec2a423536929b2cecb69dc16e1?s=256&d=identicon&r=PG",
      "display_name": "Physicist92",
      "link": "https://datascience.stackexchange.com/users/123598/physicist92"
    },
    "is_answered": true,
    "view_count": 2605,
    "answer_count": 4,
    "score": 14,
    "last_activity_date": 1636200962,
    "creation_date": 1630130928,
    "last_edit_date": 1630244990,
    "question_id": 100578,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/100578/do-you-actually-need-math-for-your-data-science-job",
    "title": "Do you actually need math for your data science job?",
    "body": "<p>I am a physicist working in a data scientist role. I was told everywhere that my degree is a very good starting point because I know a lot of math and it is crucial for this job. But other than understanding the math behind the models' calculations I don't use any math. Okay sometimes I need to create principal components or carry out SVD but these are Just algorithms that anyone can look up on the internet.</p>\n<p>So honestly I am a bit worried because I might be doing something wrong. Can you please share your experiences? <em>Important note:</em> May be that I don't use deep learning for my job.</p>\n"
  },
  {
    "tags": [
      "nlp",
      "transformer"
    ],
    "owner": {
      "account_id": 16130802,
      "reputation": 253,
      "user_id": 127231,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2a7d35b9fef8b4c817abf9e35b84f5e2?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Dametime",
      "link": "https://datascience.stackexchange.com/users/127231/dametime"
    },
    "is_answered": true,
    "view_count": 14242,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1676942314,
    "creation_date": 1637001378,
    "question_id": 104179,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/104179/is-the-transformer-decoder-an-autoregressive-model",
    "title": "Is the Transformer decoder an autoregressive model?",
    "body": "<p>I have been trying to find an answer to these questions, but I only find conflicting information. Is the transformer as a whole autoregressive or not? And what about the decoder? I understand that the decoder during inference proceeds autoregressively, but I am not sure about during training time.</p>\n<p>Here are posts saying that the Transformer is not autoregressive:</p>\n<p><a href=\"https://datascience.stackexchange.com/questions/93144/minimal-working-example-or-tutorial-showing-how-to-use-pytorchs-nn-transformerd\">Minimal working example or tutorial showing how to use Pytorch&#39;s nn.TransformerDecoder for batch text generation in training and inference modes?</a></p>\n<p>Here are some saying that it is:</p>\n<p><a href=\"https://datascience.stackexchange.com/questions/81727/what-would-be-the-target-input-for-transformer-decoder-during-test-phase?rq=1\">What would be the target input for Transformer Decoder during test phase?</a></p>\n<p><a href=\"https://www.tensorflow.org/text/tutorials/transformer\" rel=\"noreferrer\">https://www.tensorflow.org/text/tutorials/transformer</a></p>\n<p><a href=\"https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0\" rel=\"noreferrer\">https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0</a></p>\n<p><a href=\"https://huggingface.co/transformers/summary.html#seq-to-seq-models\" rel=\"noreferrer\">https://huggingface.co/transformers/summary.html#seq-to-seq-models</a></p>\n"
  },
  {
    "tags": [
      "nlp",
      "bert",
      "language-model",
      "gpt",
      "research"
    ],
    "owner": {
      "account_id": 939983,
      "reputation": 243,
      "user_id": 152976,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8d50a1fbe6b9a7baa3803721429ecb5b?s=256&d=identicon&r=PG",
      "display_name": "Ethan",
      "link": "https://datascience.stackexchange.com/users/152976/ethan"
    },
    "is_answered": true,
    "view_count": 21245,
    "accepted_answer_id": 123060,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1712326813,
    "creation_date": 1691025089,
    "question_id": 123053,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/123053/why-does-everyone-use-bert-in-research-instead-of-llama-or-gpt-or-palm-etc",
    "title": "Why does everyone use BERT in research instead of LLAMA or GPT or PaLM, etc?",
    "body": "<p>It could be that I'm misunderstanding the problems space and the iterations of LLAMA, GPT, and PaLM are all based on BERT like many language models are, but every time I see a new paper in improving language models it takes BERT as a based an adds some kind of fine-tuning or filtering or something. I don't understand why BERT became the default in research circles when all anyone hears about publicly is GPT-2,3,4 or more recently LLAMA-2. I have a feeling it has something to do with BERT being open-source, but that can't be the whole story. This question might not be specific enough, please let me know. Thanks.</p>\n"
  },
  {
    "tags": [
      "llm",
      "prompt-engineering"
    ],
    "owner": {
      "account_id": 2602735,
      "reputation": 243,
      "user_id": 145766,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6d5d4d58fa8028f218ee509899156eac?s=256&d=identicon&r=PG",
      "display_name": "Plop",
      "link": "https://datascience.stackexchange.com/users/145766/plop"
    },
    "is_answered": true,
    "view_count": 7159,
    "accepted_answer_id": 128276,
    "answer_count": 6,
    "score": 14,
    "last_activity_date": 1712744938,
    "creation_date": 1710331324,
    "last_edit_date": 1710336669,
    "question_id": 128274,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/128274/why-does-prompt-engineering-work-since-prompt-engineering-questions-dont-appea",
    "title": "Why does prompt engineering work, since prompt engineering questions don&#39;t appear as training data?",
    "body": "<p>One can find advice on prompt engineering telling basically the following thing: if you are seeking advice about a topic X, start your prompt to an LLM by</p>\n<blockquote>\n<p>You are an expert in X, you have pedagogical skills, and you are very\ngood at synthesizing information blahblahblah...</p>\n</blockquote>\n<p>I don't understand why this should work at all. Let's simplify the matter: assume an LLM is trained only on stackoverflow questions and answers. Since no question on SO starts with such a sentence, the pre-prompt actually makes the prompt more different from training data that it was before.</p>\n<p><strong>More generally, why would writing prompts like asking questions to a being, and moreover telling this being what it is give us better output? Moreover, is this even measurably true?</strong></p>\n<p>My hand-waving understanding of LLMs is that since they basically are excellent for predicting the following words of a context, they have, as a side-effect, learnt to answer questions just because they have learnt that on the Internet, questions are usually followed by answers.</p>\n"
  },
  {
    "tags": [
      "bigdata",
      "performance",
      "databases",
      "nosql"
    ],
    "owner": {
      "account_id": 1113411,
      "reputation": 461,
      "user_id": 199,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://www.gravatar.com/avatar/3f2e521fe728bb5d6e87db48df3c9114?s=256&d=identicon&r=PG",
      "display_name": "Filipe Ferminiano",
      "link": "https://datascience.stackexchange.com/users/199/filipe-ferminiano"
    },
    "is_answered": true,
    "view_count": 277,
    "accepted_answer_id": 122,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1496691023,
    "creation_date": 1400302383,
    "last_edit_date": 1496691023,
    "question_id": 113,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/113/when-a-relational-database-has-better-performance-than-a-no-relational",
    "title": "When a relational database has better performance than a no relational",
    "body": "<p>When a relational database, like MySQL, has better performance than a no relational, like MongoDB?</p>\n\n<p>I saw a question on Quora other day, about why Quora still uses MySQL as their backend, and that their performance is still good.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification"
    ],
    "owner": {
      "account_id": 70472,
      "reputation": 233,
      "user_id": 275,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/76ac8689ea84f78c85aa0a929e83f5f3?s=256&d=identicon&r=PG",
      "display_name": "super_seabass",
      "link": "https://datascience.stackexchange.com/users/275/super-seabass"
    },
    "is_answered": true,
    "view_count": 7496,
    "accepted_answer_id": 197,
    "answer_count": 4,
    "score": 13,
    "last_activity_date": 1596711849,
    "creation_date": 1400795246,
    "last_edit_date": 1400815640,
    "question_id": 196,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/196/algorithm-for-generating-classification-rules",
    "title": "Algorithm for generating classification rules",
    "body": "<p>So we have potential for a machine learning application that fits fairly neatly into the traditional problem domain solved by classifiers, i.e., we have a set of attributes describing an item and a \"bucket\" that they end up in. However, rather than create models of probabilities like in Naive Bayes or similar classifiers, we want our output to be a set of roughly human-readable rules that can be reviewed and modified by an end user.</p>\n\n<p>Association rule learning looks like the family of algorithms that solves this type of problem, but these algorithms seem to focus on identifying common combinations of features and don't include the concept of a final bucket that those features might point to. For example, our data set looks something like this:</p>\n\n<pre><code>Item A { 4-door, small, steel } =&gt; { sedan }\nItem B { 2-door, big,   steel } =&gt; { truck }\nItem C { 2-door, small, steel } =&gt; { coupe }\n</code></pre>\n\n<p>I just want the rules that say \"if it's big and a 2-door, it's a truck,\" not the rules that say \"if it's a 4-door it's also small.\" </p>\n\n<p>One workaround I can think of is to simply use association rule learning algorithms and ignore the rules that don't involve an end bucket, but that seems a bit hacky. Have I missed some family of algorithms out there? Or perhaps I'm approaching the problem incorrectly to begin with?</p>\n"
  },
  {
    "tags": [
      "classification",
      "algorithms",
      "encoding"
    ],
    "owner": {
      "account_id": 1822136,
      "reputation": 4117,
      "user_id": 84,
      "user_type": "registered",
      "accept_rate": 82,
      "profile_image": "https://www.gravatar.com/avatar/5394d14f632e89b2dfc937e3660f0079?s=256&d=identicon&r=PG",
      "display_name": "Rubens",
      "link": "https://datascience.stackexchange.com/users/84/rubens"
    },
    "is_answered": true,
    "view_count": 560,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1532650266,
    "creation_date": 1403105243,
    "last_edit_date": 1480700950,
    "question_id": 453,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/453/what-is-the-difference-between-global-and-universal-compression-methods",
    "title": "What is the difference between global and universal compression methods?",
    "body": "<p>I understand that compression methods may be split into two main sets: </p>\n\n<ol>\n<li>global</li>\n<li>local</li>\n</ol>\n\n<p>The first set works regardless of the data being processed, i.e., they do not rely on any characteristic of the data, and thus need not to perform any preprocessing on any part of the dataset (before the compression itself). On the other hand, local methods analyze the data, extracting information that usually improves the compression rate.</p>\n\n<p>While reading about some of these methods, I noticed that <a href=\"http://en.wikipedia.org/wiki/Universal_code_%28data_compression%29#Universal_and_non-universal_codes\" rel=\"noreferrer\">the unary method is not universal</a>, which surprised me since I thought \"globality\" and \"universality\" referred to the same thing. The unary method does not rely on characteristics of the data to yield its encoding (i.e., it is a global method), and therefore it should be global/universal, shouldn't it?</p>\n\n<p><strong>My primary questions:</strong></p>\n\n<ul>\n<li>What is the difference between universal and global methods? </li>\n<li>Aren't these classifications synonyms?</li>\n</ul>\n"
  },
  {
    "tags": [
      "bigdata",
      "text-mining",
      "recommender-system"
    ],
    "owner": {
      "account_id": 4632083,
      "reputation": 251,
      "user_id": 986,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/a518fb71c93d5492ac9300ed04768d99?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "R.D",
      "link": "https://datascience.stackexchange.com/users/986/r-d"
    },
    "is_answered": true,
    "view_count": 2516,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1509955629,
    "creation_date": 1403129458,
    "last_edit_date": 1509955629,
    "question_id": 461,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/461/preference-matching-algorithm",
    "title": "Preference Matching Algorithm",
    "body": "<p>There's this side project I'm working on where I need to structure a solution to the following problem.</p>\n\n<p>I have two groups of people (clients). Group <code>A</code> intends to buy and group <code>B</code> intends to sell a determined product <code>X</code>. The product has a series of attributes <code>x_i</code>, and my objective is to facilitate the transaction between <code>A</code> and <code>B</code> by matching their preferences. The main idea is to point out to each member of <code>A</code> a corresponding in <code>B</code> whose product better suits his needs, and vice versa.</p>\n\n<p>Some complicating aspects of the problem:</p>\n\n<ol>\n<li><p>The list of attributes is not finite. The buyer might be interested in a very particular characteristic or some kind of design, which is rare among the population and I can't predict. Can't previously list all the attributes;</p></li>\n<li><p>Attributes might be continuous, binary, or non-quantifiable (ex: price, functionality, design);</p></li>\n</ol>\n\n<p>Any suggestion on how to approach this problem and solve it in an automated way?</p>\n\n<p>I would also appreciate some references to other similar problems if possible.</p>\n\n<hr>\n\n<p>Great suggestions! Many similarities in to the way I'm thinking of approaching the problem.</p>\n\n<p>The main issue on mapping the attributes is that the level of detail to which the product should be described depends on each buyers. Lets take an example of a car. The product car has lots and lots of attributes that range from its performance, mechanical structure, price etc.</p>\n\n<p>Suppose I just want a cheap car, or an electric car. Ok, that's easy to map because they represent main features of this product. But lets say, for instance, that I want a car with Dual-Clutch transmission or Xenon headlights. Well there might be many cars on the data base with this attributes but I wouldn't ask the seller to fill in this level of detail to their product prior to the information that there is someone looking them. Such a procedure would require every seller fill a complex, very detailed, form just try to sell his car on the platform. Just wouldn't work.</p>\n\n<p>But still, my challenge is to try to be as detailed as necessary in the search to make a good match. So the way I'm thinking is mapping main aspects of the product, those that are probably relevant to everyone, to narrow down de group of potential sellers.</p>\n\n<p>Next step would be a refined search. In order to avoid creating a too detailed form I could ask buyers and sellers to write a free text of their specification. And then use some word matching algorithm to find possible matches. Although I understand that this is not a proper solution to the problem because the seller cannot guess what the buyer needs. But might get me close.  </p>\n\n<p>The weighting criteria suggested is great. It allows me to quantify the level to which the seller matches the buyers needs. The scaling part might be a problem though, because the importance of each attribute varies from client to client. I'm thinking of using some kind of pattern recognition or just asking de buyer to input the level of importance of each attribute.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "statistics",
      "glm"
    ],
    "owner": {
      "account_id": 2758844,
      "reputation": 323,
      "user_id": 1021,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b0e293f9c711297ee5e23c58d24e96c6?s=256&d=identicon&r=PG",
      "display_name": "user77571",
      "link": "https://datascience.stackexchange.com/users/1021/user77571"
    },
    "is_answered": true,
    "view_count": 16964,
    "accepted_answer_id": 489,
    "answer_count": 4,
    "score": 13,
    "last_activity_date": 1480938180,
    "creation_date": 1403200944,
    "last_edit_date": 1436355470,
    "question_id": 488,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/488/is-glm-a-statistical-or-machine-learning-model",
    "title": "Is GLM a statistical or machine learning model?",
    "body": "<p>I thought that generalized linear model (GLM) would be considered a statistical model, but a friend told me that some papers classify it as a machine learning technique. Which one is true (or more precise)? Any explanation would be appreciated.</p>\n"
  },
  {
    "tags": [
      "cross-validation",
      "sampling"
    ],
    "owner": {
      "account_id": 241216,
      "reputation": 5474,
      "user_id": 97,
      "user_type": "registered",
      "accept_rate": 77,
      "profile_image": "https://www.gravatar.com/avatar/68acdc33e278ab19debf230662a7ad96?s=256&d=identicon&r=PG",
      "display_name": "IgorS",
      "link": "https://datascience.stackexchange.com/users/97/igors"
    },
    "is_answered": true,
    "view_count": 12136,
    "accepted_answer_id": 513,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1404201870,
    "creation_date": 1403287066,
    "last_edit_date": 1403890599,
    "question_id": 511,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/511/cross-validation-k-fold-vs-repeated-random-sub-sampling",
    "title": "Cross-validation: K-fold vs Repeated random sub-sampling",
    "body": "<p>I wonder which type of model cross-validation to choose for classification problem: K-fold or random sub-sampling (bootstrap sampling)?</p>\n\n<p>My best guess is to use 2/3 of the data set (which is ~1000 items) for training and 1/3 for validation.</p>\n\n<p>In this case K-fold gives only three iterations(folds), which is not enough to see stable average error.</p>\n\n<p>On the other hand I don't like random sub-sampling feature: that some items won't be ever selected for training/validation, and some will be used more than once.</p>\n\n<p>Classification algorithms used: random forest &amp; logistic regression.</p>\n"
  },
  {
    "tags": [
      "classification",
      "performance"
    ],
    "owner": {
      "account_id": 57272,
      "reputation": 395,
      "user_id": 474,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/3da837d1227e7df8153c68d502f64994?s=256&d=identicon&r=PG",
      "display_name": "Dave Challis",
      "link": "https://datascience.stackexchange.com/users/474/dave-challis"
    },
    "is_answered": true,
    "view_count": 386,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1596711836,
    "creation_date": 1403967438,
    "last_edit_date": 1496691018,
    "question_id": 623,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/623/measuring-performance-of-different-classifiers-with-different-sample-sizes",
    "title": "Measuring performance of different classifiers with different sample sizes",
    "body": "<p>I'm currently using several different classifiers on various entities extracted from text, and using precision/recall as a summary of how well each separate classifier performs across a given dataset.</p>\n\n<p>I'm wondering if there's a meaningful way of comparing the performance of these classifiers in a similar way, but which also takes into account the total numbers of each entity in the test data that's being classified?</p>\n\n<p>Currently, I'm using precision/recall as a measure of performance, so might have something like:</p>\n\n<pre><code>                    Precision Recall\nPerson classifier   65%       40%\nCompany classifier  98%       90%\nCheese classifier   10%       50%\nEgg classifier      100%      100%\n</code></pre>\n\n<p>However, the dataset I'm running these on might contain 100k people, 5k companies, 500 cheeses, and 1 egg.</p>\n\n<p>So is there a summary statistic I can add to the above table which also takes into account the total number of each item? Or is there some way of measuring the fact that e.g. 100% prec/rec on the Egg classifier might not be meaningful with only 1 data item?</p>\n\n<p>Let's say we had hundreds of such classifiers, I guess I'm looking for a good way to answer questions like \"Which classifiers are underperforming? Which classifiers lack sufficient test data to tell whether they're underperforming?\". </p>\n"
  },
  {
    "tags": [
      "bigdata",
      "data-mining",
      "efficiency",
      "state-of-the-art"
    ],
    "owner": {
      "account_id": 1822136,
      "reputation": 4117,
      "user_id": 84,
      "user_type": "registered",
      "accept_rate": 82,
      "profile_image": "https://www.gravatar.com/avatar/5394d14f632e89b2dfc937e3660f0079?s=256&d=identicon&r=PG",
      "display_name": "Rubens",
      "link": "https://datascience.stackexchange.com/users/84/rubens"
    },
    "is_answered": true,
    "view_count": 2059,
    "accepted_answer_id": 1065,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1409423767,
    "creation_date": 1405185952,
    "last_edit_date": 1405220746,
    "question_id": 730,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/730/is-fpgrowth-still-considered-state-of-the-art-in-frequent-pattern-mining",
    "title": "Is FPGrowth still considered &quot;state of the art&quot; in frequent pattern mining?",
    "body": "<p>As far as I know the development of algorithms to solve the Frequent Pattern Mining (FPM) problem, the road of improvements have some main checkpoints. Firstly, the <a href=\"http://en.wikipedia.org/wiki/Apriori_algorithm\">Apriori</a> algorithm was proposed in 1993, by <a href=\"http://dl.acm.org/citation.cfm?id=170072\">Agrawal et al.</a>, along with the formalization of the problem. The algorithm was able to <em>strip-off</em> some sets from the <code>2^n - 1</code> sets (powerset) by using a lattice to maintain the data. A drawback of the approach was the need to re-read the database to compute the frequency of each set expanded.</p>\n\n<p>Later, on year 1997, <a href=\"http://www.computer.org/csdl/trans/tk/2000/03/k0372-abs.html\">Zaki et al.</a> proposed the algorithm <a href=\"http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_Eclat_Algorithm\">Eclat</a>, which <em>inserted</em> the resulting frequency of each set inside the lattice. This was done by adding, at each node of the lattice, the set of transaction-ids that had the items from root to the referred node. The main contribution is that one does not have to re-read the entire dataset to know the frequency of each set, but the memory required to keep such data structure built may exceed the size of the dataset itself.</p>\n\n<p>In 2000, <a href=\"http://dl.acm.org/citation.cfm?doid=335191.335372\">Han et al.</a> proposed an algorithm named <a href=\"http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Frequent_Pattern_Mining/The_FP-Growth_Algorithm\">FPGrowth</a>, along with a prefix-tree data structure named FPTree. The algorithm was able to provide significant data compression, while also granting that only frequent itemsets would be yielded (without candidate itemset generation). This was done mainly by sorting the items of each transaction in decreasing order, so that the most frequent items are the ones with the least repetitions in the tree data structure. Since the frequency only descends while traversing the tree in-depth, the algorithm is able to <em>strip-off</em> non-frequent itemsets.</p>\n\n<p><strong>Edit</strong>:</p>\n\n<p><strike>As far as I know, this may be considered a state-of-the-art algorithm, but I'd like to know about other proposed solutions. What other algorithms for FPM are considered \"state-of-the-art\"? What is the <em>intuition</em>/<em>main-contribution</em> of such algorithms?</strike></p>\n\n<p>Is the FPGrowth algorithm still considered \"state of the art\" in frequent pattern mining? If not, what algorithm(s) may extract frequent itemsets from large datasets more efficiently?</p>\n"
  },
  {
    "tags": [
      "nlp",
      "databases"
    ],
    "owner": {
      "account_id": 278712,
      "reputation": 298,
      "user_id": 1163,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/1ad295c828d2862df905bff480f0f0da?s=256&d=identicon&r=PG",
      "display_name": "Phonon",
      "link": "https://datascience.stackexchange.com/users/1163/phonon"
    },
    "is_answered": true,
    "view_count": 3376,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1408184719,
    "creation_date": 1405986791,
    "question_id": 802,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/802/efficient-database-model-for-storing-data-indexed-by-n-grams",
    "title": "Efficient database model for storing data indexed by n-grams",
    "body": "<p>I'm working on an application which requires creating a very large database of n-grams that exist in a large text corpus.</p>\n\n<p>I need three efficient operation types: Lookup and insertion indexed by the n-gram itself, and querying for all n-grams that contain a sub-n-gram.</p>\n\n<p>This sounds to me like the database should be a gigantic document tree, and document databases, e.g. Mongo, should be able to do the job well, but I've never used those at scale.</p>\n\n<p>Knowing the Stack Exchange question format, I'd like to clarify that I'm not asking for suggestions on specific technologies, but rather a type of database that I should be looking for to implement something like this at scale.</p>\n"
  },
  {
    "tags": [
      "nlp",
      "text-mining",
      "feature-extraction"
    ],
    "owner": {
      "account_id": 368960,
      "reputation": 661,
      "user_id": 2750,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://i.sstatic.net/03C6h.jpg?s=256",
      "display_name": "MaticDiba",
      "link": "https://datascience.stackexchange.com/users/2750/maticdiba"
    },
    "is_answered": true,
    "view_count": 2876,
    "accepted_answer_id": 858,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1524103367,
    "creation_date": 1406531989,
    "last_edit_date": 1498833937,
    "question_id": 853,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/853/unsupervised-feature-learning-for-ner",
    "title": "Unsupervised feature learning for NER",
    "body": "<p>I have implemented NER system with the use of CRF algorithm with my handcrafted features that gave quite good results. The thing is that I used lots of different features including POS tags and lemmas.</p>\n\n<p>Now I want to make the same NER for different language. The problem here is that I can't use POS tags and lemmas. I started reading articles about deep learning and unsupervised feature learning.</p>\n\n<p>My question is: </p>\n\n<p>Is it possible to use methods for unsupervised feature learning with CRF algorithm? Did anyone try this and got any good result? Is there any article or tutorial about this matter?</p>\n\n<p>I still don't completely understand this way of feature creation so I don't want to spend to much time for something that won't work. So any information would be really helpful. To create whole NER system based on deep learning is a bit to much for now.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "nlp",
      "feature-selection",
      "feature-extraction"
    ],
    "owner": {
      "account_id": 1304470,
      "reputation": 231,
      "user_id": 3064,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/c223d7ebf83718b8009c24a654616c3e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "working",
      "link": "https://datascience.stackexchange.com/users/3064/working"
    },
    "is_answered": true,
    "view_count": 4415,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1492680031,
    "creation_date": 1408900180,
    "question_id": 1034,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/1034/what-features-are-generally-used-from-parse-trees-in-classification-process-in-n",
    "title": "What features are generally used from Parse trees in classification process in NLP?",
    "body": "<p>I am exploring different types of parse tree structures. The two widely known parse tree structures are \na) Constituency based parse tree and \nb) Dependency based parse tree structures. </p>\n\n<p>I am able to use generate both types of parse tree structures using Stanford NLP package. However, I am not sure how to use these tree structures for my classification task. </p>\n\n<p>For e.g If I want to do sentiment analysis and want to categorize text into positive and negative classes, what features can I derive from parse tree structures for my classification task?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "text-mining",
      "beginner"
    ],
    "owner": {
      "account_id": 4512078,
      "reputation": 131,
      "user_id": 3215,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7439db1d88b611eb4ecd007400f515b0?s=256&d=identicon&r=PG",
      "display_name": "Grzegorz E.",
      "link": "https://datascience.stackexchange.com/users/3215/grzegorz-e"
    },
    "is_answered": true,
    "view_count": 7151,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1490427116,
    "creation_date": 1409918891,
    "last_edit_date": 1445646042,
    "question_id": 1078,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/1078/unstructured-text-classification",
    "title": "Unstructured text classification",
    "body": "<p>I'm going to classify unstructured text documents, namely web sites of unknown structure. The number of classes to which I am classifying is limited (at this point, I believe there is no more than three). Does anyone have a suggested for how I might get started?</p>\n\n<p>Is the \"bag of words\" approach feasible here? Later, I could add another classification stage based on document structure (perhaps decision trees).</p>\n\n<p>I am somewhat familiar with Mahout and Hadoop, so I prefer Java-based solutions. If needed, I can switch to Scala and/or Spark engine (the ML library).</p>\n"
  },
  {
    "tags": [
      "text-mining",
      "scraping"
    ],
    "owner": {
      "account_id": 4426354,
      "reputation": 1919,
      "user_id": 2723,
      "user_type": "registered",
      "accept_rate": 53,
      "profile_image": "https://i.sstatic.net/U7HAo.png?s=256",
      "display_name": "Hack-R",
      "link": "https://datascience.stackexchange.com/users/2723/hack-r"
    },
    "is_answered": true,
    "view_count": 373,
    "accepted_answer_id": 2624,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1506924995,
    "creation_date": 1417794714,
    "last_edit_date": 1447954939,
    "question_id": 2618,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/2618/ethically-and-cost-effectively-scaling-data-scrapes",
    "title": "Ethically and Cost-effectively Scaling Data Scrapes",
    "body": "<p>Few things in life give me pleasure like scraping structured and unstructured data from the Internet and making use of it in my models. </p>\n\n<p>For instance, the Data Science Toolkit (or <code>RDSTK</code> for R programmers) allows me to pull lots of good location-based data using IP's or addresses and the <code>tm.webmining.plugin</code> for R's <code>tm</code> package makes scraping financial and news data straightfoward. When going beyond such (semi-) structured data I tend to use <code>XPath</code>.</p>\n\n<p>However, I'm constantly getting throttled by limits on the number of queries you're allowed to make. I think Google limits me to about 50,000 requests per 24 hours, which is a problem for Big Data.</p>\n\n<p>From a <em>technical</em> perspective getting around these limits is easy -- just switch IP addresses and purge other identifiers from your environment. However, this presents both ethical and financial concerns (I think?).</p>\n\n<p>Is there a solution that I'm overlooking?</p>\n"
  },
  {
    "tags": [
      "data-mining",
      "graphs",
      "databases",
      "social-network-analysis"
    ],
    "owner": {
      "account_id": 5088209,
      "reputation": 1940,
      "user_id": 5091,
      "user_type": "registered",
      "accept_rate": 38,
      "profile_image": "https://i.sstatic.net/zzgFQ.jpg?s=256",
      "display_name": "Sreejithc321",
      "link": "https://datascience.stackexchange.com/users/5091/sreejithc321"
    },
    "is_answered": true,
    "view_count": 6825,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1435303947,
    "creation_date": 1418877366,
    "last_edit_date": 1433264453,
    "question_id": 3719,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/3719/neo4j-vs-orientdb-vs-titan",
    "title": "Neo4j vs OrientDB vs Titan",
    "body": "<p>I am working on a data-science project related on social relationship mining and need to store data in some graph databases. Initially I chose Neo4j as the database. But it seams Neo4j doesn't scale well. The alternative I found out are Titan  and oriebtDB. I have gone through <a href=\"http://db-engines.com/en/system/Neo4j%3BOrientDB%3BTitan\" rel=\"noreferrer\">this</a> comparison on these three Databases, But I would like to get more details on  these databases. So Could some one help me in choosing the best one. Mainly I would like to compare performance, scaling, on line documentation/tutorials available, Python library support, query language complexity and graph algorithm support of these databases. Also is there any other good database options ?</p>\n"
  },
  {
    "tags": [
      "time-series"
    ],
    "owner": {
      "account_id": 2247995,
      "reputation": 2749,
      "user_id": 303,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b783ab7b39873c411ed32b676f1e0e5c?s=256&d=identicon&r=PG",
      "display_name": "Lucas Morin",
      "link": "https://datascience.stackexchange.com/users/303/lucas-morin"
    },
    "is_answered": true,
    "view_count": 14012,
    "accepted_answer_id": 3801,
    "answer_count": 5,
    "score": 13,
    "last_activity_date": 1420506386,
    "creation_date": 1419766154,
    "last_edit_date": 1419767412,
    "question_id": 3770,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/3770/how-to-merge-monthly-daily-and-weekly-data",
    "title": "How to merge monthly, daily and weekly data?",
    "body": "<p>Google Trends returns weekly data so I have to find a way to merge them with my daily/monthly data. </p>\n\n<p>What I have done so far is to break each serie into daily data, for exemple: </p>\n\n<p>from:</p>\n\n<p>2013-03-03 - 2013-03-09 37</p>\n\n<p>to:</p>\n\n<p>2013-03-03 37 \n2013-03-04 37\n2013-03-05 37\n2013-03-06 37\n2013-03-07 37\n2013-03-08 37\n2013-03-09 37</p>\n\n<p>But this is adding a lot of complexity to my problem. I was trying to predict google searchs from the last 6 months values, or 6 values in monthly data. Daily data would imply a work on 180 past values. (I have 10 years of data so 120 points in monthly data / 500+ in weekly data/ 3500+ in daily data)</p>\n\n<p>The other approach would be to \"merge\" daily data in weekly/monthly data. But some questions arise from this process. Some data can be averaged because their sum represent something. Rainfall for example, the amount of rain in a given week will be the sum of the amounts for each days composing the weeks. </p>\n\n<p>In my case I am dealing with prices, financial rates and other things. For the prices it is common in my field to take volume exchanged into account, so the weekly data would be a weighted average. For financial rates it is a bit more complex a some formulas are involved to build weekly rates from daily rates.  For the other things i don't know the underlying properties. I think those properties are important to avoid meaningless indicators (an average of fiancial rates would be a non-sense for example). </p>\n\n<p>So three questions: </p>\n\n<p><strong>For known and unknown properties, how should I proceed to go from daily to weekly/monthly data ?</strong></p>\n\n<p>I feel like breaking weekly/monthly data into daily data like i've done is somewhat wrong because I am introducing quantities that have no sense in real life. So almost the same question: </p>\n\n<p><strong>For known and unknown properties, how should I proceed to go from weekly/monthly to daily data ?</strong></p>\n\n<p>Last but not least: <strong>when given two time series with different time steps, what is better: Using the Lowest or the biggest time step ?</strong> I think this is a compromise between the number of data and the complexity of the model but I can't see any strong argument to choose between those options. </p>\n\n<p>Edit: if you know a tool (in R Python even Excel) to do it easily it would be very appreciated.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "svm"
    ],
    "owner": {
      "account_id": 5677427,
      "reputation": 131,
      "user_id": 7944,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/a7c61313bcd775ee4c34f88cd2140374?s=256&d=identicon&r=PG",
      "display_name": "SVM",
      "link": "https://datascience.stackexchange.com/users/7944/svm"
    },
    "is_answered": true,
    "view_count": 6088,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1517582420,
    "creation_date": 1422114748,
    "last_edit_date": 1422178767,
    "question_id": 4936,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/4936/what-happens-when-we-train-a-linear-svm-on-non-linearly-separable-data",
    "title": "What happens when we train a linear SVM on non-linearly separable data?",
    "body": "<p>What happens when we train a basic support vector machine (linear kernel and no soft-margin) on non-linearly separable data? The optimisation problem is not feasible, so what does the minimisation algorithm return?</p>\n"
  },
  {
    "tags": [
      "knowledge-base"
    ],
    "owner": {
      "account_id": 6267339,
      "reputation": 155,
      "user_id": 9488,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/877731905619029/picture?type=large",
      "display_name": "Antonio Edgar Martinez",
      "link": "https://datascience.stackexchange.com/users/9488/antonio-edgar-martinez"
    },
    "is_answered": true,
    "view_count": 3706,
    "closed_date": 1494189666,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1499170232,
    "creation_date": 1430957768,
    "last_edit_date": 1499170232,
    "question_id": 5703,
    "link": "https://datascience.stackexchange.com/questions/5703/are-ontologies-and-the-semantic-web-dead",
    "closed_reason": "Needs more focus",
    "title": "Are ontologies and the Semantic Web dead?",
    "body": "<p>Is the Semantic Web dead? Are ontologies dead?</p>\n\n<p>I am developing a work plan for my thesis about <em>\"A knowledge base through a set ontology for interest groups around wetlands\"</em>. I have been researching and developing ontologies for it but I am still unclear about many things. What is the modeling language for ontologies?</p>\n\n<p>Which methodology for ontologies is better? <a href=\"http://semanticweb.org/wiki/OTK_methodology\" rel=\"noreferrer\">OTK</a> or <a href=\"http://semanticweb.org/wiki/METHONTOLOGY\" rel=\"noreferrer\">METHONTOLOGY</a>?</p>\n\n<p>Is there any program that does  as does </p>\n\n<p><a href=\"http://www.hd.uib.no/AcoHum/abs/Mejia.htm\" rel=\"noreferrer\">Cratilo</a> is a software for analyzing of textual corpora and for extraction of specific terms of the domain of study (it is developed by professors Jorge Antonio Mejia, Francisco Javier Alvarez and John Albeiro Snchez, Institute of Philosophy the University of Antioquia). It enables lexical analysis of texts, identifying the words that appear their frequency and location in the text. Through a process of recognition, Cratylus identifies all the words in the text and builds a database becomes the draft analysis of the work. Are there other similar tools?</p>\n\n<p>Can the terms found by Cratilo be used to create a knowledge base?</p>\n\n<p>What are the existing open semantic frameworks that can be used for such things? </p>\n\n<p>Is there software that automatically creates RDF, OWL, and XML? How does Tails work? Jena? Sesame? </p>\n"
  },
  {
    "tags": [
      "nlp",
      "word-embeddings"
    ],
    "owner": {
      "account_id": 169656,
      "reputation": 5862,
      "user_id": 843,
      "user_type": "registered",
      "accept_rate": 33,
      "profile_image": "https://i.sstatic.net/Z99mk.jpg?s=256",
      "display_name": "Franck Dernoncourt",
      "link": "https://datascience.stackexchange.com/users/843/franck-dernoncourt"
    },
    "is_answered": true,
    "view_count": 11043,
    "protected_date": 1619364287,
    "accepted_answer_id": 6521,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1619362030,
    "creation_date": 1437367697,
    "last_edit_date": 1438106116,
    "question_id": 6506,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/6506/shall-i-use-the-euclidean-distance-or-the-cosine-similarity-to-compute-the-seman",
    "title": "Shall I use the Euclidean Distance or the Cosine Similarity to compute the semantic similarity of two words?",
    "body": "<p>I want to compute the semantic similarity of two words using their vector representations (obtained using e.g. word2vec, GloVe, etc.). Shall I use the Euclidean Distance or the Cosine Similarity?</p>\n\n<p>The <a href=\"http://nlp.stanford.edu/projects/glove/\" rel=\"noreferrer\">GloVe website</a> mentions both measures without telling the pros and cons of each:</p>\n\n<blockquote>\n  <p>The Euclidean distance (or cosine similarity) between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words.</p>\n</blockquote>\n"
  },
  {
    "tags": [
      "beginner",
      "career"
    ],
    "owner": {
      "account_id": 102447,
      "reputation": 341,
      "user_id": 10879,
      "user_type": "registered",
      "accept_rate": 25,
      "profile_image": "https://www.gravatar.com/avatar/fe80cff5f1586741e1f03d02168d882f?s=256&d=identicon&r=PG",
      "display_name": "Volatil3",
      "link": "https://datascience.stackexchange.com/users/10879/volatil3"
    },
    "is_answered": true,
    "view_count": 1525,
    "answer_count": 8,
    "community_owned_date": 1533002494,
    "score": 13,
    "last_activity_date": 1659438359,
    "creation_date": 1437768607,
    "last_edit_date": 1450868587,
    "question_id": 6576,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/6576/i-am-a-programmer-how-do-i-get-into-field-of-data-science",
    "title": "I am a programmer, how do I get into field of Data Science?",
    "body": "<p>First of all this term sounds so obscure.</p>\n\n<p>Anyways..I am a software programmer. One of the languages I can code is Python. Speaking of Data I can use SQL and can do Data Scraping. What I figured out so far after reading soo many articles that Data Science is all about good at:</p>\n\n<p>1- Stats</p>\n\n<p>2- Algebra</p>\n\n<p>3- Data Analysis</p>\n\n<p>4- Visualisation.</p>\n\n<p>5- Machine Learning.</p>\n\n<p><strong>What I know so far:</strong></p>\n\n<p>1- Python Programming\n2- Data scrapping in Python</p>\n\n<p>Can you experts guide me or suggest a roadmap to brush up both theory and practical? I have given around 8 months of time frame to myself.</p>\n"
  },
  {
    "tags": [
      "feature-selection",
      "random-forest",
      "scikit-learn"
    ],
    "owner": {
      "account_id": 169656,
      "reputation": 5862,
      "user_id": 843,
      "user_type": "registered",
      "accept_rate": 33,
      "profile_image": "https://i.sstatic.net/Z99mk.jpg?s=256",
      "display_name": "Franck Dernoncourt",
      "link": "https://datascience.stackexchange.com/users/843/franck-dernoncourt"
    },
    "is_answered": true,
    "view_count": 17463,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1438710920,
    "creation_date": 1438710275,
    "last_edit_date": 1492087841,
    "question_id": 6683,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/6683/feature-selection-using-feature-importances-in-random-forests-with-scikit-learn",
    "title": "Feature selection using feature importances in random forests with scikit-learn",
    "body": "<p>I have <a href=\"http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\" rel=\"noreferrer\">plotted the feature importances in random forests with scikit-learn</a>. In order to improve the prediction using random forests, how can I use the plot information to remove features? I.e. how to spot whether a feature is useless or even worse decrease of the random forests performance, based on the plot information? The plot is based on the attribute <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" rel=\"noreferrer\"><code>feature_importances_</code></a> and I use the classifier <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\" rel=\"noreferrer\"><code>sklearn.ensemble.RandomForestClassifier</code></a>.</p>\n\n<p>I am aware that there exist <a href=\"https://datascience.stackexchange.com/a/6651/843\">other techniques for feature selection</a>, but in this question I want to focus on how to use feature <code>feature_importances_</code>.</p>\n\n<hr>\n\n<p>Examples of such feature importance plots:</p>\n\n<p><a href=\"https://i.sstatic.net/ySztK.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/ySztK.png\" alt=\"enter image description here\"></a></p>\n\n<p><a href=\"https://i.sstatic.net/xeXHY.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/xeXHY.png\" alt=\"enter image description here\"></a></p>\n"
  },
  {
    "tags": [
      "algorithms"
    ],
    "owner": {
      "account_id": 2682407,
      "reputation": 233,
      "user_id": 12438,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/c78217dbfe4e5091ee7ee5876581caed?s=256&d=identicon&r=PG",
      "display_name": "Josh Brown Kramer",
      "link": "https://datascience.stackexchange.com/users/12438/josh-brown-kramer"
    },
    "is_answered": true,
    "view_count": 735,
    "accepted_answer_id": 24529,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1510243835,
    "creation_date": 1440518648,
    "question_id": 6899,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/6899/efficient-algorithm-to-compute-the-roc-curve-for-a-classifier-consisting-of-an-e",
    "title": "Efficient algorithm to compute the ROC curve for a classifier consisting of an ensemble of disjoint classifiers",
    "body": "<p>Suppose I have classifiers C_1 ... C_n that are disjoint in the sense that no two will return true on the same input (e.g. the nodes in a decision tree).  I want to build a new classifier that is the union of some subset of these (e.g. I want to decide on which leaves of a decision tree to give a positive classification).  Of course, in doing so there will be a trade off between sensitivity and positive predictive value.  So I would like to see a ROC curve.  In principle I could do this by enumerating all subsets of the classifiers and computing the resulting sensitivity and PPV.  However, this is prohibitively expensive if n is more than 30 or so.  On the other hand, there are almost certainly some combinations that are not Pareto optimal, so there might be some branch and bound strategy, or something, that avoids most of the computation in many cases.</p>\n\n<p>I would like advice about whether this approach is likely to be fruitful and whether there is any work or if you have any ideas about efficiently computing the ROC curve in the situation above.</p>\n"
  },
  {
    "tags": [
      "classification",
      "random-forest",
      "decision-trees",
      "class-imbalance"
    ],
    "owner": {
      "account_id": 420531,
      "reputation": 293,
      "user_id": 14029,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/c1b65827e4b0ad1078efb12b17d27b1a?s=256&d=identicon&r=PG",
      "display_name": "user798275",
      "link": "https://datascience.stackexchange.com/users/14029/user798275"
    },
    "is_answered": true,
    "view_count": 24360,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1557179159,
    "creation_date": 1447344597,
    "question_id": 8820,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/8820/unbalanced-classes-how-to-minimize-false-negatives",
    "title": "Unbalanced classes -- How to minimize false negatives?",
    "body": "<p>I have a dataset that has a binary class attribute. There are 623 instances with class +1 (cancer positive) and 101,671 instances with class -1 (cancer negative).</p>\n\n<p>I've tried various algorithms (Naive Bayes, Random Forest, AODE, C4.5) and all of them have unacceptable false negative ratios. Random Forest has the highest overall prediction accuracy (99.5%) and the lowest false negative ratio, but still misses 79% of positive classes (i.e. fails to detect 79% of malignant tumors).</p>\n\n<p>Any ideas how I can improve this situation?</p>\n\n<p>Thanks!</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "julia"
    ],
    "owner": {
      "account_id": 6444670,
      "reputation": 8476,
      "user_id": 11097,
      "user_type": "registered",
      "accept_rate": 75,
      "profile_image": "https://i.sstatic.net/z0jss.png?s=256",
      "display_name": "Dawny33",
      "link": "https://datascience.stackexchange.com/users/11097/dawny33"
    },
    "is_answered": true,
    "view_count": 12149,
    "accepted_answer_id": 8923,
    "answer_count": 5,
    "score": 13,
    "last_activity_date": 1596635891,
    "creation_date": 1447913093,
    "question_id": 8909,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/8909/best-julia-library-for-neural-networks",
    "title": "Best Julia library for neural networks",
    "body": "<p>I have been using this library for basic neural network construction and analysis.</p>\n\n<p>However, it does not have support for building multi-layered neural networks, etc.</p>\n\n<p>So, I would like to know of any nice libraries for doing advanced neural networks and Deep Learning in Julia.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "dataset",
      "data-cleaning",
      "data"
    ],
    "owner": {
      "account_id": 7322944,
      "reputation": 629,
      "user_id": 14200,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://lh6.googleusercontent.com/-xWUFTu_pds0/AAAAAAAAAAI/AAAAAAAAALA/P9U9tQBilgM/s256-rj/photo.jpg",
      "display_name": "jmvllt",
      "link": "https://datascience.stackexchange.com/users/14200/jmvllt"
    },
    "is_answered": true,
    "view_count": 4402,
    "accepted_answer_id": 9230,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1449363201,
    "creation_date": 1448449885,
    "question_id": 9020,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9020/do-i-have-to-standardize-my-new-polynomial-features",
    "title": "Do I have to standardize my new polynomial features?",
    "body": "<p>I have a vector X with n features previously standardized.</p>\n\n<p>If I want to generate new polynomial features (let say adding square features), do I need to do another standardization on these new features after the computing ? </p>\n\n<p>Because knowing that my original features are already between 0 and 1, the new polynomial features will also be between 0 and 1. But the higher the degree is, the higher the \"ratio\" between the original one and the polynomial one will be.</p>\n\n<p>Also,  would that be better to do the square computing on the values of the non standardized features, and then standardize these new features ?</p>\n\n<p>By the way, when I say \"standardization\", I m thinking about substrat each value by the mean and divide it by the std.</p>\n\n<p>Thanks in advance,</p>\n\n<p>Regards.</p>\n"
  },
  {
    "tags": [
      "data"
    ],
    "owner": {
      "account_id": 7570768,
      "reputation": 131,
      "user_id": 15180,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/130e2a15c1389fa0ca8c16906faab3f1?s=256&d=identicon&r=PG",
      "display_name": "user15180",
      "link": "https://datascience.stackexchange.com/users/15180/user15180"
    },
    "is_answered": true,
    "view_count": 35989,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1615678145,
    "creation_date": 1451932557,
    "last_edit_date": 1615678145,
    "question_id": 9616,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/9616/how-to-create-us-state-choropleth-map",
    "title": "How to create US state choropleth map",
    "body": "<p>I have a value associated with each US state (let's pretend it's the average temperature in January for each state). I want to display this data as a heat map of the United States. To be clear, it would be a map of the US with each state having a color from a color gradient that corresponds to a quantitative value. I don't know what this plot type is called, so I'm having trouble figuring out how to do this in MATLAB or another program, but does anyone know an easy way to do this? I have a vector of data (of length 50) and just need to make a US map plot.</p>\n"
  },
  {
    "tags": [
      "classification",
      "clustering",
      "time-series"
    ],
    "owner": {
      "account_id": 3436315,
      "reputation": 131,
      "user_id": 15230,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/fTj4X.png?s=256",
      "display_name": "JusefPol",
      "link": "https://datascience.stackexchange.com/users/15230/jusefpol"
    },
    "is_answered": true,
    "view_count": 729,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1513607371,
    "creation_date": 1452155562,
    "last_edit_date": 1513607371,
    "question_id": 9667,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/9667/classify-customers-based-on-2-features-and-a-time-series-of-events",
    "title": "Classify Customers based on 2 features AND a Time series of events",
    "body": "<p>I need help on what should be my next step in an algorithm I am designing.</p>\n\n<p>Due to NDAs, I can't disclose much, but I'll try to be generic and understandable.</p>\n\n<p>Basically, after several steps in the algorithms, I have this:</p>\n\n<p>For each customer that I have, and events that they do during a month, during first steps I have clustered the events into several categories (each customer will have the events separated into categories that go from 1 to x being x between 1 to 25, generally the first categories have more density of events than the others).</p>\n\n<p>For each category and customer I have created a time series aggregating the events of the month per hour (getting patterns of when these events are being done). Also I am using a couple of normalizing variables based on the number of days over a month (30 days) that the guy performs at least one event, and the number of days with at least one event over the total of days with at least one event (aggregating all clusters). The first one gives me a ratio of how active the customer is during the month, and the second one weights the category against the others.</p>\n\n<p>The final table looks like this</p>\n\n<pre><code>|*Identifier*|  *firstCat* | *feature1* | *feature2*  |   {      *(TIME SERIES)*   }\n\nCustomerID  |  ClusterID |  DaysOver30 | DaysOverTotal | Events9AM Events10AM ... \n\n xx | 1 | 0,69 |  0,72 |  0,2   0,13   ...\n\n xx | 2 | 0,11 |  0,28 |  0,1   0,45   ...\n\n xy | 1 | 0,23 |  0,88 |  0,00  0,60   ...\n\n xy | 2 | 0,11 |  0,08 |  1,00  0,00   ...\n\n xy | 3 | 0,10 |  0,04 |  0,40  0,60   ...\n</code></pre>\n\n<p>The time series variables are the percentage over the total of events per day on each specific category (this means that per each row adding up all variables should be 1). The reason of doing it like that is because for example a time series with events <code>0 0 0 1 0</code> and <code>1 1 1 2 1</code> are completely different, and standardizing to normal would give similar results. And due to high skew between different categories, I check the values on the time series independently with the others.</p>\n\n<p>What I need to do now is to identify these categories (remember, they can be from 1 to x being x any number from 1 to 25) into 3 tags: tag A, tag B and None of Them. Looking at these variables I can manually identify which tag they belong to, and the idea is to identify manually as much as I can and use any classifier algorithm to learn from that and identify all of them.</p>\n\n<p>My idea was to use multiple logistic regressions on the table, but all the variables of the time series are correlated (since they are a linear combination of each other), so I thought I better use a clustering algorithm only over the time series using euclidean distance to categorize the different patterns and use the result and the other two normalizing variables in the logistic regression.</p>\n\n<p>The other concern that I have is that this approach takes each row independently from the others, and in theory, for each customer there should be only 0 or 1 tag A, 0 or 1 tag B and the remaining of them should be None (another tip is that normally Tag A and B are between first categories, because is highly dependent on the normalizing features (if days over total is High, there is a high possibility that the row is either A or B, depending on the Time Series Pattern). </p>\n\n<p>Edit: This is no longer a concern, I will just perform two different logistic regressions, one for Tag A or Other and another for Tag B or another, with the result probabilities I can select only the best of each.</p>\n\n<p>The dataset is enormous and the final algorithm needs to be applied using SQL (on Teradata), but for getting the coefficients of the logistic regression, or the centers of the clustering I get a sample and use R.</p>\n"
  },
  {
    "tags": [
      "terminology"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 12217,
    "accepted_answer_id": 10263,
    "answer_count": 4,
    "score": 13,
    "last_activity_date": 1591090193,
    "creation_date": 1455528853,
    "last_edit_date": 1455634860,
    "question_id": 10250,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10250/what-is-the-difference-between-objective-error-criterion-cost-loss-fun",
    "title": "What is the difference between (objective / error / criterion / cost / loss) function in the context of neural networks?",
    "body": "<p>The title says it all: I have seen three terms for functions so far, that seem to be the same / similar:</p>\n\n<ul>\n<li>error function</li>\n<li>criterion function</li>\n<li>cost function</li>\n<li>objective function</li>\n<li>loss function</li>\n</ul>\n\n<p>I was working on classification problems</p>\n\n<p>$$E(W) = \\frac{1}{2} \\sum_{x \\in E}(t_x-o(x))^2$$</p>\n\n<p>where $W$ are the weights, $E$ is the evaluation set, $t_x$ is the desired output (the class) of $x$ and $o(x)$ is the given output. This function\nseems to be commonly called \"error function\".</p>\n\n<p>But while reading about this topic, I've also seen the terms \"criterion function\" and \"objective function\". Do they all mean  the same for neural nets?</p>\n\n<ul>\n<li><a href=\"https://class.coursera.org/neuralnets-2012-001/lecture/47\" rel=\"noreferrer\">Geoffrey Hinton</a> called cross-entropy for softmax-neurons and $E(W) = \\frac{1}{2} \\sum_{x \\in E}(t_x-o(x))^2$ a <em>cost function</em>.</li>\n</ul>\n"
  },
  {
    "tags": [
      "machine-learning",
      "bigdata",
      "word2vec"
    ],
    "owner": {
      "account_id": 2578672,
      "reputation": 471,
      "user_id": 16964,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/2XDLo.jpg?s=256",
      "display_name": "Nomiluks",
      "link": "https://datascience.stackexchange.com/users/16964/nomiluks"
    },
    "is_answered": true,
    "view_count": 5446,
    "answer_count": 4,
    "score": 13,
    "last_activity_date": 1566112321,
    "creation_date": 1457643680,
    "last_edit_date": 1566112321,
    "question_id": 10642,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/10642/can-we-take-of-benefit-of-using-transfer-learning-while-training-a-word2vec-mode",
    "title": "Can we take of benefit of using transfer learning while training a word2vec models?",
    "body": "<p>I am looking to find a pre-trained weights of an already trained models like <strong>Google News data</strong> etc. I found it hard to train a new model with enough amount (10 GB etc) of data for myself. So, I want to take benefit from transfer learning in which I would able to get pre-trained layer weights and retrain those weights on my domain specific words. So, definitely it will take relatively less time in training. Any sort of help will be highly appreciated.\nThanks in advance :)</p>\n"
  },
  {
    "tags": [
      "pca"
    ],
    "owner": {
      "account_id": 6432245,
      "reputation": 385,
      "user_id": 14898,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://www.gravatar.com/avatar/09db501f3cc596e11717e6e59bebfe23?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "pr338",
      "link": "https://datascience.stackexchange.com/users/14898/pr338"
    },
    "is_answered": true,
    "view_count": 7521,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1458132860,
    "creation_date": 1458102530,
    "question_id": 10730,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/10730/how-many-dimensions-to-reduce-to-when-doing-pca",
    "title": "How many dimensions to reduce to when doing PCA?",
    "body": "<p>How to choose K for PCA? K is the number of dimensions to project down to. The only requirement is to not lose too much information. I understand it depends on the data, but I'm looking more for a simple general overview about what characteristics to consider when choosing K.</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning"
    ],
    "owner": {
      "account_id": 2741124,
      "reputation": 263,
      "user_id": 17752,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/6KLWB.jpg?s=256",
      "display_name": "Jo&#235;l",
      "link": "https://datascience.stackexchange.com/users/17752/jo%c3%abl"
    },
    "is_answered": true,
    "view_count": 671,
    "accepted_answer_id": 23897,
    "answer_count": 4,
    "score": 13,
    "last_activity_date": 1552702685,
    "creation_date": 1460264817,
    "question_id": 11118,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/11118/alphago-and-other-game-programs-using-reinforcement-learning-without-human-dat",
    "title": "AlphaGo (and other game programs using reinforcement-learning) without human database",
    "body": "<p>I am not a specialist of the subject, and my question is probably very naive. It stems from an essay to understand the powers and limitation of reinforcement learning as used in the AlphaGo program.</p>\n\n<p>The program AlphaGo has been built using, among other things (Monte-Carlo exploration of trees, etc.), neural networks which are trained from a huge database of human-played go games, and which are then reinforced by letting play versions of the program against itself many times.</p>\n\n<p>Now I wonder what would happen is we tried to build such a program without human database, i.e. starting with a basic program of Go just knowing rules and some method to explore trees, and letting play against itself to improve its neural network. Will we, after many games against itself, arrive at a program able to compete with or beat the best human players? And if so, how many games (in order of magnitude) would be needed for that? Or on the contrary, will such a program converge toward a much weaker player?</p>\n\n<p>I assume that the experiment has not been made, since AlphaGo is so recent. But the answer may nevertheless be obvious to a specialist. Otherwise any educated guess will interest me.</p>\n\n<p>One can also ask the same question for \"simpler\" games. If we use roughly the same reinforcement-learning technics used for AlphaGo, but with \nno use of human database, for a Chess program, would we eventually get\na program able to beat the best human? And if so, how fast? Has this been tried? Or if not for Chess, what about Checkers, or even simpler games? </p>\n\n<p>Thanks a lot.   </p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "supervised-learning"
    ],
    "owner": {
      "account_id": 266740,
      "reputation": 4199,
      "user_id": 12515,
      "user_type": "registered",
      "accept_rate": 37,
      "profile_image": "https://i.sstatic.net/G0s1w.jpg?s=256",
      "display_name": "Ryan Zotti",
      "link": "https://datascience.stackexchange.com/users/12515/ryan-zotti"
    },
    "is_answered": true,
    "view_count": 3832,
    "accepted_answer_id": 11127,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1466438193,
    "creation_date": 1460309290,
    "question_id": 11126,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/11126/supervised-learning-vs-reinforcement-learning-for-a-simple-self-driving-rc-car",
    "title": "Supervised learning vs reinforcement learning for a simple self driving rc car",
    "body": "<p>I'm building a remote-controlled self driving car for fun. I'm using a Raspberry Pi as the onboard computer; and I'm using various plug-ins, such as a Raspberry Pi camera and distance sensors, for feedback on the car's surroundings. I'm using OpenCV to turn the video frames into tensors, and I'm using Google's TensorFlow to build a convoluted neural network to learn road boundaries and obstacles. My main question is, should I use supervised learning to teach the car to drive or should I provide objectives and penalties and do reinforcement learning (i.e, get to point B as fast as possible while not hitting anything and staying within the road boundaries)? Below is a list of the pros and cons that I've come up with.</p>\n\n<p><strong>Supervised learning pros:</strong></p>\n\n<ul>\n<li>The inputs to the learning algorithm are pretty straightforward. The car learns to associate the video frame tensor and sensor distance readings with forward, backward, and angular wheel displacement</li>\n<li>I can more or less teach the car to drive exactly how I want (without overfitting, of course)</li>\n<li>I've done tons of supervised learning problems before, and this approach seems to comfortably fit my existing skill set</li>\n</ul>\n\n<p><strong>Supervised learning cons:</strong></p>\n\n<ul>\n<li>It's not clear how to teach speed, and the correct speed is pretty arbitrary as long as the car doesn't go so fast that it veers off the road. I suppose I could drive fast during training, but this seems like a crude approach. Maybe I could manually add in a constant variable during training that corresponds to the speed for that training session, and then when the learning algorithm is deployed, I set this variable according to the speed I want? </li>\n</ul>\n\n<p><strong>Reinforcement learning pros:</strong></p>\n\n<ul>\n<li>If I build my car with the specific purpose of racing other people's self driving cars, reinforcement learning seems to be the natural way to tell my car to \"get there as fast as possible\" </li>\n<li>I've read that RL is sometimes used for autonomous drones, so in theory it should be easier in cars because I don't have to worry about up and down</li>\n</ul>\n\n<p><strong>Reinforcement learning cons:</strong></p>\n\n<ul>\n<li><p>I feel like reinforcement learning would require a lot of additional sensors, and frankly my foot-long car doesn't have that much space inside considering that it also needs to fit a battery, the Raspberry Pi, and a breadboard</p></li>\n<li><p>The car will behave very erratically at first, so much so that maybe it destroys itself. It might also take an unreasonably long time to learn (e.g., months or years)</p></li>\n<li>I can't incoporate explicit rules later on, e.g., stop at a toy red-light. With supervised learning, I could incorporate numerous SL algorithms (e.g., a Haar Cascade classifier for identifying stoplights) into a configurable rules engine that gets evaluated between each video frame. The rules engine would thus be able to override the driving SL algorithm if it saw a red stoplight even though the stoplight might not have been part of the training of the driving algorithm. RL seems too continous to do this (ie., stop only at the terminal state)</li>\n<li>I don't have a lot experience with applied reinforcement learning, although I definitely want to learn it regardless</li>\n</ul>\n"
  },
  {
    "tags": [
      "python",
      "visualization",
      "pca",
      "jupyter"
    ],
    "owner": {
      "account_id": 1845340,
      "reputation": 330,
      "user_id": 19094,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/a479e6496fa8b7ba12a238ec95003a81?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "scottlittle",
      "link": "https://datascience.stackexchange.com/users/19094/scottlittle"
    },
    "is_answered": true,
    "view_count": 7201,
    "answer_count": 4,
    "score": 13,
    "last_activity_date": 1596593495,
    "creation_date": 1464396913,
    "last_edit_date": 1502262511,
    "question_id": 11970,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/11970/how-do-i-make-an-interactive-pca-scatterplot-in-python",
    "title": "How do I make an interactive PCA scatterplot in Python?",
    "body": "<p>The <a href=\"https://matplotlib.org\" rel=\"noreferrer\">matplotlib</a> library is very capable but lacks interactiveness, especially inside Jupyter Notebook.  I would like a good offline plotting tool like <a href=\"https://plot.ly\" rel=\"noreferrer\">plot.ly</a>.</p>\n"
  },
  {
    "tags": [
      "tensorflow",
      "loss-function"
    ],
    "owner": {
      "account_id": 8821536,
      "reputation": 181,
      "user_id": 21464,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/519e698356e98f47a5c45170f6c02d93?s=256&d=identicon&r=PG",
      "display_name": "Cole",
      "link": "https://datascience.stackexchange.com/users/21464/cole"
    },
    "is_answered": true,
    "view_count": 14423,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1540277713,
    "creation_date": 1469039993,
    "question_id": 12886,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/12886/tensorflow-adjusting-cost-function-for-imbalanced-data",
    "title": "Tensorflow Adjusting Cost Function for Imbalanced Data",
    "body": "<p>I have a classification problem with highly imbalanced data. I have read that over and undersampling as well as changing the cost for underrepresented categorical outputs will lead to better fitting. Before this was done tensorflow would categorize each input as the majority group (and gain over 90% accuracy, as meaningless as that is). </p>\n\n<p>I have noticed that the log of the inverse percentage of each group has made the best multiplier that I have tried. Is there a more standard manipulation for the cost function? Is this implemented correctly? </p>\n\n<pre><code>from collections import Counter\ncounts = Counter(category_train)\nweightsArray =[]\nfor i in range(n_classes):\n    weightsArray.append(math.log(category_train.shape[0]/max(counts[i],1))+1)\n\nclass_weight = tf.constant(weightsArray)\nweighted_logits = tf.mul(pred, class_weight)\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(weighted_logits, y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n</code></pre>\n"
  },
  {
    "tags": [
      "neural-network",
      "decision-trees",
      "normalization"
    ],
    "owner": {
      "account_id": 4906381,
      "reputation": 245,
      "user_id": 22080,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6175cb17429df974c1c12e6116b37954?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Merlin1896",
      "link": "https://datascience.stackexchange.com/users/22080/merlin1896"
    },
    "is_answered": true,
    "view_count": 30636,
    "accepted_answer_id": 13221,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1470477393,
    "creation_date": 1470256083,
    "question_id": 13178,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/13178/how-to-normalize-data-for-neural-network-and-decision-forest",
    "title": "How to normalize data for Neural Network and Decision Forest",
    "body": "<p>I have a data set with 20000 samples, each has 12 different features. Each sample is either in category 0 or 1. I want to train a neural network and a decision forest to categorize the samples so that I can compare the results and both techniques. </p>\n\n<p>The first thing I stumbled upon is the proper normalization of the data. One feature is in the range $[0,10^6]$, another one in $[30,40]$ and there is one feature that mostly takes the value 8 and sometimes 7. So as I read in different sources, proper normalization of the input data is crucial for neural networks. As I found out, there are many possible ways to normalize the data, for example:</p>\n\n<ol>\n<li><strong>Min-Max Normalization</strong>: The input range is linearly transformed to the interval $[0,1]$ (or alternatively $[-1,1]$, does that matter?)</li>\n<li><strong>Z-Score Normalization</strong>: The data is transformed to have zero mean and unit variance:\n$$y_{new}=\\frac{y_{old}-\\text{mean}}{\\sqrt{\\text{Var}}}$$</li>\n</ol>\n\n<p>Which normalization should I choose? Is normalization also needed for decision forests? With Z-Score normalization, the different features of my test data do not lie in the same range. Could this be a problem? Should every feature normalized with the same algorithm, so that I decide either to use Min-Max for all features or Z-Score for all features?</p>\n\n<p>Are there combinations where the data is mapped to $[-1,1]$ and also has zero mean (which would imply a non-linear transformation of the data and hence a change in the variance and other features of the input data).</p>\n\n<p>I feel a bit lost because I can't find references which answer these questions.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "predictive-modeling",
      "bigdata"
    ],
    "owner": {
      "account_id": 809076,
      "reputation": 253,
      "user_id": 24153,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/30f6f708b05f5226735550782f523c8f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "iLoeng",
      "link": "https://datascience.stackexchange.com/users/24153/iloeng"
    },
    "is_answered": true,
    "view_count": 10035,
    "accepted_answer_id": 13903,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1591287936,
    "creation_date": 1473288000,
    "question_id": 13901,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/13901/machine-learning-best-practices-for-big-dataset",
    "title": "Machine Learning Best Practices for Big Dataset",
    "body": "<p>I am about to graduate from my Master and had learnt about machine learning as well as performed research projects with it. I wonder about the best practices in the industry when performing machine learning tasks with Big Datasets (like 100s GB or TB). Appreciate if fellow data scientists can share their experience. Here are my questions:</p>\n\n<ol>\n<li>Obviously, very large datasets take longer time to train (can be days or weeks). Many times we need to train various models (SVM, Neural Network, etc.) to compare and find better performance model. I suspect, in industry projects, we want the results as quick as possible but produce the best performance. Are there any tips for reducing the training &amp; testing time? If you recommend subsetting the dataset, I will be interested to learn how best to subset the dataset to cover all or majority of scenarios from the dataset.</li>\n<li>We know that performing cross validation is better as it may reduce over-fitting. However, cross validation also takes time to train and the model trained with cross validation may not be implemented straight (speaking from python sklearn experience: I need to train the model with dataset again after cross validation testing for it to be implemented). Do you normally do cross validation in your big data projects or getting by with the train-test split?</li>\n</ol>\n\n<p>Appreciate the feedback.  </p>\n"
  },
  {
    "tags": [
      "python",
      "nlp"
    ],
    "owner": {
      "account_id": 6767222,
      "reputation": 343,
      "user_id": 20974,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://www.gravatar.com/avatar/277e93886e6825e1bf7f23b8840fc329?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "edwin",
      "link": "https://datascience.stackexchange.com/users/20974/edwin"
    },
    "is_answered": true,
    "view_count": 3498,
    "accepted_answer_id": 14933,
    "answer_count": 4,
    "score": 13,
    "last_activity_date": 1609864123,
    "creation_date": 1478074553,
    "last_edit_date": 1478513660,
    "question_id": 14883,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/14883/extract-information-from-sentence",
    "title": "Extract information from sentence",
    "body": "<p>I'm creating a simple chatbot. I want to obtain the information from the user response. An example scenario:</p>\n\n<pre><code>Bot : Hi, what is your name?\nUser: My name is Edwin.\n</code></pre>\n\n<p>I wish to extract the name Edwin from the sentence. However, the user can response in different ways such as </p>\n\n<pre><code>User: Edwin is my name.\nUser: I am Edwin.\nUser: Edwin. \n</code></pre>\n\n<p>I'm tried to rely on the dependency relations between words but the result does not do well.</p>\n\n<p>Any idea on what technique I could use to tackle this problem?</p>\n\n<p><strong>[UPDATED]</strong></p>\n\n<p>I tested with named entity recognition together with part of speech tagger and parser. I found out that most model is trained in a way that the first character of the entity for the person name or the proper noun must be upper case. This may be true for normal document, but it is irrelevant for a chatbot. E.g. </p>\n\n<pre><code>User: my name is edwin.\n</code></pre>\n\n<p>Most NER failed to recognize this.</p>\n"
  },
  {
    "tags": [
      "tensorflow"
    ],
    "owner": {
      "account_id": 8732164,
      "reputation": 161,
      "user_id": 25772,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/3ef4deedccf84301f07bb0b52d327caf?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "HenrySky",
      "link": "https://datascience.stackexchange.com/users/25772/henrysky"
    },
    "is_answered": true,
    "view_count": 24103,
    "accepted_answer_id": 15038,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1510407107,
    "creation_date": 1478759924,
    "last_edit_date": 1499438581,
    "question_id": 15032,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/15032/how-to-do-batch-inner-product-in-tensorflow",
    "title": "How to do batch inner product in Tensorflow?",
    "body": "<p>I have two tensor <code>a:[batch_size, dim]</code>  <code>b:[batch_size, dim]</code>.\nI want to do inner product for every pair in the batch, generating <code>c:[batch_size, 1]</code>, where <code>c[i,0]=a[i,:].T*b[i,:]</code>. How?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "image-classification",
      "image-recognition"
    ],
    "owner": {
      "account_id": 400425,
      "reputation": 613,
      "user_id": 15664,
      "user_type": "registered",
      "accept_rate": 17,
      "profile_image": "https://www.gravatar.com/avatar/94ade09ec7d2c56582adb92141375ce8?s=256&d=identicon&r=PG",
      "display_name": "spore234",
      "link": "https://datascience.stackexchange.com/users/15664/spore234"
    },
    "is_answered": true,
    "view_count": 9270,
    "accepted_answer_id": 16602,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1560098095,
    "creation_date": 1485684176,
    "last_edit_date": 1560098095,
    "question_id": 16601,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/16601/reason-for-square-images-in-deep-learning",
    "title": "Reason for square images in deep learning",
    "body": "<p>Most of the advanced deep learning models like VGG, ResNet, etc. require square images as input, usually with a pixel size of <span class=\"math-container\">$224x224$</span>.</p>\n\n<p>Is there a reason why the input has to be of equal shape, or can I build a convnet model with say <span class=\"math-container\">$100x200$</span> as well (if I want to do facIAL recognition for example and I have portrait images)?</p>\n\n<p>Is there increased benefit with a larger pixel size, say <span class=\"math-container\">$512x512$</span>?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "visualization",
      "scikit-learn",
      "data",
      "decision-trees"
    ],
    "owner": {
      "account_id": 1046162,
      "reputation": 245,
      "user_id": 28544,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/201400217/picture?type=large",
      "display_name": "Tim Lindsey",
      "link": "https://datascience.stackexchange.com/users/28544/tim-lindsey"
    },
    "is_answered": true,
    "view_count": 20201,
    "accepted_answer_id": 37571,
    "answer_count": 4,
    "score": 13,
    "last_activity_date": 1535561319,
    "creation_date": 1485995372,
    "question_id": 16693,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/16693/interpreting-decision-tree-in-context-of-feature-importances",
    "title": "Interpreting Decision Tree in context of feature importances",
    "body": "<p>I'm trying to understand how to fully understand the decision process of a decision tree classification model built with sklearn. The 2 main aspect I'm looking at are a graphviz representation of the tree and the list of feature importances. What I don't understand is how the feature importance is determined in the context of the tree. For example, here is my list of feature importances:</p>\n\n<p>Feature ranking:\n1. FeatureA (0.300237)</p>\n\n<ol start=\"2\">\n<li><p>FeatureB (0.166800)</p></li>\n<li><p>FeatureC (0.092472)</p></li>\n<li><p>FeatureD (0.075009)</p></li>\n<li><p>FeatureE (0.068310)</p></li>\n<li><p>FeatureF (0.067118)</p></li>\n<li><p>FeatureG (0.066510)</p></li>\n<li><p>FeatureH (0.043502)</p></li>\n<li><p>FeatureI (0.040281)</p></li>\n<li><p>FeatureJ (0.039006)</p></li>\n<li><p>FeatureK (0.032618)</p></li>\n<li><p>FeatureL (0.008136)</p></li>\n<li><p>FeatureM (0.000000)</p></li>\n</ol>\n\n<p>However, when I look at the top of the tree, it looks like this:<a href=\"https://i.sstatic.net/yJ2xy.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/yJ2xy.png\" alt=\"Tree Snippet\"></a></p>\n\n<p>In fact, some of the features that are ranked \"most important\" don't appear until much further down the tree, and the top of the tree is FeatureJ which is one of the lowest ranked features. My naive assumption would be that the most important features would be ranked near the top of the tree to have the greatest impact. If that's incorrect, then what is it that makes a feature \"important\"?</p>\n"
  },
  {
    "tags": [
      "python",
      "association-rules"
    ],
    "owner": {
      "account_id": 4962749,
      "reputation": 2785,
      "user_id": 17310,
      "user_type": "registered",
      "accept_rate": 25,
      "profile_image": "https://www.gravatar.com/avatar/6b2ddcbdf322159e5ff9fa987d2af7c9?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Edamame",
      "link": "https://datascience.stackexchange.com/users/17310/edamame"
    },
    "is_answered": true,
    "view_count": 29755,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1658507656,
    "creation_date": 1486332769,
    "last_edit_date": 1658507656,
    "question_id": 16769,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/16769/best-frequent-itemset-package-in-python",
    "title": "Best frequent itemset package in python",
    "body": "<p>Could anyone please recommend a good frequent itemset package in python? I only need to find frequent itemset, no need of finding the association rules.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "convolutional-neural-network",
      "gpu",
      "parameter-estimation"
    ],
    "owner": {
      "account_id": 3076402,
      "reputation": 1071,
      "user_id": 12206,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://i.sstatic.net/9RN4n.png?s=256",
      "display_name": "Simon",
      "link": "https://datascience.stackexchange.com/users/12206/simon"
    },
    "is_answered": true,
    "view_count": 24134,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1613443840,
    "creation_date": 1488402588,
    "last_edit_date": 1613443840,
    "question_id": 17286,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/17286/cnn-memory-consumption",
    "title": "CNN memory consumption",
    "body": "<p>I'd like to be able to estimate whether a proposed model is small enough to be trained on a GPU with a given amount of memory</p>\n\n<p>If I have a simple CNN architecture like this:</p>\n\n<ul>\n<li><code>Input</code>: 50x50x3</li>\n<li><code>C1</code>: 32 3x3 kernels, with padding (I guess in reality theyre actually 3x3x3 given the input depth?)</li>\n<li><code>P1</code>: 2x2 with stride 2</li>\n<li><code>C2</code>: 64 3x3 kernels, with padding</li>\n<li><code>P2</code>: 2x2 with stride 2</li>\n<li><code>FC</code>: 500 neurons</li>\n<li><code>Output</code>: softmax 10 classes</li>\n<li>Mini batch size of 64</li>\n</ul>\n\n<p>Assuming 32bit floating point values, how do you calculate the memory cost of each layer of the network during training? and then the total memory required to train such a model?</p>\n"
  },
  {
    "tags": [
      "deep-learning"
    ],
    "owner": {
      "account_id": 400425,
      "reputation": 613,
      "user_id": 15664,
      "user_type": "registered",
      "accept_rate": 17,
      "profile_image": "https://www.gravatar.com/avatar/94ade09ec7d2c56582adb92141375ce8?s=256&d=identicon&r=PG",
      "display_name": "spore234",
      "link": "https://datascience.stackexchange.com/users/15664/spore234"
    },
    "is_answered": true,
    "view_count": 8049,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1537900381,
    "creation_date": 1488970886,
    "last_edit_date": 1488977977,
    "question_id": 17444,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/17444/deep-learning-for-non-image-non-nlp-tasks",
    "title": "deep learning for non-image non-NLP tasks?",
    "body": "<p>So far there are many interesting applications for deep learning in computer vision or natural language processing.</p>\n\n<p>How is it in other more traditional fields?\nFor example, I have traditional socio-demographic variables plus maybe a lot of lab measurements and want to predict a certain disease. Would this be a deep learning application if I have lots of observations?\nHow would I construct a network here, I think all the fancy layers (convolutional etc.) are not really necessary?! Just make it deep?</p>\n\n<p>On my specific data set, I tried some common machine learning algorithms like random forests, gbm etc with mixed results regarding accuracy.\nI have limited deep learning experience with image recognition.</p>\n"
  },
  {
    "tags": [
      "feature-extraction",
      "unsupervised-learning",
      "gan"
    ],
    "owner": {
      "account_id": 3259418,
      "reputation": 251,
      "user_id": 17947,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/hvvbS.jpg?s=256",
      "display_name": "exAres",
      "link": "https://datascience.stackexchange.com/users/17947/exares"
    },
    "is_answered": true,
    "view_count": 6900,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1575432602,
    "creation_date": 1489065453,
    "question_id": 17471,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/17471/how-to-use-gan-for-unsupervised-feature-extraction-from-images",
    "title": "How to use GAN for unsupervised feature extraction from images?",
    "body": "<p>I have understood how GAN works while two networks (generative and discriminative) compete with each other. I have built a DCGAN (GAN with convolutional discriminator and de-convolutional generator) which now successfully generates handwritten digits similar to those in MNIST dataset.</p>\n\n<p>I have read a lot about GAN's applications for extracting features from images. How can use my trained GAN model (on MNIST dataset) to extract feature from MNIST handwritten digist images?</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "convolutional-neural-network"
    ],
    "owner": {
      "account_id": 8699802,
      "reputation": 253,
      "user_id": 31075,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/GlJwX.png?s=256",
      "display_name": "D3181",
      "link": "https://datascience.stackexchange.com/users/31075/d3181"
    },
    "is_answered": true,
    "view_count": 37652,
    "accepted_answer_id": 39993,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1682499322,
    "creation_date": 1492117011,
    "last_edit_date": 1613443400,
    "question_id": 18341,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/18341/how-are-weights-represented-in-a-convolution-neural-network",
    "title": "How are weights represented in a convolution neural network?",
    "body": "<p>I have been trying to develop a convolution neural network following some <a href=\"https://cs231n.github.io/convolutional-networks/\" rel=\"noreferrer\">guides</a> <a href=\"https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/\" rel=\"noreferrer\">online</a>. However, most guides I have encountered gloss over an important detail, which is how to programmatically represent the weights in a CNN.</p>\n\n<p>As far as I understand, in a \"regular\" neural network, the weight of a connection is a numerical value, which is adjusted in order to reduce the error; then back-propagation is used to further update the weights, reducing thus the error, etc. </p>\n\n<p>However, in a CNN, the input is an array of numbers (the image), and a subset of those (the filter) to calculate the mean error, by multiplying the filter pixels by the original pixels. </p>\n\n<p>So, is there a weight neuron for each filter (kernel or feature map) of the image? Or is a single weight neuron represented by the sum of all the mean error's calculated from convolving the filter over the receptive field, such that you have one value, in the end, that is the total error for the entire image?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "neural-network",
      "keras"
    ],
    "owner": {
      "account_id": 5448615,
      "reputation": 141,
      "user_id": 32011,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4b83681e0f685274b5c84457252656e2?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Aveiur",
      "link": "https://datascience.stackexchange.com/users/32011/aveiur"
    },
    "is_answered": true,
    "view_count": 9492,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1723039824,
    "creation_date": 1494154406,
    "last_edit_date": 1723039824,
    "question_id": 18810,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/18810/how-to-check-for-dead-relu-neurons",
    "title": "How to check for dead relu neurons",
    "body": "<p>Background:\nWhile fitting neural networks with relu activation, I found that sometimes the prediction becomes near constant. I believe that this is due to the relu neurons dieing during training as stated here. (<a href=\"https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks\">What is the &quot;dying ReLU&quot; problem in neural networks?</a>)</p>\n<p>Question:\nWhat I'm hoping to do is to implement a check in the code itself to check if the neurons are dead. After that, the code could refit the network if needed.</p>\n<p>As such, what is a good citeria to check for dead neurons? Currently, I'm thinking of checking for low variance in the prediction as a citeria.</p>\n<p>If it helps, I'm using keras.</p>\n"
  },
  {
    "tags": [
      "r",
      "pandas"
    ],
    "owner": {
      "account_id": 446733,
      "reputation": 326,
      "user_id": 10975,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/3ac5824c6cc4817a754a186d6a45a08b?s=256&d=identicon&r=PG",
      "display_name": "Max Flander",
      "link": "https://datascience.stackexchange.com/users/10975/max-flander"
    },
    "is_answered": true,
    "view_count": 14793,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1663499554,
    "creation_date": 1494891224,
    "question_id": 18989,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/18989/naming-conventions-for-dataframes",
    "title": "Naming conventions for dataframes",
    "body": "<p>I often find myself writing code like the following (oversimplfied example)</p>\n\n<pre><code>df = read_csv('customer_data_export.csv')\ndf2 = df.query(\"date &gt; '2017-01-10'\")\ndata = df_filtered.groupby('transaction_id').sum()\nplot_data = pivot_table(data, columns='weekday', rows='n_items')\n# Etc etc\n</code></pre>\n\n<p>Basically the problem is that while it's relatively easy to come up with semantic names for columns (as random variables) it's hard for me to come up with meaningful names for each step of transformed dataframe. Additionally I prefer to have short names to make the code easier to type. (Working in Jupyter notebook, the tab-completion isn't the best).</p>\n\n<p>What are some best practices that people follow with this kind of thing? </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "ai",
      "theory"
    ],
    "owner": {
      "account_id": 8669058,
      "reputation": 185,
      "user_id": 23850,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-t9fmqTUCqqo/AAAAAAAAAAI/AAAAAAAAAGk/WAQwwt8eu9A/s256-rj/photo.jpg",
      "display_name": "Sairaam Venkatraman",
      "link": "https://datascience.stackexchange.com/users/23850/sairaam-venkatraman"
    },
    "is_answered": true,
    "view_count": 1794,
    "protected_date": 1574264400,
    "answer_count": 8,
    "score": 13,
    "last_activity_date": 1608695854,
    "creation_date": 1495179077,
    "last_edit_date": 1608695854,
    "question_id": 19077,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/19077/difference-between-machine-learning-and-artificial-intelligence",
    "title": "Difference between machine learning and artificial intelligence",
    "body": "<p>Is there any difference between machine learning and artificial intelligence? Or do these terms refer to the same thing?</p>\n"
  },
  {
    "tags": [
      "python",
      "neural-network",
      "keras",
      "rnn",
      "pytorch"
    ],
    "owner": {
      "account_id": 6936603,
      "reputation": 303,
      "user_id": 29767,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/uKvJb.jpg?s=256",
      "display_name": "Vadim",
      "link": "https://datascience.stackexchange.com/users/29767/vadim"
    },
    "is_answered": true,
    "view_count": 1789,
    "accepted_answer_id": 20473,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1620060728,
    "creation_date": 1500283468,
    "last_edit_date": 1620060728,
    "question_id": 20472,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/20472/an-artificial-neural-network-ann-with-an-arbitrary-number-of-inputs-and-output",
    "title": "An Artificial Neural Network (ANN) with an arbitrary number of inputs and outputs",
    "body": "<p>I would like to use ANNs for my problem, but the issue is my inputs and outputs node numbers are not fixed.</p>\n<p>I did some google searches before asking my question and found that the RNN may help me with my problem. But, all examples which I've found somehow have a <em>defined</em> number of input and output nodes.</p>\n<p>So, I'm looking for a strategy, how to make it real or at least some examples, preferable in Keras or PyTorch.</p>\n<p>More details about my issue:</p>\n<p>I have two inputs lists, where the length of the first one is fixed and equals two, e.g.:</p>\n<pre><code>in_1 = [2,2] \n</code></pre>\n<p>but the length of the second list is flexible, the length can be from three to inf, e.g.:</p>\n<pre><code>in_2 = [1,1,2,2]\n</code></pre>\n<p>or</p>\n<pre><code>in_2 = [1,1,1,2,2,2,3,3,3]\n</code></pre>\n<p>Also, input lists depend on each other. The first list shows the dimension of the output list. So if in_1 = [2,2], means the output must have a possibility to be reshaped to [2,2] form.</p>\n<p>Currently, I'm thinking to combine two input list into one:</p>\n<pre><code>in = in_1 + in_2 = [2, 2, 1, 1, 2, 2]\n</code></pre>\n<p>Moreover, the output has the same length as the <strong>in_2</strong> list, e.g.:</p>\n<p>if input lists are:</p>\n<pre><code>in_1 = [2, 2]\nin_2 = [1, 1, 2, 2]\n</code></pre>\n<p>Output should be:</p>\n<pre><code>out = [1, 2, 1, 2]\n</code></pre>\n<p>Any ideas are welcome!</p>\n"
  },
  {
    "tags": [
      "python",
      "pandas",
      "dataframe"
    ],
    "owner": {
      "account_id": 10613061,
      "reputation": 543,
      "user_id": 35420,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/DAGfA.jpg?s=256",
      "display_name": "Kevin",
      "link": "https://datascience.stackexchange.com/users/35420/kevin"
    },
    "is_answered": true,
    "view_count": 21722,
    "accepted_answer_id": 22105,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1502280371,
    "creation_date": 1500579805,
    "last_edit_date": 1501990916,
    "question_id": 20587,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/20587/find-the-consecutive-zeros-in-a-dataframe-and-do-a-conditional-replacement",
    "title": "Find the consecutive zeros in a DataFrame and do a conditional replacement",
    "body": "<p>I have a dataset like this:</p>\n\n<h3>Sample Dataframe</h3>\n\n<pre><code>import pandas as pd\n\ndf = pd.DataFrame({\n    'names': ['A','B','C','D','E','F','G','H','I','J','K','L'],\n    'col1': [0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0],\n    'col2': [0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0]})\n</code></pre>\n\n<p>I'd like to replace some of the <code>0</code>'s in <code>col1</code> and <code>col2</code> with <code>1</code>'s, but not replace the <code>0</code>'s if three or more <code>0</code>'s are consecutive in the same column.  How can this be done with pandas?</p>\n\n<h3>Original Dataset:</h3>\n\n<pre><code>names   col1    col2\nA   0   0\nB   1   0\nC   0   0\nD   1   0\nE   1   1\nF   1   0\nG   0   1\nH   0   0\nI   0   1\nJ   1   0\nK   0   0\nL   0   0\n</code></pre>\n\n<h3>Desired Dataset:</h3>\n\n<pre><code>names   col1    col2\nA   1   0\nB   1   0\nC   1   0\nD   1   0\nE   1   1\nF   1   1\nG   0   1\nH   0   1\nI   0   1\nJ   1   0\nK   1   0\nL   1   0\n</code></pre>\n"
  },
  {
    "tags": [
      "deep-learning"
    ],
    "migrated_from": {
      "other_site": {
        "styling": {
          "tag_background_color": "#FFF",
          "tag_foreground_color": "#000",
          "link_color": "#0077CC"
        },
        "related_sites": [
          {
            "relation": "meta",
            "api_site_parameter": "ai.meta",
            "site_url": "https://ai.meta.stackexchange.com",
            "name": "Artificial Intelligence Meta Stack Exchange"
          },
          {
            "relation": "chat",
            "site_url": "https://chat.stackexchange.com?tab=site&host=ai.stackexchange.com",
            "name": "Chat Stack Exchange"
          }
        ],
        "markdown_extensions": [
          "MathJax"
        ],
        "launch_date": 1639665491,
        "open_beta_date": 1471907237,
        "closed_beta_date": 1470164400,
        "site_state": "normal",
        "high_resolution_icon_url": "https://cdn.sstatic.net/Sites/ai/Img/apple-touch-icon@2.png",
        "favicon_url": "https://cdn.sstatic.net/Sites/ai/Img/favicon.ico",
        "icon_url": "https://cdn.sstatic.net/Sites/ai/Img/apple-touch-icon.png",
        "audience": "people interested in conceptual questions about life and challenges in a world where &quot;cognitive&quot; functions can be mimicked in purely digital environment",
        "site_url": "https://ai.stackexchange.com",
        "api_site_parameter": "ai",
        "logo_url": "https://cdn.sstatic.net/Sites/ai/Img/apple-touch-icon.png",
        "name": "Artificial Intelligence",
        "site_type": "main_site"
      },
      "on_date": 1502719276,
      "question_id": 3810
    },
    "owner": {
      "account_id": 4842611,
      "reputation": 917,
      "user_id": 37932,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/2u77o.jpg?s=256",
      "display_name": "aerin",
      "link": "https://datascience.stackexchange.com/users/37932/aerin"
    },
    "is_answered": true,
    "view_count": 1780,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1687368115,
    "creation_date": 1502665482,
    "question_id": 22242,
    "link": "https://datascience.stackexchange.com/questions/22242/sort-numbers-using-only-2-hidden-layers",
    "title": "Sort numbers using only 2 hidden layers",
    "body": "<p>I'm reading the cornerstone paper <a href=\"https://arxiv.org/pdf/1409.3215v1.pdf%3B\" rel=\"noreferrer\">Sequence to Sequence Learning\nwith Neural Networks</a> by Ilya Sutskever and Quoc Le. On the first page, it briefly mentions that: </p>\n\n<pre><code>A surprising example of the power of DNNs is their ability to sort\nN N-bit numbers using only 2 hidden layers of quadratic size \n</code></pre>\n\n<p>Can anyone briefly outline how to sort numbers using only 2 hidden layers?</p>\n"
  },
  {
    "tags": [
      "tsne"
    ],
    "owner": {
      "account_id": 14926,
      "reputation": 533,
      "user_id": 34165,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/1sz79.png?s=256",
      "display_name": "geometrikal",
      "link": "https://datascience.stackexchange.com/users/34165/geometrikal"
    },
    "is_answered": true,
    "view_count": 6223,
    "accepted_answer_id": 22489,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1503592882,
    "creation_date": 1502870864,
    "question_id": 22299,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/22299/does-nearest-neighbour-make-any-sense-with-t-sne",
    "title": "Does nearest neighbour make any sense with t-SNE?",
    "body": "<p>Answers on here have stated that <a href=\"https://datascience.stackexchange.com/questions/17314/are-t-sne-dimensions-meaningful\">the dimensions in t-SNE are meaningless</a>, and that the <a href=\"https://datascience.stackexchange.com/questions/10802/can-closer-points-be-considered-more-similar-in-t-sne-visualization\">distances between points are not a measure of similarity</a>.</p>\n\n<p>However, can we say anything about a point based on it's nearest neighbours in t-SNE space? <a href=\"https://datascience.stackexchange.com/questions/19025/t-sne-why-equal-data-values-are-visually-not-close\">This answer</a> to why points that are exactly the same are not clustered suggests the ratio of distances between points is similar between lower and higher dimensional representations.</p>\n\n<p>For example, the image below shows t-SNE on one of my datasets (15 classes).</p>\n\n<p>Can I say that <code>cro 479</code> (top right) is an outlier? Is <code>fra 1353</code> (bottom left) is more similar to <code>cir 375</code> than the other images in the <code>fra</code> class, etc? Or could these just be artefacts, e.g. <code>fra 1353</code> got stuck on the other side of a few clusters and couldn't force its way through to the other <code>fra</code> class?</p>\n\n<p><a href=\"https://i.sstatic.net/NqNxt.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/NqNxt.png\" alt=\"enter image description here\"></a></p>\n"
  },
  {
    "tags": [
      "linear-regression",
      "regularization"
    ],
    "owner": {
      "account_id": 8786532,
      "reputation": 273,
      "user_id": 40388,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6630eb9a3518ebe0706f449ac4adbc5e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "astudentofmaths",
      "link": "https://datascience.stackexchange.com/users/40388/astudentofmaths"
    },
    "is_answered": true,
    "view_count": 5373,
    "accepted_answer_id": 23877,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1508267897,
    "creation_date": 1507838092,
    "question_id": 23728,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/23728/why-using-l1-regularization-over-l2",
    "title": "Why using L1 regularization over L2?",
    "body": "<p>Conducting a linear regression model using a loss function, why should I use $L_1$ instead of $L_2$ regularization?</p>\n\n<p>Is it better at preventing overfitting? Is it deterministic (so always a unique solution)? Is it better at feature selection (because producing sparse models)? Does it disperse the weights among the features?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "scikit-learn",
      "regression",
      "feature-selection",
      "linear-regression"
    ],
    "owner": {
      "account_id": 9562942,
      "reputation": 131,
      "user_id": 41490,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-n4Wp3ZnStBA/AAAAAAAAAAI/AAAAAAAACtw/dzkBzAEGeXE/s256-rj/photo.jpg",
      "display_name": "nlahri",
      "link": "https://datascience.stackexchange.com/users/41490/nlahri"
    },
    "is_answered": true,
    "view_count": 59150,
    "closed_date": 1511006794,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1510069715,
    "creation_date": 1509973138,
    "last_edit_date": 1509983223,
    "question_id": 24405,
    "link": "https://datascience.stackexchange.com/questions/24405/how-to-do-stepwise-regression-using-sklearn",
    "closed_reason": "Duplicate",
    "title": "How to do stepwise regression using sklearn?",
    "body": "<p>I could not find a way to stepwise regression in scikit learn. I have checked all other posts on Stack Exchange on this topic. Answers to all of them suggests using f_regression.</p>\n\n<p>But f_regression does not do stepwise regression but only give F-score and pvalues corresponding to each of the regressors, which is only the first step in stepwise regression.</p>\n\n<p>What to do after 1st regressors with the best f-score is chosen?</p>\n"
  },
  {
    "tags": [
      "algorithms",
      "anomaly-detection",
      "outlier",
      "terminology",
      "definitions"
    ],
    "owner": {
      "account_id": 271958,
      "reputation": 19500,
      "user_id": 8820,
      "user_type": "registered",
      "accept_rate": 44,
      "profile_image": "https://www.gravatar.com/avatar/e86681e49622d52817b36fd2a4c936b7?s=256&d=identicon&r=PG",
      "display_name": "Martin Thoma",
      "link": "https://datascience.stackexchange.com/users/8820/martin-thoma"
    },
    "is_answered": true,
    "view_count": 9113,
    "answer_count": 4,
    "score": 13,
    "last_activity_date": 1638600126,
    "creation_date": 1510744652,
    "question_id": 24760,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/24760/what-is-the-difference-between-outlier-detection-and-anomaly-detection",
    "title": "What is the difference between outlier detection and anomaly detection?",
    "body": "<p>I would like to know the difference in terms of applications (e.g. which one is credit card fraud detection?) and in terms of used  techniques.</p>\n\n<p>Example papers which define the task would be welcome.</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "clustering",
      "time-series",
      "unsupervised-learning",
      "autoencoder"
    ],
    "owner": {
      "account_id": 7710102,
      "reputation": 255,
      "user_id": 35479,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/oa54S.jpg?s=256",
      "display_name": "Tendero",
      "link": "https://datascience.stackexchange.com/users/35479/tendero"
    },
    "is_answered": true,
    "view_count": 23853,
    "accepted_answer_id": 25713,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1746268284,
    "creation_date": 1513361291,
    "last_edit_date": 1746268284,
    "question_id": 25712,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/25712/how-can-autoencoders-be-used-for-clustering",
    "title": "How can autoencoders be used for clustering?",
    "body": "<p>Suppose I have a set of time-domain signals with absolutely <strong>no labels</strong>. I want to cluster them in 2 or 3 classes. Autoencoders are unsupervised networks that learn to compress the inputs. So given an input <span class=\"math-container\">$x^{(i)}$</span>, weights <span class=\"math-container\">$W_1$</span> and <span class=\"math-container\">$W_2$</span>, biases <span class=\"math-container\">$b_1$</span> and <span class=\"math-container\">$b_2$</span>, and output <span class=\"math-container\">$\\hat{x}^{(i)}$</span>, we can find the following relationships:</p>\n<p><span class=\"math-container\">$$z^{(i)} =W_1x^{(i)}+b_1$$</span>\n<span class=\"math-container\">$$\\hat{x}^{(i)} =W_2z^{(i)}+b_2$$</span></p>\n<p>So <span class=\"math-container\">$z^{(i)}$</span> would be a compressed form of <span class=\"math-container\">$x^{(i)}$</span>, and <span class=\"math-container\">$\\hat{x}^{(i)}$</span> the reconstruction of the latter. So far so good.</p>\n<p>What I don't understand is how this could be used for clustering (if there is any way to do it at all). For example, in the first figure of <a href=\"https://www.sciencedirect.com/science/article/pii/S1877050917318707/\" rel=\"nofollow noreferrer\">this paper</a>, there is a block diagram I'm not sure I understand. It uses the <span class=\"math-container\">$z^{(i)}$</span> as the inputs to the feed-forward network, but there is no mention to how that network is trained. I don't know if there is something I'm ignoring or if the paper is incomplete. Also, <a href=\"http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/\" rel=\"nofollow noreferrer\">this tutorial</a> at the end shows the weights learned by the autoencoder, and they seem to be kernels a CNN would learn to classify images. So... I guess the autoencoder's weights can be used somehow in a feed-forward network for classification, but I'm not sure how.</p>\n<p>My doubts are:</p>\n<ol>\n<li>If <span class=\"math-container\">$x^{(i)}$</span> is a time-domain signal of length <span class=\"math-container\">$N$</span> (i.e. <span class=\"math-container\">$x^{(i)}\\in\\mathbb{R}^{1\\times N}$</span>), can <span class=\"math-container\">$z^{(i)}$</span> only be a vector as well? In other words, would it make sense for <span class=\"math-container\">$z^{(i)}$</span> to be a <em>matrix</em> with one of its dimensions greater than <span class=\"math-container\">$1$</span>? I believe it would not, but I just want to check.</li>\n<li>Which of these quantities would be the input to a classifier? For example, if I want to use a classic MLP that has as many output units as classes I want to classify the signals in, what should I put at the input of this fully-connected network (<span class=\"math-container\">$z^{(i)}$</span>,<span class=\"math-container\">$\\hat{x}^{(i)}$</span>, any other thing)?</li>\n<li>How can I use the learned weights and biases in this MLP? Remember that we assumed that <strong>absolutely no labels</strong> are available, so it is impossible to train the network. I think the learned <span class=\"math-container\">$W_i$</span> and <span class=\"math-container\">$b_i$</span> should be useful somehow in the fully-connected network, but I don't see how to use them.</li>\n</ol>\n<p>Observation: note that I used an MLP as an example because it is the most basic architecture, but the question applies to any other neural network that could be used to classify time-domain signals.</p>\n"
  },
  {
    "tags": [
      "classification",
      "recommender-system",
      "cosine-distance"
    ],
    "owner": {
      "account_id": 205758,
      "reputation": 651,
      "user_id": 10522,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://www.gravatar.com/avatar/0c6aabd6b849d1f50d89fd1292a3fef0?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Victor",
      "link": "https://datascience.stackexchange.com/users/10522/victor"
    },
    "is_answered": true,
    "view_count": 25576,
    "accepted_answer_id": 26453,
    "answer_count": 4,
    "score": 13,
    "last_activity_date": 1608703167,
    "creation_date": 1515513911,
    "question_id": 26446,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/26446/can-i-use-cosine-similarity-as-a-distance-metric-in-a-knn-algorithm",
    "title": "Can I use cosine similarity as a distance metric in a KNN algorithm",
    "body": "<p>Most discussions of KNN mention Euclidean,Manhattan and Hamming distances, but they dont mention cosine similarity metric. Is there a reason for this?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "scikit-learn",
      "cosine-distance"
    ],
    "owner": {
      "account_id": 11992647,
      "reputation": 233,
      "user_id": 44645,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6efcbb1b3453f527451492a38e4b523c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Olivia Brown",
      "link": "https://datascience.stackexchange.com/users/44645/olivia-brown"
    },
    "is_answered": true,
    "view_count": 36878,
    "accepted_answer_id": 26649,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1620134079,
    "creation_date": 1516022564,
    "last_edit_date": 1518383887,
    "question_id": 26648,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/26648/cosine-similarity-returns-matrix-instead-of-single-value",
    "title": "cosine_similarity returns matrix instead of single value",
    "body": "<p>I am using below code to compute cosine similarity between the 2 vectors. It returns a matrix instead of a single value <code>0.8660254</code>.</p>\n\n<blockquote>\n  <p>[[ 1.         0.8660254]</p>\n  \n  <p>[ 0.8660254  1.       ]]</p>\n</blockquote>\n\n<pre><code>from sklearn.metrics.pairwise import cosine_similarity\nvec1 = [1,1,0,1,1]\nvec2 = [0,1,0,1,1]\nprint(cosine_similarity([vec1, vec2]))\n</code></pre>\n"
  },
  {
    "tags": [
      "time-series",
      "rnn",
      "lstm",
      "generative-models"
    ],
    "owner": {
      "account_id": 4113101,
      "reputation": 428,
      "user_id": 44637,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/bd28015dbb4f4ced5b455ee4d20d46a6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Hastu",
      "link": "https://datascience.stackexchange.com/users/44637/hastu"
    },
    "is_answered": true,
    "view_count": 24380,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1579400144,
    "creation_date": 1516049480,
    "last_edit_date": 1579400144,
    "question_id": 26663,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/26663/when-to-use-stateful-lstm",
    "title": "When to use Stateful LSTM?",
    "body": "<p>I'm trying to use LSTM on time-series data in order to generate future sequences that looks like the original sequences in term of values and progression direction. \nMy approach is: </p>\n\n<ul>\n<li>train RNN to predict a value based on 25 past values then use the model to recursively generate future predictions by appending the predicted values to the original sequence and shift the old values ... </li>\n</ul>\n\n<p>Playing with LSTM cells, I found that the model is not able to learn to generate sequences that looks like the original data. It only predict next value then start converging to an 'equilibrium' or static value which is the same whatever the input sequence is. </p>\n\n<p>I'm wondering if Stateful LSTM would help to learn better from past values and try to predict something close to what it have already? My goal here is to generate sequences that looks like something that the model have seen already.</p>\n\n<p>Please let me know if I'm missing something or if you had a similar situation and you found the best approach to generate timeseries sequences that looks like what the model learned in the past.</p>\n"
  },
  {
    "tags": [
      "neural-network",
      "nlp",
      "lstm",
      "rnn"
    ],
    "owner": {
      "account_id": 72038,
      "reputation": 2119,
      "user_id": 23240,
      "user_type": "registered",
      "accept_rate": 20,
      "profile_image": "https://i.sstatic.net/wqY8n.jpg?s=256",
      "display_name": "I_Play_With_Data",
      "link": "https://datascience.stackexchange.com/users/23240/i-play-with-data"
    },
    "is_answered": true,
    "view_count": 9532,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1671478716,
    "creation_date": 1517586312,
    "question_id": 27392,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/27392/so-whats-the-catch-with-lstm",
    "title": "So what&#39;s the catch with LSTM?",
    "body": "<p>I am expanding my knowledge of the Keras package and I have been tooling with some of the available models. I have an NLP binary classification problem that I'm trying to solve and have been applying different models. </p>\n\n<p>After working with some results and reading more and more about LSTM, it seems like this approach is far superior to anything else I've tried (across multiple datasets). I keep thinking to myself, \"why/when would you <em>not</em> use LSTM?\". The use of the additional gates, inherent to LSTM, makes perfect sense to me after having some models that suffer from vanishing gradients. </p>\n\n<p>So what's the catch with LSTM? Where do they not do so well? I know there is no such thing as a \"one size fits all\" algorithm, so there must be a downside to LSTM.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "deep-learning"
    ],
    "owner": {
      "account_id": 6683525,
      "reputation": 173,
      "user_id": 44044,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/489c0ea1bff08c62af182b56b130ac7c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "porfgian",
      "link": "https://datascience.stackexchange.com/users/44044/porfgian"
    },
    "is_answered": true,
    "view_count": 21039,
    "accepted_answer_id": 28517,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1580482553,
    "creation_date": 1519985640,
    "last_edit_date": 1519985722,
    "question_id": 28512,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/28512/train-new-data-to-pre-trained-model",
    "title": "Train new data to pre-trained model",
    "body": "<p>Let's say I've trained my model and made my predictions. </p>\n\n<p>My question is... How can I append some new data to my pre-trained model without retrain the model from the beginning.</p>\n"
  },
  {
    "tags": [
      "classification",
      "tools",
      "annotation"
    ],
    "owner": {
      "account_id": 6894591,
      "reputation": 1364,
      "user_id": 27432,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/a83c5543d18485c47c75899a560c0565?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "S van Balen",
      "link": "https://datascience.stackexchange.com/users/27432/s-van-balen"
    },
    "is_answered": true,
    "view_count": 12222,
    "protected_date": 1658244762,
    "accepted_answer_id": 28698,
    "answer_count": 5,
    "community_owned_date": 1548893148,
    "score": 13,
    "last_activity_date": 1593695889,
    "creation_date": 1520156710,
    "question_id": 28594,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/28594/what-are-helpful-annotation-tools-if-any",
    "title": "What are helpful annotation tools (if any)",
    "body": "<p>I'm looking for tools that would help me and my team annotate training sets. I work in an environment with large sets of data, some of which are un- or semi-structured. In many cases there are registration that help in finding a grounded truth. In many cases however a curated set is needed, even if it just were for evaluation. A complicating factor is that some of the data can not leave the premise.</p>\n\n<p>We are looking to annotate an object detection task, but I anticipate an image segmentation task, a text classification task and a sentiment detection task in the near future. </p>\n\n<p>What I'm looking for is a system that can help a group make an annotation, preferably in a way that motivates the annotators by showing group progress, relative individual progress and perhaps personal inter annotator agreement.</p>\n"
  },
  {
    "tags": [
      "word2vec",
      "word-embeddings",
      "matrix-factorisation"
    ],
    "owner": {
      "account_id": 8071682,
      "reputation": 1347,
      "user_id": 17208,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/af7bee44fa2f121f32e165ee5c879d75?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Robin",
      "link": "https://datascience.stackexchange.com/users/17208/robin"
    },
    "is_answered": true,
    "view_count": 5556,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1621033317,
    "creation_date": 1520935557,
    "question_id": 29019,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/29019/why-do-we-need-2-matrices-for-word2vec-or-glove",
    "title": "Why do we need 2 matrices for word2vec or GloVe",
    "body": "<p>Word2vec and GloVe are the two most known words embedding methods. Many works pointed that these two models are actually very close to each other and that under some assumptions, they perform a matrix factorization of the ppmi of the co-occurrences of the words in the corpus. </p>\n\n<p>Still, I can't understand why we actually need two matrices (and not one) for these models. Couldn't we use the same one for U and V ? Is it a problem with the gradient descent or is there another reason ?</p>\n\n<p>Someone told me it might be because the embeddings u and v of one word should be far enough to represent the fact that a word rarely appears in its own context. But it is not clear to me.   </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "keras",
      "tensorflow",
      "transfer-learning"
    ],
    "owner": {
      "account_id": 1554834,
      "reputation": 347,
      "user_id": 17539,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/407d1774cc79b26ca440f2bc1b4103d5?s=256&d=identicon&r=PG",
      "display_name": "Borealis",
      "link": "https://datascience.stackexchange.com/users/17539/borealis"
    },
    "is_answered": true,
    "view_count": 18941,
    "accepted_answer_id": 30671,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1611956660,
    "creation_date": 1524461107,
    "last_edit_date": 1594640484,
    "question_id": 30659,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/30659/what-are-the-consequences-of-not-freezing-layers-in-transfer-learning",
    "title": "What are the consequences of not freezing layers in transfer learning?",
    "body": "<p>I am trying to fine tune some code from a <a href=\"https://www.kaggle.com/fujisan/use-keras-pre-trained-vgg16-acc-98\" rel=\"noreferrer\">Kaggle kernel</a>. The model uses pretrained VGG16 weights (via 'imagenet') for transfer learning. However, I notice there is no layer freezing of layers as is recommended in a <a href=\"https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\" rel=\"noreferrer\">keras blog</a>. One approach would be to freeze the all of the VGG16 layers and use only the last 4 layers in the code during compilation, for example:</p>\n\n<pre><code>for layer in model.layers[:-5]:\n    layer.trainable = False\n</code></pre>\n\n<p>Supposedly, this will use the imagenet weights for the top layers and train only the last 5 layers. What are the consequences of not freezing the VGG16 layers? </p>\n\n<pre><code>from keras.models import Sequential, Model, load_model\nfrom keras import applications\nfrom keras import optimizers\nfrom keras.layers import Dropout, Flatten, Dense\n\nimg_rows, img_cols, img_channel = 224, 224, 3\n\nbase_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_rows, img_cols, img_channel))\n\nadd_model = Sequential()\nadd_model.add(Flatten(input_shape=base_model.output_shape[1:]))\nadd_model.add(Dense(256, activation='relu'))\nadd_model.add(Dense(1, activation='sigmoid'))\n\nmodel = Model(inputs=base_model.input, outputs=add_model(base_model.output))\nmodel.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n              metrics=['accuracy'])\n\nmodel.summary()\n</code></pre>\n"
  },
  {
    "tags": [
      "clustering",
      "predictive-modeling",
      "predictor-importance"
    ],
    "owner": {
      "account_id": 5752084,
      "reputation": 290,
      "user_id": 41888,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/886e50b945ee508aa1178ec2a13fd65e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Tanguy",
      "link": "https://datascience.stackexchange.com/users/41888/tanguy"
    },
    "is_answered": true,
    "view_count": 7736,
    "answer_count": 6,
    "score": 13,
    "last_activity_date": 1638360633,
    "creation_date": 1524734490,
    "last_edit_date": 1524747702,
    "question_id": 30860,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/30860/is-it-possible-to-cluster-data-according-to-a-target",
    "title": "Is it possible to cluster data according to a target?",
    "body": "<p>I was wondering if there exists techniques to cluster data according to a target. For example, suppose we want to find groups of customers likely to churn: </p>\n\n<ul>\n<li>Target is churn.</li>\n<li>We want to find clusters exhibiting the same behaviour according to the fact that they are likely to churn (or not). Therefore, variables not explaining churn behaviour should not influence how clusters are built.</li>\n</ul>\n\n<p>I have done this analysis the following way: </p>\n\n<ol>\n<li>Predict target (e.g. using a Random Forest) and retrieve \"most important features\" (from feature importance analysis).</li>\n<li>Cluster samples with selected features (e.g. using k-means).</li>\n</ol>\n\n<p>However, I am afraid the clustering technique used in the 2nd step might not catch behaviours found in the 1st step which might explain churn (suppose there is a complex interaction in some trees in the RF, this interaction might not be cought in the k-means algorithm).</p>\n\n<p>I was thinking of another way of doing this by using a neural network:</p>\n\n<ol>\n<li>Predict target using a neural network with several layers, and for each sample retrieve activations from a given layer.</li>\n<li>Cluster samples with their activations.</li>\n</ol>\n\n<p>If the performance of the neural network is good and if the layer from which activations are retrieved is carefully chosen (not too close to the input or the output layer), I suppose the clusters could show customers displaying the same behaviour explaining the target.</p>\n\n<p>I did not find any articles having this approach. Did anyone deal with the same issue or have other ideas?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "cnn",
      "computer-vision"
    ],
    "owner": {
      "account_id": 8453205,
      "reputation": 2015,
      "user_id": 51129,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/29013c1e5a2906d8f3d08f27c953095e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "N.IT",
      "link": "https://datascience.stackexchange.com/users/51129/n-it"
    },
    "is_answered": true,
    "view_count": 9337,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1608616556,
    "creation_date": 1526302270,
    "last_edit_date": 1526304053,
    "question_id": 31641,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/31641/what-is-difference-between-fully-connected-layer-and-bilinear-layer-in-cnn",
    "title": "What is difference between Fully Connected layer and Bilinear layer in CNN?",
    "body": "<p>What is the difference between <code>Fully Connected</code> layers and <code>Bilinear</code> layers in deep learning?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "keras",
      "lstm"
    ],
    "owner": {
      "account_id": 5612565,
      "reputation": 420,
      "user_id": 53361,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7c8b0a5706eb83e39b4aaf1861458fee?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Jazz",
      "link": "https://datascience.stackexchange.com/users/53361/jazz"
    },
    "is_answered": true,
    "view_count": 14776,
    "accepted_answer_id": 32950,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1579314119,
    "creation_date": 1528463446,
    "last_edit_date": 1592305723,
    "question_id": 32831,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/32831/batch-size-of-stateful-lstm-in-keras",
    "title": "Batch Size of Stateful LSTM in keras",
    "body": "<p>My Model is defined as below:</p>\n<pre><code>## defining the model\nbatch_size = 1\n\ndef my_model():\n\n   \n   input_x = Input(batch_shape=(batch_size, look_back, 4), name='input')\n   drop = Dropout(0.5)\n   \n   lstm_1 = LSTM(100, return_sequences=True, batch_input_shape=(batch_size, look_back, 4), name='3dLSTM', stateful=True)(input_x)\n   lstm_1_drop = drop(lstm_1)\n   lstm_2 = LSTM(100, batch_input_shape=(batch_size, look_back, 4), name='2dLSTM', stateful=True)(lstm_1_drop)\n   lstm_2_drop = drop(lstm_2)\n\n   y1 = Dense(1, activation='relu', name='op1')(lstm_2_drop)\n   y2 = Dense(1, activation='relu', name='op2')(lstm_2_drop)\n   \n   model = Model(inputs=input_x, outputs=[y1,y2])\n   model.compile(loss='mse', optimizer='adam',metrics=['mse'])\n   model.summary()\n   return model\n\nmodel = my_model()\n</code></pre>\n<p>It is a Stateful LSTM model with batch size =1. My <code>model.fit</code> looks like this :</p>\n<pre><code># Train the model\nhistory = model.fit(\n    x_train,\n    [y_11_train,y_22_train],\n    epochs=1, \n    batch_size=batch_size, \n    verbose=0, \n    shuffle=False)\n\nmodel.reset_states()\n</code></pre>\n<p>My model runs well and outputs results. But I am unable to increase or alter the <code>batch_size</code> as flexibly as we could do when the model is in stateless condition.\nAs in for bigger size of dataset the model seems to be training forever as the <code>batch_size</code> here is just 1. And as we know we can't randomly put any <code>batch_size</code> value for <code>stateful LSTM</code> as it needs to be a divisible factor.</p>\n<p>I have gone through some blogs which describes changing <code>batch_size</code> by using different batch sizes for training and predicting using the <code>get_weights()</code> and <code>set_weights()</code> functions in the Keras API ref: <a href=\"https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/\" rel=\"noreferrer\">https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/</a>, still it seems to be that the <code>batch_size</code> that is being used here is less.</p>\n<p>My question is : Cant we use batch size like 32, 64, 128 in stateful LSTMs? If yes then how do I implement it in my above given model, if not then what are the alternatives?</p>\n<p>Looking for valuable suggestions.</p>\n<p><strong>Post Edit</strong></p>\n<p>In stateful LSTM <code>model.reset_states()</code> should be after every epoch, Hence I set the resetting of the states after each epoch in the following way:</p>\n<pre><code>for i in range(100):\n    start = time.time()\n    history = model.fit(x_train, [y_11_train,y_22_train], epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n    model.reset_states()\n\n    print(&quot;Epoch&quot;,i, time.time()-start,&quot;s&quot;)\n</code></pre>\n"
  },
  {
    "tags": [
      "machine-learning",
      "model-evaluations",
      "scoring",
      "metric",
      "nlg"
    ],
    "owner": {
      "account_id": 5434043,
      "reputation": 3618,
      "user_id": 54395,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/qcAy4.png?s=256",
      "display_name": "Bruno Lubascher",
      "link": "https://datascience.stackexchange.com/users/54395/bruno-lubascher"
    },
    "is_answered": true,
    "view_count": 2453,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1610919719,
    "creation_date": 1534066322,
    "last_edit_date": 1534090943,
    "question_id": 36817,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/36817/why-is-the-f-measure-preferred-for-classification-tasks",
    "title": "Why is the F-measure preferred for classification tasks?",
    "body": "<p><strong>Why is the F-measure usually used for (supervised) classification tasks, whereas the G-measure (or FowlkesMallows index) is  generally used for (unsupervised) clustering tasks?</strong></p>\n\n<p>The F-measure is the <strong>harmonic</strong> mean of the <strong>precision</strong> and <strong>recall</strong>.</p>\n\n<p>The G-measure (or FowlkesMallows index) is the <strong>geometric</strong> mean of the <strong>precision</strong> and <strong>recall</strong>.</p>\n\n<p>Below is a plot of the different means.</p>\n\n<p><a href=\"https://i.sstatic.net/o9bUr.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/o9bUr.png\" alt=\"enter image description here\"></a></p>\n\n<p>F1 (harmonic) $= 2\\cdot\\frac{precision\\cdot recall}{precision + recall}$</p>\n\n<p>Geometric $= \\sqrt{precision\\cdot recall}$</p>\n\n<p>Arithmetic $= \\frac{precision + recall}{2}$</p>\n\n<p>The reason I ask is that I need to decide which average to use in a NLG task, where I measured <strong>BLEU</strong> and <strong>ROUGE</strong> ( where BLEU is equivalent to precision and ROUGE to recall). <strong>How should I calculate the mean of these scores?</strong></p>\n"
  },
  {
    "tags": [
      "python",
      "deep-learning",
      "keras",
      "tensorflow",
      "cross-validation"
    ],
    "owner": {
      "account_id": 14164883,
      "reputation": 788,
      "user_id": 57731,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b2d72beba671ce005fda579f8d99e24e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Noran",
      "link": "https://datascience.stackexchange.com/users/57731/noran"
    },
    "is_answered": true,
    "view_count": 13291,
    "protected_date": 1571580319,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1571551841,
    "creation_date": 1534409968,
    "last_edit_date": 1534436067,
    "question_id": 37009,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/37009/k-fold-cross-validation-when-using-fit-generator-and-flow-from-directory-in-ke",
    "title": "K-fold cross validation when using fit_generator and flow_from_directory() in Keras",
    "body": "<p>I am using <code>flow_from_directory()</code> and <code>fit_generator</code> in my deep learning model, and I want to use cross validation method to train the CNN model.</p>\n\n<pre><code>datagen = ImageDataGenerator(rotation_range=15,width_shift_range=0.2,\n                             height_shift_range=0.2,shear_range=0.2,\n                             zoom_range=0.2,horizontal_flip=True,\n                             fill_mode='nearest')\n\nimage_size = (224, 224)\nbatch = 32\n\ntrain_generator = datagen.flow_from_directory(train_data,\n                                              target_size=image_size,\n                                              batch_size=batch,\n                                              classes= classes_array)\n</code></pre>\n\n<p>I found this <a href=\"https://www.youtube.com/watch?v=SIyMm5DFwQ8&amp;t=1016s\" rel=\"noreferrer\">Youtube video</a> and this <a href=\"https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/\" rel=\"noreferrer\">Tutorial</a>, But it is not use <code>flow_from_directory()</code>.</p>\n\n<p>Do you have any idea how do I use k-fold cross validation when using <code>fit_generator</code> and <code>flow_from_directory()</code> in Keras?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "categorical-data"
    ],
    "owner": {
      "account_id": 13325016,
      "reputation": 281,
      "user_id": 50405,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/bffbd612702bdcf552be1708275fd315?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Marisa",
      "link": "https://datascience.stackexchange.com/users/50405/marisa"
    },
    "is_answered": true,
    "view_count": 10774,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1538070260,
    "creation_date": 1535376701,
    "last_edit_date": 1535445359,
    "question_id": 37480,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/37480/how-can-i-do-classification-with-categorical-data-which-is-not-fixed",
    "title": "How can I do classification with categorical data which is not fixed?",
    "body": "<p>I have a classification problem with both categorical and numerical data. The problem I'm facing is that my categorical data is not fixed, that means that the new candidate whose label I want to predict may have a new category which was not observed beforehand. </p>\n\n<p>For example, if my categorical data was <code>sex</code>, the only possible labels would be <code>female</code>, <code>male</code> and <code>other</code>, no matter what. However, my categorical variable is <code>city</code> so it could happen that the person I am trying to predict has a new city that my classifier has never seen. </p>\n\n<p>I am wondering if there is a way to do the classification in these terms or if I should do the training again considering this new categorical data. </p>\n"
  },
  {
    "tags": [
      "time-series",
      "reinforcement-learning",
      "forecasting"
    ],
    "owner": {
      "account_id": 9339752,
      "reputation": 609,
      "user_id": 58384,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-iUME3M1jaSk/AAAAAAAAAAI/AAAAAAAAAQI/kPELBaK0IAM/s256-rj/photo.jpg",
      "display_name": "Osama Dar",
      "link": "https://datascience.stackexchange.com/users/58384/osama-dar"
    },
    "is_answered": true,
    "view_count": 16147,
    "accepted_answer_id": 37597,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1535639097,
    "creation_date": 1535615183,
    "last_edit_date": 1535639097,
    "question_id": 37589,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/37589/can-reinforcement-learning-be-applied-for-time-series-forecasting",
    "title": "Can Reinforcement learning be applied for time series forecasting?",
    "body": "<p>Can Reinforcement learning be applied for time series forecasting? </p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "keras",
      "tensorflow",
      "computer-vision",
      "convolutional-neural-network"
    ],
    "owner": {
      "account_id": 14164883,
      "reputation": 788,
      "user_id": 57731,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b2d72beba671ce005fda579f8d99e24e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Noran",
      "link": "https://datascience.stackexchange.com/users/57731/noran"
    },
    "is_answered": true,
    "view_count": 24621,
    "accepted_answer_id": 38331,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1613177624,
    "creation_date": 1537097355,
    "last_edit_date": 1613177624,
    "question_id": 38327,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/38327/optimizer-for-convolutional-neural-network",
    "title": "Optimizer for Convolutional neural network",
    "body": "<p>What is the best optimizer for Convolutional neural network (CNN)? </p>\n\n<p>Can I use RMSProp for CNN or only for RNN?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "neural-network",
      "deep-learning",
      "multiclass-classification",
      "activation-function"
    ],
    "owner": {
      "account_id": 8062976,
      "reputation": 131,
      "user_id": 60257,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0d68c8970f384e130488f48932d92943?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "bharath chandra",
      "link": "https://datascience.stackexchange.com/users/60257/bharath-chandra"
    },
    "is_answered": true,
    "view_count": 37536,
    "answer_count": 5,
    "score": 13,
    "last_activity_date": 1643661206,
    "creation_date": 1538815308,
    "last_edit_date": 1538855771,
    "question_id": 39264,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/39264/how-does-sigmoid-activation-work-in-multi-class-classification-problems",
    "title": "How does Sigmoid activation work in multi-class classification problems",
    "body": "<p>I know that for a problem with multiple classes we usually use softmax, but can we also use sigmoid? I have tried to implement digit classification with sigmoid at the output layer, it works. What I don't understand is how does it work?</p>\n"
  },
  {
    "tags": [
      "scikit-learn",
      "decision-trees"
    ],
    "owner": {
      "account_id": 13096083,
      "reputation": 301,
      "user_id": 60636,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/5e4a18805b87f11e1e66ead55cafef50?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "James Flash",
      "link": "https://datascience.stackexchange.com/users/60636/james-flash"
    },
    "is_answered": true,
    "view_count": 23448,
    "accepted_answer_id": 41442,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1604151484,
    "creation_date": 1542634747,
    "last_edit_date": 1542743596,
    "question_id": 41417,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/41417/how-max-features-parameter-works-in-decisiontreeclassifier",
    "title": "How max_features parameter works in DecisionTreeClassifier?",
    "body": "<p>What is the parameter <code>max_features</code> in <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\" rel=\"noreferrer\"><code>DecisionTreeClassifier</code></a> responsible for?</p>\n\n<p>I thought it defines the number of features the tree uses to generate its nodes. But in spite of the different values of this parameter (n = 1 and 2), my tree employs both features that I have. What changes so?</p>\n\n<p><code>max_features = 2</code></p>\n\n<p><a href=\"https://i.sstatic.net/wAQ5U.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/wAQ5U.png\" alt=\"enter image description here\"></a></p>\n\n<p><code>max_features = 1</code></p>\n\n<p><a href=\"https://i.sstatic.net/gWuKl.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/gWuKl.png\" alt=\"enter image description here\"></a></p>\n\n<p>You can see <code>x1</code> and <code>x2</code> are used in both cases</p>\n"
  },
  {
    "tags": [
      "jupyter",
      "anaconda"
    ],
    "owner": {
      "account_id": 9606341,
      "reputation": 307,
      "user_id": 63282,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6a6ad70a317f3a54202c179b33f69e3f?s=256&d=identicon&r=PG",
      "display_name": "Desmond",
      "link": "https://datascience.stackexchange.com/users/63282/desmond"
    },
    "is_answered": true,
    "view_count": 35702,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1580418480,
    "creation_date": 1543310951,
    "question_id": 41732,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/41732/conda-verification-failed",
    "title": "Conda Verification Failed",
    "body": "<p>I was trying to install jupyter package for anaconda in my current environment but constantly getting the following error.</p>\n\n<blockquote>\n  <p>Preparing transaction: done Verifying transaction: failed</p>\n  \n  <p>CondaVerificationError: The package for ipython located at\n  D:\\Anaconda\\pkgs\\ipython-7.1.1-py36h39e3cac_0 appears to be corrupted.\n  The path 'Lib/site-packages/IPython/lib/tests/test.wav' specified in\n  the package manifest cannot be found.</p>\n  \n  <p>CondaVerificationError: The package for notebook located at\n  D:\\Anaconda\\pkgs\\notebook-5.7.2-py36_0 appears to be corrupted. The\n  path\n  'Lib/site-packages/notebook/static/components/MathJax/extensions/a11y/invalid_keypress.mp3'\n  specified in the package manifest cannot be found.</p>\n</blockquote>\n\n<p>I have Googled many Q&amp;A for this problem, mostly I found:</p>\n\n<pre><code>conda clean --packages --tarballs \nconda clean --all\n</code></pre>\n\n<p>After trying both I tried the installation again, but none worked...</p>\n\n<p>Any solution for this?</p>\n"
  },
  {
    "tags": [
      "python",
      "keras",
      "tensorflow",
      "gpu",
      "nvidia"
    ],
    "owner": {
      "account_id": 14863330,
      "reputation": 133,
      "user_id": 63548,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-loNc6_wt6qc/AAAAAAAAAAI/AAAAAAAADOk/sBX77x3Ytys/s256-rj/photo.jpg",
      "display_name": "Deni Avinash",
      "link": "https://datascience.stackexchange.com/users/63548/deni-avinash"
    },
    "is_answered": true,
    "view_count": 44509,
    "protected_date": 1571072072,
    "accepted_answer_id": 41958,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1685466232,
    "creation_date": 1543711065,
    "last_edit_date": 1685466232,
    "question_id": 41956,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/41956/how-to-make-my-neural-netwok-run-on-gpu-instead-of-cpu",
    "title": "How to make my Neural Netwok run on GPU instead of CPU",
    "body": "<p>I have installed Anaconda3 and have installed latest versions of Keras and Tensorflow. </p>\n\n<p>Running this command :</p>\n\n<pre><code>from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\n</code></pre>\n\n<p>I find the Notebook is running in CPU:</p>\n\n<p>[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 2888773992010593937\n]</p>\n\n<p>This is my Nvidia version:</p>\n\n<p>nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2018 NVIDIA Corporation\nBuilt on Sat_Aug_25_21:08:04_Central_Daylight_Time_2018\nCuda compilation tools, release 10.0, V10.0.130</p>\n\n<p>running nvidia-smi, I'm getting this result:\n<a href=\"https://i.sstatic.net/pPrFY.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/pPrFY.png\" alt=\"enter image description here\"></a></p>\n\n<p>I want to make the neural network to train on GPU. Please help me in switching from CPU to GPU. </p>\n"
  },
  {
    "tags": [
      "bayesian",
      "hyperparameter-tuning"
    ],
    "owner": {
      "account_id": 5119265,
      "reputation": 509,
      "user_id": 57825,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4243ae0d8226839abe4ef8ea36d8fe88?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "AAC",
      "link": "https://datascience.stackexchange.com/users/57825/aac"
    },
    "is_answered": true,
    "view_count": 6983,
    "accepted_answer_id": 44165,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1547756762,
    "creation_date": 1543966085,
    "question_id": 42133,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/42133/what-makes-a-tree-structured-parzen-estimator-tree-structured",
    "title": "What makes a Tree-Structured Parzen Estimator &quot;tree-structured?&quot;",
    "body": "<p>From what I understand the Tree-Structured Parzen Estimator (TPE) creates two probability models based on hyperparameters that exceed the performance of some threshold and hyperparameters that don't. </p>\n\n<p>What I don't fully understand is why TPE is \"tree-structured.\" Is the simple clustering of y > thresh and y &lt; thresh sufficient to call it a \"tree?\" When I envision a \"tree structure\" I usually think of many different \"forks,\" perhaps some that are not binary and some that are forks of forks. Although a tree with just one fork is still technically a tree, the name \"Tree-Structured Parzen Estimator\" seems like it would describe something much more complex.</p>\n\n<p>My question is whether or not I am missing some other deeper conceptual \"forks\" inherent in TPE that would turn it into a \"tree\" when you look at the bigger picture.</p>\n"
  },
  {
    "tags": [
      "machine-learning"
    ],
    "owner": {
      "account_id": 11552010,
      "reputation": 233,
      "user_id": 54265,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ea10b4492d6aa57dc50b129498e108d5?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Niklas",
      "link": "https://datascience.stackexchange.com/users/54265/niklas"
    },
    "is_answered": true,
    "view_count": 23811,
    "accepted_answer_id": 44143,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1601796603,
    "creation_date": 1547662564,
    "question_id": 44108,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/44108/difference-between-a-target-and-a-label-in-machine-learning",
    "title": "Difference between a target and a label in machine learning",
    "body": "<p>If I have a supervised learning system (for example for the MNIST dataset) I have features (pixel values of MNIST data) and labels (correct digit-value).</p>\n\n<p>However sometimes people use the word target (instead of label).</p>\n\n<p>Are target and label interchangeable? Is label just used for classification? Target both for classification and regression?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "keras",
      "tensorflow",
      "multiclass-classification",
      "class-imbalance"
    ],
    "owner": {
      "account_id": 1417101,
      "reputation": 261,
      "user_id": 66737,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e430883e036caa5bcd2f9751420b6727?s=256&d=identicon&r=PG",
      "display_name": "dbm",
      "link": "https://datascience.stackexchange.com/users/66737/dbm"
    },
    "is_answered": true,
    "view_count": 13832,
    "answer_count": 6,
    "score": 13,
    "last_activity_date": 1615050348,
    "creation_date": 1548979335,
    "last_edit_date": 1548980327,
    "question_id": 44883,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/44883/deep-network-not-able-to-learn-imbalanced-data-beyond-the-dominant-class",
    "title": "Deep network not able to learn imbalanced data beyond the dominant class",
    "body": "<p>I have data with 5 output classes. The training data has the following no of samples for these 5 classes:\n[706326,  32211,   2856,   3050,    901]</p>\n\n<p>I am using the following keras (tf.keras) code:</p>\n\n<pre><code>class_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(y_train),\n                                                 y_train)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(50, input_shape=(dataX.shape[1],)),\n    tf.keras.layers.Dropout(rate = 0.5),\n    tf.keras.layers.Dense(50, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(rate = 0.5),\n    tf.keras.layers.Dense(50, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(rate = 0.5),\n    tf.keras.layers.Dense(50, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(rate = 0.5),\n    tf.keras.layers.Dense(5, activation=tf.nn.softmax) ])\n     adam = tf.keras.optimizers.Adam(lr=0.5)\n\nmodel.compile(optimizer=adam, \n              loss='sparse_categorical_crossentropy',\n              metrics=[metrics.sparse_categorical_accuracy])    \n     model.fit(X_train,y_train, epochs=5, batch_size=32, class_weight=class_weights)\n\ny_pred = np.argmax(model.predict(X_test), axis=1)\n</code></pre>\n\n<p>The first line on class_weight is taken from one of the answers in to this question: <a href=\"https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\">How to set class weights for imbalanced classes in Keras?</a></p>\n\n<p>I know about this answer: <a href=\"https://datascience.stackexchange.com/questions/16840/multi-class-neural-net-always-predicting-1-class-after-optimization?rq=1\">Multi-class neural net always predicting 1 class after optimization</a> . The difference is that in that case, the class weights wasn't used whereas I am using it.</p>\n\n<p>I am using sparse_categorical_crossentropy which accepts categories as integers (don't need to convert them to one-hot encoding), but I also tried categorical_crossentropy and still the same problem.</p>\n\n<p>I have of course tried different learning rate, batch_size, no of epochs, optimizer, and depth/length of the network. But it always is stuck at ~0.94 accuracy which is essentially I would get if I predict the first class all the time.</p>\n\n<p>Not sure what is missing here. Any error? Or should I use some other specialized deep network?</p>\n"
  },
  {
    "tags": [
      "nlp",
      "attention-mechanism"
    ],
    "owner": {
      "account_id": 9897059,
      "reputation": 251,
      "user_id": 67219,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/10211902465825465/picture?type=large",
      "display_name": "Sean Lee",
      "link": "https://datascience.stackexchange.com/users/67219/sean-lee"
    },
    "is_answered": true,
    "view_count": 9310,
    "accepted_answer_id": 55353,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1610816887,
    "creation_date": 1550029428,
    "question_id": 45475,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/45475/variable-input-output-length-for-transformer",
    "title": "Variable input/output length for Transformer",
    "body": "<p>I was reading the paper \"Attention is all you need\" (<a href=\"https://arxiv.org/pdf/1706.03762.pdf\" rel=\"noreferrer\">https://arxiv.org/pdf/1706.03762.pdf</a> ) and came across this site \n<a href=\"http://jalammar.github.io/illustrated-transformer/\" rel=\"noreferrer\">http://jalammar.github.io/illustrated-transformer/</a> which provided a great breakdown of the architecture of the Transformer. </p>\n\n<p>Unfortunately, I was unable to find any explanation of why it works with input/output lengths that are not equal (eg. input: je suis tudiant and expected output: i am a student). </p>\n\n<p>My main confusion is this. From what I understand, when we are passing the output from the encoder to the decoder (say <span class=\"math-container\">$3 \\times 10$</span> in this case), we do so via a Multi-Head Attention layer, which takes in 3 inputs:</p>\n\n<ol>\n<li>A Query (from encoder), of dimension <span class=\"math-container\">$3 \\times k_1$</span></li>\n<li>A Key (from encoder), of dimension <span class=\"math-container\">$3 \\times k_1$</span></li>\n<li>A Value (from decoder), of dimension <span class=\"math-container\">$L_0 \\times k_1$</span>, where <span class=\"math-container\">$L_0$</span> refers to the number of words in the (masked) output sentence.</li>\n</ol>\n\n<p>Given that the Multi-Head Attention should take in 3 matrices which have the same number of rows (or at least this is what I have understood from its architecture), how do we deal with the problem of varying output lengths?</p>\n"
  },
  {
    "tags": [
      "pandas",
      "dataframe",
      "difference"
    ],
    "owner": {
      "account_id": 4931656,
      "reputation": 133,
      "user_id": 52219,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/055db49e288a9f3d8db24a8cb5320f4e?s=256&d=identicon&r=PG",
      "display_name": "Parth S.",
      "link": "https://datascience.stackexchange.com/users/52219/parth-s"
    },
    "is_answered": true,
    "view_count": 59852,
    "accepted_answer_id": 46436,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1665740396,
    "creation_date": 1551400771,
    "last_edit_date": 1551490260,
    "question_id": 46434,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/46434/dataframe-columns-difference-use",
    "title": "dataframe.columns.difference() use",
    "body": "<p>I am trying to find the working of <code>dataframe.columns.difference()</code> but couldn't find a satisfactory explanation about it. Can anyone explain the working of this method in detail? </p>\n"
  },
  {
    "tags": [
      "pandas",
      "dataframe",
      "excel",
      "data-table"
    ],
    "owner": {
      "account_id": 1695045,
      "reputation": 345,
      "user_id": 37045,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/3z8rb.jpg?s=256",
      "display_name": "Della",
      "link": "https://datascience.stackexchange.com/users/37045/della"
    },
    "is_answered": true,
    "view_count": 74926,
    "accepted_answer_id": 46451,
    "answer_count": 5,
    "score": 13,
    "last_activity_date": 1707817555,
    "creation_date": 1551417834,
    "last_edit_date": 1611260488,
    "question_id": 46437,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/46437/how-to-write-multiple-data-frames-in-an-excel-sheet",
    "title": "How to Write Multiple Data Frames in an Excel Sheet",
    "body": "<p>I have multiple data frames with same column names. I want to write them together to an excel sheet stacked vertically on top of each other. And between each, there will be a text occupying a row. This is what I have in mind. </p>\n\n<p><a href=\"https://i.sstatic.net/VsmKz.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/VsmKz.png\" alt=\"enter image description here\"></a></p>\n\n<p>I tried the pandas.ExcelWriter() method, but each dataframe overwrites the previous frame in the sheet, instead of appending. </p>\n\n<p>Note that, I still need multiple sheets for different dataframe, but also multiple dataframes on each sheet. Is it possible? Or any other python library which can dynamically generate the excel sheet from pandas dataframes?</p>\n"
  },
  {
    "tags": [
      "python",
      "clustering",
      "pandas"
    ],
    "owner": {
      "account_id": 10208261,
      "reputation": 141,
      "user_id": 70946,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/BrI9o.jpg?s=256",
      "display_name": "Lola",
      "link": "https://datascience.stackexchange.com/users/70946/lola"
    },
    "is_answered": true,
    "view_count": 101414,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1646331226,
    "creation_date": 1554470406,
    "last_edit_date": 1592305723,
    "question_id": 48693,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/48693/perform-k-means-clustering-over-multiple-columns",
    "title": "Perform k-means clustering over multiple columns",
    "body": "<p>I am trying to perform k-means clustering on multiple columns. My data set is composed of 4 numerical columns and 1 categorical column. I already researched previous questions but the answers are not satisfactory.</p>\n<p>I know how to perform the algorithm on two columns, but I'm finding it quite difficult to apply the same algorithm on 4 numerical columns.</p>\n<p>I am not really interested in visualizing the data for now, but in having the clusters displayed in the table.The picture shows that the first row belongs to cluster number 2, and so on. That is exactly what I need to achieve, but using 4 numerical columns, therefore each row must belong to a certain cluster.</p>\n<p>Do you have any idea on how to achieve this? Any idea would be of great help. Thanks in advance! :<a href=\"https://i.sstatic.net/FqLMH.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/FqLMH.png\" alt=\"enter image description here\" /></a></p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "cnn",
      "image-classification",
      "image-preprocessing"
    ],
    "owner": {
      "account_id": 10774251,
      "reputation": 1471,
      "user_id": 50146,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/W7XNJ.jpg?s=256",
      "display_name": "deepguy",
      "link": "https://datascience.stackexchange.com/users/50146/deepguy"
    },
    "is_answered": true,
    "view_count": 4579,
    "accepted_answer_id": 54020,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1667605406,
    "creation_date": 1558585066,
    "last_edit_date": 1576590841,
    "question_id": 52435,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/52435/finding-outliers-in-image-dataset",
    "title": "Finding outliers in Image dataset",
    "body": "<p>I have been working on an image classification tasks for which I am extracting the image frames from the video stream collected for different classes.</p>\n\n<p>I have already trained an image classification model (using transfer learning) however due to the outliers (or overlap in the class distribution) accuracy of the model is poor. And not able to generalize the new images / video streams. </p>\n\n<p>Could you please help me with the below queries <br /></p>\n\n<ol>\n<li><p>How the sample is distributed in each class ? Can I use any visualization techniques (for example : histogram) to see the sample distribution.</p></li>\n<li><p>And also going through the image one by one is tedious process so is there a technique with which I can find the outliers (outlier images) from the samples. So that I can remove outliers before training the model.</p></li>\n</ol>\n\n<p>Any updates on this..</p>\n\n<p>Thank you</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python"
    ],
    "owner": {
      "account_id": 16054103,
      "reputation": 131,
      "user_id": 75264,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/288754b2bd682c1e81353d1c700124f8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Siri1993",
      "link": "https://datascience.stackexchange.com/users/75264/siri1993"
    },
    "is_answered": true,
    "view_count": 72417,
    "protected_date": 1567008852,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1631677839,
    "creation_date": 1559399017,
    "last_edit_date": 1559420721,
    "question_id": 53048,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/53048/expected-2d-array-got-scalar-array-instead",
    "title": "Expected 2D array, got scalar array instead",
    "body": "<p>Can anyone help me with this error. I did the following code but it does not work and I am getting the following error:</p>\n\n<pre><code>ValueError: Expected 2D array, got scalar array instead:\narray=6.5. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample. \n</code></pre>\n\n<p>My code:</p>\n\n<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport pandas\ndataset = pandas.read_excel('PEG RATIOS.xlsx')\n\nX = dataset.iloc[:, 2].values\nX =X.reshape(-1,1)\ny = dataset.iloc[:, 3].values\ny = y.reshape (-1,1)\n\n\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree = 4)\nX_poly = poly_reg.fit_transform(X)\npoly_reg.fit(X_poly, y)\nlin_reg_2 = LinearRegression()\nlin_reg_2.fit(X_poly, y)\n\nX_grid = np.arange(min(X), max(X), 0.1)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.scatter(X, y, color = 'red')\nplt.plot(X_grid, lin_reg_2.predict(poly_reg.fit_transform(X_grid)), color = 'blue')\nplt.title('PEG Ratios verrus Exoected Growth: Semiconductor Firms')\nplt.xlabel('Expected Growth rate')\nplt.ylabel('PEGH Ratio')\nplt.show()\nlin_reg_2.predict(poly_reg.fit_transform(6.5))\n</code></pre>\n"
  },
  {
    "tags": [
      "deep-learning",
      "accuracy",
      "overfitting"
    ],
    "owner": {
      "account_id": 12994076,
      "reputation": 329,
      "user_id": 71515,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/QJxIR.jpg?s=256",
      "display_name": "Nikhil.Nixel",
      "link": "https://datascience.stackexchange.com/users/71515/nikhil-nixel"
    },
    "is_answered": true,
    "view_count": 64099,
    "accepted_answer_id": 53257,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1582554409,
    "creation_date": 1559742874,
    "last_edit_date": 1559826407,
    "question_id": 53256,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/53256/how-to-know-if-a-model-is-overfitting-or-underfitting-by-looking-at-graph",
    "title": "How to know if a model is overfitting or underfitting by looking at graph",
    "body": "<p>Just recently got my hands on tensorboard, but can you tell me what features should I look for in the graph (Accuracy and Validation Accuracy)\nAnd please do enlighten me about the concept of underfitting as well.</p>\n"
  },
  {
    "tags": [
      "python",
      "pandas",
      "dataframe"
    ],
    "owner": {
      "account_id": 8107981,
      "reputation": 303,
      "user_id": 38091,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4e2a2e1863d0764986bed4ea76268362?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Harshith",
      "link": "https://datascience.stackexchange.com/users/38091/harshith"
    },
    "is_answered": true,
    "view_count": 47139,
    "closed_date": 1568057338,
    "accepted_answer_id": 58893,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1568092910,
    "creation_date": 1568021589,
    "question_id": 58890,
    "link": "https://datascience.stackexchange.com/questions/58890/delete-drop-only-the-rows-which-has-all-values-as-nan-in-pandas",
    "closed_reason": "Not suitable for this site",
    "title": "Delete/Drop only the rows which has all values as NaN in pandas",
    "body": "<p>I have a Dataframe, i need to drop the rows which has all the values as NaN.</p>\n\n<pre><code>ID      Age    Gender\n601     21       M\n501     NaN      F\nNaN     NaN      NaN\n</code></pre>\n\n<p>The resulting data frame should look like.</p>\n\n<pre><code>Id     Age    Gender\n601     21      M\n501    NaN      F\n</code></pre>\n\n<p>I used df.drop(axis = 0), this will delete the rows if there is even one NaN value in row. Is there a way to do as required?</p>\n"
  },
  {
    "tags": [
      "data",
      "training",
      "smote"
    ],
    "owner": {
      "account_id": 12807198,
      "reputation": 761,
      "user_id": 65133,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/9eaf0efe385f5ca2e7f49887986e20b4?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "yamini goel",
      "link": "https://datascience.stackexchange.com/users/65133/yamini-goel"
    },
    "is_answered": true,
    "view_count": 20972,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1603457027,
    "creation_date": 1571300466,
    "question_id": 61858,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/61858/oversampling-undersampling-only-train-set-only-or-both-train-and-validation-set",
    "title": "Oversampling/Undersampling only train set only or both train and validation set",
    "body": "<p>I am working on a dataset with class imbalance problem. Now, I know one needs to oversample or undersample only the train set and not the test set. But my issue is: whether to oversample the train set and then split it to train and validate set or first split into train and val set and then perform sampling(over/under) on the train set only</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "statistics",
      "feature-selection",
      "performance"
    ],
    "owner": {
      "account_id": 15001531,
      "reputation": 2725,
      "user_id": 64876,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-XkVFAldxdns/AAAAAAAAAAI/AAAAAAAAAAA/AKxrwcYw7mFQ9BqeKrYJYmarsmObRK9Lsw/mo/s256-rj/photo.jpg",
      "display_name": "The Great",
      "link": "https://datascience.stackexchange.com/users/64876/the-great"
    },
    "is_answered": true,
    "view_count": 10101,
    "accepted_answer_id": 66346,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1617886316,
    "creation_date": 1578828894,
    "last_edit_date": 1617886316,
    "question_id": 66345,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/66345/why-ml-model-produces-different-results-despite-random-state-defined-and-how-to",
    "title": "Why ML model produces different results despite random_state defined? And how to set global random seed for sklearn",
    "body": "<p>I have been running few ML models on same set of data for a binary classification problem with class proportion of 33:67.</p>\n<p>I had the same algorithms and same set of hyperparamters during yesterday and today's run.</p>\n<p>Please note that I also have the parameter <code>random_state</code> in each estimator function as shown below</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>np.random.seed(42)\nsvm=SVC()  # i replace the estimator here for diff algos\nsvm_cv=GridSearchCV(svm,op_param_grid,cv=10,scoring='f1')\nsvm_cv.fit(X_train_std,y_train)\n</code></pre>\n<p>q1) Why does this change happens even when I have <code>random_state</code> configured?</p>\n<p>q2) Is there anything else that I should do to reproduce the same results every time I run?</p>\n<p>Please find below the results that are different? Here <code>auc-Y</code> denotes yesterday's run</p>\n<p><a href=\"https://i.sstatic.net/7eYat.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/7eYat.png\" alt=\"enter image description here\" /></a></p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "python",
      "deep-learning",
      "keras",
      "lstm"
    ],
    "owner": {
      "account_id": 9686325,
      "reputation": 242,
      "user_id": 78699,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/4ymUA.jpg?s=256",
      "display_name": "lsfischer",
      "link": "https://datascience.stackexchange.com/users/78699/lsfischer"
    },
    "is_answered": true,
    "view_count": 32035,
    "accepted_answer_id": 66602,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1685465879,
    "creation_date": 1579190623,
    "last_edit_date": 1685465879,
    "question_id": 66594,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/66594/activation-function-between-lstm-layers",
    "title": "Activation function between LSTM layers",
    "body": "<p>I'm aware the LSTM cell uses both sigmoid and tanh activation functions internally, however when creating a stacked LSTM architecture does it make sense to pass their outputs through an activation function (e.g. ReLU)?</p>\n\n<p>So do we prefer this:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>model = LSTM(100, activation=\"relu\", return_sequences=True, input_shape(timesteps, n_features))\nmodel = LSTM(50, activation=\"relu\", return_sequences=True)(model)\n...\n</code></pre>\n\n<p>over this?</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>model = LSTM(100, return_sequences=True, input_shape(timesteps, n_features))\nmodel = LSTM(50, return_sequences=True)(model)\n...\n</code></pre>\n\n<p>From my empirical results when creating an LSTM-autoencoder I've found them to be quite similar.</p>\n"
  },
  {
    "tags": [
      "predictive-modeling",
      "algorithms"
    ],
    "owner": {
      "account_id": 7975314,
      "reputation": 348,
      "user_id": 80461,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/fbf74500b97d722cdc862171e64f5145?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Mariah",
      "link": "https://datascience.stackexchange.com/users/80461/mariah"
    },
    "is_answered": true,
    "view_count": 1418,
    "answer_count": 5,
    "score": 13,
    "last_activity_date": 1606425008,
    "creation_date": 1579287727,
    "last_edit_date": 1579291854,
    "question_id": 66651,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/66651/in-industry-what-type-of-new-data-science-algorithms-does-one-develop",
    "title": "In industry, what type of new data science algorithms does one develop?",
    "body": "<p>I've seen several job descriptions for data science which include developing a novel algorithm to be a part of production environments.\nCan you give some input of what could be meant here exactly? Would they mean an algorithm that behaves somewhat of an ETL: getting the data, cleaning it, storing it, and running a known model on it? Or something more complex like building variations of known prediction algorithms? Some examples will really be nice, as I'm studying to go into this field.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "nlp",
      "transformer",
      "attention-mechanism"
    ],
    "owner": {
      "account_id": 5546205,
      "reputation": 263,
      "user_id": 78637,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8a00934db8fe14793cc5c642c10fb8e3?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Joff",
      "link": "https://datascience.stackexchange.com/users/78637/joff"
    },
    "is_answered": true,
    "view_count": 8506,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1693970568,
    "creation_date": 1582462489,
    "question_id": 68553,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/68553/why-does-the-transformer-positional-encoding-use-both-sine-and-cosine",
    "title": "Why does the transformer positional encoding use both sine and cosine?",
    "body": "<p>In the transformer architecture they use positional encoding (explained in <a href=\"https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model\">this answer</a> and I get how it is constructed. </p>\n\n<p>I am wondering why it needs to use both sine and cosine though instead of just one or the other?</p>\n"
  },
  {
    "tags": [
      "neural-network"
    ],
    "owner": {
      "account_id": 1633482,
      "reputation": 419,
      "user_id": 46563,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/0OQkl.png?s=256",
      "display_name": "Shinobii",
      "link": "https://datascience.stackexchange.com/users/46563/shinobii"
    },
    "is_answered": true,
    "view_count": 4824,
    "protected_date": 1586352484,
    "accepted_answer_id": 68896,
    "answer_count": 4,
    "score": 13,
    "last_activity_date": 1586349345,
    "creation_date": 1582922285,
    "question_id": 68893,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/68893/how-can-i-provide-an-answer-to-neural-network-skeptics",
    "title": "How can I provide an answer to Neural Network skeptics?",
    "body": "<p>After given several talks on NN's, I always have a skeptic that wants a real measure of how well the model is. How do you know the model is truly accurate?</p>\n\n<p>I explain the use of test data etc. to evaluate the total error, however, there is always someone who wants to know about the error associated with each weight?</p>\n\n<p>Can anyone enlighten me as to how I can satisfy these types of questions?</p>\n\n<p>It has become a real issue.</p>\n"
  },
  {
    "tags": [
      "python",
      "pandas",
      "matplotlib",
      "tableau"
    ],
    "owner": {
      "account_id": 12898536,
      "reputation": 143,
      "user_id": 92553,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/L7LC9.png?s=256",
      "display_name": "Uralan",
      "link": "https://datascience.stackexchange.com/users/92553/uralan"
    },
    "is_answered": true,
    "view_count": 3018,
    "accepted_answer_id": 71406,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1741632740,
    "creation_date": 1585483241,
    "question_id": 71404,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/71404/what-do-pythons-pandas-matplotlib-seaborn-bring-to-the-table-that-tableau-does",
    "title": "What do Python&#39;s pandas/matplotlib/seaborn bring to the table that Tableau does not?",
    "body": "<p>I spent the past year learning Python. As a person who thought coding was impossible to learn for those outside of the CS/IT sphere, I was obviously gobsmacked by the power of a few lines of Python code!</p>\n\n<p>Having arrived at an intermediate level overall, I was pretty proud of myself as it greatly expands my possibilities in data analysis and visualization compared to Excel (aside from the millions of other uses there are for Python).</p>\n\n<p>Purely in terms of data analysis and visualization:</p>\n\n<p><strong>what does approaching the same data set with</strong> <code>pandas/matplotlib/seaborn/numpy</code> <strong>bring to the table as opposed to using <em>Tableau</em>?</strong></p>\n\n<p>(sidenote: I was greatly disappointed to see all my hard-earned Python data wrangling skills were available in such a user-friendly GUI... :'( )</p>\n"
  },
  {
    "tags": [
      "correlation"
    ],
    "owner": {
      "account_id": 18725340,
      "reputation": 131,
      "user_id": 98234,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8142f332796d246bd72758355c96cde7?s=256&d=identicon&r=PG",
      "display_name": "Ashley",
      "link": "https://datascience.stackexchange.com/users/98234/ashley"
    },
    "is_answered": true,
    "view_count": 4534,
    "answer_count": 8,
    "score": 13,
    "last_activity_date": 1593361704,
    "creation_date": 1590919792,
    "question_id": 75167,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/75167/if-a-and-b-are-correlated-and-a-and-c-are-correlated-why-is-it-possible-for-b-a",
    "title": "If A and B are correlated and A and C are correlated. Why is it possible for B and C to be uncorrelated?",
    "body": "<p>Let's say\nA and B are correlated\nA and C are correlated\nB and C is uncorrelated\nHow is it possible for B and C to be uncorrelated when they are both correlated to A? </p>\n"
  },
  {
    "tags": [
      "machine-learning"
    ],
    "owner": {
      "account_id": 22104593,
      "reputation": 239,
      "user_id": 132065,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/209950564333436/picture?type=large",
      "display_name": "Ahmad Turani",
      "link": "https://datascience.stackexchange.com/users/132065/ahmad-turani"
    },
    "is_answered": true,
    "view_count": 3040,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1671290760,
    "creation_date": 1671265403,
    "question_id": 117119,
    "content_license": "CC BY-SA 4.0",
    "link": "https://datascience.stackexchange.com/questions/117119/1-of-data-for-training-99-of-data-for-testing",
    "title": "1% of data for training 99% of data for testing",
    "body": "<p>I got feedback from a reviewer. It is really important for me to answer to this question. I would appreciate of any help.</p>\n<p>it was mentioned that 1% of the data was used for training while 99% was used for testing. This is unusual and it calls for careful evaluation of the actual need for using ML tools for the problem. In short, if just 1% is sufficient to build a ML model, it may mean that the data is essentially trivial such that using ML may not be at all necessary. For this reason, it would be good for the authors to provide a rather strong justification for the motivation of this work</p>\n<p>So, actually we went with 10 and 90 and got same result. We wanted to show that with less amount of training data we could provide good prediction. Any idea we could write that for 80 and 20 there is not much difference?</p>\n"
  },
  {
    "tags": [
      "bigdata",
      "google",
      "search"
    ],
    "owner": {
      "account_id": 975679,
      "reputation": 231,
      "user_id": 189,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/62b2afdccdf0a4aa366c46b99aed8e2b?s=256&d=identicon&r=PG",
      "display_name": "resgh",
      "link": "https://datascience.stackexchange.com/users/189/resgh"
    },
    "is_answered": true,
    "view_count": 568,
    "accepted_answer_id": 91,
    "answer_count": 3,
    "score": 12,
    "last_activity_date": 1409424002,
    "creation_date": 1400152947,
    "last_edit_date": 1400208416,
    "question_id": 89,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/89/how-does-a-query-into-a-huge-database-return-with-negligible-latency",
    "title": "How does a query into a huge database return with negligible latency?",
    "body": "<p>For example, when searching something in Google, results return nigh-instantly.</p>\n\n<p>I understand that Google sorts and indexes pages with algorithms etc., but I imagine it infeasible for the results of every single possible query to be indexed (and results are personalized, which renders this even more infeasible)?</p>\n\n<p>Moreover, wouldn't the hardware latency in Google's hardware be huge? Even if the data in Google were all stored in TB/s SSDs, I imagine the hardware latency to be huge, given the sheer amount of data to process.</p>\n\n<p>Does MapReduce help solve this problem?</p>\n\n<p>EDIT: Okay, so I understand that popular searches can be cached in memory. But what about unpopular searches? Even for the most obscure search I have conducted, I don't think the search has ever been reported to be larger than 5 seconds. How is this possible?</p>\n"
  },
  {
    "tags": [
      "tools",
      "data-stream-mining"
    ],
    "owner": {
      "account_id": 9698,
      "reputation": 315,
      "user_id": 200,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8723a22cc67390f68ce5fd57f4d2e02b?s=256&d=identicon&r=PG",
      "display_name": "Tahir Akhtar",
      "link": "https://datascience.stackexchange.com/users/200/tahir-akhtar"
    },
    "is_answered": true,
    "view_count": 148,
    "answer_count": 2,
    "score": 12,
    "last_activity_date": 1400484830,
    "creation_date": 1400270870,
    "last_edit_date": 1400484830,
    "question_id": 107,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/107/opensource-tools-for-help-in-mining-stream-of-leader-board-scores",
    "title": "Opensource tools for help in mining stream of leader board scores",
    "body": "<p>Consider a stream containing <a href=\"http://en.m.wikipedia.org/wiki/Tuple\" rel=\"noreferrer\">tuples</a> <code>(user, new_score)</code> representing users' scores in an online game. The stream could have 100-1,000 new elements per second. The game has 200K to 300K unique players. </p>\n\n<p>I would like to have some standing queries like: </p>\n\n<ol>\n<li>Which players posted more than x scores in a sliding window of one hour</li>\n<li>Which players gained x% score in a sliding window of one hour</li>\n</ol>\n\n<p>My question is which open source tools can I employ to jumpstart this project? I am considering <a href=\"http://esper.codehaus.org/\" rel=\"noreferrer\">Esper</a> at the moment. </p>\n\n<p>Note: I have just completed reading \"Mining Data Streams\" (chapter 4 of <a href=\"http://infolab.stanford.edu/~ullman/mmds.html\" rel=\"noreferrer\">Mining of Massive Datasets</a>) and I am quite new to mining data streams.</p>\n"
  },
  {
    "tags": [
      "social-network-analysis",
      "time-series",
      "javascript",
      "visualization"
    ],
    "owner": {
      "account_id": 1503498,
      "reputation": 916,
      "user_id": 173,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Z4uAf.jpg?s=256",
      "display_name": "Wojciech Walczak",
      "link": "https://datascience.stackexchange.com/users/173/wojciech-walczak"
    },
    "is_answered": true,
    "view_count": 5005,
    "accepted_answer_id": 190,
    "answer_count": 3,
    "score": 12,
    "last_activity_date": 1400760889,
    "creation_date": 1400650176,
    "last_edit_date": 1400651518,
    "question_id": 176,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/176/how-to-animate-growth-of-a-social-network",
    "title": "How to animate growth of a social network?",
    "body": "<p>I am seeking for a library/tool to visualize how social network changes when new nodes/edges are added to it.</p>\n\n<p>One of the existing solutions is <a href=\"http://www.stanford.edu/group/sonia/\">SoNIA: Social Network Image Animator</a>. It let's you make movies like <a href=\"https://www.youtube.com/watch?v=yGSNCED6mDc\">this one</a>. </p>\n\n<p>SoNIA's documentation says that it's broken at the moment, and besides this I would prefer JavaScript-based solution instead. So, my question is: are you familiar with any tools or are you able to point me to some libraries which would make this task as easy as possible?</p>\n\n<p>Right after posting this question I'll dig into <a href=\"http://sigmajs.org/\">sigma.js</a>, so please consider this library covered.</p>\n\n<p>In general, my input data would be something like this:</p>\n\n<pre><code>time_elapsed; node1; node2\n1; A; B\n2; A; C\n3; B; C\n</code></pre>\n\n<p>So, here we have three points in time (1, 2, 3), three nodes (A, B, C), and three edges, which represent a triadic closure between the three considered nodes.</p>\n\n<p>Moreover, every node will have two attributes (age and gender), so I would like to be able to change the shape/colour of the nodes.</p>\n\n<p>Also, after adding a new node, it would be perfect to have some ForceAtlas2 or similar algorithm to adjust the layout of the graph.</p>\n"
  },
  {
    "tags": [
      "parallel",
      "clustering",
      "aws"
    ],
    "owner": {
      "account_id": 4494374,
      "reputation": 871,
      "user_id": 250,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/cDteV.jpg?s=256",
      "display_name": "Therriault",
      "link": "https://datascience.stackexchange.com/users/250/therriault"
    },
    "is_answered": true,
    "view_count": 1964,
    "accepted_answer_id": 208,
    "answer_count": 3,
    "score": 12,
    "last_activity_date": 1487495569,
    "creation_date": 1400874354,
    "question_id": 205,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/205/instances-vs-cores-when-using-ec2",
    "title": "Instances vs. cores when using EC2",
    "body": "<p>Working on what could often be called \"medium data\" projects, I've been able to parallelize my code (mostly for modeling and prediction in Python) on a single system across anywhere from 4 to 32 cores. Now I'm looking at scaling up to clusters on EC2 (probably with StarCluster/IPython, but open to other suggestions as well), and have been puzzled by how to reconcile distributing work across cores on an instance vs. instances on a cluster.</p>\n\n<p>Is it even practical to parallelize across instances as well as across cores on each instance? If so, can anyone give a quick rundown of the pros + cons of running many instances with few cores each vs. a few instances with many cores? Is there a rule of thumb for choosing the right ratio of instances to cores per instance? </p>\n\n<p>Bandwidth and RAM are non-trivial concerns in my projects, but it's easy to spot when those are the bottlenecks and readjust. It's much harder, I'd imagine, to benchmark the right mix of cores to instances without repeated testing, and my projects vary too much for any single test to apply to all circumstances. Thanks in advance, and if I've just failed to google this one properly, feel free to point me to the right answer somewhere else!</p>\n"
  },
  {
    "tags": [
      "bigdata",
      "efficiency",
      "apache-hadoop",
      "distributed"
    ],
    "owner": {
      "account_id": 3818521,
      "reputation": 347,
      "user_id": 339,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e31bbf9b2079cd69df18ba1e07c2c8e2?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "mbbce",
      "link": "https://datascience.stackexchange.com/users/339/mbbce"
    },
    "is_answered": true,
    "view_count": 7213,
    "accepted_answer_id": 242,
    "answer_count": 2,
    "score": 12,
    "last_activity_date": 1402443208,
    "creation_date": 1401618351,
    "last_edit_date": 1401659942,
    "question_id": 227,
    "content_license": "CC BY-SA 3.0",
    "link": "https://datascience.stackexchange.com/questions/227/tradeoffs-between-storm-and-hadoop-mapreduce",
    "title": "Tradeoffs between Storm and Hadoop (MapReduce)",
    "body": "<p>Can someone kindly tell me about the trade-offs involved when choosing between Storm and MapReduce in Hadoop Cluster for data processing? Of course, aside from the obvious one, that Hadoop (processing via MapReduce in a Hadoop Cluster) is a batch processing system, and Storm is a real-time processing system.</p>\n\n<p>I have worked a bit with Hadoop Eco System, but I haven't worked with Storm. After looking through a lot of presentations and articles, I still haven't been able to find a satisfactory and comprehensive answer.</p>\n\n<p>Note: The term tradeoff here is not meant to compare to similar things. It is meant to represent the consequences of getting results real-time that are absent from a batch processing system. </p>\n"
  }
]