[
  {
    "tags": [
      "philosophy",
      "decision-theory",
      "mythology-of-ai",
      "death"
    ],
    "owner": {
      "account_id": 6161610,
      "reputation": 1949,
      "user_id": 1812,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-cXt5wYhFuAc/AAAAAAAAAAI/AAAAAAAAAEE/cZR9o9lxnx0/s256-rj/photo.jpg",
      "display_name": "Josh B.",
      "link": "https://ai.stackexchange.com/users/1812/josh-b"
    },
    "is_answered": true,
    "view_count": 62104,
    "protected_date": 1472577260,
    "accepted_answer_id": 1769,
    "answer_count": 12,
    "score": 182,
    "last_activity_date": 1639468807,
    "creation_date": 1472485754,
    "last_edit_date": 1583446917,
    "question_id": 1768,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1768/could-a-paradox-kill-an-ai",
    "title": "Could a paradox kill an AI?",
    "body": "<p>In <a href=\"https://en.wikipedia.org/wiki/Portal_2\" rel=\"noreferrer\">Portal 2</a> we see that AI's can be \"<em>killed</em>\" by thinking about a paradox.</p>\n\n<p><a href=\"https://i.sstatic.net/wkUSC.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/wkUSC.png\" alt=\"Portal Paradox Poster\"></a></p>\n\n<p>I assume this works by forcing the AI into an infinite loop which would essentially \"<em>freeze</em>\" the computer's consciousness.</p>\n\n<p><strong>Questions:</strong> </p>\n\n<ul>\n<li>Would this confuse the AI technology we have today to the point of destroying it? </li>\n<li>If so, why? </li>\n<li>And if not, could it be possible in the future?</li>\n</ul>\n"
  },
  {
    "tags": [
      "machine-learning",
      "comparison",
      "terminology",
      "ai-field"
    ],
    "owner": {
      "account_id": 5780101,
      "reputation": 1305,
      "user_id": 69,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/637eff3f3244f3658d31a25ea7636d82?s=256&d=identicon&r=PG",
      "display_name": "intcreator",
      "link": "https://ai.stackexchange.com/users/69/intcreator"
    },
    "is_answered": true,
    "view_count": 17628,
    "protected_date": 1560977380,
    "answer_count": 9,
    "score": 102,
    "last_activity_date": 1637230715,
    "creation_date": 1470153926,
    "last_edit_date": 1637230715,
    "question_id": 35,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/35/what-is-the-difference-between-artificial-intelligence-and-machine-learning",
    "title": "What is the difference between artificial intelligence and machine learning?",
    "body": "<p>These two terms seem to be related, especially in their application in computer science and software engineering.</p>\n<ul>\n<li>Is one a subset of another?</li>\n<li>Is one a tool used to build a system for the other?</li>\n<li>What are their differences and why are they significant?</li>\n</ul>\n"
  },
  {
    "tags": [
      "philosophy",
      "ethics",
      "autonomous-vehicles",
      "decision-theory"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 8122,
    "protected_date": 1488918323,
    "accepted_answer_id": 1790,
    "answer_count": 14,
    "score": 102,
    "last_activity_date": 1593211056,
    "creation_date": 1470164277,
    "last_edit_date": 1593211056,
    "question_id": 111,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/111/how-could-self-driving-cars-make-ethical-decisions-about-who-to-kill",
    "title": "How could self-driving cars make ethical decisions about who to kill?",
    "body": "<p>Obviously, self-driving cars aren't perfect, so imagine that the Google car (as an example) got into a difficult situation.</p>\n<p>Here are a few examples of unfortunate situations caused by a set of events:</p>\n<ul>\n<li>The car is heading toward a crowd of 10 people crossing the road, so it cannot stop in time, but it can avoid killing 10 people by hitting the wall (killing the passengers),</li>\n<li>Avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car,</li>\n<li>Killing an animal on the street in favour of a human being,</li>\n<li>Purposely changing lanes to crash into another car to avoid killing a dog,</li>\n</ul>\n<p>And here are a few dilemmas:</p>\n<ul>\n<li>Does the algorithm recognize the difference between a human being and an animal?</li>\n<li>Does the size of the human being or animal matter?</li>\n<li>Does it count how many passengers it has vs. people in the front?</li>\n<li>Does it &quot;know&quot; when babies/children are on board?</li>\n<li>Does it take into the account the age (e.g. killing the older first)?</li>\n</ul>\n<p>How would an algorithm decide what it should do from the technical perspective? Is it being aware of above (counting the probability of kills), or not (killing people just to avoid its own destruction)? </p>\n<p>Related articles:</p>\n<ul>\n<li><a href=\"https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/\" rel=\"noreferrer\">Why Self-Driving Cars Must Be Programmed to Kill</a></li>\n<li><a href=\"https://www.technologyreview.com/s/539731/how-to-help-self-driving-cars-make-ethical-decisions/\" rel=\"noreferrer\">How to Help Self-Driving Cars Make Ethical Decisions</a></li>\n</ul>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "deep-learning",
      "convolutional-neural-networks",
      "natural-language-processing"
    ],
    "owner": {
      "account_id": 910144,
      "reputation": 1171,
      "user_id": 2522,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/mq30u.jpg?s=256",
      "display_name": "Asciiom",
      "link": "https://ai.stackexchange.com/users/2522/asciiom"
    },
    "is_answered": true,
    "view_count": 93991,
    "accepted_answer_id": 2009,
    "answer_count": 4,
    "score": 102,
    "last_activity_date": 1692567248,
    "creation_date": 1474626838,
    "last_edit_date": 1563046127,
    "question_id": 2008,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2008/how-can-neural-networks-deal-with-varying-input-sizes",
    "title": "How can neural networks deal with varying input sizes?",
    "body": "<p>As far as I can tell, neural networks have a <strong>fixed number of neurons</strong> in the input layer.</p>\n\n<p>If neural networks are used in a context like NLP, sentences or blocks of text of varying sizes are fed to a network. \nHow is the <strong>varying input size</strong> reconciled with the <strong>fixed size</strong> of the input layer of the network? In other words, how is such a network made flexible enough to deal with an input that might be anywhere from one word to multiple pages of text?</p>\n\n<p>If my assumption of a fixed number of input neurons is wrong and new input neurons are added to/removed from the network to match the input size I don't see how these can ever be trained.</p>\n\n<p>I give the example of NLP, but lots of problems have an inherently unpredictable input size. I'm interested in the general approach for dealing with this.</p>\n\n<p>For images, it's clear you can up/downsample to a fixed size, but, for text, this seems to be an impossible approach since adding/removing text changes the meaning of the original input.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "convolutional-neural-networks",
      "explainable-ai"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 20085,
    "protected_date": 1561335965,
    "accepted_answer_id": 4044,
    "answer_count": 7,
    "score": 100,
    "last_activity_date": 1637348077,
    "creation_date": 1470708536,
    "last_edit_date": 1637348077,
    "question_id": 1479,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1479/do-scientists-know-what-is-happening-inside-artificial-neural-networks",
    "title": "Do scientists know what is happening inside artificial neural networks?",
    "body": "<p>Do scientists or research experts know from the kitchen what is happening inside complex &quot;deep&quot; neural network with at least millions of connections firing at an instant? Do they understand the process behind this (e.g. what is happening inside and how it works exactly), or it is a subject of debate?</p>\n<p>For example this <a href=\"https://www.cs.nyu.edu/%7Efergus/papers/zeilerECCV2014.pdf\" rel=\"noreferrer\">study</a> says:</p>\n<blockquote>\n<p>However there is no clear understanding of <em>why</em> they perform so well, or <em>how</em> they might be improved.</p>\n</blockquote>\n<p>So does this mean that scientists actually don't know how complex convolutional network models work?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "comparison",
      "supervised-learning",
      "self-supervised-learning",
      "representation-learning"
    ],
    "owner": {
      "account_id": 4148701,
      "reputation": 42397,
      "user_id": 2444,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e6fce047cba96a601a9dac66df4cd2e8?s=256&d=identicon&r=PG",
      "display_name": "nbro",
      "link": "https://ai.stackexchange.com/users/2444/nbro"
    },
    "is_answered": true,
    "view_count": 86812,
    "protected_date": 1567022225,
    "accepted_answer_id": 10624,
    "answer_count": 3,
    "score": 99,
    "last_activity_date": 1605840404,
    "creation_date": 1550347378,
    "last_edit_date": 1605840404,
    "question_id": 10623,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10623/what-is-self-supervised-learning-in-machine-learning",
    "title": "What is self-supervised learning in machine learning?",
    "body": "<p>What is self-supervised learning in machine learning? How is it different from supervised learning?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "comparison",
      "model-based-methods",
      "model-free-methods"
    ],
    "owner": {
      "account_id": 4836670,
      "reputation": 1021,
      "user_id": 10720,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/49682ea6c11b9a5c1ff679ba93367ce8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "mynameisvinn",
      "link": "https://ai.stackexchange.com/users/10720/mynameisvinn"
    },
    "is_answered": true,
    "view_count": 104012,
    "protected_date": 1559169504,
    "answer_count": 6,
    "score": 92,
    "last_activity_date": 1654946796,
    "creation_date": 1510063809,
    "last_edit_date": 1550343696,
    "question_id": 4456,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4456/whats-the-difference-between-model-free-and-model-based-reinforcement-learning",
    "title": "What&#39;s the difference between model-free and model-based reinforcement learning?",
    "body": "<p>What's the difference between model-free and model-based reinforcement learning? </p>\n\n<p>It seems to me that any model-free learner, learning through trial and error, could be reframed as model-based. In that case, when would model-free learners be appropriate?</p>\n"
  },
  {
    "tags": [
      "convolutional-neural-networks",
      "computer-vision",
      "image-recognition",
      "deep-neural-networks",
      "adversarial-ml"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 8043,
    "protected_date": 1583448593,
    "accepted_answer_id": 250,
    "answer_count": 9,
    "score": 88,
    "last_activity_date": 1613677705,
    "creation_date": 1470157527,
    "last_edit_date": 1613677705,
    "question_id": 92,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/92/how-is-it-possible-that-deep-neural-networks-are-so-easily-fooled",
    "title": "How is it possible that deep neural networks are so easily fooled?",
    "body": "<p>The following <a href=\"http://www.evolvingai.org/fooling\" rel=\"noreferrer\">page</a>/<a href=\"http://www.evolvingai.org/files/DNNsEasilyFooled_cvpr15.pdf\" rel=\"noreferrer\">study</a> demonstrates that the deep neural networks are easily fooled by giving high confidence predictions for unrecognisable images, e.g.</p>\n\n<p><a href=\"https://i.sstatic.net/7pgrH.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/7pgrH.jpg\" alt=\"Evolved images that are unrecognisable to humans, but that state-of-the-art DNNs trained on ImageNet believe with &gt;= 99.6% certainty to be a familiar object. This result highlights differences between how DNNs and humans recognise objects. Directly and indirectly encoded images\"></a></p>\n\n<p><a href=\"https://i.sstatic.net/pBm48.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/pBm48.png\" alt=\"Evolving images to match DNN classes produces a tremendous diversity of images. The mean DNN confidence scores for these images is 99.12% for the listed class, meaning that the DNN believes with near-certainty that the image is that type of thing. Shown are images selected to showcase diversity from 5 independent evolutionary runs. The images shed light on what the DNN network cares about, and what it does not, when classifying an image. For example, a school bus is alternating yellow and black lines, but does not need to have a windshield or wheels\"></a></p>\n\n<p>How this is possible? Can you please explain ideally in plain English?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "natural-language-processing",
      "recurrent-neural-networks",
      "long-short-term-memory",
      "transformer"
    ],
    "owner": {
      "account_id": 3779609,
      "reputation": 1833,
      "user_id": 9863,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/08ea907d3b57005305612f88c0a85b03?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "DRV",
      "link": "https://ai.stackexchange.com/users/9863/drv"
    },
    "is_answered": true,
    "view_count": 116732,
    "accepted_answer_id": 20084,
    "answer_count": 4,
    "score": 82,
    "last_activity_date": 1724166337,
    "creation_date": 1586261131,
    "last_edit_date": 1586275738,
    "question_id": 20075,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/20075/why-does-the-transformer-do-better-than-rnn-and-lstm-in-long-range-context-depen",
    "title": "Why does the transformer do better than RNN and LSTM in long-range context dependencies?",
    "body": "<p>I am reading the article <a href=\"https://towardsdatascience.com/transformers-141e32e69591\" rel=\"noreferrer\">How Transformers Work</a> where the author writes</p>\n\n<blockquote>\n  <p>Another problem with RNNs, and LSTMs, is that it’s hard to parallelize the work for processing sentences, since you have to process word by word. Not only that but there is no model of <strong>long and short-range dependencies</strong>. </p>\n</blockquote>\n\n<p>Why exactly does the transformer do better than RNN and LSTM in <strong>long-range context dependencies</strong>?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "social",
      "explainable-ai"
    ],
    "owner": {
      "account_id": 2304865,
      "reputation": 2859,
      "user_id": 16565,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/e4iCE.jpg?s=256",
      "display_name": "malioboro",
      "link": "https://ai.stackexchange.com/users/16565/malioboro"
    },
    "is_answered": true,
    "view_count": 11164,
    "protected_date": 1567358423,
    "accepted_answer_id": 14247,
    "answer_count": 9,
    "score": 71,
    "last_activity_date": 1642116312,
    "creation_date": 1567254519,
    "last_edit_date": 1597834342,
    "question_id": 14224,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/14224/why-do-we-need-explainable-ai",
    "title": "Why do we need explainable AI?",
    "body": "<p>If the original purpose for developing AI was to help humans in some tasks and that purpose still holds, why should we care about its explainability? For example, in deep learning, as long as the intelligence helps us to the best of their abilities and carefully arrives at its decisions, why would we need to know how its intelligence works?</p>\n"
  },
  {
    "tags": [
      "reference-request",
      "recurrent-neural-networks",
      "long-short-term-memory",
      "hyperparameter-optimization",
      "hyper-parameters"
    ],
    "owner": {
      "account_id": 10465107,
      "reputation": 1029,
      "user_id": 6645,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/87326d687b83789020f7e73a17621555?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Stephen Johnson",
      "link": "https://ai.stackexchange.com/users/6645/stephen-johnson"
    },
    "is_answered": true,
    "view_count": 126765,
    "protected_date": 1583595507,
    "answer_count": 4,
    "score": 69,
    "last_activity_date": 1651744805,
    "creation_date": 1492176903,
    "last_edit_date": 1651744805,
    "question_id": 3156,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm",
    "title": "How to select number of hidden layers and number of memory cells in an LSTM?",
    "body": "<p>I am trying to find some existing research on how to select the number of hidden layers and the size of these of an LSTM-based RNN.</p>\n<p>Is there an article where this problem is being investigated, i.e., how many memory cells should one use? I assume it totaly depends on the application and in which context the model is being used, but what does the research say?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "tensorflow",
      "python",
      "programming-languages"
    ],
    "owner": {
      "account_id": 7831764,
      "reputation": 845,
      "user_id": 7268,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/xZr6F.jpg?s=256",
      "display_name": "Douglas Ferreira",
      "link": "https://ai.stackexchange.com/users/7268/douglas-ferreira"
    },
    "is_answered": true,
    "view_count": 47435,
    "protected_date": 1588675187,
    "accepted_answer_id": 3503,
    "answer_count": 10,
    "score": 68,
    "last_activity_date": 1639753037,
    "creation_date": 1497518028,
    "last_edit_date": 1639753037,
    "question_id": 3494,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3494/why-is-python-such-a-popular-language-in-the-ai-field",
    "title": "Why is Python such a popular language in the AI field?",
    "body": "<p>First of all, I'm a beginner studying AI and this is not an opinion-oriented question or one to compare programming languages. I'm not implying that Python is the best language. But the fact is that most of the famous AI frameworks have primary support for Python. They can even be multilanguage supported, for example, TensorFlow that supports Python, C++, or CNTK from Microsoft that supports C# and C++, but the most used is Python (I mean more documentation, examples, bigger community, support, etc). Even if you choose C# (developed by Microsoft and my primary programming language), you must have the Python environment set up.</p>\n<p>I read in other forums that Python is preferred for AI because the code is simplified and cleaner, good for fast prototyping.</p>\n<p>I was watching a movie with AI thematics (Ex Machina). In some scenes, the main character hacks the interface of the house automation. Guess which language was on the scene? Python.</p>\n<p>So, what is the big deal with Python? Why is there a growing association between Python and AI?</p>\n"
  },
  {
    "tags": [
      "convolutional-neural-networks",
      "weights",
      "filters"
    ],
    "owner": {
      "account_id": 5705590,
      "reputation": 803,
      "user_id": 14389,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2973e21d652ad1171bba48f95720bf83?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Ryan Chase",
      "link": "https://ai.stackexchange.com/users/14389/ryan-chase"
    },
    "is_answered": true,
    "view_count": 61997,
    "answer_count": 13,
    "score": 66,
    "last_activity_date": 1740484809,
    "creation_date": 1521686180,
    "last_edit_date": 1639867463,
    "question_id": 5769,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5769/in-a-cnn-does-each-new-filter-have-different-weights-for-each-input-channel-or",
    "title": "In a CNN, does each new filter have different weights for each input channel, or are the same weights of each filter used across input channels?",
    "body": "<p>My understanding is that the convolutional layer of a convolutional neural network has four dimensions: <code>input_channels, filter_height, filter_width, number_of_filters</code>.  Furthermore, it is my understanding that each new filter just gets convoluted over ALL of the <code>input_channels</code> (or feature/activation maps from the previous layer).</p>\n<p>HOWEVER, the graphic below from CS231 shows each filter (in red) being applied to a SINGLE CHANNEL, rather than the same filter being used across channels.  This seems to indicate that there is a separate filter for EACH channel (in this case I'm assuming they're the three color channels of an input image, but the same would apply for all input channels).</p>\n<p>This is confusing - is there a different unique filter for each input channel?</p>\n<p><a href=\"https://i.sstatic.net/3m7mW.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/3m7mW.png\" alt=\"Convolutional filters diagram\" /></a></p>\n<p>This is the <a href=\"http://cs231n.github.io/convolutional-networks/\" rel=\"noreferrer\">source</a>.</p>\n<p>The above image seems contradictory to an excerpt from O'reilly's <a href=\"https://rads.stackoverflow.com/amzn/click/com/1491925612\" rel=\"noreferrer\" rel=\"nofollow noreferrer\">&quot;Fundamentals of Deep Learning&quot;</a>:</p>\n<blockquote>\n<p>...filters don't just operate on a single feature map. They operate on the entire volume of feature maps that have been generated at a particular layer...As a result, feature maps must be able to operate over volumes, not just areas</p>\n</blockquote>\n<p>...Also, it is my understanding that these images below are indicating a <strong>THE SAME</strong> filter is just convolved over all three input channels (contradictory to what's shown in the CS231 graphic above):</p>\n<p><a href=\"https://i.sstatic.net/VdqER.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/VdqER.png\" alt=\"Application of a volumetric convolutional filter to an RGB image\" /></a></p>\n<p><a href=\"https://i.sstatic.net/kczF0.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/kczF0.png\" alt=\"Convolutions on an RGB image\" /></a></p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "incremental-learning",
      "catastrophic-forgetting"
    ],
    "owner": {
      "account_id": 6476695,
      "reputation": 2258,
      "user_id": 4199,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-o_4nx3c8sHQ/AAAAAAAAAAI/AAAAAAAAAPU/lVuNexWGyPU/s256-rj/photo.jpg",
      "display_name": "zooby",
      "link": "https://ai.stackexchange.com/users/4199/zooby"
    },
    "is_answered": true,
    "view_count": 17424,
    "protected_date": 1562931352,
    "accepted_answer_id": 13293,
    "answer_count": 4,
    "score": 66,
    "last_activity_date": 1737209449,
    "creation_date": 1562729384,
    "last_edit_date": 1639312096,
    "question_id": 13289,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/13289/are-neural-networks-prone-to-catastrophic-forgetting",
    "title": "Are neural networks prone to catastrophic forgetting?",
    "body": "<p>Imagine you show a neural network a picture of a lion 100 times and label it with &quot;dangerous&quot;, so it learns that lions are dangerous.</p>\n<p>Now imagine that previously you have shown it millions of images of lions and alternatively labeled it as &quot;dangerous&quot; and &quot;not dangerous&quot;, such that the probability of a lion being dangerous is 50%.</p>\n<p>But those last 100 times have pushed the neural network into being very positive about regarding the lion as &quot;dangerous&quot;, thus ignoring the last million lessons.</p>\n<p>Therefore, it seems there is a flaw in neural networks, in that they can change their mind too quickly based on recent evidence. Especially if that previous evidence was in the middle.</p>\n<p>Is there a neural network model that keeps track of how much evidence it has seen? (Or would this be equivalent to letting the learning rate decrease by <span class=\"math-container\">$1/T$</span> where <span class=\"math-container\">$T$</span> is the number of trials?)</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "reference-request",
      "applications"
    ],
    "owner": {
      "account_id": 1963605,
      "reputation": 634,
      "user_id": 31649,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/DQwOe.jpg?s=256",
      "display_name": "AB_IM",
      "link": "https://ai.stackexchange.com/users/31649/ab-im"
    },
    "is_answered": true,
    "view_count": 12604,
    "accepted_answer_id": 18577,
    "answer_count": 11,
    "score": 60,
    "last_activity_date": 1611326307,
    "creation_date": 1583947652,
    "last_edit_date": 1611326307,
    "question_id": 18576,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/18576/what-are-some-well-known-problems-where-neural-networks-dont-do-very-well",
    "title": "What are some well-known problems where neural networks don&#39;t do very well?",
    "body": "<p><strong>Background:</strong> It's well-known that neural networks offer great performance across a large number of tasks, and this is largely a consequence of <a href=\"https://ai.stackexchange.com/questions/13317/where-can-i-find-the-proof-of-the-universal-approximation-theorem/25917#25917\">their universal approximation capabilities</a>.  However, in this post I'm curious about the <em>opposite</em>:</p>\n<p><strong>Question:</strong> Namely, what are some <em>well-known</em> cases, problems or real-world applications where neural networks don't do very well?</p>\n<hr />\n<p><strong>Specification:</strong>\nI'm looking for specific <em>regression</em> tasks (with accessible data-sets) where neural networks are not the state-of-the-art.  The regression task should be &quot;naturally suitable&quot;, so no sequential or time-dependent data (in which case an RNN or reservoir computer would be more natural).</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "social",
      "neo-luddism"
    ],
    "owner": {
      "account_id": 6823977,
      "reputation": 691,
      "user_id": 29713,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/da1c4f581b03210fa20579bc66d1dd1d?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "M14",
      "link": "https://ai.stackexchange.com/users/29713/m14"
    },
    "is_answered": true,
    "view_count": 11467,
    "protected_date": 1575069284,
    "accepted_answer_id": 15462,
    "answer_count": 13,
    "score": 58,
    "last_activity_date": 1568912114,
    "creation_date": 1568614249,
    "last_edit_date": 1568668755,
    "question_id": 15449,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/15449/how-could-artificial-intelligence-harm-us",
    "title": "How could artificial intelligence harm us?",
    "body": "<p>We often hear that artificial intelligence may harm or even kill humans, so it might prove dangerous.</p>\n\n<p>How could artificial intelligence harm us?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "chat-bots",
      "natural-language-understanding",
      "chatgpt"
    ],
    "owner": {
      "account_id": 6617635,
      "reputation": 1041,
      "user_id": 41448,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/RSniT.jpg?s=256",
      "display_name": "milez",
      "link": "https://ai.stackexchange.com/users/41448/milez"
    },
    "is_answered": true,
    "view_count": 51823,
    "protected_date": 1670761037,
    "accepted_answer_id": 38262,
    "answer_count": 2,
    "score": 58,
    "last_activity_date": 1701545349,
    "creation_date": 1670152984,
    "last_edit_date": 1671218453,
    "question_id": 38150,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/38150/how-does-chatgpt-retain-the-context-of-previous-questions",
    "title": "How does ChatGPT retain the context of previous questions?",
    "body": "<p>One of the innovations with OpenAI's ChatGPT is how natural it is for users to interact with it.</p>\n<p>What is the technical enabler for ChatGPT to maintain the context of previous questions in its answers? For example, ChatGPT understands a prompt of &quot;tell me more&quot; and expands on it's previous answer.</p>\n<p>Does it use activations from previous questions? Is there a separate input for the context? How does it work?</p>\n"
  },
  {
    "tags": [
      "deep-neural-networks",
      "terminology",
      "fuzzy-logic"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 2942,
    "protected_date": 1587462620,
    "accepted_answer_id": 32,
    "answer_count": 6,
    "score": 51,
    "last_activity_date": 1645665923,
    "creation_date": 1470152876,
    "last_edit_date": 1539859473,
    "question_id": 10,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10/what-is-fuzzy-logic",
    "title": "What is fuzzy logic?",
    "body": "<p>I'm new to A.I. and I'd like to know in simple words, what is the fuzzy logic concept? How does it help, and when is it used?</p>\n"
  },
  {
    "tags": [
      "chat-bots",
      "chatgpt",
      "language-model"
    ],
    "owner": {
      "account_id": 14132826,
      "reputation": 611,
      "user_id": 66457,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e50f21ae07d70eb20af391eec87d286e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Nicolas Zein",
      "link": "https://ai.stackexchange.com/users/66457/nicolas-zein"
    },
    "is_answered": true,
    "view_count": 24243,
    "answer_count": 3,
    "score": 51,
    "last_activity_date": 1688964445,
    "creation_date": 1673177258,
    "last_edit_date": 1688959507,
    "question_id": 38660,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/38660/was-chatgpt-trained-on-stack-overflow-data",
    "title": "Was ChatGPT trained on Stack Overflow data?",
    "body": "<p>Has ChatGPT used highly rated and upvoted questions/answers from Stack Overflow in its training data?</p>\n<p>For me it makes complete sense to take answers that have upwards of 100 upvotes and include them in your training data, but people around me seem to think this hypothesis doesn't make sense. Is there a way to confirm this?</p>\n"
  },
  {
    "tags": [
      "terminology",
      "definitions",
      "agi",
      "comparison",
      "narrow-ai"
    ],
    "owner": {
      "account_id": 40019,
      "reputation": 2533,
      "user_id": 55,
      "user_type": "registered",
      "accept_rate": 43,
      "profile_image": "https://i.sstatic.net/FkIiK.png?s=256",
      "display_name": "WilliamKF",
      "link": "https://ai.stackexchange.com/users/55/williamkf"
    },
    "is_answered": true,
    "view_count": 24082,
    "protected_date": 1496833279,
    "accepted_answer_id": 141,
    "answer_count": 3,
    "score": 50,
    "last_activity_date": 1561062724,
    "creation_date": 1470156155,
    "last_edit_date": 1561062724,
    "question_id": 74,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/74/what-is-the-difference-between-strong-ai-and-weak-ai",
    "title": "What is the difference between strong-AI and weak-AI?",
    "body": "<p>I've heard the terms strong-AI and weak-AI used.  Are these well defined terms or subjective ones?  How are they generally defined?</p>\n"
  },
  {
    "tags": [
      "quantum-computing"
    ],
    "owner": {
      "account_id": 6241661,
      "reputation": 1521,
      "user_id": 29,
      "user_type": "registered",
      "accept_rate": 78,
      "profile_image": "https://i.sstatic.net/JfeF9.png?s=256",
      "display_name": "wythagoras",
      "link": "https://ai.stackexchange.com/users/29/wythagoras"
    },
    "is_answered": true,
    "view_count": 2184,
    "protected_date": 1562712358,
    "accepted_answer_id": 114,
    "answer_count": 5,
    "score": 49,
    "last_activity_date": 1640367924,
    "creation_date": 1470153985,
    "last_edit_date": 1470165229,
    "question_id": 36,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/36/to-what-extent-can-quantum-computers-help-to-develop-artificial-intelligence",
    "title": "To what extent can quantum computers help to develop Artificial Intelligence?",
    "body": "<p>What aspects of quantum computers, if any, can help to further develop Artificial Intelligence?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "q-learning",
      "policy-gradients",
      "comparison"
    ],
    "owner": {
      "account_id": 4238770,
      "reputation": 605,
      "user_id": 15298,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0b14195f08bb1855a205bf106053ff7b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Tejas Ramdas",
      "link": "https://ai.stackexchange.com/users/15298/tejas-ramdas"
    },
    "is_answered": true,
    "view_count": 26998,
    "accepted_answer_id": 6199,
    "answer_count": 2,
    "score": 48,
    "last_activity_date": 1587557154,
    "creation_date": 1524885076,
    "last_edit_date": 1550245146,
    "question_id": 6196,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6196/what-is-the-relation-between-q-learning-and-policy-gradients-methods",
    "title": "What is the relation between Q-learning and policy gradients methods?",
    "body": "<p>As far as I understand, Q-learning and policy gradients (PG) are the two major approaches used to solve RL problems. While Q-learning aims to predict the reward of a certain action taken in a certain state, policy gradients directly predict the action itself.</p>\n\n<p>However, both approaches appear identical to me, i.e. predicting the maximum reward for an action (Q-learning) is equivalent to predicting the probability of taking the action directly (PG). Is the difference in the way the loss is back-propagated?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "philosophy",
      "agi"
    ],
    "owner": {
      "account_id": 945168,
      "reputation": 688,
      "user_id": 19102,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/48xBa.png?s=256",
      "display_name": "verdery",
      "link": "https://ai.stackexchange.com/users/19102/verdery"
    },
    "is_answered": true,
    "view_count": 16513,
    "protected_date": 1571275667,
    "accepted_answer_id": 15744,
    "answer_count": 19,
    "score": 48,
    "last_activity_date": 1617289215,
    "creation_date": 1570234718,
    "last_edit_date": 1570235690,
    "question_id": 15730,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/15730/can-digital-computers-understand-infinity",
    "title": "Can digital computers understand infinity?",
    "body": "<p>As a human being, we can think infinity. In principle, if we have enough resources (time etc.), we can count infinitely many things (including abstract, like numbers, or real).</p>\n\n<p>For example, at least, we can take into account integers. We can think, principally, and \"understand\" infinitely many numbers that are displayed on the screen. Nowadays, we are trying to design artificial intelligence which is capable at least human being. However, I am stuck with infinity. I try to find a way how can teach a model (deep or not) to understand infinity. I define \"understanding' in a functional approach. For example, If a computer can differentiate 10 different numbers or things, it means that it really understand these different things somehow. This is the basic straight forward approach to \"understanding\".</p>\n\n<p>As I mentioned before, humans understand infinity because they are capable, at least, counting infinite integers, in principle. From this point of view, if I want to create a model, the model is actually a function in an abstract sense, this model must differentiate infinitely many numbers. Since computers are digital machines which have limited capacity to model such an infinite function, how can I create a model that differentiates infinitely many integers?</p>\n\n<p>For example, we can take a deep learning vision model that recognizes numbers on the card. This model must assign a number to each different card to differentiate each integer. Since there exist infinite numbers of integer, how can the model assign different number to each integer, like a human being, on the digital computers? If it cannot differentiate infinite things, how does it understand infinity?</p>\n\n<p>If I take into account real numbers, the problem becomes much harder.</p>\n\n<p>What is the point that I am missing? Are there any resources that focus on the subject?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "backpropagation",
      "time-complexity"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user9947"
    },
    "is_answered": true,
    "view_count": 62985,
    "accepted_answer_id": 5730,
    "answer_count": 4,
    "score": 44,
    "last_activity_date": 1635969969,
    "creation_date": 1521372415,
    "last_edit_date": 1550766419,
    "question_id": 5728,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5728/what-is-the-time-complexity-for-training-a-neural-network-using-back-propagation",
    "title": "What is the time complexity for training a neural network using back-propagation?",
    "body": "<p>Suppose that a NN contains <span class=\"math-container\">$n$</span> hidden layers, <span class=\"math-container\">$m$</span> training examples, <span class=\"math-container\">$x$</span> features, and <span class=\"math-container\">$n_i$</span> nodes in each layer. What is the time complexity to train this NN using back-propagation? </p>\n\n<p>I have a basic idea about how they find the time complexity of algorithms, but here there are 4 different factors to consider here i.e. iterations, layers, nodes in each layer, training examples, and maybe more factors. I found an answer <a href=\"https://www.researchgate.net/post/What_is_the_time_complexity_of_Multilayer_Perceptron_MLP_and_other_neural_networks\" rel=\"noreferrer\">here</a> but it was not clear enough.</p>\n\n<p>Are there other factors, apart from those I mentioned above, that influence the time complexity of the training algorithm of a NN?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "capsule-neural-network"
    ],
    "owner": {
      "account_id": 443353,
      "reputation": 2148,
      "user_id": 144,
      "user_type": "registered",
      "accept_rate": 30,
      "profile_image": "https://i.sstatic.net/ZAzOl.jpg?s=256",
      "display_name": "rcpinto",
      "link": "https://ai.stackexchange.com/users/144/rcpinto"
    },
    "is_answered": true,
    "view_count": 21527,
    "accepted_answer_id": 1313,
    "answer_count": 6,
    "score": 43,
    "last_activity_date": 1591716350,
    "creation_date": 1470299286,
    "last_edit_date": 1591702053,
    "question_id": 1294,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1294/how-do-capsule-neural-networks-work",
    "title": "How do capsule neural networks work?",
    "body": "<p>Geoffrey Hinton has been researching something he calls \"capsules theory\" in neural networks. What is it? How do capsule neural networks work?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "policy-gradients",
      "actor-critic-methods",
      "reinforce",
      "reward-design"
    ],
    "owner": {
      "account_id": 5780751,
      "reputation": 712,
      "user_id": 6019,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/1031340060210684/picture?type=large",
      "display_name": "Moln&#225;r Istv&#225;n",
      "link": "https://ai.stackexchange.com/users/6019/moln%c3%a1r-istv%c3%a1n"
    },
    "is_answered": true,
    "view_count": 17301,
    "protected_date": 1599256286,
    "accepted_answer_id": 2981,
    "answer_count": 5,
    "score": 43,
    "last_activity_date": 1641801178,
    "creation_date": 1489501567,
    "last_edit_date": 1641801144,
    "question_id": 2980,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2980/how-should-i-handle-invalid-actions-when-using-reinforce",
    "title": "How should I handle invalid actions (when using REINFORCE)?",
    "body": "<p>I want to create an AI which can play five-in-a-row/Gomoku. I want to use reinforcement learning for this.</p>\n<p>I use the <em>policy gradient</em> method, namely REINFORCE, with baseline. For the value and policy function approximation, I use a <em>neural network</em>. It has convolutional and fully connected layers. All of the layers, except for the output, are shared. The policy's output layer has <span class=\"math-container\">$8 \\times 8=64$</span> (the size of the board) output unit and <em>softmax</em> on them. So it is stochastic.</p>\n<p>But what if the network produces a very high probability for an invalid action? An invalid action is when the agent wants to check a square that has one <code>X</code> or <code>O</code> in it. I think it can be stuck in that game state.</p>\n<p>How should I handle this situation?</p>\n<p>My guess is to use the <em>actor-critic</em> method. For an invalid action, we should give a negative reward and pass the turn to the opponent.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "prediction",
      "primality-test"
    ],
    "owner": {
      "account_id": 2597332,
      "reputation": 543,
      "user_id": 7458,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4f49014a5e14365cde7d5da764a0ab7e?s=256&d=identicon&r=PG",
      "display_name": "Fullk33",
      "link": "https://ai.stackexchange.com/users/7458/fullk33"
    },
    "is_answered": true,
    "view_count": 24950,
    "accepted_answer_id": 7598,
    "answer_count": 5,
    "score": 43,
    "last_activity_date": 1664438134,
    "creation_date": 1495811731,
    "last_edit_date": 1605358460,
    "question_id": 3389,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3389/could-a-neural-network-detect-primes",
    "title": "Could a neural network detect primes?",
    "body": "<p>I am not looking for an efficient way to find primes (which of course is a <a href=\"https://en.wikipedia.org/wiki/AKS_primality_test\" rel=\"noreferrer\">solved problem</a>). This is more of a &quot;what if&quot; question.</p>\n<p>So, in theory, could you train a neural network to predict whether or not a given number <span class=\"math-container\">$n$</span> is composite or prime? How would such a network be laid out?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "convolutional-neural-networks",
      "comparison",
      "terminology",
      "definitions"
    ],
    "owner": {
      "account_id": 5129611,
      "reputation": 2945,
      "user_id": 145,
      "user_type": "moderator",
      "accept_rate": 62,
      "profile_image": "https://i.sstatic.net/IIYyh.png?s=256",
      "display_name": "Mithical",
      "link": "https://ai.stackexchange.com/users/145/mithical"
    },
    "is_answered": true,
    "view_count": 83017,
    "protected_date": 1715597011,
    "answer_count": 5,
    "score": 43,
    "last_activity_date": 1715596146,
    "creation_date": 1520370115,
    "last_edit_date": 1584119973,
    "question_id": 5546,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5546/what-is-the-difference-between-a-convolutional-neural-network-and-a-regular-neur",
    "title": "What is the difference between a convolutional neural network and a regular neural network?",
    "body": "<p>I've seen these terms thrown around this site a lot, specifically in the tags <a href=\"/questions/tagged/convolutional-neural-networks\" class=\"post-tag\" title=\"show questions tagged &#39;convolutional-neural-networks&#39;\" rel=\"tag\">convolutional-neural-networks</a> and <a href=\"/questions/tagged/neural-networks\" class=\"post-tag\" title=\"show questions tagged &#39;neural-networks&#39;\" rel=\"tag\">neural-networks</a>.</p>\n\n<p>I know that a neural network is a system based loosely on the human brain. But what's the difference between a <em>convolutional</em> neural network and a regular neural network? Is one just a lot more complicated and, ahem, <em>convoluted</em> than the other? </p>\n"
  },
  {
    "tags": [
      "chatgpt",
      "gpt-4",
      "prompt",
      "prompt-design"
    ],
    "owner": {
      "account_id": 4153260,
      "reputation": 787,
      "user_id": 2317,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/3492e6631a5b35e255a348d92e47b2c7?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Volker Siegel",
      "link": "https://ai.stackexchange.com/users/2317/volker-siegel"
    },
    "is_answered": true,
    "view_count": 54057,
    "answer_count": 4,
    "score": 43,
    "last_activity_date": 1730806763,
    "creation_date": 1680120603,
    "last_edit_date": 1688115640,
    "question_id": 39837,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/39837/meaning-of-roles-in-the-api-of-gpt-4-chatgpt-system-user-assistant",
    "title": "Meaning of roles in the API of GPT-4/ChatGPT (system/user/assistant)",
    "body": "<p>In the API of GPT-4 and ChatGPT, the prompt for a chat conversation is a list of messages, each marked as one of three roles: <code>system</code>, <code>user</code> or <code>assistant</code>.*</p>\n<p>I understand which information this represents - but what does the model with that information?</p>\n<p>Is there a difference between a chain of messages with user and assistant alternating, compared to the same messages, with all the same role of either user or assistant?</p>\n<p>To make a difference, it seems the role would need to be encoded into the prompt of the language model. Otherwise, it would be just the concatenation of the previous messages.</p>\n<p><strong>So, what is the effect of the roles?</strong></p>\n<hr />\n<ul>\n<li>There is also the role <code>function</code> since recently, but that has a specific well defined purpose, so it is not relevant here.</li>\n</ul>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "terminology",
      "activation-functions"
    ],
    "owner": {
      "account_id": 2476500,
      "reputation": 982,
      "user_id": 12957,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-A95kTyPOdM4/AAAAAAAAAAI/AAAAAAAAAFY/PNTfMFaLEXY/s256-rj/photo.jpg",
      "display_name": "Mohsin",
      "link": "https://ai.stackexchange.com/users/12957/mohsin"
    },
    "is_answered": true,
    "view_count": 28111,
    "accepted_answer_id": 5521,
    "answer_count": 5,
    "score": 41,
    "last_activity_date": 1617289127,
    "creation_date": 1520026044,
    "last_edit_date": 1571881852,
    "question_id": 5493,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5493/what-is-the-purpose-of-an-activation-function-in-neural-networks",
    "title": "What is the purpose of an activation function in neural networks?",
    "body": "<p>It is said that activation functions in neural networks help introduce <strong><em>non-linearity</em></strong>.</p>\n\n<ul>\n<li>What does this mean?</li>\n<li>What does <strong><em>non-linearity</em></strong> mean in this context?</li>\n<li>How does the introduction of this <strong><em>non-linearity</em></strong> help?</li>\n<li>Are there any other purposes of <strong><em>activation functions</em></strong>?</li>\n</ul>\n"
  },
  {
    "tags": [
      "chatgpt"
    ],
    "owner": {
      "account_id": 1134207,
      "reputation": 384,
      "user_id": 9092,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/401b42cd42a7e2c89d872509e2614efd?s=256&d=identicon&r=PG",
      "display_name": "Peter Franek",
      "link": "https://ai.stackexchange.com/users/9092/peter-franek"
    },
    "is_answered": true,
    "view_count": 31668,
    "accepted_answer_id": 39687,
    "answer_count": 2,
    "score": 41,
    "last_activity_date": 1699625523,
    "creation_date": 1679308768,
    "last_edit_date": 1684877264,
    "question_id": 39686,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/39686/how-is-chatgpt-aware-of-todays-date",
    "title": "How is ChatGPT aware of today&#39;s date?",
    "body": "<p>I asked ChatGPT (3.5 and 4) about current date and (s)he answered correctly. In subsequent conversation (s)he was not able to explain how (s)he has this knowledge.</p>\n<p>I always thought that the model only sees the conversation above and a pretrained NN is used. How is the information about current date injected into his/her knowledge?</p>\n"
  },
  {
    "tags": [
      "turing-test",
      "agi",
      "intelligent-agent",
      "narrow-ai"
    ],
    "owner": {
      "account_id": 7263447,
      "reputation": 501,
      "user_id": 9,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/r4U5R.jpg?s=256",
      "display_name": "Rob Murray",
      "link": "https://ai.stackexchange.com/users/9/rob-murray"
    },
    "is_answered": true,
    "view_count": 4583,
    "protected_date": 1559169361,
    "answer_count": 6,
    "score": 40,
    "last_activity_date": 1523477795,
    "creation_date": 1470153170,
    "last_edit_date": 1470319810,
    "question_id": 15,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/15/is-the-turing-test-or-any-of-its-variants-a-reliable-test-of-artificial-intell",
    "title": "Is the Turing Test, or any of its variants, a reliable test of artificial intelligence?",
    "body": "<p>The <a href=\"https://en.wikipedia.org/wiki/Turing_test\">Turing Test</a> was the first test of artificial intelligence and is now a bit outdated. The <a href=\"https://en.wikipedia.org/wiki/Turing_test#Total_Turing_test\">Total Turing Test</a> aims to be a more modern test which requires a much more sophisticated system. What techniques can we use to identify an artificial intelligence (weak AI) and an <a href=\"https://en.wikipedia.org/wiki/Artificial_general_intelligence\">artificial general intelligence</a> (strong AI)?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "definitions",
      "agi",
      "superintelligence",
      "singularity"
    ],
    "owner": {
      "account_id": 40019,
      "reputation": 2533,
      "user_id": 55,
      "user_type": "registered",
      "accept_rate": 43,
      "profile_image": "https://i.sstatic.net/FkIiK.png?s=256",
      "display_name": "WilliamKF",
      "link": "https://ai.stackexchange.com/users/55/williamkf"
    },
    "is_answered": true,
    "view_count": 1929,
    "protected_date": 1583564494,
    "accepted_answer_id": 45,
    "answer_count": 4,
    "score": 40,
    "last_activity_date": 1605619279,
    "creation_date": 1470153218,
    "last_edit_date": 1568651019,
    "question_id": 17,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/17/what-is-the-concept-of-the-technological-singularity",
    "title": "What is the concept of the technological singularity?",
    "body": "<p>I've heard the idea of the technological singularity, what is it and how does it relate to Artificial Intelligence? Is this the theoretical point where Artificial Intelligence machines have progressed to the point where they grow and learn on their own beyond what humans can do and their growth takes off?  How would we know when we reach this point?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "deep-learning",
      "research",
      "programming-languages"
    ],
    "owner": {
      "account_id": 6352088,
      "reputation": 609,
      "user_id": 3323,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/mLqqcceD.jpg?s=256",
      "display_name": "Alecto",
      "link": "https://ai.stackexchange.com/users/3323/alecto"
    },
    "is_answered": true,
    "view_count": 34189,
    "protected_date": 1589400577,
    "accepted_answer_id": 2237,
    "answer_count": 3,
    "score": 40,
    "last_activity_date": 1674003793,
    "creation_date": 1477885087,
    "last_edit_date": 1639151202,
    "question_id": 2236,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2236/why-is-lisp-such-a-good-language-for-ai",
    "title": "Why is Lisp such a good language for AI?",
    "body": "<p>I've heard before from computer scientists and from researchers in the area of AI that that Lisp is a good language for research and development in artificial intelligence.</p>\n<ul>\n<li>Does this still apply, with the proliferation of neural networks and deep learning?</li>\n<li>What was their reasoning for this?</li>\n<li>What languages are current deep-learning systems currently built in?</li>\n</ul>\n"
  },
  {
    "tags": [
      "machine-learning",
      "terminology",
      "word-embedding",
      "latent-variable",
      "embeddings"
    ],
    "owner": {
      "account_id": 4148701,
      "reputation": 42397,
      "user_id": 2444,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e6fce047cba96a601a9dac66df4cd2e8?s=256&d=identicon&r=PG",
      "display_name": "nbro",
      "link": "https://ai.stackexchange.com/users/2444/nbro"
    },
    "is_answered": true,
    "view_count": 23708,
    "accepted_answer_id": 20646,
    "answer_count": 5,
    "score": 39,
    "last_activity_date": 1624026333,
    "creation_date": 1552823796,
    "last_edit_date": 1607260661,
    "question_id": 11285,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11285/what-is-the-difference-between-latent-and-embedding-spaces",
    "title": "What is the difference between latent and embedding spaces?",
    "body": "<p>In general, the word &quot;latent&quot; means &quot;hidden&quot; and &quot;to embed&quot; means &quot;to incorporate&quot;. In machine learning, the expressions &quot;hidden (or latent) space&quot; and &quot;embedding space&quot; occur in several contexts. More specifically, an <a href=\"https://en.wikipedia.org/wiki/Word_embedding\" rel=\"noreferrer\">embedding</a> can refer to a vector representation of a word. An <a href=\"https://en.wikipedia.org/wiki/Embedding\" rel=\"noreferrer\">embedding space</a> can refer to a subspace of a bigger space, so we say that the subspace is <em>embedded</em> in the bigger space. The word &quot;latent&quot; comes up in contexts like <a href=\"https://en.wikipedia.org/wiki/Hidden_Markov_model\" rel=\"noreferrer\">hidden Markov models (HMMs)</a> or <a href=\"https://en.wikipedia.org/wiki/Autoencoder\" rel=\"noreferrer\">auto-encoders</a>.</p>\n<p>What is the difference between these spaces? In some contexts, do these two expressions refer to the same concept?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "recurrent-neural-networks",
      "long-short-term-memory",
      "transformer"
    ],
    "owner": {
      "account_id": 5523531,
      "reputation": 3020,
      "user_id": 12201,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/08ea907d3b57005305612f88c0a85b03?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "chessprogrammer",
      "link": "https://ai.stackexchange.com/users/12201/chessprogrammer"
    },
    "is_answered": true,
    "view_count": 25670,
    "accepted_answer_id": 22960,
    "answer_count": 2,
    "score": 39,
    "last_activity_date": 1672856158,
    "creation_date": 1597032073,
    "last_edit_date": 1600350991,
    "question_id": 22957,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/22957/how-can-transformers-handle-arbitrary-length-input",
    "title": "How can Transformers handle arbitrary length input?",
    "body": "<p>The transformer, introduced in the paper <a href=\"https://arxiv.org/abs/1706.03762\" rel=\"noreferrer\">Attention Is All You Need</a>, is a popular new neural network architecture that is commonly viewed as an alternative to recurrent neural networks, like LSTMs and GRUs.</p>\n<p>However, having gone through the paper, as well as several online explanations, I still have trouble wrapping my head around how they work. How can a non-recurrent structure be able to deal with inputs of arbitrary length?</p>\n"
  },
  {
    "tags": [
      "agi",
      "philosophy",
      "natural-language-understanding",
      "chatgpt",
      "chinese-room-argument"
    ],
    "owner": {
      "account_id": 3681639,
      "reputation": 501,
      "user_id": 68600,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/VQ7Yr.jpg?s=256",
      "display_name": "Guntram Blohm",
      "link": "https://ai.stackexchange.com/users/68600/guntram-blohm"
    },
    "is_answered": true,
    "view_count": 17345,
    "protected_date": 1680872128,
    "accepted_answer_id": 39297,
    "answer_count": 7,
    "score": 39,
    "last_activity_date": 1680802490,
    "creation_date": 1677316571,
    "last_edit_date": 1677399816,
    "question_id": 39293,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/39293/is-the-chinese-room-an-explanation-of-how-chatgpt-works",
    "title": "Is the &quot;Chinese room&quot; an explanation of how ChatGPT works?",
    "body": "<p>Sorry if this question makes no sense. I'm a software developer but know very little about AI.</p>\n<p>Quite a while ago, I read about the Chinese room, and the person inside who has had a lot of training/instructions how to combine symbols, and, as a result, is very good at combining symbols in a &quot;correct&quot; way, for whatever definition of correct. I said &quot;training/instructions&quot; because, for the purpose of this question, it doesn't really make a difference if the &quot;knowledge&quot; was acquired by parsing many many examples and getting a &quot;feeling&quot; for what's right and what's wrong (AI/learning), or by a very detailed set of instructions (algorithmic).</p>\n<p>So, the person responds with perfectly reasonable sentences, without ever understanding Chinese, or the content of its input.</p>\n<p>Now, as far as I understand ChatGPT (and I might be completely wrong here), that's exactly what ChatGPT does. It has been trained on a huge corpus of text, and thus has a very good feeling which words go together well and which don't, and, given a sentence, what's the most likely continuation of this sentence. But that doesn't really mean it understands the content of the sentence, it only knows how to chose words based on what it has seen. And because it doesn't really understand any content, it mostly gives answers that are correct, but sometimes it's completely off because it &quot;doesn't really understand Chinese&quot; and doesn't know what it's talking about.</p>\n<p>So, my question: is this &quot;juggling of Chinese symbols without understanding their meaning&quot; an adequate explanation of how ChatGPT works, and if not, where's the difference? And if yes, how far is AI from models that can actually understand (for some definition of &quot;understand&quot;) textual content?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "terminology",
      "gpt",
      "language-model",
      "gpt-3"
    ],
    "owner": {
      "account_id": 8102914,
      "reputation": 503,
      "user_id": 34358,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/jD8Bm.jpg?s=256",
      "display_name": "Tom D&#246;rr",
      "link": "https://ai.stackexchange.com/users/34358/tom-d%c3%b6rr"
    },
    "is_answered": true,
    "view_count": 47824,
    "accepted_answer_id": 32478,
    "answer_count": 1,
    "score": 38,
    "last_activity_date": 1683729838,
    "creation_date": 1637458447,
    "last_edit_date": 1637581165,
    "question_id": 32477,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/32477/what-is-the-temperature-in-the-gpt-models",
    "title": "What is the &quot;temperature&quot; in the GPT models?",
    "body": "<p>What does the temperature parameter mean when talking about the GPT models?</p>\n<p>I know that a higher temperature value means more randomness, but I want to know how randomness is introduced.</p>\n<p>Does temperature mean we add noise to the weights/activations or do we add randomness when choosing a token in the softmax layer?</p>\n"
  },
  {
    "tags": [
      "convolutional-neural-networks",
      "tensorflow",
      "keras",
      "bayesian-deep-learning",
      "uncertainty-quantification"
    ],
    "owner": {
      "account_id": 5531362,
      "reputation": 1369,
      "user_id": 16871,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/1bbac7394bd9f4920abbce95fcd001c6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Alexander Soare",
      "link": "https://ai.stackexchange.com/users/16871/alexander-soare"
    },
    "is_answered": true,
    "view_count": 11319,
    "accepted_answer_id": 17722,
    "answer_count": 6,
    "score": 37,
    "last_activity_date": 1633654245,
    "creation_date": 1580228183,
    "last_edit_date": 1633654245,
    "question_id": 17721,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/17721/why-do-cnns-sometimes-make-highly-confident-mistakes-and-how-can-one-combat-th",
    "title": "Why do CNN&#39;s sometimes make highly confident mistakes, and how can one combat this problem?",
    "body": "<p>I trained a simple CNN on the MNIST database of handwritten digits to 99% accuracy. I'm feeding in a bunch of handwritten digits, and non-digits from a document.</p>\n\n<p>I want the CNN to report errors, so I set a threshold of 90% certainty below which my algorithm assumes that what it's looking at is not a digit. </p>\n\n<p>My problem is that the CNN is 100% certain of many incorrect guesses. In the example below, the CNN reports 100% certainty that it's a 0. How do I make it report failure?</p>\n\n<p><a href=\"https://i.sstatic.net/nPBYU.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/nPBYU.png\" alt=\"Handwritten digit input - does not look like a digit\"></a></p>\n\n<p><strong>My thoughts on this</strong>:\nMaybe the CNN is not really 100% certain that this is a zero. Maybe it just thinks that it can't be anything else, and it's being forced to choose (because of normalisation on the output vector). Is there any way I can get insight into what the CNN \"thought\" before I forced it to choose?</p>\n\n<p>PS: I'm using Keras on Tensorflow with Python.</p>\n\n<p><strong>Edit</strong></p>\n\n<p>Because someone asked. Here is the context of my problem:</p>\n\n<p>This came from me applying a heuristic algorithm for segmentation of sequences of connected digits. In the image above, the left part is actually a 4, and the right is the curve bit of a 2 without the base. The algorithm is supposed to step through segment cuts, and when it finds a confident match, remove that cut and continue moving along the sequence. It works really well for some cases, but of course it's totally reliant on being able to tell if what it's looking at is not a good match for a digit. Here's an example of where it kind of did okay.</p>\n\n<p><a href=\"https://i.sstatic.net/rVDz4.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/rVDz4.png\" alt=\"Various input images for handwritten digit recognition\"></a></p>\n\n<p>My next best option is to do inference on all permutations and maximise combined score. That's more expensive.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "reference-request",
      "computational-complexity",
      "intractability"
    ],
    "owner": {
      "account_id": 5982534,
      "reputation": 2111,
      "user_id": 6779,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/975485022464794/picture?type=large",
      "display_name": "k.c. sayz &#39;k.c sayz&#39;",
      "link": "https://ai.stackexchange.com/users/6779/k-c-sayz-k-c-sayz"
    },
    "is_answered": true,
    "view_count": 8188,
    "answer_count": 7,
    "score": 36,
    "last_activity_date": 1611067164,
    "creation_date": 1571592721,
    "last_edit_date": 1611067164,
    "question_id": 15986,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/15986/what-are-examples-of-promising-ai-ml-techniques-that-are-computationally-intract",
    "title": "What are examples of promising AI/ML techniques that are computationally intractable?",
    "body": "<p>To produce tangible results in the field of AI/ML, one must take theoretical results under the lens of computational complexity.</p>\n<p>Indeed, minimax effectively solves any two-person &quot;board game&quot; with win/loss conditions, but the algorithm quickly becomes untenable for games of large enough size, so it's practically useless asides from toy problems.</p>\n<p>In fact, this issue seems to cut at the heart of intelligence itself: the <a href=\"https://plato.stanford.edu/entries/frame-problem/\" rel=\"noreferrer\"><em>Frame Problem</em></a> highlights this by observing that any &quot;intelligent&quot; agent that operates under logical axioms must somehow deal with the explosive growth of computational complexity.</p>\n<p>So, we need to deal with <em>computational complexity</em>: but that doesn't mean researchers must limit themselves with practical concerns. In the past, multilayered perceptrons were thought to be intractable (I think), and thus we couldn't evaluate their utility until recently. I've heard that Bayesian techniques are conceptually elegant, but they become computationally intractable once your dataset becomes large, and thus we usually use variational methods to compute the posterior, instead of naively using the exact solution.</p>\n<p>I'm looking for more examples like this: What are examples of promising (or neat/interesting) AI/ML techniques that are computationally intractable (or uncomputable)?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "math",
      "applications"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 28803,
    "protected_date": 1583565386,
    "accepted_answer_id": 158,
    "answer_count": 6,
    "score": 35,
    "last_activity_date": 1666707055,
    "creation_date": 1470173852,
    "last_edit_date": 1633694351,
    "question_id": 154,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/154/is-it-possible-to-train-the-neural-network-to-solve-math-equations",
    "title": "Is it possible to train the neural network to solve math equations?",
    "body": "<p>I'm aware that neural networks are probably not designed to do that, however asking hypothetically, is it possible to train the deep neural network (or similar) to solve math equations?</p>\n\n<p>So given the 3 inputs: 1st number, operator sign represented by the number (1 - <code>+</code>, 2 - <code>-</code>, 3 - <code>/</code>, 4 - <code>*</code>, and so on), and the 2nd number, then after training the network should give me the valid results.</p>\n\n<p>Example 1 (<code>2+2</code>):</p>\n\n<ul>\n<li>Input 1: <code>2</code>; Input 2: <code>1</code> (<code>+</code>); Input 3: <code>2</code>; Expected output: <code>4</code></li>\n<li>Input 1: <code>10</code>; Input 2: <code>2</code> (<code>-</code>); Input 3: <code>10</code>; Expected output: <code>0</code></li>\n<li>Input 1: <code>5</code>; Input 2: <code>4</code> (<code>*</code>); Input 3: <code>5</code>; Expected output: <code>25</code></li>\n<li>and so</li>\n</ul>\n\n<p>The above can be extended to more sophisticated examples.</p>\n\n<p>Is that possible? If so, what kind of network can learn/achieve that?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "natural-language-processing",
      "bert",
      "text-generation"
    ],
    "owner": {
      "account_id": 5866004,
      "reputation": 453,
      "user_id": 20170,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/cVA9W.png?s=256",
      "display_name": "ch271828n",
      "link": "https://ai.stackexchange.com/users/20170/ch271828n"
    },
    "is_answered": true,
    "view_count": 45204,
    "accepted_answer_id": 10628,
    "answer_count": 3,
    "score": 35,
    "last_activity_date": 1632347478,
    "creation_date": 1543067079,
    "last_edit_date": 1593764883,
    "question_id": 9141,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/9141/can-bert-be-used-for-sentence-generating-tasks",
    "title": "Can BERT be used for sentence generating tasks?",
    "body": "<p>I am a new learner in NLP. I am interested in the sentence generating task. As far as I am concerned, one state-of-the-art method is the <a href=\"https://github.com/karpathy/char-rnn\" rel=\"noreferrer\">CharRNN</a>, which uses RNN to generate a sequence of words.</p>\n\n<p>However, <a href=\"https://arxiv.org/abs/1810.04805\" rel=\"noreferrer\">BERT</a> has come out several weeks ago and is very powerful. Therefore, I am wondering whether this task can also be done with the help of BERT? I am a new learner in this field, and thank you for any advice!</p>\n"
  },
  {
    "tags": [
      "python",
      "comparison",
      "r"
    ],
    "owner": {
      "account_id": 14411020,
      "reputation": 503,
      "user_id": 27652,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/5aedd802dca0d3b7397664dfac645ae8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "ItsMeMario",
      "link": "https://ai.stackexchange.com/users/27652/itsmemario"
    },
    "is_answered": true,
    "view_count": 17005,
    "protected_date": 1565095811,
    "closed_date": 1574137587,
    "accepted_answer_id": 13787,
    "answer_count": 8,
    "score": 35,
    "locked_date": 1614861044,
    "last_activity_date": 1603762008,
    "creation_date": 1564957459,
    "last_edit_date": 1603762008,
    "question_id": 13775,
    "link": "https://ai.stackexchange.com/questions/13775/is-a-switch-from-r-to-python-worth-it",
    "closed_reason": "Not suitable for this site",
    "title": "Is a switch from R to Python worth it?",
    "body": "<p>I just finished a 1-year Data Science master's program where we were taught R. I found that Python is more popular and has a larger community in AI.</p>\n<p>What are the advantages that Python may have over R in terms of features applicable to the field of Data Science and AI (other than popularity and larger community)? What positions in Data Science and AI would be more Python-heavy than R-heavy (especially comparing industry, academic, and government job positions)? In short, is Python worthwhile in all job situations or can I get by with only R in some positions?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "convolutional-neural-networks",
      "datasets",
      "image-segmentation",
      "fully-convolutional-networks"
    ],
    "owner": {
      "account_id": 4161171,
      "reputation": 617,
      "user_id": 13257,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/46fbb3af211b4baca82f45d7424aa54c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "MattSt",
      "link": "https://ai.stackexchange.com/users/13257/mattst"
    },
    "is_answered": true,
    "view_count": 43374,
    "protected_date": 1638351338,
    "accepted_answer_id": 14364,
    "answer_count": 5,
    "score": 34,
    "last_activity_date": 1638306525,
    "creation_date": 1525472463,
    "last_edit_date": 1592080476,
    "question_id": 6274,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6274/how-can-i-deal-with-images-of-variable-dimensions-when-doing-image-segmentation",
    "title": "How can I deal with images of variable dimensions when doing image segmentation?",
    "body": "<p>I'm facing the problem of having images of different dimensions as inputs in a segmentation task. Note that the images do not even have the same aspect ratio.</p>\n\n<p>One common approach that I found in general in deep learning is to crop the images, as it is also suggested <a href=\"https://datascience.stackexchange.com/q/16601/10640\"><strong>here</strong></a>. However, in my case, I cannot crop the image and keep its center or something similar, since, in segmentation, I want the output to be of the same dimensions as the input.</p>\n\n<p><a href=\"https://pdfs.semanticscholar.org/d65b/cb276bc6fc9445a78fe26de76a43e09ccd27.pdf\" rel=\"noreferrer\"><strong>This</strong></a> paper suggests that in a segmentation task one can feed the same image multiple times to the network but with a different scale and then aggregate the results. If I understand this approach correctly, it would only work if all the input images have the same aspect ratio. Please correct me if I am wrong.</p>\n\n<p>Another alternative would be to just resize each image to fixed dimensions. I think this was also proposed by the answer to <a href=\"https://ai.stackexchange.com/q/2403/2444\"><strong>this</strong></a> question. However, it is not specified in what way images are resized.</p>\n\n<p>I considered taking the maximum width and height in the dataset and resizing all the images to that fixed size in an attempt to avoid information loss. However, I believe that our network might have difficulties with distorted images as the edges in an image might not be clear. </p>\n\n<ol>\n<li><p>What is possibly the best way to resize your images before feeding them to the network?</p></li>\n<li><p>Is there any other option that I am not aware of for solving the problem of having images of different dimensions?</p></li>\n<li><p>Also, which of these approaches you think is the best taking into account the computational complexity but also the possible loss of performance by the network? </p></li>\n</ol>\n\n<p>I would appreciate if the answers to my questions include some link to a source if there is one.</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "chat-bots",
      "question-answering",
      "board-games",
      "chatgpt"
    ],
    "owner": {
      "account_id": 242612,
      "reputation": 443,
      "user_id": 64749,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/73c478b60c371abe43bb7ac15ff3666d?s=256&d=identicon&r=PG",
      "display_name": "iammilind",
      "link": "https://ai.stackexchange.com/users/64749/iammilind"
    },
    "is_answered": true,
    "view_count": 11979,
    "accepted_answer_id": 38272,
    "answer_count": 6,
    "score": 34,
    "last_activity_date": 1682083125,
    "creation_date": 1670815908,
    "last_edit_date": 1682073466,
    "question_id": 38270,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/38270/how-does-an-ai-like-chatgpt-answer-a-question-in-a-subject-which-it-may-not-know",
    "title": "How does an AI like ChatGPT answer a question in a subject which it may not know?",
    "body": "<p>After seeing <a href=\"https://meta.stackoverflow.com/q/421831\">StackOverflow's banning of ChatGPT</a>, I explored it out of curiosity. It's marvellous as it can write code by itself!</p>\n<p>Later to check if it knows chess as well like Google-Deepmind's AlphaZero AI, I asked below questions:</p>\n<blockquote>\n<p><strong>Me</strong>: Does openai know how to play chess?<br />\n<strong>OpenAI</strong>: It is not clear whether OpenAI knows how to play chess. As an artificial intelligence research organization, OpenAI has developed a number of advanced machine learning algorithms and techniques that allow AI systems to learn and adapt to new tasks and situations. ...</p>\n</blockquote>\n<blockquote>\n<p><strong>Me</strong>: In a chess if white has a king &amp; bishop and if black has only a king, then who can win with a perfect play?<br />\n<strong>OpenAI</strong>: In a chess game where white has a king and a bishop and black has only a king, <strong>it is possible for either player to win with perfect play</strong>. This type of endgame is known as a king and bishop versus king endgame, and it is considered one of the most challenging in chess. ...</p>\n</blockquote>\n<p>Now this answer is entirely wrong. Because as soon as only a bishop (or a knight) remains apart from the kings, it's instantly <a href=\"https://chess.stackexchange.com/questions/30561/is-it-possible-to-win-with-a-king-and-one-bishop-vs-a-king\">declared as a draw</a>!</p>\n<p><strong>Question</strong>: Shouldn't the AI reject my question on a subject which is not in its known criteria?<br />\nIt does so for many other subjects.</p>\n<hr />\n<p><strong>Note</strong>: Should we replace <code>bishop = rook</code> then ChatGPT answers exactly the same answer with replacing those pieces. However that happens to be true.</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "transformer",
      "attention",
      "gpt",
      "large-language-models"
    ],
    "owner": {
      "account_id": 11887562,
      "reputation": 2780,
      "user_id": 34383,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/c3938672a162d7f04ee40a146836de6d?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Robin van Hoorn",
      "link": "https://ai.stackexchange.com/users/34383/robin-van-hoorn"
    },
    "is_answered": true,
    "view_count": 52718,
    "accepted_answer_id": 40180,
    "answer_count": 1,
    "score": 34,
    "last_activity_date": 1685456475,
    "creation_date": 1682278110,
    "last_edit_date": 1685456475,
    "question_id": 40179,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/40179/how-does-the-decoder-only-transformer-architecture-work",
    "title": "How does the (decoder-only) transformer architecture work?",
    "body": "<p>How does the (decoder-only) transformer architecture work which is used in impressive models such as GPT-4?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "hyperparameter-optimization",
      "artificial-neuron",
      "hyper-parameters",
      "layers"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 1582,
    "accepted_answer_id": 12,
    "answer_count": 4,
    "score": 33,
    "last_activity_date": 1611100447,
    "creation_date": 1470152482,
    "last_edit_date": 1611100447,
    "question_id": 4,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4/how-to-find-the-optimal-number-of-neurons-per-layer",
    "title": "How to find the optimal number of neurons per layer?",
    "body": "<p>When you're writing your algorithm, how do you know how many neurons you need per single layer? Are there any methods for finding the optimal number of them, or is it a rule of thumb?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "papers",
      "importance-sampling",
      "sample-efficiency"
    ],
    "owner": {
      "account_id": 4443341,
      "reputation": 473,
      "user_id": 12574,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Jijnd.jpg?s=256",
      "display_name": "Gokul NC",
      "link": "https://ai.stackexchange.com/users/12574/gokul-nc"
    },
    "is_answered": true,
    "view_count": 21967,
    "accepted_answer_id": 5247,
    "answer_count": 2,
    "score": 32,
    "last_activity_date": 1657206642,
    "creation_date": 1518002437,
    "last_edit_date": 1602577476,
    "question_id": 5246,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5246/what-is-sample-efficiency-and-how-can-importance-sampling-be-used-to-achieve-it",
    "title": "What is sample efficiency, and how can importance sampling be used to achieve it?",
    "body": "<p>For instance, the title of <a href=\"https://arxiv.org/abs/1611.01224\" rel=\"noreferrer\">this paper</a> reads: \"Sample Efficient Actor-Critic with Experience Replay\".</p>\n\n<p>What is <em>sample efficiency</em>, and how can <em>importance sampling</em> be used to achieve it?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "reference-request",
      "proofs",
      "function-approximation",
      "universal-approximation-theorems"
    ],
    "owner": {
      "account_id": 4965836,
      "reputation": 485,
      "user_id": 27047,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/100002195175477/picture?type=large",
      "display_name": "Leroy Od",
      "link": "https://ai.stackexchange.com/users/27047/leroy-od"
    },
    "is_answered": true,
    "view_count": 19412,
    "accepted_answer_id": 13319,
    "answer_count": 3,
    "score": 32,
    "last_activity_date": 1639654875,
    "creation_date": 1562834414,
    "last_edit_date": 1594728806,
    "question_id": 13317,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/13317/where-can-i-find-the-proof-of-the-universal-approximation-theorem",
    "title": "Where can I find the proof of the universal approximation theorem?",
    "body": "<p>The <a href=\"https://en.wikipedia.org/wiki/Universal_approximation_theorem\" rel=\"noreferrer\">Wikipedia article for the universal approximation</a> theorem cites a version of the universal approximation theorem for Lebesgue-measurable functions from <a href=\"https://arxiv.org/pdf/1709.02540.pdf\" rel=\"noreferrer\">this conference paper</a>. However, the paper does not include the proofs of the theorem. Does anybody know where the proof can be found?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "deep-neural-networks",
      "terminology",
      "comparison"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 1355,
    "protected_date": 1583565328,
    "accepted_answer_id": 93,
    "answer_count": 2,
    "score": 31,
    "last_activity_date": 1598180785,
    "creation_date": 1470157170,
    "last_edit_date": 1550959520,
    "question_id": 86,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/86/how-is-a-deep-neural-network-different-from-other-neural-networks",
    "title": "How is a deep neural network different from other neural networks?",
    "body": "<p>How is a neural network having the \"deep\" adjective actually distinguished from other similar networks?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "transfer-learning",
      "incremental-learning",
      "catastrophic-forgetting"
    ],
    "owner": {
      "account_id": 1038747,
      "reputation": 459,
      "user_id": 2904,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/sjIdN.jpg?s=256",
      "display_name": "Th3Nic3Guy",
      "link": "https://ai.stackexchange.com/users/2904/th3nic3guy"
    },
    "is_answered": true,
    "view_count": 14158,
    "accepted_answer_id": 4053,
    "answer_count": 5,
    "score": 31,
    "last_activity_date": 1639061370,
    "creation_date": 1504691830,
    "last_edit_date": 1605003751,
    "question_id": 3981,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3981/is-it-possible-to-train-a-neural-network-as-new-classes-are-given",
    "title": "Is it possible to train a neural network as new classes are given?",
    "body": "<p>I would like to train a neural network (NN) where the output classes are not (all) defined from the start. More and more classes will be introduced later based on incoming data. This means that, every time I introduce a new class, I would need to retrain the NN.</p>\n<p>How can I train an NN incrementally, that is, without forgetting the previously acquired information during the previous training phases?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "ai-security",
      "ai-safety",
      "adversarial-ml"
    ],
    "owner": {
      "account_id": 9533159,
      "reputation": 495,
      "user_id": 16322,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-osHplw29vHg/AAAAAAAAAAI/AAAAAAAAABA/BuhhDfnHioI/s256-rj/photo.jpg",
      "display_name": "Surya Sg",
      "link": "https://ai.stackexchange.com/users/16322/surya-sg"
    },
    "is_answered": true,
    "view_count": 5572,
    "protected_date": 1529775383,
    "closed_date": 1642165148,
    "answer_count": 7,
    "score": 31,
    "last_activity_date": 1604574002,
    "creation_date": 1529409194,
    "last_edit_date": 1570833121,
    "question_id": 6800,
    "link": "https://ai.stackexchange.com/questions/6800/is-artificial-intelligence-vulnerable-to-hacking",
    "closed_reason": "Needs details or clarity",
    "title": "Is artificial intelligence vulnerable to hacking?",
    "body": "<p>The paper <a href=\"https://arxiv.org/pdf/1511.07528.pdf\" rel=\"nofollow noreferrer\"><em>The Limitations of Deep Learning in Adversarial Settings</em></a> explores how neural networks might be corrupted by an attacker who can manipulate the data set that the neural network trains with. The authors experiment with a neural network meant to read handwritten digits, undermining its reading ability by distorting the samples of handwritten digits that the neural network is trained with.</p>\n\n<p>I'm concerned that malicious actors might try hacking AI. For example</p>\n\n<ul>\n<li>Fooling autonomous vehicles to misinterpret stop signs vs. speed limit.</li>\n<li>Bypassing facial recognition, such as the ones for ATM.</li>\n<li>Bypassing spam filters.</li>\n<li>Fooling sentiment analysis of movie reviews, hotels, etc.</li>\n<li>Bypassing anomaly detection engines.</li>\n<li>Faking voice commands.</li>\n<li>Misclassifying machine learning based-medical predictions.</li>\n</ul>\n\n<p>What adversarial effect could disrupt the world? How we can prevent it?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "math",
      "automated-theorem-proving"
    ],
    "owner": {
      "account_id": 14091497,
      "reputation": 311,
      "user_id": 17282,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/ead7217fd32acbfa2f1552c437140790?s=256&d=identicon&r=PG",
      "display_name": "Max Mustermann Junior",
      "link": "https://ai.stackexchange.com/users/17282/max-mustermann-junior"
    },
    "is_answered": true,
    "view_count": 5439,
    "answer_count": 4,
    "score": 31,
    "last_activity_date": 1607002405,
    "creation_date": 1533376641,
    "last_edit_date": 1590088329,
    "question_id": 7416,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7416/can-neural-networks-be-used-to-prove-conjectures",
    "title": "Can neural networks be used to prove conjectures?",
    "body": "<p>Imagine I have a list (in a computer-readable form) of all problems (or statements) and proofs that math relies on. </p>\n\n<p>Could I train a neural network in such a way that, for example, I enter a problem and it generates a proof for it?</p>\n\n<p>Of course, those proofs then needed to be checked manually, but maybe the network then creates proofs from combinations of older proofs for problems yet unsolved.</p>\n\n<p>Is that possible?</p>\n\n<p>Would it be possible, for example, to solve the Collatz-conjecture or the Riemann-conjecture with this type of network? Or, if not solve, but maybe rearrange patterns in a way that mathematicians are able to use a new \"proof method\" to make a real proof?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "terminology",
      "math",
      "bellman-equations",
      "bellman-operators"
    ],
    "owner": {
      "account_id": 4148701,
      "reputation": 42397,
      "user_id": 2444,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e6fce047cba96a601a9dac66df4cd2e8?s=256&d=identicon&r=PG",
      "display_name": "nbro",
      "link": "https://ai.stackexchange.com/users/2444/nbro"
    },
    "is_answered": true,
    "view_count": 20050,
    "accepted_answer_id": 11133,
    "answer_count": 1,
    "score": 31,
    "last_activity_date": 1637230784,
    "creation_date": 1551881236,
    "last_edit_date": 1637230784,
    "question_id": 11057,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11057/what-is-the-bellman-operator-in-reinforcement-learning",
    "title": "What is the Bellman operator in reinforcement learning?",
    "body": "<p>In mathematics, the word <a href=\"https://en.wikipedia.org/wiki/Operator_(mathematics)\" rel=\"noreferrer\">operator</a> can refer to several distinct but related concepts. An operator can be defined as a function between two vector spaces, it can be defined as a function where the domain and the codomain are the same, or it can be defined as a function from functions (which are vectors) to other functions (for example, the <a href=\"https://en.wikipedia.org/wiki/Differential_operator\" rel=\"noreferrer\">differential operator</a>), that is, a high-order function (if you are familiar with functional programming).</p>\n<ul>\n<li>What is the <em>Bellman operator</em> in reinforcement learning (RL)?</li>\n<li>Why do we even need it?</li>\n<li>How is the Bellman operator related to the Bellman equations in RL?</li>\n</ul>\n"
  },
  {
    "tags": [
      "history",
      "programming-languages",
      "lisp"
    ],
    "owner": {
      "account_id": 40019,
      "reputation": 2533,
      "user_id": 55,
      "user_type": "registered",
      "accept_rate": 43,
      "profile_image": "https://i.sstatic.net/FkIiK.png?s=256",
      "display_name": "WilliamKF",
      "link": "https://ai.stackexchange.com/users/55/williamkf"
    },
    "is_answered": true,
    "view_count": 3760,
    "accepted_answer_id": 131,
    "answer_count": 4,
    "score": 30,
    "last_activity_date": 1610319794,
    "creation_date": 1470156301,
    "last_edit_date": 1523670455,
    "question_id": 77,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/77/is-lisp-still-being-used-to-tackle-ai-problems",
    "title": "Is Lisp still being used to tackle AI problems?",
    "body": "<p>I know that language of Lisp was used early on when working on artificial intelligence problems. Is it still being used today for significant work? If not, is there a new language that has taken its place as the most common one being used for work in AI today?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "tensorflow",
      "terminology",
      "bottlenecks"
    ],
    "owner": {
      "account_id": 5668560,
      "reputation": 411,
      "user_id": 11837,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/100002257370445/picture?type=large",
      "display_name": "Anurag Singh",
      "link": "https://ai.stackexchange.com/users/11837/anurag-singh"
    },
    "is_answered": true,
    "view_count": 37822,
    "accepted_answer_id": 4887,
    "answer_count": 2,
    "score": 30,
    "last_activity_date": 1641456662,
    "creation_date": 1514459548,
    "last_edit_date": 1631193927,
    "question_id": 4864,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4864/what-are-bottlenecks-in-neural-networks",
    "title": "What are &quot;bottlenecks&quot; in neural networks?",
    "body": "<p>What are &quot;bottlenecks&quot; in the context of neural networks?</p>\n<p>This term is mentioned, for example, in <a href=\"https://web.archive.org/web/20180703133602/https://www.tensorflow.org/tutorials/image_retraining\" rel=\"noreferrer\">this TensorFlow article</a>, which also uses the term &quot;bottleneck values&quot;. How does one calculate bottleneck values? How do these values help image classification?</p>\n<p>Please explain in simple words.</p>\n"
  },
  {
    "tags": [
      "reference-request",
      "philosophy",
      "artificial-consciousness"
    ],
    "owner": {
      "account_id": 8699338,
      "reputation": 401,
      "user_id": 12648,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/XE44G.jpg?s=256",
      "display_name": "Micky-D",
      "link": "https://ai.stackexchange.com/users/12648/micky-d"
    },
    "is_answered": true,
    "view_count": 5159,
    "protected_date": 1573507605,
    "answer_count": 4,
    "score": 30,
    "last_activity_date": 1639326074,
    "creation_date": 1518326820,
    "last_edit_date": 1639326074,
    "question_id": 5274,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5274/what-are-the-current-theories-on-the-development-of-a-conscious-ai",
    "title": "What are the current theories on the development of a conscious AI?",
    "body": "<p>What are the current theories on the development of a conscious AI? Is anyone even trying to develop a conscious AI?</p>\n\n<p>Is it possible that consciousness is an emergent phenomenon, that is, once we put enough complexity into our system, it will become self-aware? </p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "natural-language-understanding",
      "machine-translation",
      "google-translate"
    ],
    "owner": {
      "account_id": 150314,
      "reputation": 931,
      "user_id": 25362,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/tVWLM.png?s=256",
      "display_name": "Hans-Peter Stricker",
      "link": "https://ai.stackexchange.com/users/25362/hans-peter-stricker"
    },
    "is_answered": true,
    "view_count": 6802,
    "answer_count": 9,
    "score": 30,
    "last_activity_date": 1610972973,
    "creation_date": 1559572481,
    "last_edit_date": 1610972973,
    "question_id": 12659,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12659/what-is-the-actual-quality-of-machine-translations",
    "title": "What is the actual quality of machine translations?",
    "body": "<p>As an AI layman, till today I am confused by the promised and achieved improvements of automated translation.</p>\n<p>My impression is: <strong>there is still a very, very far way to go.</strong> Or are there other explanations why the automated translations (offered and provided e.g. by Google) of quite simple Wikipedia articles still read and sound mainly silly, are hardly readable, and only very partially helpful and useful?</p>\n<p>It may depend on personal preferences (concerning readability, helpfulness, and usefulness), but <strong>my personal expectations</strong> are disappointed sorely.</p>\n<p>The other way around: Are Google's translations nevertheless readable, helpful, and useful <strong>for a majority of users</strong>?</p>\n<p>Or does Google have reasons to <strong>retain its achievements</strong> (and not to show to the users the best they can show)?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "definitions",
      "credit-assignment-problem"
    ],
    "owner": {
      "account_id": 4148701,
      "reputation": 42397,
      "user_id": 2444,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e6fce047cba96a601a9dac66df4cd2e8?s=256&d=identicon&r=PG",
      "display_name": "nbro",
      "link": "https://ai.stackexchange.com/users/2444/nbro"
    },
    "is_answered": true,
    "view_count": 25956,
    "accepted_answer_id": 12909,
    "answer_count": 1,
    "score": 30,
    "last_activity_date": 1592649681,
    "creation_date": 1560818155,
    "last_edit_date": 1592649681,
    "question_id": 12908,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12908/what-is-the-credit-assignment-problem",
    "title": "What is the credit assignment problem?",
    "body": "<p>In reinforcement learning (RL), the <em>credit assignment problem</em> (CAP) seems to be an important problem. What is the CAP? Why is it relevant to RL?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "terminology"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user31556"
    },
    "is_answered": true,
    "view_count": 17435,
    "protected_date": 1574782729,
    "answer_count": 7,
    "score": 30,
    "last_activity_date": 1575174867,
    "creation_date": 1574602712,
    "last_edit_date": 1574609372,
    "question_id": 16741,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/16741/how-can-an-ai-train-itself-if-no-one-is-telling-it-if-its-answer-is-correct-or-w",
    "title": "How can an AI train itself if no one is telling it if its answer is correct or wrong?",
    "body": "<p>I am a programmer but not in the field of AI. A question constantly confuses me is that how can an AI be trained if we human beings are not telling it its calculation is correct?</p>\n\n<p>For example, news usually said something like \"company A has a large human face database so that it can train its facial recognition program more efficiently\". What the piece of news doesn't mention is whether a human engineer needs to tell the AI program each of the program's recognition result is accurate or not.</p>\n\n<p>Are there any engineers who are constantly telling an AI what it produced it correct or wrong? If no, how can an AI determine if the result it produces is correct or wrong?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "comparison",
      "transformer",
      "bert"
    ],
    "owner": {
      "account_id": 5523531,
      "reputation": 3020,
      "user_id": 12201,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/08ea907d3b57005305612f88c0a85b03?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "chessprogrammer",
      "link": "https://ai.stackexchange.com/users/12201/chessprogrammer"
    },
    "is_answered": true,
    "view_count": 19278,
    "accepted_answer_id": 23683,
    "answer_count": 1,
    "score": 30,
    "last_activity_date": 1717123562,
    "creation_date": 1598280995,
    "last_edit_date": 1600706900,
    "question_id": 23221,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/23221/how-is-bert-different-from-the-original-transformer-architecture",
    "title": "How is BERT different from the original transformer architecture?",
    "body": "<p>As far as I can tell, BERT is a type of Transformer architecture. What I do not understand is:</p>\n<ol>\n<li><p>How is Bert different from the original transformer architecture?</p>\n</li>\n<li><p>What tasks are better suited for BERT, and what tasks are better suited for the original architecture?</p>\n</li>\n</ol>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "math",
      "chat-bots",
      "gpt",
      "chatgpt"
    ],
    "owner": {
      "account_id": 5129611,
      "reputation": 2945,
      "user_id": 145,
      "user_type": "moderator",
      "accept_rate": 62,
      "profile_image": "https://i.sstatic.net/IIYyh.png?s=256",
      "display_name": "Mithical",
      "link": "https://ai.stackexchange.com/users/145/mithical"
    },
    "is_answered": true,
    "view_count": 37982,
    "protected_date": 1673347960,
    "answer_count": 4,
    "score": 30,
    "last_activity_date": 1684400061,
    "creation_date": 1670541142,
    "last_edit_date": 1671218401,
    "question_id": 38220,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/38220/why-is-chatgpt-bad-at-math",
    "title": "Why is ChatGPT bad at math?",
    "body": "<p>As opposed to <a href=\"https://ai.stackexchange.com/q/38217/145\">How does ChatGPT know math?</a>, I've been seeing some things floating around the Twitterverse about how ChatGPT can actually be very bad at math. For instance, I asked it &quot;If it takes 5 machines 5 minutes to make 5 devices, how long would it take 100 machines to make 100 devices?&quot;</p>\n<p>The conversation went <a href=\"https://i.sstatic.net/XDTem.png\" rel=\"noreferrer\">like this</a>:</p>\n<blockquote>\n<p><strong>Mithical</strong>: If it takes 5 machines 5 minutes to make 5 devices, how long would it take 100 machines to make 100 devices?<br />\n<strong>ChatGPT</strong>: If it takes 5 machines 5 minutes to make 5 devices, then it would take 100 machines 100 minutes to make 100 devices.<br />\n<strong>Mithical</strong>: That's not right. Check carefully and try again.<br />\n<strong>ChatGPT</strong>: I apologize if my response was not correct. Can you please provide more information or clarify your question? I may have misunderstood the scenario you described.<br />\n<strong>Mithical</strong>: It takes a machine 5 minutes to make a device. How long would it take 100 machines to make 100 devices?<br />\n<strong>ChatGPT</strong>: If it takes a single machine 5 minutes to make a single device, then it would take 100 machines 5 minutes to make 100 devices. Therefore, it would take a total of 500 minutes for 100 machines to make 100 devices.</p>\n</blockquote>\n<p>Now, unless I'm misunderstanding something here, the answer is 5 minutes. ChatGPT first makes the intuitive mistake of 100, that a human might make as well, and then goes on to (correctly, as far as I understand) say it's 5 minutes... but concludes in the same response that it's then 500 minutes.</p>\n<p>Why is this AI so bad at math? Computers are generally supposed to be good at math. Why does this model make such simple logical mistakes?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "definitions"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 8969,
    "protected_date": 1561224138,
    "accepted_answer_id": 1516,
    "answer_count": 5,
    "score": 29,
    "last_activity_date": 1563479345,
    "creation_date": 1470790554,
    "last_edit_date": 1561381637,
    "question_id": 1507,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1507/what-are-the-minimum-requirements-to-call-something-ai",
    "title": "What are the minimum requirements to call something AI?",
    "body": "<p>I believe <em>artificial intelligence</em> (AI) term is overused nowadays. For example, people see that something is self-moving and they call it AI, even if it's on autopilot (like cars or planes) or there is some simple algorithm behind it.</p>\n\n<p>What are the minimum general requirements so that we can say something is AI?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "deep-learning",
      "prediction",
      "randomness"
    ],
    "owner": {
      "account_id": 7603965,
      "reputation": 401,
      "user_id": 9170,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f3abab86d3b356c02720285a9a0edabe?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "AshTyson",
      "link": "https://ai.stackexchange.com/users/9170/ashtyson"
    },
    "is_answered": true,
    "view_count": 42795,
    "protected_date": 1539898714,
    "answer_count": 4,
    "score": 29,
    "last_activity_date": 1588697273,
    "creation_date": 1503328695,
    "last_edit_date": 1574344331,
    "question_id": 3850,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3850/can-a-neural-network-be-used-to-predict-the-next-pseudo-random-number",
    "title": "Can a neural network be used to predict the next pseudo random number?",
    "body": "<p>Is it possible to feed a neural network the output from a random number generator and expect it learn the hashing (or generator) function, so that it can predict what will be the next generated <a href=\"https://softwareengineering.stackexchange.com/q/124233/145653\">pseudo-random number</a>? </p>\n\n<p>Does something like this already exist? If research is already done on this or something related (to the prediction of pseudo-random numbers), can anyone point me to the right resources? </p>\n\n<p>Currently, I am looking at this library and its related links.\n<a href=\"https://github.com/Vict0rSch/deep_learning/tree/master/keras/recurrent\" rel=\"noreferrer\">https://github.com/Vict0rSch/deep_learning/tree/master/keras/recurrent</a></p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "reference-request",
      "network-design"
    ],
    "owner": {
      "account_id": 3356567,
      "reputation": 405,
      "user_id": 11359,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/fc6e14bdead23e763f1432eb7033043a?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Josef Ondrej",
      "link": "https://ai.stackexchange.com/users/11359/josef-ondrej"
    },
    "is_answered": true,
    "view_count": 12333,
    "protected_date": 1604629674,
    "accepted_answer_id": 4707,
    "answer_count": 4,
    "score": 29,
    "last_activity_date": 1709338947,
    "creation_date": 1512398569,
    "last_edit_date": 1640195819,
    "question_id": 4655,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4655/how-could-we-build-a-neural-network-that-is-invariant-to-permutations-of-the-inp",
    "title": "How could we build a neural network that is invariant to permutations of the inputs?",
    "body": "<blockquote>\n<p>Given a neural network <span class=\"math-container\">$f$</span> that takes as input <span class=\"math-container\">$n$</span> data points: <span class=\"math-container\">$x_1, \\dots, x_n$</span>. We say <span class=\"math-container\">$f$</span> is <strong>permutation invariant</strong> if</p>\n<p><span class=\"math-container\">$$f(x_1 ... x_n) = f(\\sigma(x_1 ... x_n))$$</span></p>\n<p>for any permutation <span class=\"math-container\">$\\sigma$</span>.</p>\n</blockquote>\n<p>How could we build such a neural network? The output can be anything (e.g. a number or vector), as long as it does not depend on the order of inputs. Is there any research work on these types of neural networks?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "comparison",
      "python",
      "programming-languages",
      "c++"
    ],
    "owner": {
      "account_id": 7039987,
      "reputation": 511,
      "user_id": 15277,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/93fba4330985b7c2c301d55af771e1b1?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Mark ellon",
      "link": "https://ai.stackexchange.com/users/15277/mark-ellon"
    },
    "is_answered": true,
    "view_count": 81363,
    "protected_date": 1569699477,
    "accepted_answer_id": 6186,
    "answer_count": 4,
    "score": 29,
    "last_activity_date": 1599209213,
    "creation_date": 1524801139,
    "last_edit_date": 1591141019,
    "question_id": 6185,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6185/why-does-c-seem-less-widely-used-than-python-in-ai",
    "title": "Why does C++ seem less widely used than Python in AI?",
    "body": "<p>I just want to know why do machine learning engineers and AI programmers use languages like Python to perform AI tasks and not C++, even though C++ is technically a more powerful language than Python.</p>\n"
  },
  {
    "tags": [
      "image-recognition",
      "reference-request",
      "voice-recognition",
      "ai-security",
      "adversarial-ml"
    ],
    "owner": {
      "account_id": 3121331,
      "reputation": 433,
      "user_id": 30335,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/3d737f33994195915863709ddf876d0b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Der F&#228;nger im Roggen",
      "link": "https://ai.stackexchange.com/users/30335/der-f%c3%a4nger-im-roggen"
    },
    "is_answered": true,
    "view_count": 4235,
    "protected_date": 1574876128,
    "accepted_answer_id": 15823,
    "answer_count": 8,
    "score": 29,
    "last_activity_date": 1570821612,
    "creation_date": 1570643129,
    "last_edit_date": 1570754092,
    "question_id": 15820,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/15820/is-there-any-research-on-the-development-of-attacks-against-artificial-intellige",
    "title": "Is there any research on the development of attacks against artificial intelligence systems?",
    "body": "<p>Is there any research on the development of attacks against artificial intelligence systems?</p>\n\n<p>For example, is there a way to generate a letter \"A\", which every human being in this world can recognize but, if it is shown to the state-of-the-art character recognition system, this system will fail to recognize it? Or spoken audio which can be easily recognized by everyone but will fail on the state-of-the-art speech recognition system. </p>\n\n<p>If there exists such a thing, is this technology a theory-based science (mathematics proved) or an experimental science (randomly add different types of noise and feed into the AI system and see how it works)? Where can I find such material?  </p>\n"
  },
  {
    "tags": [
      "chat-bots",
      "narrow-ai",
      "intelligent-personal-assistants",
      "siri",
      "cortana"
    ],
    "owner": {
      "account_id": 8761221,
      "reputation": 1082,
      "user_id": 72,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f22ad5db7375b6fb8b2193999c5297c8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Vishnu JK",
      "link": "https://ai.stackexchange.com/users/72/vishnu-jk"
    },
    "is_answered": true,
    "view_count": 1553,
    "accepted_answer_id": 1483,
    "answer_count": 4,
    "score": 28,
    "last_activity_date": 1736010085,
    "creation_date": 1470678523,
    "last_edit_date": 1736010085,
    "question_id": 1461,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1461/are-siri-and-cortana-ai-programs",
    "title": "Are Siri and Cortana AI programs?",
    "body": "<p><a href=\"https://en.wikipedia.org/wiki/Siri\" rel=\"noreferrer\">Siri</a> and <a href=\"https://en.wikipedia.org/wiki/Cortana\" rel=\"noreferrer\">Cortana</a> communicate pretty much like humans. Unlike <a href=\"https://en.wikipedia.org/wiki/Google_Now\" rel=\"noreferrer\">Google Now</a>, which mainly gives us search results when asked some questions (not setting alarms or reminders), Siri and Cortana provide us with an answer, in the same way that a person would do.</p>\n<p>So, are they actual AI programs or not?</p>\n<p>(By &quot;question&quot; I don't mean any academic-related question or asking routes/ temperature, but rather opinion based question).</p>\n"
  },
  {
    "tags": [
      "history",
      "programming-languages",
      "prolog"
    ],
    "owner": {
      "account_id": 5129611,
      "reputation": 2945,
      "user_id": 145,
      "user_type": "moderator",
      "accept_rate": 62,
      "profile_image": "https://i.sstatic.net/IIYyh.png?s=256",
      "display_name": "Mithical",
      "link": "https://ai.stackexchange.com/users/145/mithical"
    },
    "is_answered": true,
    "view_count": 11895,
    "protected_date": 1589400782,
    "accepted_answer_id": 1532,
    "answer_count": 2,
    "score": 28,
    "last_activity_date": 1471037928,
    "creation_date": 1470856638,
    "question_id": 1531,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/1531/is-prolog-still-used-in-ai",
    "title": "Is Prolog still used in AI?",
    "body": "<p>According to <a href=\"http://en.wikipedia.org/wiki/Prolog\">Wikipedia</a>,</p>\n\n<blockquote>\n  <p>Prolog is a general-purpose logic programming language associated with artificial intelligence and computational linguistics.</p>\n</blockquote>\n\n<p>Is it still used for AI?</p>\n\n<hr>\n\n<p><sub>This is based off of a question on the 2014 closed beta. The author had the UID of 330.</sub></p>\n"
  },
  {
    "tags": [
      "deep-neural-networks",
      "neural-networks",
      "image-recognition",
      "convolutional-neural-networks"
    ],
    "owner": {
      "account_id": 404018,
      "reputation": 1363,
      "user_id": 46,
      "user_type": "registered",
      "accept_rate": 29,
      "profile_image": "https://i.sstatic.net/uheW1.png?s=256",
      "display_name": "dynrepsys",
      "link": "https://ai.stackexchange.com/users/46/dynrepsys"
    },
    "is_answered": true,
    "view_count": 492,
    "answer_count": 4,
    "score": 27,
    "last_activity_date": 1610391803,
    "creation_date": 1470155935,
    "last_edit_date": 1610391803,
    "question_id": 70,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/70/is-the-pattern-recognition-capability-of-cnns-limited-to-image-processing",
    "title": "Is the pattern recognition capability of CNNs limited to image processing?",
    "body": "<p>Can a Convolutional Neural Network be used for pattern recognition in problem domains without image data? For example, by representing abstract data in an image-like format with spatial relations? Would that always be less efficient?</p>\n<p><a href=\"https://youtu.be/py5byOOHZM8?t=815\" rel=\"noreferrer\" title=\"This Developer\">This developer</a> says current development could go further but not if there's a limit outside image recognition.</p>\n"
  },
  {
    "tags": [
      "recurrent-neural-networks",
      "open-ai",
      "transformer",
      "attention",
      "gpt"
    ],
    "owner": {
      "account_id": 205095,
      "reputation": 491,
      "user_id": 9268,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/QhMRI.png?s=256",
      "display_name": "Nav",
      "link": "https://ai.stackexchange.com/users/9268/nav"
    },
    "is_answered": true,
    "view_count": 22979,
    "accepted_answer_id": 22715,
    "answer_count": 1,
    "score": 27,
    "last_activity_date": 1595929831,
    "creation_date": 1595751152,
    "last_edit_date": 1595754284,
    "question_id": 22673,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/22673/what-exactly-are-the-parameters-in-gpt-3s-175-billion-parameters-and-how-are",
    "title": "What exactly are the &quot;parameters&quot; in GPT-3&#39;s 175 billion parameters and how are they chosen/generated?",
    "body": "<p>When I studied neural networks, parameters were learning rate, batch size etc. But even GPT3's ArXiv paper does not mention anything about what exactly the parameters are, but gives a small hint that they might just be sentences.</p>\n<p><a href=\"https://i.sstatic.net/LWEEQ.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/LWEEQ.png\" alt=\"enter image description here\" /></a></p>\n<p>Even tutorial sites like <a href=\"https://www.analyticsvidhya.com/blog/2019/07/openai-gpt2-text-generator-python/\" rel=\"noreferrer\">this one</a> start talking about the usual parameters, but also say <code>&quot;model_name: This indicates which model we are using. In our case, we are using the GPT-2 model with 345 million parameters or weights&quot;</code>. So are the 175 billion &quot;parameters&quot; just neural weights? Why then are they called parameters? <a href=\"https://arxiv.org/pdf/2005.14165.pdf\" rel=\"noreferrer\">GPT3's paper</a> shows that there are only 96 layers, so I'm assuming it's not a very deep network, but extremely fat. Or does it mean that each &quot;parameter&quot; is just a representation of the encoders or decoders?</p>\n<p><a href=\"https://i.sstatic.net/dthvC.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/dthvC.png\" alt=\"enter image description here\" /></a></p>\n<p>An excerpt from <a href=\"https://minimaxir.com/2019/09/howto-gpt2/\" rel=\"noreferrer\">this website</a> shows tokens:</p>\n<blockquote>\n<p>In this case, there are two additional parameters that can be passed\nto gpt2.generate(): truncate and include_prefix. For example, if each\nshort text begins with a &lt;|startoftext|&gt; token and ends with a\n&lt;|endoftext|&gt;, then setting prefix='&lt;|startoftext|&gt;',\ntruncate=&lt;|endoftext|&gt;', and include_prefix=False, and length is\nsufficient, then gpt-2-simple will automatically extract the shortform\ntexts, even when generating in batches.</p>\n</blockquote>\n<p>So are the parameters various kinds of tokens that are manually created by humans who try to fine-tune the models? Still, 175 billion such fine-tuning parameters is too high for humans to create, so I assume the &quot;parameters&quot; are auto-generated somehow.</p>\n<p>The <a href=\"https://arxiv.org/pdf/1706.03762.pdf\" rel=\"noreferrer\">attention-based</a> paper mentions the <a href=\"http://jalammar.github.io/illustrated-gpt2/\" rel=\"noreferrer\">query-key-value weight</a> matrices as the &quot;parameters&quot;. <strong>Even if it is these weights, I'd just like to know what kind of a process generates these parameters, who chooses the parameters and specifies the relevance of words? If it's created automatically, how is it done?</strong></p>\n"
  },
  {
    "tags": [
      "open-ai",
      "chatgpt"
    ],
    "owner": {
      "account_id": 27127305,
      "reputation": 389,
      "user_id": 77392,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2b42ab6e24f8294aba29c8440f7e8384?s=256&d=identicon&r=PG",
      "display_name": "TylerDurden",
      "link": "https://ai.stackexchange.com/users/77392/tylerdurden"
    },
    "is_answered": true,
    "view_count": 15273,
    "accepted_answer_id": 42541,
    "answer_count": 4,
    "score": 27,
    "last_activity_date": 1701663606,
    "creation_date": 1698179444,
    "last_edit_date": 1698262427,
    "question_id": 42539,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/42539/why-do-chatgpt-jailbreaks-work",
    "title": "Why do ChatGPT “jailbreaks” work?",
    "body": "<p>Do developers not genuinely want to prevent them? It seems like if they are able to develop such impressive AI models then it wouldn’t be that difficult to create catch-all/wildcard mitigations to the various jailbreak methods that are devised over time.</p>\n<p>What gives? (<em>i.e.,</em> what is so difficult about plugging these holes, or why might  the developers not really want to do so?)</p>\n"
  },
  {
    "tags": [
      "neural-networks"
    ],
    "owner": {
      "account_id": 2738792,
      "reputation": 1097,
      "user_id": 7402,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/72bd5e3f73d1cd1774d2235b6bfab11d?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Blaszard",
      "link": "https://ai.stackexchange.com/users/7402/blaszard"
    },
    "is_answered": true,
    "view_count": 17278,
    "accepted_answer_id": 3803,
    "answer_count": 4,
    "score": 26,
    "last_activity_date": 1617343318,
    "creation_date": 1502429469,
    "last_edit_date": 1503333359,
    "question_id": 3801,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/3801/what-is-a-dynamic-computational-graph",
    "title": "What is a Dynamic Computational Graph?",
    "body": "<p>Frameworks like <a href=\"http://pytorch.org\" rel=\"noreferrer\">PyTorch</a> and TensorFlow through <a href=\"https://research.googleblog.com/2017/02/announcing-tensorflow-fold-deep.html\" rel=\"noreferrer\">TensorFlow Fold</a> support Dynamic Computational Graphs and are receiving attention from data scientists.</p>\n\n<p>However, there seems to be a lack of resource to aid in understanding Dynamic Computational Graphs.</p>\n\n<p>The advantage of Dynamic Computational Graphs appears to include the ability to adapt to a varying quantities in input data.  It seems like there may be automatic selection of the number of layers, the number of neurons in each layer, the activation function, and other NN parameters, depending on each input set instance during the training.  Is this an accurate characterization?</p>\n\n<p>What are the advantages of dynamic models over static models?  Is that why DCGs are receiving much attention?  In summary, what are DCGs and what are the pros and cons their use?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "comparison",
      "long-short-term-memory",
      "sequence-modeling",
      "transformer"
    ],
    "owner": {
      "account_id": 13356282,
      "reputation": 363,
      "user_id": 25859,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s256-rj/photo.jpg",
      "display_name": "shiredude95",
      "link": "https://ai.stackexchange.com/users/25859/shiredude95"
    },
    "is_answered": true,
    "view_count": 10171,
    "accepted_answer_id": 12502,
    "answer_count": 4,
    "score": 26,
    "last_activity_date": 1702658338,
    "creation_date": 1558625802,
    "last_edit_date": 1572577332,
    "question_id": 12490,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12490/can-the-decoder-in-a-transformer-model-be-parallelized-like-the-encoder",
    "title": "Can the decoder in a transformer model be parallelized like the encoder?",
    "body": "<p>Can the decoder in a transformer model be parallelized like the encoder?</p>\n\n<p>As far as I understand, the encoder has all the tokens in the sequence to compute the self-attention scores. But for a decoder, this is not possible (in both training and testing), as self-attention is calculated based on previous timestep outputs. Even if we consider some techniques, like teacher forcing, where we are concatenating expected output with obtained, this still has a sequential input from the previous timestep. </p>\n\n<p>In this case, apart from the improvement in capturing long-term dependencies, is using a transformer-decoder better than say an LSTM, when comparing purely on the basis of parallelization?</p>\n"
  },
  {
    "tags": [
      "emotional-intelligence",
      "turing-test",
      "affective-computing"
    ],
    "owner": {
      "account_id": 40019,
      "reputation": 2533,
      "user_id": 55,
      "user_type": "registered",
      "accept_rate": 43,
      "profile_image": "https://i.sstatic.net/FkIiK.png?s=256",
      "display_name": "WilliamKF",
      "link": "https://ai.stackexchange.com/users/55/williamkf"
    },
    "is_answered": true,
    "view_count": 1554,
    "accepted_answer_id": 189,
    "answer_count": 4,
    "score": 25,
    "last_activity_date": 1612141403,
    "creation_date": 1470153511,
    "last_edit_date": 1583637450,
    "question_id": 26,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/26/how-could-emotional-intelligence-be-implemented",
    "title": "How could emotional intelligence be implemented?",
    "body": "<p>I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically.  </p>\n\n<ol>\n<li><p>What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers?  </p></li>\n<li><p>Are there examples where this is already happening to a degree today?  </p></li>\n<li><p>Wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer?  </p>\n\n<p>Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence.</p></li>\n</ol>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "image-recognition",
      "convolutional-neural-networks"
    ],
    "owner": {
      "account_id": 6490650,
      "reputation": 403,
      "user_id": 6382,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6ae9251dc908d5ccfd3474fe56276988?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "WaterRocket8236",
      "link": "https://ai.stackexchange.com/users/6382/waterrocket8236"
    },
    "is_answered": true,
    "view_count": 49272,
    "accepted_answer_id": 4191,
    "answer_count": 3,
    "score": 25,
    "last_activity_date": 1634962864,
    "creation_date": 1504177475,
    "last_edit_date": 1590191194,
    "question_id": 3938,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3938/how-do-i-handle-large-images-when-training-a-cnn",
    "title": "How do I handle large images when training a CNN?",
    "body": "<p>Suppose that I have 10K images of sizes <span class=\"math-container\">$2400 \\times 2400$</span> to train a CNN. </p>\n\n<p>How do I handle such large image sizes without downsampling?</p>\n\n<p>Here are a few more specific questions.</p>\n\n<ol>\n<li><p>Are there any techniques to handle such large images which are to be trained?</p></li>\n<li><p>What batch size is reasonable to use?</p></li>\n<li><p>Are there any precautions to take, or any increase and decrease in hardware resources that I can do?</p></li>\n</ol>\n\n<p>Here are the system requirements</p>\n\n<pre><code>Ubuntu 16.04 64-bit \nRAM 16 GB\nGPU 8 GB\nHDD 500 GB\n</code></pre>\n"
  },
  {
    "tags": [
      "terminology",
      "definitions",
      "history",
      "academia",
      "ai-field"
    ],
    "owner": {
      "account_id": 14291171,
      "reputation": 379,
      "user_id": 17948,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2219f0ae5ac61d6c3670d47c5a6a21df?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Rana Wasif",
      "link": "https://ai.stackexchange.com/users/17948/rana-wasif"
    },
    "is_answered": true,
    "view_count": 2360,
    "protected_date": 1558980718,
    "answer_count": 8,
    "score": 25,
    "last_activity_date": 1679550505,
    "creation_date": 1536201656,
    "last_edit_date": 1610755273,
    "question_id": 7838,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7838/what-is-artificial-intelligence",
    "title": "What is artificial intelligence?",
    "body": "<p>What is the definition of artificial intelligence?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "training",
      "stochastic-gradient-descent",
      "batch-size"
    ],
    "owner": {
      "account_id": 9585668,
      "reputation": 401,
      "user_id": 11814,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/fnNPT.png?s=256",
      "display_name": "Sebastian Nielsen",
      "link": "https://ai.stackexchange.com/users/11814/sebastian-nielsen"
    },
    "is_answered": true,
    "view_count": 95051,
    "answer_count": 3,
    "score": 25,
    "last_activity_date": 1697353484,
    "creation_date": 1540141744,
    "last_edit_date": 1604138858,
    "question_id": 8560,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/8560/how-do-i-choose-the-optimal-batch-size",
    "title": "How do I choose the optimal batch size?",
    "body": "<blockquote>\n<p>Batch size is a term used in machine learning and refers to the number of training examples utilised in one iteration. The batch size\ncan be one of three options:</p>\n<ol>\n<li><strong>batch mode</strong>: where the batch size is equal to the total dataset thus making the iteration and epoch values equivalent</li>\n<li><strong>mini-batch mode</strong>: where the batch size is greater than one but less than the total dataset size. Usually, a number that can be divided into the total dataset size.</li>\n<li><strong>stochastic mode</strong>: where the batch size is equal to one. Therefore the gradient and the neural network parameters are updated after each sample.</li>\n</ol>\n</blockquote>\n<p>How do I choose the optimal batch size, for a given task, neural network or optimization problem?</p>\n<p>If you hypothetically didn't have to worry about computational issues, what would the optimal batch size be?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "reference-request",
      "deep-rl",
      "function-approximation",
      "action-spaces"
    ],
    "owner": {
      "account_id": 7699707,
      "reputation": 351,
      "user_id": 20627,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-4V9_WPdjLHI/AAAAAAAAAAI/AAAAAAAAAA8/REERYNsImeU/s256-rj/photo.jpg",
      "display_name": "Rikard Olsson",
      "link": "https://ai.stackexchange.com/users/20627/rikard-olsson"
    },
    "is_answered": true,
    "view_count": 12889,
    "protected_date": 1589226317,
    "answer_count": 2,
    "score": 25,
    "last_activity_date": 1640791846,
    "creation_date": 1544621278,
    "last_edit_date": 1640791738,
    "question_id": 9491,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/9491/are-there-other-approaches-to-deal-with-variable-action-spaces",
    "title": "Are there other approaches to deal with variable action spaces?",
    "body": "<p>This question is about <strong>Reinforcement Learning</strong> and <strong>variable action spaces for every/some states</strong>.</p>\n<h3>Variable action space</h3>\n<p>Let's say you have an MDP, where the number of actions varies between states (for example like in Figure 1 or Figure 2). We can express a variable action space formally as <span class=\"math-container\">$$\\forall s \\in S: \\exists s' \\in S: A(s) \\neq A(s') \\wedge s \\neq s'$$</span></p>\n<p>That is, for every state, there exists some other state which does not have the same action set.</p>\n<p>In figures 1 and 2, there's a relatively small amount of actions per state. Instead imagine states <span class=\"math-container\">$s \\in S$</span> with <span class=\"math-container\">$m_s$</span> number of actions, where <span class=\"math-container\">$1 \\leq m_s \\leq n$</span> and <span class=\"math-container\">$n$</span> is a really large integer.</p>\n<p><a href=\"https://i.sstatic.net/q2Huu.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/q2Huu.png\" alt=\"Some MDP's\" /></a></p>\n<h3>Environment</h3>\n<p>To get a better grasp of the question, here's an environment example. Take Figure 1 and let it explode into a really large directed acyclic graph with a source node, huge action space and a target node. The goal is to traverse a path, starting at any start node, such that we'll maximize the reward which we'll only receive at the target node. At every state, we can call a function <span class=\"math-container\">$M : s \\rightarrow A'$</span> that takes a state as input and returns a valid number of actions.</p>\n<h3>Approches</h3>\n<ol>\n<li><p>A naive approach to this problem (discussed <a href=\"https://ai.stackexchange.com/q/7755/2444\">here</a> and <a href=\"https://ai.stackexchange.com/q/2219/2444\">here</a>) is to define the action set equally for every state, return a negative reward whenever the performed action <span class=\"math-container\">$a \\notin A(s)$</span> and move the agent into the same state, thus letting the agent &quot;learn&quot; what actions are valid in each state. This approach has two obvious drawbacks:</p>\n<ul>\n<li><p>Learning <span class=\"math-container\">$A$</span> takes time, especially when the Q-values are not updated until either termination or some statement is fulfilled (like in <a href=\"https://datascience.stackexchange.com/q/20535/10640\">experience replay</a>)</p>\n</li>\n<li><p>We know <span class=\"math-container\">$A$</span>, why learn it?</p>\n</li>\n</ul>\n</li>\n<li><p>Another approach (<a href=\"https://ai.stackexchange.com/a/8564/2444\">first answer here</a>, also very much alike proposals from papers such as <a href=\"https://arxiv.org/pdf/1512.07679\" rel=\"noreferrer\">Deep Reinforcement Learning in Large Discrete Action Spaces</a> and <a href=\"https://arxiv.org/pdf/1705.05035\" rel=\"noreferrer\">Discrete Sequential Prediction of continuous action for Deep RL</a>) is to instead predict some scalar in continuous space and, by some method, map it into a valid action. The papers are discussing how to deal with large discrete action spaces and the proposed models seem to be a somewhat solution for this problem as well.</p>\n</li>\n<li><p>Another approach that came across was to, assuming the number of different action set <span class=\"math-container\">$n$</span> is quite small, have functions <span class=\"math-container\">$f_{\\theta_1}$</span>, <span class=\"math-container\">$f_{\\theta_2}$</span>, ..., <span class=\"math-container\">$f_{\\theta_n}$</span> that returns the action regarding that perticular state with <span class=\"math-container\">$n$</span> valid actions. In other words, the performed action of a state <span class=\"math-container\">$s$</span> with 3 number of actions will be predicted by <span class=\"math-container\">$\\underset{a}{\\text{argmax}} \\ f_{\\theta_3}(s, a)$</span>.</p>\n</li>\n</ol>\n<p>None of the approaches (1, 2 or 3) are found in papers, just pure speculations. I've searched a lot, but I cannot find papers directly regarding this matter.</p>\n<p><strong>Does anyone know any paper regarding this subject? Are there other approaches to deal with variable action spaces?</strong></p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "comparison",
      "monte-carlo-methods",
      "policy-evaluation"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user9947"
    },
    "is_answered": true,
    "view_count": 25641,
    "protected_date": 1640196534,
    "accepted_answer_id": 10818,
    "answer_count": 2,
    "score": 25,
    "last_activity_date": 1640196507,
    "creation_date": 1550827704,
    "last_edit_date": 1640196507,
    "question_id": 10812,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10812/what-is-the-difference-between-first-visit-monte-carlo-and-every-visit-monte-car",
    "title": "What is the difference between First-Visit Monte-Carlo and Every-Visit Monte-Carlo Policy Evaluation?",
    "body": "<p>I came across these 2 algorithms, but I cannot understand the difference between these 2, both in terms of implementation as well as intuitionally.</p>\n<p>So, what difference does the second point in both the slides refer to?</p>\n<p><a href=\"https://i.sstatic.net/kHuKM.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/kHuKM.png\" alt=\"enter image description here\" /></a></p>\n<p><a href=\"https://i.sstatic.net/dOhO5.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/dOhO5.png\" alt=\"enter image description here\" /></a></p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "definitions",
      "geometric-deep-learning",
      "graph-neural-networks",
      "non-euclidean-data"
    ],
    "owner": {
      "account_id": 4148701,
      "reputation": 42397,
      "user_id": 2444,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e6fce047cba96a601a9dac66df4cd2e8?s=256&d=identicon&r=PG",
      "display_name": "nbro",
      "link": "https://ai.stackexchange.com/users/2444/nbro"
    },
    "is_answered": true,
    "view_count": 23328,
    "accepted_answer_id": 20628,
    "answer_count": 5,
    "score": 25,
    "last_activity_date": 1650141782,
    "creation_date": 1552558851,
    "last_edit_date": 1628949077,
    "question_id": 11226,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11226/what-is-non-euclidean-data",
    "title": "What is non-Euclidean data?",
    "body": "<p>What is non-Euclidean data?</p>\n<p>Here are some sub-questions</p>\n<ul>\n<li><p>Where does this type of data arise? I have come across this term in the context of geometric deep learning and graph neural networks.</p>\n</li>\n<li><p>Apparently, graphs and manifolds are non-Euclidean data. Why exactly is that the case?</p>\n</li>\n<li><p>What is the difference between non-Euclidean and Euclidean data?</p>\n</li>\n<li><p>How would a dataset of non-Euclidean data look like?</p>\n</li>\n</ul>\n"
  },
  {
    "tags": [
      "deep-learning",
      "research",
      "papers"
    ],
    "owner": {
      "account_id": 7398111,
      "reputation": 669,
      "user_id": 36083,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/QNYYVbnZ.jpg?s=256",
      "display_name": "Gilad Deutsch",
      "link": "https://ai.stackexchange.com/users/36083/gilad-deutsch"
    },
    "is_answered": true,
    "view_count": 5280,
    "answer_count": 3,
    "score": 25,
    "last_activity_date": 1642117879,
    "creation_date": 1589465996,
    "last_edit_date": 1589755370,
    "question_id": 21159,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/21159/why-do-most-deep-learning-papers-not-include-an-implementation",
    "title": "Why do most deep learning papers not include an implementation?",
    "body": "<p>I'm a novice researcher, and as I started to read papers in the area of deep learning I noticed that the implementation is normally not added and is needed to be searched elsewhere, and my question is how come that's the case? The paper's authors needed to implement their models anyway in order to conduct their experimentations, so why not publish the implementation? Plus, if the implementation is not added and there's no reproducibility, what prevents authors from forging results?</p>\n"
  },
  {
    "tags": [
      "math",
      "chat-bots",
      "natural-language-understanding",
      "language-model",
      "chatgpt"
    ],
    "owner": {
      "account_id": 9914032,
      "reputation": 624,
      "user_id": 23811,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-6jIAe62Q10s/AAAAAAAAAAI/AAAAAAAAABw/kRBVixTmLmk/s256-rj/photo.jpg",
      "display_name": "Peyman",
      "link": "https://ai.stackexchange.com/users/23811/peyman"
    },
    "is_answered": true,
    "view_count": 14671,
    "answer_count": 4,
    "score": 25,
    "last_activity_date": 1684334346,
    "creation_date": 1670525004,
    "last_edit_date": 1671218366,
    "question_id": 38217,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/38217/how-does-chatgpt-know-math",
    "title": "How does ChatGPT know math?",
    "body": "<p>ChatGPT is a language model. As far as I know and If I'm not wrong, it gets text as tokens and word embeddings. So, how can it do math? For example, I asked:</p>\n<blockquote>\n<p>ME: Which one is bigger 5 or 9. <br />\nChatGPT: In this case, 9 is larger than 5.</p>\n</blockquote>\n<p>One can say, GPT saw numbers as tokens and in its training dataset there were some 9s that were bigger than 5s. So, it doesn't have actual math understanding and just sees numbers as some tokens. But I don't think that is true, because of this question:</p>\n<blockquote>\n<p>ME: Which one is bigger? 15648.25 or 9854.2547896 <br />\nChatGPT: In this case, 15648.25 is larger than 9854.2547896.</p>\n</blockquote>\n<p>We can't say it actually saw the token of <code>15648.25</code> to be bigger than the token of <code>9854.2547896</code> in its dataset!</p>\n<p>So how does this language model understand the numbers?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "definitions",
      "optical-character-recognition"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 7392,
    "accepted_answer_id": 1406,
    "answer_count": 3,
    "score": 24,
    "last_activity_date": 1583646909,
    "creation_date": 1470448663,
    "last_edit_date": 1583646909,
    "question_id": 1396,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1396/why-cant-ocr-be-perceived-as-a-good-example-of-ai",
    "title": "Why can&#39;t OCR be perceived as a good example of AI?",
    "body": "<p>On <a href=\"https://en.wikipedia.org/wiki/Artificial_intelligence\" rel=\"noreferrer\">the Wikipedia page</a> about AI, we can read:</p>\n\n<blockquote>\n  <p>Optical character recognition is no longer perceived as an exemplar of \"artificial intelligence\" having become a routine technology.</p>\n</blockquote>\n\n<p>On the other hand, the <a href=\"https://en.wikipedia.org/wiki/MNIST_database\" rel=\"noreferrer\">MNIST</a> database of handwritten digits is specially designed for training and testing neural networks and their error rates (see: <a href=\"https://en.wikipedia.org/wiki/MNIST_database#Classifiers\" rel=\"noreferrer\">Classifiers</a>).</p>\n\n<p>So, why does the above quote state that OCR is no longer exemple of AI?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "ai-design"
    ],
    "owner": {
      "account_id": 3654099,
      "reputation": 349,
      "user_id": 2420,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/SwJPH.png?s=256",
      "display_name": "Techidiot",
      "link": "https://ai.stackexchange.com/users/2420/techidiot"
    },
    "is_answered": true,
    "view_count": 1097,
    "answer_count": 2,
    "score": 24,
    "last_activity_date": 1561064096,
    "creation_date": 1473848211,
    "last_edit_date": 1561064096,
    "question_id": 1963,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1963/are-there-any-ongoing-projects-which-use-the-stack-exchange-for-machine-learning",
    "title": "Are there any ongoing projects which use the Stack Exchange for machine learning?",
    "body": "<p>Are there any ongoing AI projects which use the Stack Exchange for machine learning?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "activation-functions",
      "hyperparameter-optimization",
      "hyper-parameters"
    ],
    "owner": {
      "account_id": 5968374,
      "reputation": 706,
      "user_id": 16199,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/9bc16e3ccc6a7f57f4dd0edf83ca5913?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "gvgramazio",
      "link": "https://ai.stackexchange.com/users/16199/gvgramazio"
    },
    "is_answered": true,
    "view_count": 12951,
    "accepted_answer_id": 7089,
    "answer_count": 3,
    "score": 24,
    "last_activity_date": 1619366583,
    "creation_date": 1531094817,
    "last_edit_date": 1619366583,
    "question_id": 7088,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7088/how-to-choose-an-activation-function-for-the-hidden-layers",
    "title": "How to choose an activation function for the hidden layers?",
    "body": "<p>I choose the activation function for the output layer depending on the output that I need and the properties of the activation function that I know. For example, I choose the <em>sigmoid</em> function when I'm dealing with probabilities, a <em>ReLU</em> when I'm dealing with positive values, and a linear function when I'm dealing with general values.</p>\n<p>In hidden layers, I use a <em>leaky ReLU</em> to avoid dead neurons instead of the ReLU, and the <em>tanh</em> instead of the <em>sigmoid</em>. Of course, I don't use a linear function in hidden units.</p>\n<p>However, the choice for them in the hidden layer is mostly due to trial and error.</p>\n<p>Is there any rule of thumb of which activation function is likely to work well in some situations?</p>\n<p>Take the term <em>situations</em> as general as possible: it could be referring to the depth of the layer, to the depth of the NN, to the number of neurons for that layer, to the optimizer that we chose, to the number of input features of that layer, to the application of this NN, etc.</p>\n<p>The more activation functions I discover the more I'm confused in the choice of the function to use in hidden layers. I don't think that flipping a coin is a good way of choosing an activation function.</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "comparison",
      "terminology",
      "sutton-barto",
      "control-theory"
    ],
    "owner": {
      "account_id": 15553941,
      "reputation": 383,
      "user_id": 23276,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/A0Sis.jpg?s=256",
      "display_name": "Bionic Buffulo",
      "link": "https://ai.stackexchange.com/users/23276/bionic-buffulo"
    },
    "is_answered": true,
    "view_count": 15041,
    "protected_date": 1628555876,
    "accepted_answer_id": 23427,
    "answer_count": 2,
    "score": 24,
    "last_activity_date": 1613160203,
    "creation_date": 1553252277,
    "last_edit_date": 1613160203,
    "question_id": 11375,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11375/what-is-the-difference-between-reinforcement-learning-and-optimal-control",
    "title": "What is the difference between reinforcement learning and optimal control?",
    "body": "<p>Coming from a process (optimal) control background, I have begun studying the field of deep reinforcement learning.</p>\n<p>Sutton &amp; Barto (2015) state that</p>\n<blockquote>\n<p>particularly important (to the writing of the text) have been the contributions establishing and developing the relationships to the theory of optimal control and dynamic programming</p>\n</blockquote>\n<p>With an emphasis on the elements of reinforcement learning - that is, policy, agent, environment, etc., what are the key differences between (deep) RL and optimal control theory?</p>\n<p>In optimal control we have, controllers, sensors, actuators, plants, etc, as elements. Are these different names for similar elements in deep RL? For example, would an optimal control plant be called an environment in deep RL?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "ai-design",
      "control-theory"
    ],
    "owner": {
      "account_id": 9708483,
      "reputation": 541,
      "user_id": 22424,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/sAKz6.jpg?s=256",
      "display_name": "SeeDerekEngineer",
      "link": "https://ai.stackexchange.com/users/22424/seederekengineer"
    },
    "is_answered": true,
    "view_count": 8062,
    "accepted_answer_id": 12480,
    "answer_count": 1,
    "score": 24,
    "last_activity_date": 1558569404,
    "creation_date": 1558538677,
    "last_edit_date": 1558544559,
    "question_id": 12472,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12472/when-should-i-use-reinforcement-learning-vs-pid-control",
    "title": "When should I use Reinforcement Learning vs PID Control?",
    "body": "<p>When designing solutions to problems such as the Lunar Lander on <a href=\"https://gym.openai.com/envs/LunarLander-v2/\" rel=\"noreferrer\">OpenAIGym</a>, Reinforcement Learning is a tempting means of giving the agent adequate action control so as to successfully land.  </p>\n\n<p>But what are the instances in which control system algorithms, such as <a href=\"https://en.wikipedia.org/wiki/PID_controller\" rel=\"noreferrer\">PID controllers</a>, would do just an adequate job as, if not better than, Reinforcement Learning?</p>\n\n<p>Questions such as <a href=\"https://ai.stackexchange.com/questions/11375/what-is-the-difference-between-reinforcement-learning-and-optimal-control\">this one</a> do a great job at addressing the theory of this question, but do little to address the practical component.</p>\n\n<p>As an Artificial Intelligence engineer, what elements of a problem domain should suggest to me that a PID controller is insufficient to solve a problem, and a Reinforcement Learning algorithm should instead be used (or vice versa)?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "chatgpt",
      "benchmarks"
    ],
    "owner": {
      "account_id": 150314,
      "reputation": 931,
      "user_id": 25362,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/tVWLM.png?s=256",
      "display_name": "Hans-Peter Stricker",
      "link": "https://ai.stackexchange.com/users/25362/hans-peter-stricker"
    },
    "is_answered": true,
    "view_count": 12549,
    "answer_count": 5,
    "score": 24,
    "last_activity_date": 1706735430,
    "creation_date": 1684935179,
    "last_edit_date": 1685035512,
    "question_id": 40564,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/40564/why-does-chatgpt-fail-in-playing-20-questions",
    "title": "Why does ChatGPT fail in playing &quot;20 questions&quot;?",
    "body": "<p><a href=\"https://en.wikipedia.org/wiki/IBM_Watson\" rel=\"noreferrer\">IBM Watson</a>'s success in playing <a href=\"https://en.wikipedia.org/wiki/IBM_Watson#Jeopardy!\" rel=\"noreferrer\">&quot;Jeopardy!&quot;</a> was a landmark in the history of artificial intelligence. In the seemingly simpler game of <a href=\"https://en.wikipedia.org/wiki/Twenty_questions\" rel=\"noreferrer\">&quot;Twenty questions&quot;</a> where player B has to guess a word that player A thinks of by asking questions to be answered by &quot;Yes/No/Hm&quot; ChatGPT fails epically - at least in my personal opinion. I thought first of Chartres cathedral and it took ChatGPT 41 questions to get it (with some additional help), and then of Kant's Critique of Pure Reason where after question #30 I had to explicitly tell ChatGPT that it's a book. Then it took ten further questions. (Chat protocols can be provided. It may be seen that ChatGPT follows no or bad question policies or heuristics humans intuitively would use.)</p>\n<p>My questions are:</p>\n<ol>\n<li><p>Is there an intuitive understanding why ChatGPT plays &quot;20 questions&quot; so bad?</p>\n</li>\n<li><p>And why do even average humans play it so much better?</p>\n</li>\n<li><p>Might it be a future <a href=\"https://openreview.net/pdf?id=yzkSU5zdwD#:%7E\" rel=\"noreferrer\">emergent ability</a> which may possibly arise in ever larger LLMs?</p>\n</li>\n</ol>\n<p>I found two interesting papers on the topic</p>\n<ol>\n<li><p><a href=\"https://evanthebouncy.medium.com/llm-self-play-on-20-questions-dee7a8c63377\" rel=\"noreferrer\">LLM self-play on 20 Questions</a></p>\n</li>\n<li><p><a href=\"https://arxiv.org/ftp/arxiv/papers/2301/2301.01743.pdf\" rel=\"noreferrer\">Chatbots As Problem Solvers: Playing Twenty Questions With Role Reversals</a></p>\n</li>\n</ol>\n<p>The first one answers some of my questions partially, e.g. that &quot;gpt-3.5-turbo has a score of 68/1823 playing 20 questions with itself&quot; which sounds pretty low.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "comparison"
    ],
    "owner": {
      "account_id": 7639415,
      "reputation": 333,
      "user_id": 1727,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/5e97e979d8e0a0fae9aad184f5dae95e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Addis",
      "link": "https://ai.stackexchange.com/users/1727/addis"
    },
    "is_answered": true,
    "view_count": 4069,
    "protected_date": 1559169845,
    "accepted_answer_id": 1743,
    "answer_count": 5,
    "score": 23,
    "last_activity_date": 1594176159,
    "creation_date": 1472163550,
    "last_edit_date": 1561382931,
    "question_id": 1742,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1742/what-is-the-difference-between-machine-learning-and-deep-learning",
    "title": "What is the difference between machine learning and deep learning?",
    "body": "<p>Can someone explain to me the difference between machine learning and deep learning? Is it possible to learn deep learning without knowing machine learning?</p>\n"
  },
  {
    "tags": [
      "htm"
    ],
    "owner": {
      "account_id": 5416059,
      "reputation": 4265,
      "user_id": 2227,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://www.gravatar.com/avatar/af11dfed2f1c4e002c6a71cea4a7cfab?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "BlindKungFuMaster",
      "link": "https://ai.stackexchange.com/users/2227/blindkungfumaster"
    },
    "is_answered": true,
    "view_count": 1637,
    "accepted_answer_id": 2006,
    "answer_count": 2,
    "score": 23,
    "last_activity_date": 1598947894,
    "creation_date": 1474463156,
    "last_edit_date": 1598947894,
    "question_id": 2005,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2005/what-are-the-flaws-in-jeff-hawkinss-ai-framework",
    "title": "What are the flaws in Jeff Hawkins&#39;s AI framework?",
    "body": "<p>In 2004 <a href=\"https://en.wikipedia.org/wiki/Jeff_Hawkins\" rel=\"nofollow noreferrer\">Jeff Hawkins</a>, inventor of the palm pilot, published a very interesting book called <a href=\"https://en.wikipedia.org/wiki/On_Intelligence\" rel=\"nofollow noreferrer\">On Intelligence</a>, in which he details a theory how the human neocortex works.</p>\n<p>This theory is called <a href=\"https://en.wikipedia.org/wiki/Memory-prediction_framework\" rel=\"nofollow noreferrer\">Memory-Prediction framework</a> and it has some striking features, for example not only bottom-up (feedforward), but also top-down information processing and the ability to make simultaneous, but discrete predictions of different future scenarios (as described <a href=\"http://journal.frontiersin.org/article/10.3389/fncir.2016.00023/full\" rel=\"nofollow noreferrer\">in this paper</a>).</p>\n<p>The promise of the Memory-Prediction framework is unsupervised generation of stable high level representations of future possibilities. Something which would revolutionise probably a whole bunch of AI research areas.</p>\n<p>Hawkins founded <a href=\"https://en.wikipedia.org/wiki/Numenta\" rel=\"nofollow noreferrer\">a company</a> and proceeded to implement his ideas. Unfortunately more than ten years later the promise of his ideas is still unfulfilled. So far the implementation is only used for anomaly detection, which is kind of the opposite of what you really want to do. Instead of extracting the understanding, you'll extract the instances which your artificial cortex doesn't understand.</p>\n<p>My question is in what way Hawkins's framework falls short. What are the concrete or conceptual problems that so far prevent his theory from working in practice?</p>\n"
  },
  {
    "tags": [
      "activation-functions",
      "neuroscience",
      "brain"
    ],
    "owner": {
      "account_id": 13234817,
      "reputation": 341,
      "user_id": 15107,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ab92a1b5d3d7e64c51e83792a7de003b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "mlman",
      "link": "https://ai.stackexchange.com/users/15107/mlman"
    },
    "is_answered": true,
    "view_count": 6902,
    "accepted_answer_id": 6104,
    "answer_count": 6,
    "score": 23,
    "last_activity_date": 1720107067,
    "creation_date": 1524047773,
    "last_edit_date": 1640196642,
    "question_id": 6099,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6099/what-activation-function-does-the-human-brain-use",
    "title": "What activation function does the human brain use?",
    "body": "<p>Does the human brain use a specific activation function?</p>\n<p>I've tried doing some research, and as it's a threshold for whether the signal is sent through a neuron or not, it sounds a lot like ReLU. However, I can't find a single article confirming this. Or is it more like a step function (it sends 1 if it's above the threshold, instead of the input value)?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "activation-functions",
      "relu"
    ],
    "owner": {
      "account_id": 5968374,
      "reputation": 706,
      "user_id": 16199,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/9bc16e3ccc6a7f57f4dd0edf83ca5913?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "gvgramazio",
      "link": "https://ai.stackexchange.com/users/16199/gvgramazio"
    },
    "is_answered": true,
    "view_count": 29593,
    "answer_count": 1,
    "score": 23,
    "last_activity_date": 1589884676,
    "creation_date": 1532436462,
    "last_edit_date": 1532462603,
    "question_id": 7274,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7274/what-are-the-advantages-of-relu-vs-leaky-relu-and-parametric-relu-if-any",
    "title": "What are the advantages of ReLU vs Leaky ReLU and Parametric ReLU (if any)?",
    "body": "<p>I think that the advantage of using Leaky ReLU instead of ReLU is that in this way we cannot have vanishing gradient. Parametric ReLU has the same advantage with the only difference that the slope of the output for negative inputs is a learnable parameter while in the Leaky ReLU it's a hyperparameter.</p>\n\n<p>However, I'm not able to tell if there are cases where is more convenient to use ReLU instead of Leaky ReLU or Parametric ReLU.</p>\n"
  },
  {
    "tags": [
      "recurrent-neural-networks",
      "long-short-term-memory",
      "reference-request",
      "papers"
    ],
    "owner": {
      "account_id": 4641636,
      "reputation": 331,
      "user_id": 18649,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/7qKk0.jpg?s=256",
      "display_name": "Ahsan Tarique",
      "link": "https://ai.stackexchange.com/users/18649/ahsan-tarique"
    },
    "is_answered": true,
    "view_count": 35618,
    "answer_count": 4,
    "score": 23,
    "last_activity_date": 1629210257,
    "creation_date": 1538333745,
    "last_edit_date": 1611011968,
    "question_id": 8190,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/8190/where-can-i-find-the-original-paper-that-introduced-rnns",
    "title": "Where can I find the original paper that introduced RNNs?",
    "body": "<p>I was able to find <a href=\"https://www.bioinf.jku.at/publications/older/2604.pdf\" rel=\"noreferrer\">the original paper on LSTM</a>, but I was not able to find the paper that introduced &quot;vanilla&quot; RNNs. Where can I find it?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "transformer",
      "attention",
      "bert",
      "gpt"
    ],
    "owner": {
      "account_id": 3625986,
      "reputation": 381,
      "user_id": 37519,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/afdb576792ee07d70d7d39d77a8cb1d0?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Athena Wisdom",
      "link": "https://ai.stackexchange.com/users/37519/athena-wisdom"
    },
    "is_answered": true,
    "view_count": 14527,
    "answer_count": 2,
    "score": 23,
    "last_activity_date": 1674154259,
    "creation_date": 1616874930,
    "last_edit_date": 1616878355,
    "question_id": 27038,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/27038/why-does-gpt-2-exclude-the-transformer-encoder",
    "title": "Why does GPT-2 Exclude the Transformer Encoder?",
    "body": "<p>After looking into transformers, BERT, and GPT-2, from what I understand, GPT-2 essentially uses only the decoder part of the original transformer architecture and uses masked self-attention that can only look at prior tokens.</p>\n<p>Why does GPT-2 not require the encoder part of the original transformer architecture?</p>\n<p><strong>GPT-2 architecture with only decoder layers</strong></p>\n<p><a href=\"https://i.sstatic.net/Kb8Gq.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/Kb8Gq.png\" alt=\"enter image description here\" /></a></p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "papers",
      "hidden-layers",
      "batch-normalization"
    ],
    "owner": {
      "account_id": 215520,
      "reputation": 2602,
      "user_id": 20538,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d48d658287ec6bb32b8da3f2479b2aee?s=256&d=identicon&r=PG",
      "display_name": "Kostya",
      "link": "https://ai.stackexchange.com/users/20538/kostya"
    },
    "is_answered": true,
    "view_count": 4218,
    "answer_count": 5,
    "score": 23,
    "last_activity_date": 1648965311,
    "creation_date": 1618095951,
    "last_edit_date": 1637589243,
    "question_id": 27260,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/27260/why-does-batch-normalization-work",
    "title": "Why does Batch Normalization work?",
    "body": "<p>Adding <a href=\"https://en.wikipedia.org/wiki/Batch_normalization\" rel=\"noreferrer\">BatchNorm</a> layers improves training time and makes the whole deep model more stable.  That's an experimental fact that is widely used in machine learning practice.</p>\n<p>My question is - why does it work?</p>\n<p>The <a href=\"https://arxiv.org/abs/1502.03167\" rel=\"noreferrer\">original (2015) paper</a> motivated the introduction of the layers by stating that these layers help fixing &quot;<em>internal covariate shift</em>&quot;. The rough idea is that large shifts in the distributions of inputs of inner layers makes training less stable, leading to a decrease in the learning rate and slowing down of the training.  Batch normalization mitigates this problem by standardizing the inputs of inner layers.</p>\n<p>This explanation was harshly criticized <a href=\"https://arxiv.org/abs/1805.11604\" rel=\"noreferrer\">by the next (2018) paper</a> -- quoting the abstract:</p>\n<blockquote>\n<p>... distributional stability of layer inputs has little to do with the success of BatchNorm</p>\n</blockquote>\n<p>They demonstrate that BatchNorm only slightly affects the inner layer inputs distributions.  More than that -- they tried to <em>inject</em> some non-zero mean/variance noise into the distributions. And they still got almost the same performance.</p>\n<p>Their conclusion was that the real reason BatchNorm works was that...</p>\n<blockquote>\n<p>Instead BatchNorm makes the optimization landscape significantly smoother.</p>\n</blockquote>\n<p>Which, to my taste, is slightly tautological to saying that it improves stability.</p>\n<p>I've found two more papers trying to tackle the question: In <a href=\"https://arxiv.org/abs/2002.10444\" rel=\"noreferrer\">this paper</a> the &quot;key benefit&quot; is claimed to be the fact that Batch Normalization biases residual blocks towards the identity function. And in <a href=\"https://arxiv.org/abs/2003.01652\" rel=\"noreferrer\">this paper</a> that it &quot;avoids rank collapse&quot;.</p>\n<p>So, is there any bottom line? Why does BatchNorm work?</p>\n"
  },
  {
    "tags": [
      "social",
      "ethics",
      "chatgpt",
      "green-ai"
    ],
    "owner": {
      "account_id": 539911,
      "reputation": 333,
      "user_id": 67552,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/83b2cb1d707d143cd9aa3c55e10437c6?s=256&d=identicon&r=PG",
      "display_name": "wide_eyed_pupil",
      "link": "https://ai.stackexchange.com/users/67552/wide-eyed-pupil"
    },
    "is_answered": true,
    "view_count": 34186,
    "answer_count": 6,
    "score": 23,
    "last_activity_date": 1710747360,
    "creation_date": 1675132317,
    "last_edit_date": 1675164443,
    "question_id": 38970,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/38970/how-much-energy-consumption-is-involved-in-chat-gpt-responses-being-generated",
    "title": "How much energy consumption is involved in Chat GPT responses being generated?",
    "body": "<p>I note <a href=\"https://ai.stackexchange.com/questions/22877/how-much-computing-power-does-it-cost-to-run-gpt-3\">this question</a> was deemed off-topic, so I'm trying to clearly frame <em>this</em> <em>question</em> in terms of scope of response I'm interested in, namely <strong>ethics</strong> and <strong>sustainability</strong> issues associated with the soon-to-be proliferation of OpenAI Chat GPT types of tools for all manner of online information seeking behavior (from humans and other bots). <em>This is not a programming or specific hardware question.</em></p>\n<p><strong>On average, how much energy is consumed for each response that Open AI's public chatgpt-3 provides?</strong>\ni.e. what is the energy to run the entire system for 24 hours divided by the number of responses generated in 24 hours (ignoring energy consumed to train the system or build the hardware components).</p>\n<p>How does this compare to a Google/Duck Duck Go/Bing search inquiry?</p>\n<p>I read somewhere an OpenAI employee on the ChatGPT team that the computer power used to provide responses to queries is &quot;ridiculous&quot;, and there's documentation of the size of the memory requirements of hosting servers and parameters but without knowing its throughput for example it's hard to quantify the energy consumption.</p>\n<p>I often get more interesting results from Chat GPT than Duck Duck Go on certain types of queries where I used to know the answer but cannot remember the answer. IN these cases I can fact check for myself, I'm looking for a memory prompts with names and jargon that will remind me.</p>\n<p>Also when seeking out counter-views to my own (say critiques of degrowth or heterodoxy economics concepts) Chat GPT is good at providing names and papers/reports/books that critiques the view I provide it.</p>\n<p>In many cases more usefully than conventional search engines. Therefore, I can see the popularity of these tools ballooning rapidly, especially when the operational costs CAPEX + OPEX of the servers and maintainers is borne by large amounts of seed funding (eg OpenAI) or any other loss-leader startup wishing to ride the next wave of AI.</p>\n<p>The heart of my question is &quot;at what <code>externalized</code> costs do we gain these tools in terms of greenhouse gases, use of limited mineral resources, GPUs scarcity etc.&quot;</p>\n"
  },
  {
    "tags": [
      "history",
      "intelligence-testing"
    ],
    "owner": {
      "account_id": 6108253,
      "reputation": 1670,
      "user_id": 181,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://lh3.googleusercontent.com/-YYgQ6EQmsiQ/AAAAAAAAAAI/AAAAAAAAABU/QDDm2dKMp3c/s256-rj/photo.jpg",
      "display_name": "Left SE On 10_6_19",
      "link": "https://ai.stackexchange.com/users/181/left-se-on-10-6-19"
    },
    "is_answered": true,
    "view_count": 1399,
    "accepted_answer_id": 2095,
    "answer_count": 1,
    "score": 22,
    "last_activity_date": 1583077368,
    "creation_date": 1470593831,
    "last_edit_date": 1583077368,
    "question_id": 1451,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1451/has-the-lovelace-test-2-0-been-successfully-used-in-an-academic-setting",
    "title": "Has the Lovelace Test 2.0 been successfully used in an academic setting?",
    "body": "<p>In October 2014, Dr. Mark Riedl published an approach to testing AI intelligence, called <a href=\"http://arxiv.org/pdf/1410.6142v3.pdf\" rel=\"nofollow noreferrer\">the \"Lovelace Test 2.0\"</a>, after being inspired by the <a href=\"http://kryten.mm.rpi.edu/lovelace.pdf\" rel=\"nofollow noreferrer\">original Lovelace Test</a> (published in 2001). Mark believed that the original Lovelace Test would be impossible to pass, and therefore, suggested a weaker, and more practical version.</p>\n\n<p>The Lovelace Test 2.0 makes the assumption that for an AI to be intelligent, it must exhibit creativity. From the paper itself:</p>\n\n<blockquote>\n  <p>The Lovelace 2.0 Test is as follows: artificial agent <span class=\"math-container\">$a $</span> is challenged as follows:</p>\n  \n  <ul>\n  <li><p><span class=\"math-container\">$a$</span> must create an artifact <span class=\"math-container\">$o$</span> of type <span class=\"math-container\">$t$</span>;</p></li>\n  <li><p><span class=\"math-container\">$o$</span> must conform to a set of constraints <span class=\"math-container\">$C$</span> where <span class=\"math-container\">$c_i ∈ C$</span> is\n  any criterion expressible in natural language;</p></li>\n  <li><p>a human evaluator <span class=\"math-container\">$h$</span>, having chosen <span class=\"math-container\">$t$</span> and <span class=\"math-container\">$C$</span>, is satisfied\n  that <span class=\"math-container\">$o$</span> is a valid instance of <span class=\"math-container\">$t$</span> and meets <span class=\"math-container\">$C$</span>; and</p></li>\n  <li><p>a human referee <span class=\"math-container\">$r$</span> determines the combination of <span class=\"math-container\">$t$</span> and <span class=\"math-container\">$C$</span>\n  to not be unrealistic for an average human.</p></li>\n  </ul>\n</blockquote>\n\n<p>Since it is possible for a human evaluator to come up with some pretty easy constraints for an AI to beat, the human evaluator is then expected to keep coming up with more and more complex constraints for the AI until the AI fails. The point of the Lovelace Test 2.0 is to <em>compare</em> the creativity of different AIs, not to provide a definite dividing line between 'intelligence' and 'nonintelligence' like the Turing Test would.</p>\n\n<p>However, I am curious about whether this test has actually been used in an academic setting, or it is only seen as a thought experiment at the moment. The Lovelace Test seems easy to apply in academic settings (you only need to develop some measurable constraints that you can use to test the artificial agent), but it also may be too subjective (humans can disagree on the merits of certain constraints, and whether a creative artifact produced by an AI actually meets the final result).</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "classification",
      "comparison",
      "cross-entropy",
      "kl-divergence"
    ],
    "owner": {
      "account_id": 442600,
      "reputation": 331,
      "user_id": 6359,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f546d7e44a147ab61043e2ed2bb3c15b?s=256&d=identicon&r=PG",
      "display_name": "Josh Albert",
      "link": "https://ai.stackexchange.com/users/6359/josh-albert"
    },
    "is_answered": true,
    "view_count": 4776,
    "accepted_answer_id": 4185,
    "answer_count": 1,
    "score": 22,
    "last_activity_date": 1634124911,
    "creation_date": 1490863153,
    "last_edit_date": 1634124911,
    "question_id": 3065,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3065/why-has-the-cross-entropy-become-the-classification-standard-loss-function-and-n",
    "title": "Why has the cross-entropy become the classification standard loss function and not Kullback-Leibler divergence?",
    "body": "<p>The cross-entropy is identical to the KL divergence plus the entropy of the target distribution. The KL divergence equals zero when the two distributions are the same, which seems more intuitive to me than the entropy of the target distribution, which is what the cross-entropy is on a match.</p>\n<p>I'm not saying there's more information in one of the other except that a human view may find a zero more intuitive than a positive.\nOf course, one usually uses an evaluative method to really see how well classification occurs. But is the choice of the cross-entropy over the KL divergence historic?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning"
    ],
    "owner": {
      "account_id": 8315615,
      "reputation": 331,
      "user_id": 17050,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0a9b46ff05faf457d0d10aab73b2b12f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "elimohl",
      "link": "https://ai.stackexchange.com/users/17050/elimohl"
    },
    "is_answered": true,
    "view_count": 14408,
    "accepted_answer_id": 7323,
    "answer_count": 4,
    "score": 22,
    "last_activity_date": 1534425904,
    "creation_date": 1532268771,
    "question_id": 7247,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7247/why-do-we-need-floats-for-using-neural-networks",
    "title": "Why do we need floats for using neural networks?",
    "body": "<p>Is it possible to make a neural network that uses only integers by scaling input and output of each function to [-INT_MAX, INT_MAX]? Is there any drawbacks?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "ai-design",
      "markov-decision-process",
      "state-spaces",
      "state-representations"
    ],
    "owner": {
      "account_id": 254270,
      "reputation": 323,
      "user_id": 17853,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b8f5694163927c8fe743ffede41dcbd7?s=256&d=identicon&r=PG",
      "display_name": "Andy",
      "link": "https://ai.stackexchange.com/users/17853/andy"
    },
    "is_answered": true,
    "view_count": 19060,
    "accepted_answer_id": 7771,
    "answer_count": 2,
    "score": 22,
    "last_activity_date": 1637236884,
    "creation_date": 1535672720,
    "last_edit_date": 1637236884,
    "question_id": 7763,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7763/how-to-define-states-in-reinforcement-learning",
    "title": "How to define states in reinforcement learning?",
    "body": "<p>I am studying reinforcement learning and the variants of it. I am starting to get an understanding of how the algorithms work and how they apply to an MDP.</p>\n<p>What I don't understand is the process of defining the states of the MDP. In most examples and tutorials, they represent something simple like a square in a grid or similar.</p>\n<p>For more complex problems, like a robot learning to walk, etc.,</p>\n<ul>\n<li>How do you go about defining those states?</li>\n<li>Can you use learning or classification algorithms to &quot;learn&quot; those states?</li>\n</ul>\n"
  },
  {
    "tags": [
      "machine-learning",
      "reinforcement-learning",
      "overfitting",
      "dropout"
    ],
    "owner": {
      "account_id": 6998394,
      "reputation": 353,
      "user_id": 18372,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/5693d499e792b981f0290637e495eb9d?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Matt Hamilton",
      "link": "https://ai.stackexchange.com/users/18372/matt-hamilton"
    },
    "is_answered": true,
    "view_count": 7253,
    "accepted_answer_id": 8295,
    "answer_count": 1,
    "score": 22,
    "last_activity_date": 1539106378,
    "creation_date": 1538906150,
    "question_id": 8293,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/8293/why-do-you-not-see-dropout-layers-on-reinforcement-learning-examples",
    "title": "Why do you not see dropout layers on reinforcement learning examples?",
    "body": "<p>I've been looking at reinforcement learning, and specifically playing around with creating my own environments to use with the OpenAI Gym AI. I am using agents from the stable_baselines project to test with it.</p>\n\n<p>One thing I've noticed in virtually all RL examples is that there never seems to be any dropout layers in any of the networks. Why is this?</p>\n\n<p>I have created an environment that simulates currency prices and a simple agent, using DQN, that attempts to learn when to buy and sell. Training it over almost a million timesteps taken from a specific set of data consisting of one month's worth of 5-minute price data it seems to overfit a lot. If I then evaluate the agents and model against a different month's worth of data is performs abysmally. So sounds like classic overfitting.</p>\n\n<p>But is there a reason why you don't see dropout layers in RL networks? Is there other mechanisms to try and deal with overfitting? Or in many RL examples does it not matter? e.g. there may only be one true way to the ultimate high score in the 'breakout' game, so you might as well learn that exactly, and no need to generalise?</p>\n\n<p>Or is it deemed that the chaotic nature of the environment itself should provide enough different combinations of outcomes that you don't need to have dropout layers?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "topology",
      "architecture",
      "neurons",
      "biology"
    ],
    "owner": {
      "account_id": 7204978,
      "reputation": 321,
      "user_id": 20357,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/1274665195882772/picture?type=large",
      "display_name": "Harsh Sikka",
      "link": "https://ai.stackexchange.com/users/20357/harsh-sikka"
    },
    "is_answered": true,
    "view_count": 617,
    "answer_count": 2,
    "score": 22,
    "last_activity_date": 1644487734,
    "creation_date": 1543784966,
    "question_id": 9312,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/9312/are-modular-neural-networks-more-effective-than-large-monolithic-networks-at-an",
    "title": "Are Modular Neural Networks more effective than large, monolithic networks at any tasks?",
    "body": "<p>Modular/Multiple Neural networks (MNNs) revolve around training smaller, independent networks that can feed into each other or another higher network.</p>\n\n<p>In principle, the hierarchical organization could allow us to make sense of more complex problem spaces and reach a higher functionality, but it seems difficult to find examples of concrete research done in the past regarding this. I've found a few sources:</p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Modular_neural_network\" rel=\"noreferrer\">https://en.wikipedia.org/wiki/Modular_neural_network</a></p>\n\n<p><a href=\"https://www.teco.edu/~albrecht/neuro/html/node32.html\" rel=\"noreferrer\">https://www.teco.edu/~albrecht/neuro/html/node32.html</a></p>\n\n<p><a href=\"https://vtechworks.lib.vt.edu/bitstream/handle/10919/27998/etd.pdf?sequence=1&amp;isAllowed=y\" rel=\"noreferrer\">https://vtechworks.lib.vt.edu/bitstream/handle/10919/27998/etd.pdf?sequence=1&amp;isAllowed=y</a> </p>\n\n<p>A few concrete questions I have:</p>\n\n<ul>\n<li><p><em>Has there been any recent research into the use of MNNs?</em></p></li>\n<li><p><em>Are there any tasks where MNNs have shown better performance than large single nets?</em></p></li>\n<li><p><em>Could MNNs be used for multimodal classification, i.e. train each net on a fundamentally different type of data, (text vs image) and feed forward to a higher level intermediary that operates on all the outputs?</em></p></li>\n<li><p><em>From a software engineering perspective, aren't these more fault tolerant and easily isolatable on a distributed system?</em></p></li>\n<li><p><em>Has there been any work into dynamically adapting the topologies of subnetworks using a process like Neural Architecture Search?</em></p></li>\n<li><p><em>Generally, are MNNs practical in any way?</em></p></li>\n</ul>\n\n<p>Apologies if these questions seem naive, I've just come into ML and more broadly CS from a biology/neuroscience background and am captivated by the potential interplay. </p>\n\n<p>I really appreciate you taking the time and lending your insight!</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "q-learning",
      "deep-rl",
      "proofs",
      "function-approximation"
    ],
    "owner": {
      "account_id": 4148701,
      "reputation": 42397,
      "user_id": 2444,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e6fce047cba96a601a9dac66df4cd2e8?s=256&d=identicon&r=PG",
      "display_name": "nbro",
      "link": "https://ai.stackexchange.com/users/2444/nbro"
    },
    "is_answered": true,
    "view_count": 5827,
    "answer_count": 3,
    "score": 22,
    "last_activity_date": 1608383669,
    "creation_date": 1554488626,
    "last_edit_date": 1576684886,
    "question_id": 11679,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11679/why-doesnt-q-learning-converge-when-using-function-approximation",
    "title": "Why doesn&#39;t Q-learning converge when using function approximation?",
    "body": "<p>The tabular Q-learning algorithm is guaranteed to find the optimal <span class=\"math-container\">$Q$</span> function, <span class=\"math-container\">$Q^*$</span>, provided the following conditions (the <a href=\"https://en.wikipedia.org/wiki/Stochastic_approximation\" rel=\"noreferrer\">Robbins-Monro conditions</a>) regarding the learning rate are satisfied</p>\n\n<ol>\n<li><span class=\"math-container\">$\\sum_{t} \\alpha_t(s, a) = \\infty$</span></li>\n<li><span class=\"math-container\">$\\sum_{t} \\alpha_t^2(s, a) &lt; \\infty$</span></li>\n</ol>\n\n<p>where <span class=\"math-container\">$\\alpha_t(s, a)$</span> means the learning rate used when updating the <span class=\"math-container\">$Q$</span> value associated with state <span class=\"math-container\">$s$</span> and action <span class=\"math-container\">$a$</span> at time time step <span class=\"math-container\">$t$</span>, where <span class=\"math-container\">$0 \\leq  \\alpha_t(s, a) &lt; 1$</span> is assumed to be true, for all states <span class=\"math-container\">$s$</span> and actions <span class=\"math-container\">$a$</span>.</p>\n\n<p>Apparently, given that <span class=\"math-container\">$0 \\leq  \\alpha_t(s, a) &lt; 1$</span>, in order for the two conditions to be true, all state-action pairs must be visited infinitely often: this is also stated in the book <a href=\"http://incompleteideas.net/book/bookdraft2018mar21.pdf\" rel=\"noreferrer\">Reinforcement Learning: An Introduction</a>, apart from the fact that this should be widely known and it is the rationale behind the usage of the <span class=\"math-container\">$\\epsilon$</span>-greedy policy (or similar policies) during training.</p>\n\n<p>A complete proof that shows that <span class=\"math-container\">$Q$</span>-learning finds the optimal <span class=\"math-container\">$Q$</span> function can be found in the paper <a href=\"http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf\" rel=\"noreferrer\">Convergence of Q-learning: A Simple Proof</a> (by Francisco S. Melo). He uses concepts like <a href=\"https://en.wikipedia.org/wiki/Contraction_mapping\" rel=\"noreferrer\">contraction mapping</a> in order to define the optimal <span class=\"math-container\">$Q$</span> function (see also <a href=\"https://ai.stackexchange.com/q/11057/2444\">What is the Bellman operator in reinforcement learning?</a>), which is a fixed point of this contraction operator. He also uses a theorem (n. 2) regarding the random process that converges to <span class=\"math-container\">$0$</span>, given a few assumptions. (The proof might not be easy to follow if you are not a math guy.)</p>\n\n<p>If a neural network is used to represent the <span class=\"math-container\">$Q$</span> function, do the convergence guarantees of <span class=\"math-container\">$Q$</span>-learning still hold? Why does (or not) Q-learning converge when using function approximation? Is there a formal proof of such non-convergence of <span class=\"math-container\">$Q$</span>-learning using function approximation? </p>\n\n<p>I am looking for different types of answers, from those that give just the intuition behind the non-convergence of <span class=\"math-container\">$Q$</span>-learning when using function approximation to those that provide a formal proof (or a link to a paper with a formal proof).</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "q-learning",
      "dqn",
      "continuous-action-spaces",
      "continuous-state-spaces"
    ],
    "owner": {
      "account_id": 14071170,
      "reputation": 481,
      "user_id": 19413,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/c825c3c5bcda9c3d0d0612c478d7e08f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Bryan McGill",
      "link": "https://ai.stackexchange.com/users/19413/bryan-mcgill"
    },
    "is_answered": true,
    "view_count": 23227,
    "answer_count": 2,
    "score": 22,
    "last_activity_date": 1605994261,
    "creation_date": 1557573103,
    "last_edit_date": 1605993445,
    "question_id": 12255,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12255/can-q-learning-be-used-for-continuous-state-or-action-spaces",
    "title": "Can Q-learning be used for continuous (state or action) spaces?",
    "body": "<p>Many examples work with a table-based method for Q-learning. This may be suitable for a discrete state (observation) or action space, like a robot in a grid world, but is there a way to use Q-learning for continuous spaces like the control of a pendulum?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "keras",
      "convolution",
      "transformer",
      "feedforward-neural-networks"
    ],
    "owner": {
      "account_id": 4757969,
      "reputation": 321,
      "user_id": 29824,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/5PGh1.jpg?s=256",
      "display_name": "Eli Korvigo",
      "link": "https://ai.stackexchange.com/users/29824/eli-korvigo"
    },
    "is_answered": true,
    "view_count": 11587,
    "answer_count": 2,
    "score": 22,
    "last_activity_date": 1594512150,
    "creation_date": 1568850328,
    "last_edit_date": 1581538445,
    "question_id": 15524,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/15524/why-would-you-implement-the-position-wise-feed-forward-network-of-the-transforme",
    "title": "Why would you implement the position-wise feed-forward network of the transformer with convolution layers?",
    "body": "<p>The Transformer model introduced in <a href=\"https://arxiv.org/abs/1706.03762\" rel=\"noreferrer\">\"Attention is all you need\"</a> by Vaswani et al. incorporates a so-called position-wise feed-forward network (FFN):</p>\n\n<blockquote>\n  <p>In addition to attention sub-layers, each of the layers in our encoder\n  and decoder contains a fully connected feed-forward network, which is\n  applied to each position separately and identically. This consists of\n  two linear transformations with a ReLU activation in between.</p>\n  \n  <p><span class=\"math-container\">$$\\text{FFN}(x) = \\max(0, x \\times {W}_{1} + {b}_{1}) \\times {W}_{2} + {b}_{2}$$</span></p>\n  \n  <p>While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is <span class=\"math-container\">${d}_{\\text{model}} = 512$</span>, and the inner-layer has dimensionality <span class=\"math-container\">${d}_{ff} = 2048$</span>.</p>\n</blockquote>\n\n<p>I have seen at least one implementation in Keras that directly follows the convolution analogy. Here is an excerpt from <a href=\"https://github.com/Lsdefine/attention-is-all-you-need-keras/blob/master/transformer.py\" rel=\"noreferrer\">attention-is-all-you-need-keras</a>.</p>\n\n<pre><code>class PositionwiseFeedForward():\n    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')\n        self.w_2 = Conv1D(d_hid, 1)\n        self.layer_norm = LayerNormalization()\n        self.dropout = Dropout(dropout)\n    def __call__(self, x):\n        output = self.w_1(x) \n        output = self.w_2(output)\n        output = self.dropout(output)\n        output = Add()([output, x])\n        return self.layer_norm(output)\n</code></pre>\n\n<p>Yet, in Keras you can apply a single <code>Dense</code> layer across all time-steps using the <code>TimeDistributed</code> wrapper (moreover, a simple <code>Dense</code> layer applied to a 2D input <a href=\"https://stackoverflow.com/a/44616780/3846213\">implicitly behaves</a> like a <code>TimeDistributed</code> layer). Therefore, in Keras a stack of two Dense layers (one with a ReLU and the other one without an activation) is exactly the same thing as the aforementioned position-wise FFN. So, why would you implement it using convolutions?</p>\n\n<p><strong>Update</strong> </p>\n\n<p>Adding benchmarks in response to the answer by @mshlis:</p>\n\n<pre><code>import os\nimport typing as t\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\nimport numpy as np\n\nfrom keras import layers, models\nfrom keras import backend as K\nfrom tensorflow import Tensor\n\n\n# Generate random data\n\nn = 128000  # n samples\nseq_l = 32  # sequence length\nemb_dim = 512  # embedding size\n\nx = np.random.normal(0, 1, size=(n, seq_l, emb_dim)).astype(np.float32)\ny = np.random.binomial(1, 0.5, size=n).astype(np.int32)\n</code></pre>\n\n<hr>\n\n<pre><code># Define constructors\n\ndef ffn_dense(hid_dim: int, input_: Tensor) -&gt; Tensor:\n    output_dim = K.int_shape(input_)[-1]\n    hidden = layers.Dense(hid_dim, activation='relu')(input_)\n    return layers.Dense(output_dim, activation=None)(hidden)\n\n\ndef ffn_cnn(hid_dim: int, input_: Tensor) -&gt; Tensor:\n    output_dim = K.int_shape(input_)[-1]\n    hidden = layers.Conv1D(hid_dim, 1, activation='relu')(input_)\n    return layers.Conv1D(output_dim, 1, activation=None)(hidden)\n\n\ndef build_model(ffn_implementation: t.Callable[[int, Tensor], Tensor], \n                ffn_hid_dim: int, \n                input_shape: t.Tuple[int, int]) -&gt; models.Model:\n    input_ = layers.Input(shape=(seq_l, emb_dim))\n    ffn = ffn_implementation(ffn_hid_dim, input_)\n    flattened = layers.Flatten()(ffn)\n    output = layers.Dense(1, activation='sigmoid')(flattened)\n    model = models.Model(inputs=input_, outputs=output)\n    model.compile(optimizer='Adam', loss='binary_crossentropy')\n    return model\n</code></pre>\n\n<hr>\n\n<pre><code># Build the models\n\nffn_hid_dim = emb_dim * 4  # this rule is taken from the original paper\nbath_size = 512  # the batchsize was selected to maximise GPU load, i.e. reduce PCI IO overhead\n\nmodel_dense = build_model(ffn_dense, ffn_hid_dim, (seq_l, emb_dim))\nmodel_cnn = build_model(ffn_cnn, ffn_hid_dim, (seq_l, emb_dim))\n</code></pre>\n\n<hr>\n\n<pre><code># Pre-heat the GPU and let TF apply memory stream optimisations\n\nmodel_dense.fit(x=x, y=y[:, None], batch_size=bath_size, epochs=1)\n%timeit model_dense.fit(x=x, y=y[:, None], batch_size=bath_size, epochs=1)\n\nmodel_cnn.fit(x=x, y=y[:, None], batch_size=bath_size, epochs=1)\n%timeit model_cnn.fit(x=x, y=y[:, None], batch_size=bath_size, epochs=1)\n</code></pre>\n\n<p>I am getting 14.8 seconds per epoch with the Dense implementation:</p>\n\n<pre><code>Epoch 1/1\n128000/128000 [==============================] - 15s 116us/step - loss: 0.6332\nEpoch 1/1\n128000/128000 [==============================] - 15s 115us/step - loss: 0.5327\nEpoch 1/1\n128000/128000 [==============================] - 15s 117us/step - loss: 0.3828\nEpoch 1/1\n128000/128000 [==============================] - 14s 113us/step - loss: 0.2543\nEpoch 1/1\n128000/128000 [==============================] - 15s 116us/step - loss: 0.1908\nEpoch 1/1\n128000/128000 [==============================] - 15s 116us/step - loss: 0.1533\nEpoch 1/1\n128000/128000 [==============================] - 15s 117us/step - loss: 0.1475\nEpoch 1/1\n128000/128000 [==============================] - 15s 117us/step - loss: 0.1406\n\n14.8 s ± 170 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n</code></pre>\n\n<p>and 18.2 seconds for the CNN implementation. I am running this test on a standard Nvidia RTX 2080.\nSo, from a performance perspective there seems to be no point in actually implementing an FFN block as a CNN in Keras. Considering that the maths are the same, the choice boils down to pure aesthetics. </p>\n"
  },
  {
    "tags": [
      "social",
      "computation"
    ],
    "owner": {
      "account_id": 458906,
      "reputation": 329,
      "user_id": 33848,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/04a4899dc41ab3b0099c73c141d2e4a3?s=256&d=identicon&r=PG",
      "display_name": "Harrison Tran",
      "link": "https://ai.stackexchange.com/users/33848/harrison-tran"
    },
    "is_answered": true,
    "view_count": 4233,
    "answer_count": 3,
    "score": 22,
    "last_activity_date": 1619857368,
    "creation_date": 1582878892,
    "last_edit_date": 1582925006,
    "question_id": 18303,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/18303/is-a-dystopian-surveillance-state-computationally-possible",
    "title": "Is a dystopian surveillance state computationally possible?",
    "body": "<p>This isn't really a conspiracy theory question. More of an inquire on the global computational power and data storage logistics question.</p>\n\n<p>Most recording instruments such as cameras and microphones are typically voluntary opt in devices, in that, they have to be activated before they start recording. What happens if all of these devices were permanently activated and started recording data to some distributed global data storage?</p>\n\n<p>There are 400 hours of video uploaded to YouTube every minute.</p>\n\n<p>Let’s do some very rough math.</p>\n\n<p>I’m going to assume for the rest of this post that the average video is 1080p which is 2.5GB (or <span class=\"math-container\">$10^9$</span> bytes) per hour. From that, we get about 400 hrs * 60 mins * 2.5GB/hrs * 24 hrs = 1.5 petabytes (or <span class=\"math-container\">$10^{15}$</span> bytes) per day.</p>\n\n<p>But YouTube videos post are voluntary, and they are far from continuous video streams.</p>\n\n<p>There are about 3.5 billion smartphones in the world. If video was continuously streamed and recorded, going through the same video math above (<span class=\"math-container\">$3.5 * 10^9 * 1.5 * 10^{15} * 24)$</span> = 126 yottabytes (or <span class=\"math-container\">$10^{24}$</span> bytes) per day.</p>\n\n<p>The IDC projects there will be 175 zettabytes (or <span class=\"math-container\">$10^{21}$</span> bytes) in 2025.</p>\n\n<p>Unless my math is very wrong, it would seem as though smartphone cameras alone could produce more data in one day than all of the data created in human history in 2025.</p>\n\n<p>This, so far, has only been about the data recording, but, to implement a surveillance state, all recorded data would need to be processed by AI to intelligent flag data that is significant. How much processing power would be needed to filter 126 yottabytes into relevant information?</p>\n\n<p>Overall, this question is motivated by the spread of dystopian surveillance media like Edward Snowden NSA whistle blowing leaks or George Orwell's sentiment of \"Big Brother is Watching You\".</p>\n\n<p>Computationally, could we be surveilled, and to what extent? I imagine text messages surveillance would be the easiest, does the world have the computation power to surveil all text messages? How about audio? or video?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "papers",
      "transformer",
      "attention",
      "bert"
    ],
    "owner": {
      "account_id": 3779609,
      "reputation": 1833,
      "user_id": 9863,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/08ea907d3b57005305612f88c0a85b03?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "DRV",
      "link": "https://ai.stackexchange.com/users/9863/drv"
    },
    "is_answered": true,
    "view_count": 12472,
    "answer_count": 1,
    "score": 22,
    "last_activity_date": 1638693255,
    "creation_date": 1586609601,
    "last_edit_date": 1638286796,
    "question_id": 20176,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/20176/what-is-the-intuition-behind-the-dot-product-attention",
    "title": "What is the intuition behind the dot product attention?",
    "body": "<p>I am watching the video <a href=\"https://www.youtube.com/watch?v=iDulhoQ2pro&amp;t=6s\" rel=\"noreferrer\">Attention Is All You Need</a> by Yannic Kilcher.</p>\n<p>My question is: what is the intuition behind the dot product attention?</p>\n<p><span class=\"math-container\">$$A(q,K, V) = \\sum_i\\frac{e^{q.k_i}}{\\sum_j e^{q.k_j}} v_i$$</span></p>\n<p>becomes:</p>\n<p><span class=\"math-container\">$$A(Q,K, V) = \\text{softmax}(QK^T)V$$</span></p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "reference-request",
      "chess",
      "board-games"
    ],
    "owner": {
      "account_id": 30669519,
      "reputation": 349,
      "user_id": 81504,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Q8Sz4.jpg?s=256",
      "display_name": "stats_noob",
      "link": "https://ai.stackexchange.com/users/81504/stats-noob"
    },
    "is_answered": true,
    "view_count": 9901,
    "answer_count": 2,
    "score": 22,
    "last_activity_date": 1680451748,
    "creation_date": 1647616454,
    "last_edit_date": 1652171782,
    "question_id": 34876,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/34876/how-do-neural-networks-play-chess",
    "title": "How do neural networks play chess?",
    "body": "<p>I have been spending a few days trying to wrap my head around how and why neural networks are used to play chess.</p>\n<p>Although I know very little about how the game of chess works, I can understand the following idea. Theoretically, we could make a &quot;tree&quot; that included every possible outcome of a chess game. Through knowledge provided by chess experts, we could identify how &quot;favorable&quot; certain parts of this tree are compared to other parts of the tree. We could also use this tree to &quot;rank&quot; optimal chess moves based on how the chess board appears in the current turn (e.g. which pieces you and your opponent have left and where these pieces are situated).</p>\n<p>The problem is, this tree would be so enormous that it would be impossible to create, store and &quot;search&quot; (e.g. with the MinMax algorithm):</p>\n<p><a href=\"https://i.sstatic.net/IXbIc.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/IXbIc.png\" alt=\"enter image description here\" /></a></p>\n<p>I understand that perhaps this tree can be created using data to limit the size of the tree based on scenarios that are more likely to appear compared to all possible scenarios. For example, if a player wanted they could spend the whole game aimlessly shifting their &quot;Rook&quot; back and forth - theoretically, this outcome could occur but no player (in their sane mind) would ever do this. Thus, the tree could be constructed using actual data from millions of chess games. This for example could tell us : Based on historical data and given the current setup of the chess board, 21% of games were won when the immediate next move involved moving the Queen to &quot;F5&quot; vs only 3% of games were won when the immediate next move involved moving the Knight to &quot;F5&quot;. I suppose at each move, the data based tree could be queried to rank the optimality of each next move by checking the proportion of &quot;terminal nodes&quot; that resulted in wins for each possible move given the current chess board.</p>\n<p>However, I still see 2 problems with this approach:</p>\n<ul>\n<li><p>It is possible that we might run into a scenario(s) that never occurred within the historical data, rendering the tree useless in this scenario</p>\n</li>\n<li><p>This tree still might be too large to efficiently store and query.</p>\n</li>\n</ul>\n<p>This is probably why neural networks are being used to play chess - I tried to do some readings about this topic, but I can't seem to fully understand it. In this case, <strong>what exactly would the neural network use as a loss function? I don't see how the loss function in this case is continuous, and thus how could gradient descent be used on such a loss function?</strong></p>\n<p>Could someone please recommend some sources (e.g. YouTube Videos, Blogs, etc.) that show how a neural network can be used to play chess.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "objective-functions",
      "generative-adversarial-networks",
      "generative-model"
    ],
    "owner": {
      "account_id": 4767603,
      "reputation": 395,
      "user_id": 7858,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/48248a8939c6db9b3d90f02b72354198?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "tryingtolearn",
      "link": "https://ai.stackexchange.com/users/7858/tryingtolearn"
    },
    "is_answered": true,
    "view_count": 24868,
    "accepted_answer_id": 7130,
    "answer_count": 3,
    "score": 21,
    "last_activity_date": 1606048967,
    "creation_date": 1497351056,
    "last_edit_date": 1606048967,
    "question_id": 3488,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3488/how-can-we-process-the-data-from-both-the-true-distribution-and-the-generator",
    "title": "How can we process the data from both the true distribution and the generator?",
    "body": "<p>I'm struggling to understand the GAN loss function as provided in <a href=\"https://danieltakeshi.github.io/2017/03/05/understanding-generative-adversarial-networks/\" rel=\"nofollow noreferrer\">Understanding Generative Adversarial Networks</a> (a blog post written by Daniel Seita).</p>\n<p>In the standard cross-entropy loss, we have an output that has been run through a sigmoid function and a resulting binary classification.</p>\n<p>Sieta states</p>\n<blockquote>\n<p>Thus, For [each] data point <span class=\"math-container\">$x_1$</span> and its label, we get the following loss function ...</p>\n<p><span class=\"math-container\">$$\nH((x_1, y_1), D) = -y_1 \\log D(x_1) - (1 - y_1) \\log (1 - D(x_1))\n$$</span></p>\n</blockquote>\n<p>This is just the log of the expectation, which makes sense. However, according to this formulation of the GAN loss, how can we process the data from both the true distribution and the generator in the same iteration?</p>\n"
  },
  {
    "tags": [
      "agi",
      "problem-solving",
      "intelligence-testing",
      "ai-completeness"
    ],
    "owner": {
      "account_id": 10994020,
      "reputation": 313,
      "user_id": 25411,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-ZOD_8RgdEms/AAAAAAAAAAI/AAAAAAAAAD0/tIimydiDfxM/s256-rj/photo.jpg",
      "display_name": "Marc Perlade",
      "link": "https://ai.stackexchange.com/users/25411/marc-perlade"
    },
    "is_answered": true,
    "view_count": 2175,
    "protected_date": 1563021660,
    "accepted_answer_id": 12147,
    "answer_count": 2,
    "score": 21,
    "last_activity_date": 1559648055,
    "creation_date": 1557045940,
    "last_edit_date": 1557061760,
    "question_id": 12142,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12142/problems-that-only-humans-will-ever-be-able-to-solve",
    "title": "Problems that only humans will ever be able to solve",
    "body": "<p>With the increasing complexity of reCAPTCHA, I wondered about the existence of some problem, that only a human will ever be able to solve (or that AI won't be able to solve as long as it doesn't reproduce exactly the human brain).</p>\n\n<p>For instance, the distorted text used to be only possible to solve by humans. Although...</p>\n\n<blockquote>\n  <p>The computer now got the [distorted text] test right 99.8%, even in the most challenging situations.</p>\n</blockquote>\n\n<p>It seems also obvious that the distorted text can't be used for real human detection anymore. </p>\n\n<p>I'd also like to know whether an algorithm can be employed for the creation of such a problem (as for the distorted text), or if the originality of a human brain is necessarily needed. </p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "comparison",
      "word2vec",
      "cbow",
      "skip-gram"
    ],
    "owner": {
      "account_id": 3779609,
      "reputation": 1833,
      "user_id": 9863,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/08ea907d3b57005305612f88c0a85b03?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "DRV",
      "link": "https://ai.stackexchange.com/users/9863/drv"
    },
    "is_answered": true,
    "view_count": 20799,
    "answer_count": 2,
    "score": 21,
    "last_activity_date": 1640167866,
    "creation_date": 1584168773,
    "last_edit_date": 1640167866,
    "question_id": 18634,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/18634/what-are-the-main-differences-between-skip-gram-and-continuous-bag-of-words",
    "title": "What are the main differences between skip-gram and continuous bag of words?",
    "body": "<p>The skip-gram and continuous bag of words (CBOW) are two different types of word2vec models.</p>\n<p>What are the main differences between them? What are the pros and cons of both methods?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "convolutional-neural-networks",
      "computer-vision",
      "image-segmentation",
      "fully-convolutional-networks"
    ],
    "owner": {
      "account_id": 9197165,
      "reputation": 345,
      "user_id": 30910,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4b2fe451743122b778e6992b529c7316?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "r4bb1t",
      "link": "https://ai.stackexchange.com/users/30910/r4bb1t"
    },
    "is_answered": true,
    "view_count": 38009,
    "accepted_answer_id": 21824,
    "answer_count": 1,
    "score": 21,
    "last_activity_date": 1592483716,
    "creation_date": 1591925749,
    "last_edit_date": 1591976838,
    "question_id": 21810,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/21810/what-is-a-fully-convolution-network",
    "title": "What is a fully convolution network?",
    "body": "<p>I was surveying some <a href=\"https://arxiv.org/pdf/1512.08086.pdf\" rel=\"noreferrer\">literature</a> related to Fully Convolutional Networks and came across the following phrase, </p>\n\n<blockquote>\n  <p>A fully convolutional network is achieved by replacing the parameter-rich fully connected layers in standard CNN architectures by convolutional layers with <span class=\"math-container\">$1 \\times 1$</span> kernels.</p>\n</blockquote>\n\n<p>I have two questions.</p>\n\n<ol>\n<li><p>What is meant by <em>parameter-rich</em>? Is it called parameter rich because the fully connected layers pass on parameters without any kind of \"spatial\" reduction? </p></li>\n<li><p>Also, how do <span class=\"math-container\">$1 \\times 1$</span> kernels work? Doesn't <span class=\"math-container\">$1 \\times 1$</span> kernel simply mean that one is sliding a single pixel over the image? I am confused about this.</p></li>\n</ol>\n"
  },
  {
    "tags": [
      "machine-learning",
      "reinforcement-learning"
    ],
    "owner": {
      "account_id": 17972355,
      "reputation": 404,
      "user_id": 40485,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/041950a5ac8329b72b9f289fdf0b37da?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Arpit-Gole",
      "link": "https://ai.stackexchange.com/users/40485/arpit-gole"
    },
    "is_answered": true,
    "view_count": 3723,
    "answer_count": 2,
    "score": 21,
    "last_activity_date": 1600151571,
    "creation_date": 1600001843,
    "last_edit_date": 1600002507,
    "question_id": 23561,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/23561/what-is-the-hello-world-problem-of-reinforcement-learning",
    "title": "What is the &quot;Hello World&quot; problem of Reinforcement Learning?",
    "body": "<p>As we all know, <a href=\"https://en.wikipedia.org/wiki/%22Hello,_World!%22_program\" rel=\"noreferrer\">&quot;Hello World&quot;</a> is usually the first program that any programmer learns/implements in any language/framework.</p>\n<p>As Aurélien Géron mentioned in his <a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\" rel=\"noreferrer\">book</a> that <a href=\"http://yann.lecun.com/exdb/mnist/\" rel=\"noreferrer\">MNIST</a> is often called the <em>Hello World of Machine Learning</em>, <strong>is there any &quot;Hello World&quot; problem of Reinforcement Learning?</strong></p>\n<p>A few candidates that I could think of are <a href=\"https://en.wikipedia.org/wiki/Multi-armed_bandit\" rel=\"noreferrer\">multi armed bandits problem</a> and <a href=\"https://gym.openai.com/envs/CartPole-v0/\" rel=\"noreferrer\">Cart Pole Env</a>.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "computational-learning-theory",
      "multilayer-perceptrons",
      "function-approximation",
      "universal-approximation-theorems"
    ],
    "owner": {
      "account_id": 7026140,
      "reputation": 813,
      "user_id": 38076,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/CXAgk.jpg?s=256",
      "display_name": "mark mark",
      "link": "https://ai.stackexchange.com/users/38076/mark-mark"
    },
    "is_answered": true,
    "view_count": 1537,
    "protected_date": 1680527482,
    "answer_count": 1,
    "score": 21,
    "last_activity_date": 1681993426,
    "creation_date": 1609609759,
    "last_edit_date": 1609628790,
    "question_id": 25527,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/25527/what-is-the-number-of-neurons-required-to-approximate-a-polynomial-of-degree-n",
    "title": "What is the number of neurons required to approximate a polynomial of degree n?",
    "body": "<p>I learned about the <em>universal approximation theorem</em> from <a href=\"http://neuralnetworksanddeeplearning.com/chap4.html\" rel=\"noreferrer\">this</a> guide. It states that a network even with a single hidden layer can approximate any function within some bound, given a sufficient number of neurons. Or mathematically, <span class=\"math-container\">${|g(x)−f(x)|&lt; \\epsilon}$</span>, where <span class=\"math-container\">${g(x)}$</span> is the approximation, <span class=\"math-container\">${f(x)}$</span> is the target function and  is <span class=\"math-container\">$\\epsilon$</span> is an arbitrary bound.</p>\n<p>A polynomial of degree <span class=\"math-container\">$n$</span> has at maximum <span class=\"math-container\">$n-1$</span> turning points (where the derivative of the polynomial changes sign). With each new turning point, the approximation seems to become more complex.</p>\n<p>I'm not necessarily looking for a formula, but I'd like to get a general idea on how to figure out the sufficient number of neurons is for a reasonable approximation of a polynomial with a single layer of the neural network (you may consider &quot;reasonable&quot; to be <span class=\"math-container\">$\\epsilon = 0.0001$</span>). To ask in other words, how would adding one more neuron affect the model's ability to express a polynomial?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "transformer",
      "attention",
      "word-embedding"
    ],
    "owner": {
      "account_id": 13252799,
      "reputation": 645,
      "user_id": 43632,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s256-rj/photo.jpg",
      "display_name": "Bert Gayus",
      "link": "https://ai.stackexchange.com/users/43632/bert-gayus"
    },
    "is_answered": true,
    "view_count": 16203,
    "accepted_answer_id": 26246,
    "answer_count": 3,
    "score": 21,
    "last_activity_date": 1619873036,
    "creation_date": 1612551119,
    "question_id": 26235,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/26235/what-kind-of-word-embedding-is-used-in-the-original-transformer",
    "title": "What kind of word embedding is used in the original transformer?",
    "body": "<p>I am currently trying to understand transformers.</p>\n<p>To start, I read <a href=\"https://arxiv.org/pdf/1706.03762.pdf\" rel=\"noreferrer\">Attention Is All You Need</a> and also <a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\" rel=\"noreferrer\">this</a> tutorial.</p>\n<p>What makes me wonder is the word embedding used in the model. Is word2vec or GloVe being used? Are the word embeddings trained from scratch?</p>\n<p>In the tutorial linked above, the transformer is implemented from scratch and nn.Embedding from pytorch is used for the embeddings. I looked up this function and didn't understand it well, but I tend to think that the embeddings are trained from scratch, right?</p>\n"
  },
  {
    "tags": [
      "activation-functions",
      "probability",
      "softmax",
      "probability-theory"
    ],
    "owner": {
      "account_id": 24023903,
      "reputation": 1027,
      "user_id": 62466,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gg8FPkPrDJ6ppvEo_oMsRobnMxCg-rC31AmUyp8=k-s256",
      "display_name": "Snehal Patel",
      "link": "https://ai.stackexchange.com/users/62466/snehal-patel"
    },
    "is_answered": true,
    "view_count": 4728,
    "accepted_answer_id": 37896,
    "answer_count": 3,
    "score": 21,
    "last_activity_date": 1735107199,
    "creation_date": 1668453118,
    "question_id": 37889,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/37889/are-softmax-outputs-of-classifiers-true-probabilities",
    "title": "Are softmax outputs of classifiers true probabilities?",
    "body": "<p><strong>BACKGROUND:</strong>  The <a href=\"https://en.wikipedia.org/wiki/Softmax_function\" rel=\"noreferrer\">softmax function</a> is the most common choice for an activation function for the last dense layer of a multiclass neural network classifier.  The outputs of the softmax function have mathematical properties of probabilities and are--in practice--presumed to be (conditional) probabilities of the classes given the features:</p>\n<ol>\n<li>First, the softmax output for each class is between <span class=\"math-container\">$0$</span> and <span class=\"math-container\">$1$</span>.</li>\n<li>Second, the outputs of all the classes sum to <span class=\"math-container\">$1$</span>.</li>\n</ol>\n<p><strong>PROBLEM:</strong>  However, just because they have mathematical properties of probabilities does not automatically mean that the softmax outputs are in fact probabilities.  In fact, there are other functions that also have these mathematical properties, which are also occasionally used as activation functions.</p>\n<p><strong>QUESTION:</strong>  <em>&quot;Do softmax outputs represent probabilities in the usual sense?&quot;</em>  In other words, do they really reflect chances or likelihoods? (I use likelihood in the colloquial sense here.)</p>\n"
  },
  {
    "tags": [
      "agi",
      "theory-of-computation",
      "halting-problem",
      "turing-machine"
    ],
    "owner": {
      "account_id": 40019,
      "reputation": 2533,
      "user_id": 55,
      "user_type": "registered",
      "accept_rate": 43,
      "profile_image": "https://i.sstatic.net/FkIiK.png?s=256",
      "display_name": "WilliamKF",
      "link": "https://ai.stackexchange.com/users/55/williamkf"
    },
    "is_answered": true,
    "view_count": 3280,
    "accepted_answer_id": 170,
    "answer_count": 2,
    "score": 20,
    "last_activity_date": 1634085091,
    "creation_date": 1470172604,
    "last_edit_date": 1634085091,
    "question_id": 148,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/148/what-limits-if-any-does-the-halting-problem-put-on-artificial-intelligence",
    "title": "What limits, if any, does the halting problem put on Artificial Intelligence?",
    "body": "<p>Given the proven <a href=\"https://en.wikipedia.org/wiki/Halting_problem\">halting problem</a> for <a href=\"https://en.wikipedia.org/wiki/Turing_machine\">Turing machines</a>, can we infer limits on the ability of strong Artificial Intelligence?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "models"
    ],
    "owner": {
      "account_id": 443353,
      "reputation": 2148,
      "user_id": 144,
      "user_type": "registered",
      "accept_rate": 30,
      "profile_image": "https://i.sstatic.net/ZAzOl.jpg?s=256",
      "display_name": "rcpinto",
      "link": "https://ai.stackexchange.com/users/144/rcpinto"
    },
    "is_answered": true,
    "view_count": 2355,
    "answer_count": 3,
    "score": 20,
    "last_activity_date": 1476450515,
    "creation_date": 1470175141,
    "question_id": 156,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/156/are-there-any-computational-models-of-mirror-neurons",
    "title": "Are there any computational models of mirror neurons?",
    "body": "<p>From Wikipedia:</p>\n\n<blockquote>\n  <p>A mirror neuron is a neuron that fires both when an animal acts and when the animal observes the same action performed by another.</p>\n</blockquote>\n\n<p>Mirror neurons are related to imitation learning, a very useful feature that is missing in current real-world A.I. implementations. Instead of learning from input-output examples (supervised learning) or from rewards (reinforcement learning), an agent with mirror neurons would be able to learn by simply observing other agents, translating their movements to its own coordinate system. What do we have on this subject regarding computational models?</p>\n"
  },
  {
    "tags": [
      "genetic-algorithms",
      "applications",
      "evolutionary-algorithms"
    ],
    "owner": {
      "account_id": 8692869,
      "reputation": 853,
      "user_id": 30,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/YEyuc.png?s=256",
      "display_name": "Disenchanted Lurker",
      "link": "https://ai.stackexchange.com/users/30/disenchanted-lurker"
    },
    "is_answered": true,
    "view_count": 7536,
    "accepted_answer_id": 243,
    "answer_count": 5,
    "score": 20,
    "last_activity_date": 1610062674,
    "creation_date": 1470244925,
    "last_edit_date": 1609979997,
    "question_id": 240,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/240/what-exactly-are-genetic-algorithms-and-what-sort-of-problems-are-they-good-for",
    "title": "What exactly are genetic algorithms and what sort of problems are they good for?",
    "body": "<p>I've noticed that a few questions on this site mention genetic algorithms and it made me realize that I don't really know much about those.</p>\n\n<p>I have heard the term before, but it's not something I've ever used, so I don't have much idea about how they work and what they are good for. All I know is that they involve some sort of evolution and randomly changing values.</p>\n\n<p>Can you give me a short explanation, preferably including some sort of practical example that illustrates the basic principles?</p>\n"
  },
  {
    "tags": [
      "ai-safety",
      "asimovs-laws"
    ],
    "owner": {
      "account_id": 6823451,
      "reputation": 768,
      "user_id": 71,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/a9714bf38c26918f9b49e3eb7ecc0f08?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "3442",
      "link": "https://ai.stackexchange.com/users/71/3442"
    },
    "is_answered": true,
    "view_count": 754,
    "accepted_answer_id": 1355,
    "answer_count": 3,
    "score": 20,
    "last_activity_date": 1607123570,
    "creation_date": 1470354541,
    "last_edit_date": 1607123570,
    "question_id": 1348,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1348/are-asimovs-laws-flawed-by-design-or-are-they-feasible-in-practice",
    "title": "Are Asimov&#39;s Laws flawed by design, or are they feasible in practice?",
    "body": "<p>Isaac Asimov's famous <a href=\"https://en.wikipedia.org/wiki/Three_Laws_of_Robotics\">Three Laws of Robotics</a> originated in the context of Asimov's science fiction stories. In those stories, the three laws serve as a safety measure, in order to avoid untimely or manipulated situations from exploding in havoc.</p>\n\n<p>More often than not, Asimov's narratives would find a way to break them, leading the writer to make several modifications to the laws themselves. For instance, in some of his stories, he <a href=\"https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#First_Law_modified\">modified the First Law</a>, <a href=\"https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#Zeroth_Law_added\">added a Fourth (or Zeroth) Law</a>, or even <a href=\"https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#Removal_of_the_Three_Laws\">removed all Laws altogether</a>.</p>\n\n<p>However, it is easy to argue that, in popular culture, and even in the field of AI research itself, the Laws of Robotics are taken quite seriously. Ignoring the side problem of the different, subjective, and mutually-exclusive interpretations of the laws, are there any arguments proving the laws themselves intrinsically flawed by their design, or, alternatively, strong enough for use in reality? Likewise, has a better, stricter security heuristics set being designed for the purpose?</p>\n"
  },
  {
    "tags": [
      "object-recognition",
      "decision-theory",
      "autonomous-vehicles",
      "google"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 1084,
    "accepted_answer_id": 1578,
    "answer_count": 1,
    "score": 20,
    "last_activity_date": 1561043415,
    "creation_date": 1470939507,
    "last_edit_date": 1592387840,
    "question_id": 1561,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1561/would-googles-self-driving-car-stop-when-it-sees-somebody-with-a-t-shirt-with-a",
    "title": "Would Google&#39;s self-driving-car stop when it sees somebody with a T-shirt with a stop sign printed on it?",
    "body": "<p>In <a href=\"https://www.technologyreview.com/s/530276/hidden-obstacles-for-googles-self-driving-cars/\" rel=\"noreferrer\">Hidden Obstacles for Google’s Self-Driving Cars</a> article we can read that:</p>\n<blockquote>\n<p>Google’s cars can detect and respond to stop signs that aren’t on its map, a feature that was introduced to deal with temporary signs used at construction sites.</p>\n<p>Google says that its cars can identify almost all unmapped stop signs, and would remain safe if they miss a sign because the vehicles are always looking out for traffic, pedestrians and other obstacles.</p>\n</blockquote>\n<p>What would happen if a car spotted somebody in front of it (but not on the collision path) wearing a T-shirt that has a stop sign printed on it. Would it react and stop the car?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "emotional-intelligence"
    ],
    "owner": {
      "account_id": 48222,
      "reputation": 3797,
      "user_id": 33,
      "user_type": "moderator",
      "profile_image": "https://www.gravatar.com/avatar/5003c2212ad32f32666c821406e7525c?s=256&d=identicon&r=PG",
      "display_name": "mindcrime",
      "link": "https://ai.stackexchange.com/users/33/mindcrime"
    },
    "is_answered": true,
    "view_count": 1163,
    "answer_count": 11,
    "score": 20,
    "last_activity_date": 1516689289,
    "creation_date": 1471630132,
    "last_edit_date": 1473534328,
    "question_id": 1700,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/1700/what-purpose-would-be-served-by-developing-ais-that-experience-human-like-emoti",
    "title": "What purpose would be served by developing AI&#39;s that experience human-like emotions?",
    "body": "<p>In a <a href=\"http://www.wsj.com/articles/whats-next-for-artificial-intelligence-1465827619\">recent Wall Street Journal article</a>, Yann LeCunn makes the following statement:</p>\n\n<blockquote>\n  <p>The next step in achieving human-level ai is creating intelligent—but not autonomous—machines. The AI system in your car will get you safely home, but won’t choose another destination once you’ve gone inside. From there, we’ll add basic drives, along with emotions and moral values. If we create machines that learn as well as our brains do, it’s easy to imagine them inheriting human-like qualities—and flaws. </p>\n</blockquote>\n\n<p>Personally, I have generally taken the position that talking about emotions for artificial intelligences is silly, because there would be no <em>reason</em> to create AI's that experience emotions.  Obviously Yann disagrees.  So the question is:  what end would be served by doing this?  Does an AI <em>need</em> emotions to serve as a useful tool?  </p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "game-ai",
      "neat",
      "card-games"
    ],
    "owner": {
      "account_id": 1429295,
      "reputation": 321,
      "user_id": 9660,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/5c4cbaa1cd75e86194bde02b8a04c0d5?s=256&d=identicon&r=PG",
      "display_name": "pcaston2",
      "link": "https://ai.stackexchange.com/users/9660/pcaston2"
    },
    "is_answered": true,
    "view_count": 22410,
    "answer_count": 5,
    "score": 20,
    "last_activity_date": 1632069413,
    "creation_date": 1505679140,
    "last_edit_date": 1612120079,
    "question_id": 4048,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4048/how-can-i-design-and-train-a-neural-network-to-play-a-card-game-similar-to-magi",
    "title": "How can I design and train a neural network to play a card game (similar to Magic: The Gathering)?",
    "body": "<p><strong>Introduction</strong></p>\n<p>I am currently writing an engine to play a card game, as there is no engine yet for this particular game.</p>\n<p><strong>About the game</strong></p>\n<p>The game is similar to <a href=\"https://en.wikipedia.org/wiki/Magic:_The_Gathering\" rel=\"noreferrer\">Magic: The Gathering</a>. There is a commander, which has health and abilities. Players have an energy pool, which they use to put minions and spells on the board. Minions have health, attack values, costs, etc. Cards also have abilities, these are not easily enumerated. Cards are played from the hand, new cards are drawn from a deck. These are all aspects it would be helpful for the neural network to consider.</p>\n<p><strong>Idea</strong></p>\n<p>I am hoping to be able to introduce a neural network to the game afterwards, and have it learn to play the game. So, I'm writing the engine in such a way that is helpful for an AI player. There are choice points, and at those points, a list of valid options is presented. Random selection would be able to play the game (albeit not well).</p>\n<p>I have learned a lot about neural networks (mostly NEAT and HyperNEAT) and even built my own implementation. Neural networks are usually applied to image recognition tasks or to control a simple agent.</p>\n<p><strong>Problem/question</strong></p>\n<p>I'm not sure if or how I would apply neural networks to make selections with cards, which have a complex synergy. How could I design and train a neural network for this game, such that it can take into account all the variables? Is there a common approach?</p>\n<p>I know that <a href=\"https://github.com/bnordli/rftg\" rel=\"noreferrer\">Keldon wrote a good AI for RftG</a>, which has a decent amount of complexity, but I am not sure how he managed to build such an AI.</p>\n<p>Any advice? Is it feasible? Are there any good examples of this? How were the inputs mapped?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "classification",
      "image-recognition",
      "implementation",
      "multiclass-classification"
    ],
    "owner": {
      "account_id": 5630616,
      "reputation": 321,
      "user_id": 11845,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/cfedec8198a068163996e8905e99d405?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Sergey Kravchenko",
      "link": "https://ai.stackexchange.com/users/11845/sergey-kravchenko"
    },
    "is_answered": true,
    "view_count": 14604,
    "answer_count": 2,
    "score": 20,
    "last_activity_date": 1610829448,
    "creation_date": 1514743481,
    "last_edit_date": 1610829448,
    "question_id": 4889,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4889/how-to-implement-an-unknown-class-in-multi-class-classification-with-neural-ne",
    "title": "How to implement an &quot;unknown&quot; class in multi-class classification with neural networks?",
    "body": "<p>For example, I need to detect classes for MNIST data. But I want to have not 10 classes for digits, but also I want to have 11th class &quot;not a digit&quot;, so that any letter, any other type of image, or random noise would be classified as &quot;not a digit&quot;. Similarly, with CIFAR-10, I want to have the 11th &quot;unknown&quot; class to classify any image that contains something out of the available 10 classes of CIFAR-10.</p>\n<p>So, how to implement such a feature? Maybe there are some examples somewhere, preferable with Keras.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "comparison",
      "brain"
    ],
    "owner": {
      "account_id": 976103,
      "reputation": 491,
      "user_id": 14612,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/29652ad59c284fe1edfd688e481d77b4?s=256&d=identicon&r=PG",
      "display_name": "Andreas Storvik Strauman",
      "link": "https://ai.stackexchange.com/users/14612/andreas-storvik-strauman"
    },
    "is_answered": true,
    "view_count": 4480,
    "accepted_answer_id": 5957,
    "answer_count": 3,
    "score": 20,
    "last_activity_date": 1639406300,
    "creation_date": 1523223266,
    "last_edit_date": 1639406300,
    "question_id": 5955,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5955/how-are-artificial-neural-networks-and-the-biological-neural-networks-similar-an",
    "title": "How are Artificial Neural Networks and the Biological Neural Networks similar and different?",
    "body": "<p>I've heard multiple times that \"Neural Networks are the best approximation we have to model the human brain\", and I think it is commonly known that Neural Networks are modelled after our brain.</p>\n\n<p>I strongly suspect that this model has been simplified, but how much? </p>\n\n<p>How much does, say, the vanilla NN differ from what we know about the human brain? Do we even know?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "terminology"
    ],
    "owner": {
      "account_id": 8954213,
      "reputation": 713,
      "user_id": 11566,
      "user_type": "registered",
      "accept_rate": 40,
      "profile_image": "https://www.gravatar.com/avatar/08ea907d3b57005305612f88c0a85b03?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "echo",
      "link": "https://ai.stackexchange.com/users/11566/echo"
    },
    "is_answered": true,
    "view_count": 17717,
    "answer_count": 2,
    "score": 20,
    "last_activity_date": 1684831574,
    "creation_date": 1523307743,
    "last_edit_date": 1542399113,
    "question_id": 5970,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5970/what-is-the-difference-between-an-observation-and-a-state-in-reinforcement-learn",
    "title": "What is the difference between an observation and a state in reinforcement learning?",
    "body": "<p>I'm studying reinforcement learning. It seems that \"state\" and \"observation\" mean exactly the same thing. They both capture the current state of the game. </p>\n\n<p>Is there a difference between the two terms? Is the observation maybe the state after the action has been taken?</p>\n"
  },
  {
    "tags": [
      "comparison",
      "definitions",
      "search",
      "graph-search",
      "tree-search"
    ],
    "owner": {
      "account_id": 5699100,
      "reputation": 433,
      "user_id": 15391,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/5ccdfa32084dc96cd06a682f3938d506?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "xava",
      "link": "https://ai.stackexchange.com/users/15391/xava"
    },
    "is_answered": true,
    "view_count": 53847,
    "protected_date": 1564174777,
    "answer_count": 1,
    "score": 20,
    "last_activity_date": 1621082568,
    "creation_date": 1526393201,
    "last_edit_date": 1573406922,
    "question_id": 6426,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6426/what-is-the-difference-between-tree-search-and-graph-search",
    "title": "What is the difference between tree search and graph search?",
    "body": "<p>I have read various answers to this question at different places, but I am still missing something. </p>\n\n<p>What I have understood is that a graph search holds a closed list, with all expanded nodes, so they don't get explored again. However, if you apply breadth-first-search or uniformed-cost search at a search tree, you do the same. You have to keep the expanded nodes in memory. </p>\n"
  },
  {
    "tags": [
      "game-ai",
      "applications",
      "monte-carlo-tree-search",
      "minimax",
      "alpha-beta-pruning"
    ],
    "owner": {
      "account_id": 11532576,
      "reputation": 487,
      "user_id": 16906,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0c86d2559c9c6a063867e0544d238a34?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "JoeyB",
      "link": "https://ai.stackexchange.com/users/16906/joeyb"
    },
    "is_answered": true,
    "view_count": 12608,
    "protected_date": 1574448467,
    "accepted_answer_id": 7165,
    "answer_count": 3,
    "score": 20,
    "last_activity_date": 1607357164,
    "creation_date": 1531755444,
    "last_edit_date": 1574448437,
    "question_id": 7159,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7159/how-do-i-choose-the-best-algorithm-for-a-board-game-like-checkers",
    "title": "How do I choose the best algorithm for a board game like checkers?",
    "body": "<p>How do I choose the best algorithm for a board game like checkers?</p>\n\n<p>So far, I have considered only three algorithms, namely, minimax, alpha-beta pruning, and Monte Carlo tree search (MCTS). Apparently, both the alpha-beta pruning and MCTS are extensions of the basic minimax algorithm.</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "terminology",
      "actor-critic-methods",
      "comparison",
      "advantage-actor-critic"
    ],
    "owner": {
      "account_id": 2738792,
      "reputation": 1097,
      "user_id": 7402,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/72bd5e3f73d1cd1774d2235b6bfab11d?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Blaszard",
      "link": "https://ai.stackexchange.com/users/7402/blaszard"
    },
    "is_answered": true,
    "view_count": 11425,
    "protected_date": 1590191886,
    "accepted_answer_id": 7425,
    "answer_count": 4,
    "score": 20,
    "last_activity_date": 1693004115,
    "creation_date": 1533221948,
    "last_edit_date": 1589451582,
    "question_id": 7390,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7390/what-is-the-difference-between-actor-critic-and-advantage-actor-critic",
    "title": "What is the difference between actor-critic and advantage actor-critic?",
    "body": "<p>I'm struggling to understand the difference between actor-critic and advantage actor-critic.</p>\n\n<p>At least, I know they are different from asynchronous advantage actor-critic (A3C), as A3C adds an asynchronous mechanism that uses multiple worker agents interacting with their own copy of the environment and reports the gradient to the global agent. </p>\n\n<p>But what is the difference between the actor-critic and advantage actor-critic (A2C)? Is it simply with or without <em>advantage</em> function? But, then, does the actor-critic have any other implementation except for the use of advantage function?</p>\n\n<p>Or maybe are they synonyms and actor-critic is just a shorthand for A2C?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "long-short-term-memory",
      "deep-rl",
      "comparison",
      "experience-replay"
    ],
    "owner": {
      "account_id": 9065118,
      "reputation": 373,
      "user_id": 17365,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/tmOvh.gif?s=256",
      "display_name": "Kevin. Fang",
      "link": "https://ai.stackexchange.com/users/17365/kevin-fang"
    },
    "is_answered": true,
    "view_count": 19655,
    "accepted_answer_id": 7725,
    "answer_count": 1,
    "score": 20,
    "last_activity_date": 1571761381,
    "creation_date": 1535335100,
    "last_edit_date": 1555934935,
    "question_id": 7721,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7721/how-does-lstm-in-deep-reinforcement-learning-differ-from-experience-replay",
    "title": "How does LSTM in deep reinforcement learning differ from experience replay?",
    "body": "<p>In the paper <a href=\"https://arxiv.org/pdf/1507.06527.pdf\" rel=\"noreferrer\">Deep Recurrent Q-Learning for Partially Observable MDPs</a>, the author processed the Atari game frames with an LSTM layer at the end. My questions are: </p>\n\n<ul>\n<li><p>How does this method differ from the <a href=\"https://datascience.stackexchange.com/questions/20535/understanding-experience-replay-in-reinforcement-learning\">experience replay</a>, as they both use past information in the training? </p></li>\n<li><p>What's the typical application of both techniques? </p></li>\n<li><p>Can they work together?</p></li>\n<li><p>If they can work together, does it mean that the <em>state</em> is no longer a single state but a set of contiguous states?</p></li>\n</ul>\n"
  },
  {
    "tags": [
      "automated-theorem-proving",
      "automated-reasoning"
    ],
    "owner": {
      "account_id": 19666247,
      "reputation": 201,
      "user_id": 41418,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/8ef90c3cb3910715e3c9f57cc2ece123?s=256&d=identicon&r=PG",
      "display_name": "Ivan Ivanov",
      "link": "https://ai.stackexchange.com/users/41418/ivan-ivanov"
    },
    "is_answered": true,
    "view_count": 1968,
    "answer_count": 1,
    "score": 20,
    "last_activity_date": 1604721760,
    "creation_date": 1601924935,
    "question_id": 23910,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/23910/why-is-automated-theorem-proving-so-hard",
    "title": "Why is automated theorem proving so hard?",
    "body": "<p>The problem of automated theorem proving (ATP) seems to be very similar to playing board games (e.g. chess, go, etc.): it can also be naturally stated as a problem of a decision tree traversal. However, there is a dramatic difference in progress on those 2 tasks: board games are successfully being solved by reinforcement learning techniques nowadays (see AlphaGo and AlphaZero), but ATP is still nowhere near to automatically proving even freshman-level theorems. What does make ATP so hard compared to board games playing?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "recurrent-neural-networks",
      "feedforward-neural-networks",
      "hyperparameter-optimization",
      "hidden-layers"
    ],
    "owner": {
      "account_id": 2212818,
      "reputation": 293,
      "user_id": 202,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0cfff136062c79e30de9744fac255522?s=256&d=identicon&r=PG",
      "display_name": "v01d",
      "link": "https://ai.stackexchange.com/users/202/v01d"
    },
    "is_answered": true,
    "view_count": 586,
    "accepted_answer_id": 183,
    "answer_count": 2,
    "score": 19,
    "last_activity_date": 1586717647,
    "creation_date": 1470202290,
    "last_edit_date": 1586717647,
    "question_id": 182,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/182/how-do-i-decide-the-optimal-number-of-layers-for-a-neural-network",
    "title": "How do I decide the optimal number of layers for a neural network?",
    "body": "<p>How do I decide the optimal number of layers for a neural network (feedforward or recurrent)?</p>\n"
  },
  {
    "tags": [
      "deep-learning"
    ],
    "owner": {
      "account_id": 1448821,
      "reputation": 7286,
      "user_id": 42,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ab21a7fe07af4e3a153d65c420e87e85?s=256&d=identicon&r=PG",
      "display_name": "NietzscheanAI",
      "link": "https://ai.stackexchange.com/users/42/nietzscheanai"
    },
    "is_answered": true,
    "view_count": 4235,
    "accepted_answer_id": 5803,
    "answer_count": 4,
    "score": 19,
    "last_activity_date": 1522070272,
    "creation_date": 1470247526,
    "last_edit_date": 1470319794,
    "question_id": 248,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/248/issues-with-and-alternatives-to-deep-learning-approaches",
    "title": "Issues with and alternatives to Deep Learning approaches?",
    "body": "<p>Over the last 50 years, the rise/fall/rise in popularity of neural nets has acted as something of a 'barometer' for AI research.</p>\n\n<p>It's clear from the questions on this site that people are interested in applying Deep Learning (DL) to a wide variety of difficult problems.</p>\n\n<p>I therefore have two questions:</p>\n\n<ol>\n<li>Practitioners - What do you find to be the main obstacles to\napplying DL 'out of the box' to your problem? </li>\n<li>Researchers - What\ntechniques do you use (or have developed) that might help address\npractical issues? Are they within DL or do they offer an\nalternative approach?</li>\n</ol>\n"
  },
  {
    "tags": [
      "neural-networks",
      "comparison",
      "recurrent-neural-networks"
    ],
    "owner": {
      "account_id": 5129611,
      "reputation": 2945,
      "user_id": 145,
      "user_type": "moderator",
      "accept_rate": 62,
      "profile_image": "https://i.sstatic.net/IIYyh.png?s=256",
      "display_name": "Mithical",
      "link": "https://ai.stackexchange.com/users/145/mithical"
    },
    "is_answered": true,
    "view_count": 651,
    "answer_count": 1,
    "score": 19,
    "last_activity_date": 1508037595,
    "creation_date": 1470838208,
    "last_edit_date": 1471940368,
    "question_id": 1525,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/1525/could-a-boltzmann-machine-store-more-patterns-than-a-hopfield-net",
    "title": "Could a Boltzmann machine store more patterns than a Hopfield net?",
    "body": "<p><sub>This is from a closed beta for AI, with this question being posted by user number 47. All credit to them. </sub></p>\n\n<hr>\n\n<p>According to <a href=\"https://en.wikipedia.org/wiki/Boltzmann_machine\" rel=\"noreferrer\">Wikipedia</a>,</p>\n\n<blockquote>\n  <p>Boltzmann machines can be seen as the stochastic, generative counterpart of Hopfield nets.</p>\n</blockquote>\n\n<p>Both are recurrent neural networks that can be trained to learn of bit patterns. Then when presented with a partial pattern, the net will retrieve the full complete pattern.</p>\n\n<p>Hopfield networks have been proven to have a capacity of 0.138 (e.g. approximately 138 bit vectors can be recalled from storage for every 1000 nodes, Hertz 1991).</p>\n\n<p>As a Boltzmann machine is stochastic, my understanding is that it would not necessarily always show the same pattern when the energy difference between one stored pattern and another is similar. But because of this stochasticity, maybe it allows for denser pattern storage but without the guarantee that you'll always get the \"closest\" pattern in terms of energy difference. Would this be true? Or would a Hopfield net be able to store more patterns?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "comparison",
      "backpropagation"
    ],
    "owner": {
      "account_id": 7474192,
      "reputation": 191,
      "user_id": 2143,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-V7GFVXr2fJA/AAAAAAAAAAI/AAAAAAAAAB4/zEVLXqDoGuI/s256-rj/photo.jpg",
      "display_name": "Aspie96",
      "link": "https://ai.stackexchange.com/users/2143/aspie96"
    },
    "is_answered": true,
    "view_count": 516,
    "answer_count": 1,
    "score": 19,
    "last_activity_date": 1590238858,
    "creation_date": 1472982302,
    "last_edit_date": 1590238413,
    "question_id": 1851,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1851/are-these-two-versions-of-back-propagation-equivalent",
    "title": "Are these two versions of back-propagation equivalent?",
    "body": "<p>Just for fun, I am trying to develop a neural network.</p>\n\n<p>Now, for backpropagation I saw two techniques.</p>\n\n<p>The first one is used <a href=\"http://courses.cs.washington.edu/courses/cse599/01wi/admin/Assignments/bpn.html\" rel=\"nofollow noreferrer\">here</a> and in many other places too.</p>\n\n<p>What it does is:</p>\n\n<ul>\n<li>It computes the error for each output neuron.</li>\n<li>It backpropagates it into the network (calculating an error for each inner neuron).</li>\n<li><p>It updates the weights with the formula: <span class=\"math-container\">$\\Delta w_{l, m, n}=k \\cdot E_{l+1, n} \\cdot N_{l, m}$</span>, where </p>\n\n<ul>\n<li><span class=\"math-container\">$\\Delta w_{l, m, n}$</span> is the change in weight, </li>\n<li><span class=\"math-container\">$k$</span> is the learning speed, </li>\n<li><span class=\"math-container\">$E_{l+1, n}$</span> is the error of the neuron receiving the input from the synapse, and </li>\n<li><span class=\"math-container\">$ N_{l, m}$</span> is the output sent on the synapse.</li>\n</ul></li>\n<li><p>It repeats for each entry of the dataset, as many times as required.</p></li>\n</ul>\n\n<p>However, the neural network proposed in <a href=\"https://www.youtube.com/watch?v=bxe2T-V8XRs&amp;list=PL77aoaxdgEVDrHoFOMKTjDdsa0p9iVtsR\" rel=\"nofollow noreferrer\">this tutorial</a> (also available on GitHub) uses a different technique:</p>\n\n<ul>\n<li>It uses an error function (the other method does have an error function, but it does not use it for training).</li>\n<li>It has another function which can compute the final error starting from the weights.</li>\n<li>It minimizes that function (through gradient descent).</li>\n</ul>\n\n<p>Now, which method should be used?</p>\n\n<p>I think the first one is the most used one (because I saw different examples using it), but does it work as well?</p>\n\n<p>In particular, I don't know:</p>\n\n<ul>\n<li>Isn't it more subject to local minimums (since it doesn't use quadratic functions)?</li>\n<li>Since the variation of each weight is influenced by the output value of its output neuron, don't entries of the dataset which just happen to produce higher values in the neurons (not just the output ones) influence the weights more than other entries?</li>\n</ul>\n\n<p>Now, I do prefer the first technique, because I find it simpler to implement and easier to think about.</p>\n\n<p>Though, if it does have the problems I mentioned (which I hope it doesn't), is there any actual reason to use it over the second method?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "deep-rl",
      "q-learning",
      "dqn",
      "target-network"
    ],
    "owner": {
      "account_id": 3879041,
      "reputation": 327,
      "user_id": 11584,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4cdabbec68b4220322af987fe9e4486b?s=256&d=identicon&r=PG",
      "display_name": "amitection",
      "link": "https://ai.stackexchange.com/users/11584/amitection"
    },
    "is_answered": true,
    "view_count": 6792,
    "accepted_answer_id": 6983,
    "answer_count": 2,
    "score": 19,
    "last_activity_date": 1737357304,
    "creation_date": 1530517643,
    "last_edit_date": 1640196961,
    "question_id": 6982,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6982/why-does-dqn-require-two-different-networks",
    "title": "Why does DQN require two different networks?",
    "body": "<p>I was going through <a href=\"https://github.com/transedward/pytorch-dqn/blob/master/dqn_learn.py\" rel=\"noreferrer\">this</a> implementation of DQN and I see that on line 124 and 125 two different Q networks have been initialized. From my understanding, I think one network predicts the appropriate action and the second network predicts the target Q values for finding the Bellman error.</p>\n\n<p>Why can we not just make one single network that simply predicts the Q value and use it for both the cases? My best guess that it's been done to reduce the computation time, otherwise we would have to find out the q value for each action and then select the best one. Is this the only reason? Am I missing something?</p>\n"
  },
  {
    "tags": [
      "reference-request",
      "research",
      "books",
      "academia",
      "education"
    ],
    "owner": {
      "account_id": 9458018,
      "reputation": 371,
      "user_id": 12021,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6f7fa802ea797579f98f30303905b5fa?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Surya Bhusal",
      "link": "https://ai.stackexchange.com/users/12021/surya-bhusal"
    },
    "is_answered": true,
    "view_count": 2791,
    "accepted_answer_id": 7353,
    "answer_count": 3,
    "score": 19,
    "last_activity_date": 1640101934,
    "creation_date": 1532979478,
    "last_edit_date": 1640101934,
    "question_id": 7352,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7352/what-are-the-mathematical-prerequisites-for-an-ai-researcher",
    "title": "What are the mathematical prerequisites for an AI researcher?",
    "body": "<p>What are the mathematical prerequisites for understanding the core part of various algorithms involved in artificial intelligence and developing one's own algorithms?</p>\n<p>Please, refer to some specific books.</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "dqn",
      "game-ai",
      "action-spaces",
      "board-games"
    ],
    "owner": {
      "account_id": 12148656,
      "reputation": 375,
      "user_id": 21278,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/fe54f1c4798aa72950daef38926f6917?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "ytolochko",
      "link": "https://ai.stackexchange.com/users/21278/ytolochko"
    },
    "is_answered": true,
    "view_count": 5954,
    "answer_count": 1,
    "score": 19,
    "last_activity_date": 1610409457,
    "creation_date": 1551878612,
    "last_edit_date": 1610409457,
    "question_id": 11055,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11055/how-to-deal-with-a-huge-action-space-where-at-every-step-there-is-a-variable",
    "title": "How to deal with a huge action space, where, at every step, there is a variable number of legal actions?",
    "body": "<p>I am working on creating an RL-based AI for a certain board game. Just as a general overview of the game so that you understand what it's all about: It's a discrete turn-based game with a board of size <span class=\"math-container\">$n \\times n$</span> (<span class=\"math-container\">$n$</span> depending on the number of players). Each player gets an <span class=\"math-container\">$m$</span> number of pieces, which the player must place on the board. In the end, the one who has the least number of pieces wins. There are of course rules as to how the pieces can be placed so that not all placements are legal at every move.</p>\n<p>I have the game working in an OpenAI's gym environment (i.e. control by <code>step</code> function), have the board representation as the observation, and I have defined the reward function.</p>\n<p>The thing I am struggling with right now is to meaningfully represent the action space.</p>\n<p>I looked into how AlphaZero tackles chess. The action space there is <span class=\"math-container\">$8*8*73 = 4672$</span>: for every possible tile on the board, there are 73 movement-related modalities. So, for every move, the algorithm comes up with 4672 values, the illegal ones are set to zero and non-zero ones are re-normalized.</p>\n<p>Now, I am not sure if such an approach would be feasible for my use-case, as my calculations show that I have a theoretical cap of ~30k possible actions (<span class=\"math-container\">$n * n * m$</span>) if using the same way of calculation. I am not sure if this would still work on, especially considering that I don't have the DeepMind computing resources at hand.</p>\n<p>Therefore, my question: <em>Is there any other way of doing it apart from selecting the legal actions from all theoretically possible ones?</em></p>\n<p>The legal actions would be just a fraction of the ~30k possible ones. However, at every step, the legal actions would change because every new piece determines the new placement possibilities (also, the already placed pieces are not available anymore, i.e. action space generally gets smaller with every step).</p>\n<p>I am thinking of games, like Starcraft 2, where action space must be larger still and they demonstrate good results, not only by DeepMind but also by private enthusiasts with for example DQN.</p>\n<p>I would appreciate any ideas, hints, or readings!</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "definitions",
      "geometric-deep-learning",
      "graphs",
      "graph-neural-networks"
    ],
    "owner": {
      "account_id": 4148701,
      "reputation": 42397,
      "user_id": 2444,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e6fce047cba96a601a9dac66df4cd2e8?s=256&d=identicon&r=PG",
      "display_name": "nbro",
      "link": "https://ai.stackexchange.com/users/2444/nbro"
    },
    "is_answered": true,
    "view_count": 5218,
    "accepted_answer_id": 11201,
    "answer_count": 3,
    "score": 19,
    "last_activity_date": 1629906322,
    "creation_date": 1552386589,
    "last_edit_date": 1604672657,
    "question_id": 11166,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11166/what-is-geometric-deep-learning",
    "title": "What is geometric deep learning?",
    "body": "<p>What is geometric deep learning (GDL)?</p>\n<p>Here are a few sub-questions</p>\n<ul>\n<li>How is it different from deep learning?</li>\n<li>Why do we need GDL?</li>\n<li>What are some applications of GDL?</li>\n</ul>\n"
  },
  {
    "tags": [
      "recurrent-neural-networks",
      "long-short-term-memory",
      "hidden-layers",
      "seq2seq",
      "encoder-decoder"
    ],
    "owner": {
      "account_id": 11909125,
      "reputation": 825,
      "user_id": 30885,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/98d37dbcca45af6ec9419b1d3211a6fb?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user8714896",
      "link": "https://ai.stackexchange.com/users/30885/user8714896"
    },
    "is_answered": true,
    "view_count": 34281,
    "accepted_answer_id": 16833,
    "answer_count": 4,
    "score": 19,
    "last_activity_date": 1708244722,
    "creation_date": 1572326721,
    "last_edit_date": 1610901816,
    "question_id": 16133,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/16133/what-exactly-is-a-hidden-state-in-an-lstm-and-rnn",
    "title": "What exactly is a hidden state in an LSTM and RNN?",
    "body": "<p>I'm working on a project, where we use an encoder-decoder architecture. We decided to use an LSTM for both the encoder and decoder due to its <strong>hidden states</strong>. In my specific case, the hidden state of the encoder is passed to the decoder, and this would allow the model to learn better latent representations.</p>\n<p>Does this make sense?</p>\n<p>I am a bit confused about this because I really don't know what the hidden state is. Moreover, we're using separate LSTMs for the encoder and decoder, so I can't see how the hidden state from the encoder LSTM can be useful to the decoder LSTM because only the encoder LSTM really understands it.</p>\n"
  },
  {
    "tags": [
      "definitions",
      "unsupervised-learning",
      "self-supervised-learning"
    ],
    "owner": {
      "account_id": 11887562,
      "reputation": 2780,
      "user_id": 34383,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/c3938672a162d7f04ee40a146836de6d?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Robin van Hoorn",
      "link": "https://ai.stackexchange.com/users/34383/robin-van-hoorn"
    },
    "is_answered": true,
    "view_count": 17093,
    "answer_count": 4,
    "score": 19,
    "last_activity_date": 1722159684,
    "creation_date": 1683485525,
    "last_edit_date": 1683532280,
    "question_id": 40341,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/40341/what-is-the-difference-between-self-supervised-and-unsupervised-learning",
    "title": "What is the difference between self-supervised and unsupervised learning?",
    "body": "<p>What is the difference between <a href=\"https://ai.stackexchange.com/questions/10623/what-is-self-supervised-learning-in-machine-learning?noredirect=1&amp;lq=1\">self-supervised</a> and unsupervised learning? The terms logically overlap (and maybe self-supervised learning is a subset of unsupervised learning?), but I cannot pinpoint exactly what that difference is.</p>\n<p>What are the commonly agreed-upon 'definitions' of these terms? What is an example of unsupervised learning that is definitely not self-supervised learning?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "reference-request",
      "natural-language-understanding",
      "semantics"
    ],
    "owner": {
      "account_id": 6444670,
      "reputation": 1381,
      "user_id": 101,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://i.sstatic.net/z0jss.png?s=256",
      "display_name": "Dawny33",
      "link": "https://ai.stackexchange.com/users/101/dawny33"
    },
    "is_answered": true,
    "view_count": 381,
    "accepted_answer_id": 1345,
    "answer_count": 2,
    "score": 18,
    "last_activity_date": 1640196311,
    "creation_date": 1470214865,
    "last_edit_date": 1640196311,
    "question_id": 198,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/198/what-research-has-been-done-in-the-domain-of-identifying-sarcasm-in-text",
    "title": "What research has been done in the domain of &quot;identifying sarcasm in text&quot;?",
    "body": "<p>Identifying sarcasm is considered one of the most difficult open-ended problems in the domain of ML and NLP/NLU.</p>\n<p>So, was there any considerable research done on that front? If yes, then what is the accuracy like? Please, also, explain the NLP model briefly.</p>\n"
  },
  {
    "tags": [
      "multilayer-perceptrons",
      "history",
      "perceptron",
      "ai-winter",
      "xor-problem"
    ],
    "owner": {
      "account_id": 443353,
      "reputation": 2148,
      "user_id": 144,
      "user_type": "registered",
      "accept_rate": 30,
      "profile_image": "https://i.sstatic.net/ZAzOl.jpg?s=256",
      "display_name": "rcpinto",
      "link": "https://ai.stackexchange.com/users/144/rcpinto"
    },
    "is_answered": true,
    "view_count": 7713,
    "answer_count": 4,
    "score": 18,
    "last_activity_date": 1721944800,
    "creation_date": 1470296046,
    "last_edit_date": 1611012853,
    "question_id": 1288,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1288/did-minsky-and-papert-know-that-multi-layer-perceptrons-could-solve-xor",
    "title": "Did Minsky and Papert know that multi-layer perceptrons could solve XOR?",
    "body": "<p>In their famous book entitled <a href=\"https://rads.stackoverflow.com/amzn/click/com/0262631113\" rel=\"noreferrer\" rel=\"nofollow noreferrer\">Perceptrons: An Introduction to Computational Geometry</a>, Minsky and Papert show that a perceptron can't solve the XOR problem. This contributed to the first AI winter, resulting in funding cuts for neural networks. However, now we know that a multilayer perceptron can solve the XOR problem easily.</p>\n\n<p>Backprop wasn't known at the time, but did they know about manually building multilayer perceptrons? Did Minsky &amp; Papert know that multilayer perceptrons could solve XOR at the time they wrote the book, albeit not knowing how to train it?</p>\n"
  },
  {
    "tags": [
      "genetic-algorithms",
      "evolutionary-algorithms",
      "novelty-search"
    ],
    "owner": {
      "account_id": 443353,
      "reputation": 2148,
      "user_id": 144,
      "user_type": "registered",
      "accept_rate": 30,
      "profile_image": "https://i.sstatic.net/ZAzOl.jpg?s=256",
      "display_name": "rcpinto",
      "link": "https://ai.stackexchange.com/users/144/rcpinto"
    },
    "is_answered": true,
    "view_count": 6246,
    "answer_count": 2,
    "score": 18,
    "last_activity_date": 1574383722,
    "creation_date": 1470299983,
    "last_edit_date": 1574383722,
    "question_id": 1296,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1296/how-does-novelty-search-work",
    "title": "How does novelty search work?",
    "body": "<p>In <a href=\"https://web.archive.org/web/20190519181402/http://fabelier.org/novelty-search-and-open-ended-evolution-by-ken-stanley/\" rel=\"noreferrer\">this article</a>, the author claims that guiding evolution by novelty alone (without explicit goals) can solve problems even better than using explicit goals. In other words, using a novelty measure as a fitness function for a genetic algorithm works better than a goal-directed fitness function. How is that possible?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "tensorflow",
      "regression"
    ],
    "owner": {
      "account_id": 1259203,
      "reputation": 283,
      "user_id": 2472,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/536ab18f11a1222292ccfbc9d1cdda75?s=256&d=identicon&r=PG",
      "display_name": "Souradeep Nanda",
      "link": "https://ai.stackexchange.com/users/2472/souradeep-nanda"
    },
    "is_answered": true,
    "view_count": 31683,
    "accepted_answer_id": 1990,
    "answer_count": 10,
    "score": 18,
    "last_activity_date": 1731066612,
    "creation_date": 1474197623,
    "last_edit_date": 1606827762,
    "question_id": 1987,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1987/how-to-classify-data-which-is-spiral-in-shape",
    "title": "How to classify data which is spiral in shape?",
    "body": "<p>I have been messing around in <a href=\"http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=spiral&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.73263&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false\" rel=\"noreferrer\">tensorflow playground</a>. One of the input data sets is a spiral. No matter what input parameters I choose, no matter how wide and deep the neural network I make, I cannot fit the spiral. How do data scientists fit data of this shape?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "classification",
      "prediction"
    ],
    "owner": {
      "account_id": 9896169,
      "reputation": 181,
      "user_id": 4394,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/076c352d2de8a2b504cc105d7ad793c8?s=256&d=identicon&r=PG",
      "display_name": "Aditya Gupta",
      "link": "https://ai.stackexchange.com/users/4394/aditya-gupta"
    },
    "is_answered": true,
    "view_count": 2406,
    "answer_count": 4,
    "score": 18,
    "last_activity_date": 1571882437,
    "creation_date": 1482353257,
    "last_edit_date": 1571882437,
    "question_id": 2524,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2524/what-makes-neural-networks-so-good-at-predictions",
    "title": "What makes neural networks so good at predictions?",
    "body": "<p>I am new to neural-network and I am trying to understand mathematically what makes neural networks so good at classification problems. </p>\n\n<p>By taking the example of a small neural network (for example, one with 2 inputs, 2 nodes in a hidden layer and 2 nodes for the output), all you have is a complex function at the output which is mostly sigmoid over a linear combination of the sigmoid.</p>\n\n<p>So, how does that make them good at prediction? Does the final function lead to some sort of curve fitting?</p>\n"
  },
  {
    "tags": [
      "neural-networks"
    ],
    "owner": {
      "account_id": 10465107,
      "reputation": 1029,
      "user_id": 6645,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/87326d687b83789020f7e73a17621555?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Stephen Johnson",
      "link": "https://ai.stackexchange.com/users/6645/stephen-johnson"
    },
    "is_answered": true,
    "view_count": 26216,
    "accepted_answer_id": 3263,
    "answer_count": 4,
    "score": 18,
    "last_activity_date": 1537209717,
    "creation_date": 1493903197,
    "question_id": 3262,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/3262/1-hidden-layer-with-1000-neurons-vs-10-hidden-layers-with-100-neurons",
    "title": "1 hidden layer with 1000 neurons vs. 10 hidden layers with 100 neurons",
    "body": "<p>These types of questions may be problem-dependent, but I have tried to find research that addresses the question whether the number of hidden layers and their size (number of neurons in each layer) really matter or not.</p>\n\n<p>So my question is, does it really matter if we for example have 1 large hidden layer of 1000 neurons vs. 10 hidden layers with 100 neurons each?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "algorithm",
      "sutton-barto",
      "reinforce"
    ],
    "owner": {
      "account_id": 8140149,
      "reputation": 373,
      "user_id": 17565,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/3288fc667a0d999e11086b0eed014538?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Diego Orellana",
      "link": "https://ai.stackexchange.com/users/17565/diego-orellana"
    },
    "is_answered": true,
    "view_count": 2608,
    "accepted_answer_id": 7681,
    "answer_count": 4,
    "score": 18,
    "last_activity_date": 1657068189,
    "creation_date": 1534961209,
    "last_edit_date": 1561809599,
    "question_id": 7680,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7680/why-does-the-discount-rate-in-the-reinforce-algorithm-appear-twice",
    "title": "Why does the discount rate in the REINFORCE algorithm appear twice?",
    "body": "<p>I was reading the book <a href=\"http://incompleteideas.net/book/bookdraft2017nov5.pdf\" rel=\"noreferrer\">Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto</a> (complete draft, November 5, 2017). </p>\n\n<p>On page 271, the pseudo-code for the episodic Monte-Carlo Policy-Gradient Method is presented. Looking at this pseudo-code I can't understand why it seems that the discount rate appears 2 times, once in the update state and a second time inside the return. [See the figure below] </p>\n\n<p><a href=\"https://i.sstatic.net/dxDnP.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/dxDnP.png\" alt=\"enter image description here\"></a></p>\n\n<p>It seems that the return for the steps after step 1 are just a truncation of the return of the first step. Also, if you look just one page above in the book you find an equation with just 1 discount rate (the one inside the return.) </p>\n\n<p>Why then does the pseudo-code seem to be different? My guess is that I am misunderstanding something: </p>\n\n<p><span class=\"math-container\">$$\n{\\mathbf{\\theta}}_{t+1} ~\\dot{=}~\\mathbf{\\theta}_t + \\alpha G_t \\frac{{\\nabla}_{\\mathbf{\\theta}} \\pi \\left(A_t \\middle| S_t, \\mathbf{\\theta}_{t} \\right)}{\\pi \\left(A_t \\middle| S_t, \\mathbf{\\theta}_{t} \\right)}.\n\\tag{13.6}\n$$</span></p>\n"
  },
  {
    "tags": [
      "reference-request",
      "ethics",
      "explainable-ai"
    ],
    "owner": {
      "account_id": 4148701,
      "reputation": 42397,
      "user_id": 2444,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e6fce047cba96a601a9dac66df4cd2e8?s=256&d=identicon&r=PG",
      "display_name": "nbro",
      "link": "https://ai.stackexchange.com/users/2444/nbro"
    },
    "is_answered": true,
    "view_count": 3433,
    "protected_date": 1618626360,
    "answer_count": 3,
    "score": 18,
    "last_activity_date": 1642192628,
    "creation_date": 1560640067,
    "last_edit_date": 1642168630,
    "question_id": 12870,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12870/which-explainable-artificial-intelligence-techniques-are-there",
    "title": "Which explainable artificial intelligence techniques are there?",
    "body": "<p><a href=\"https://en.wikipedia.org/wiki/Explainable_artificial_intelligence\" rel=\"noreferrer\">Explainable artificial intelligence (XAI)</a> is concerned with the development of techniques that can enhance the interpretability, accountability, and transparency of artificial intelligence and, in particular, machine learning algorithms and models, especially black-box ones, such as artificial neural networks, so that these can also be adopted in areas, like healthcare, where the interpretability and understanding of the results (e.g. classifications) are required.</p>\n<p>Which XAI techniques are there?</p>\n<p>If there are many, to avoid making this question too broad, you can just provide a few examples (the most famous or effective ones), and, for people interested in more techniques and details, you can also provide one or more references/surveys/books that go into the details of XAI. The idea of this question is that people could easily find one technique that they could study to understand what XAI really is or how it can be approached.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "transformer",
      "attention",
      "embeddings"
    ],
    "owner": {
      "account_id": 6083264,
      "reputation": 401,
      "user_id": 104,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6c988ec0f6ac8741298c84c228895b48?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "nalzok",
      "link": "https://ai.stackexchange.com/users/104/nalzok"
    },
    "is_answered": true,
    "view_count": 4354,
    "answer_count": 3,
    "score": 18,
    "last_activity_date": 1703431811,
    "creation_date": 1655704654,
    "question_id": 35990,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/35990/why-are-embeddings-added-not-concatenated",
    "title": "Why are embeddings added, not concatenated?",
    "body": "<p>Let's consider the following example from BERT</p>\n<p><a href=\"https://i.sstatic.net/YLGSz.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/YLGSz.png\" alt=\"enter image description here\" /></a></p>\n<p>I cannot understand why &quot;the input embeddings are the <em>sum</em> of the token embeddings, the segmentation embeddings, and the position embeddings&quot;. The thing is, these embeddings carry different types of information, so intuitively adding them together doesn't really make sense. I mean, you cannot add 2 meters to 3 kilograms, but you can make a tuple (2 meters, 3 kilograms), so I think it's more natural to concatenate these embedding together. By adding them together, we are assuming the information about token, segmentation, and position can be simultaneously represented in the same embedding space, but that sounds like a bold claim.</p>\n<p>Other transformers, like ViTMAE, seem to follow the trend of <a href=\"https://github.com/huggingface/transformers/blob/v4.20.0/src/transformers/models/vit_mae/modeling_vit_mae.py#L795\" rel=\"noreferrer\">adding position embeddings to other &quot;semantic&quot; embeddings</a>. What's the rationale behind the practice?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "philosophy",
      "definitions"
    ],
    "owner": {
      "account_id": 8057826,
      "reputation": 429,
      "user_id": 5,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Kk1s7.png?s=256",
      "display_name": "skistaddy",
      "link": "https://ai.stackexchange.com/users/5/skistaddy"
    },
    "is_answered": true,
    "view_count": 599,
    "accepted_answer_id": 97,
    "answer_count": 1,
    "score": 17,
    "last_activity_date": 1561381854,
    "creation_date": 1470157475,
    "last_edit_date": 1561381854,
    "question_id": 91,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/91/are-search-engines-considered-ai",
    "title": "Are search engines considered AI?",
    "body": "<p>Are search engines considered AI because of the way they analyze what you search for and remember it? Or how they send you ads of what you've searched for recently? </p>\n\n<p>Is this considered AI or just smart?</p>\n"
  },
  {
    "tags": [
      "game-ai",
      "monte-carlo-tree-search",
      "monte-carlo-methods",
      "alphago"
    ],
    "owner": {
      "account_id": 6444670,
      "reputation": 1381,
      "user_id": 101,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://i.sstatic.net/z0jss.png?s=256",
      "display_name": "Dawny33",
      "link": "https://ai.stackexchange.com/users/101/dawny33"
    },
    "is_answered": true,
    "view_count": 2800,
    "accepted_answer_id": 1361,
    "answer_count": 1,
    "score": 17,
    "last_activity_date": 1640196836,
    "creation_date": 1470380630,
    "last_edit_date": 1640196836,
    "question_id": 1358,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1358/how-does-monte-carlo-search-work",
    "title": "How does &quot;Monte-Carlo search&quot; work?",
    "body": "<p>I have heard about this concept in a Reddit post about AlphaGo. I have tried to go through the paper and the article, but could not really make sense of the algorithm.</p>\n<p>So, can someone give an easy-to-understand explanation of how the Monte-Carlo search algorithm work and how is it being used in building game-playing AI bots?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "natural-language-processing"
    ],
    "owner": {
      "account_id": 3551531,
      "reputation": 275,
      "user_id": 2426,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/23e8e06760b889abbd4eaa020edd78f2?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "AvahW",
      "link": "https://ai.stackexchange.com/users/2426/avahw"
    },
    "is_answered": true,
    "view_count": 1658,
    "accepted_answer_id": 1971,
    "answer_count": 3,
    "score": 17,
    "last_activity_date": 1567601836,
    "creation_date": 1473865141,
    "last_edit_date": 1559861981,
    "question_id": 1970,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1970/how-would-an-ai-learn-language",
    "title": "How would an AI learn language?",
    "body": "<p>I was think about AIs and how they would work, when I realised that I couldn't think of a way that an AI could be taught language. A child tends to learn language through associations of language and pictures to an object (e.g., people saying the word \"dog\" while around a dog, and later realising that  people say \"a dog\" and \"a car\" and learn what \"a\" means, etc.). However, a text based AI couldn't use this method to learn, as they wouldn't have access to any sort of input device.</p>\n\n<p>The only way I could come up with is programming in every word, and rule, in the English language (or whatever language it is meant to 'speak' in), however that would, potentially, take years to do.</p>\n\n<p>Does anyone have any ideas on how this could be done? Or if it has been done already, if so how?</p>\n\n<p>By the way, in this context, I am using AI to mean an Artificial Intelligence system with near-human intelligence, and no prior knowledge of language.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "algorithm",
      "computer-vision"
    ],
    "owner": {
      "account_id": 3302011,
      "reputation": 353,
      "user_id": 1366,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d07f33df80651f4d22e5a37293a52b14?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Gottfried William",
      "link": "https://ai.stackexchange.com/users/1366/gottfried-william"
    },
    "is_answered": true,
    "view_count": 1183,
    "answer_count": 1,
    "score": 17,
    "last_activity_date": 1616947335,
    "creation_date": 1477723779,
    "last_edit_date": 1575506061,
    "question_id": 2231,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2231/are-information-processing-rules-from-gestalt-psychology-still-used-in-computer",
    "title": "Are information processing rules from Gestalt psychology still used in computer vision today?",
    "body": "<p>Decades ago there were and are books in machine vision, which by implementing various information processing rules from gestalt psychology, got impressive results with little code or special hardware in image identification and visual processing.</p>\n\n<p>Are such methods being used or worked on today? Was any progress made on this? Or was this research program dropped? By today, I mean 2016, not 1995 or 2005.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "classification",
      "applications",
      "comparison"
    ],
    "owner": {
      "account_id": 5885227,
      "reputation": 293,
      "user_id": 5388,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/2F6fZ.jpg?s=256",
      "display_name": "Alexander",
      "link": "https://ai.stackexchange.com/users/5388/alexander"
    },
    "is_answered": true,
    "view_count": 1943,
    "accepted_answer_id": 3003,
    "answer_count": 2,
    "score": 17,
    "last_activity_date": 1559170331,
    "creation_date": 1489664506,
    "last_edit_date": 1559169922,
    "question_id": 3002,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3002/when-is-deep-learning-overkill",
    "title": "When is deep learning overkill?",
    "body": "<p>For example, for classifying emails as spam, is it worthwhile - from a  time/accuracy perspective - to apply <em>deep learning</em> (if possible) instead of another machine learning algorithm? Will deep learning make other machine learning algorithms like <em>naive Bayes</em> unnecessary?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "applications"
    ],
    "owner": {
      "account_id": 5723980,
      "reputation": 367,
      "user_id": 7816,
      "user_type": "registered",
      "accept_rate": 20,
      "profile_image": "https://graph.facebook.com/748796320/picture?type=large",
      "display_name": "Mark Markrowave Charlton",
      "link": "https://ai.stackexchange.com/users/7816/mark-markrowave-charlton"
    },
    "is_answered": true,
    "view_count": 8830,
    "answer_count": 3,
    "score": 17,
    "last_activity_date": 1604242245,
    "creation_date": 1497586216,
    "last_edit_date": 1542612891,
    "question_id": 3502,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3502/are-there-any-applications-of-reinforcement-learning-other-than-games",
    "title": "Are there any applications of reinforcement learning other than games?",
    "body": "<p>Is there a way to teach reinforcement learning in applications other than games? </p>\n\n<p>The only examples I can find on the Internet are of game agents. I understand that VNC's control the input to the games via the reinforcement network. Is it possible to set this up with say a CAD software?</p>\n"
  },
  {
    "tags": [
      "agi",
      "social",
      "narrow-ai",
      "futurism"
    ],
    "owner": {
      "account_id": 7240311,
      "reputation": 221,
      "user_id": 8918,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/27edba56ea7ff334ab2c88551351da7a?s=256&d=identicon&r=PG",
      "display_name": "tuomastik",
      "link": "https://ai.stackexchange.com/users/8918/tuomastik"
    },
    "is_answered": true,
    "view_count": 793,
    "answer_count": 4,
    "score": 17,
    "last_activity_date": 1639773082,
    "creation_date": 1502140168,
    "last_edit_date": 1639773082,
    "question_id": 3775,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3775/how-to-reinvent-jobs-replaced-by-ai",
    "title": "How to reinvent jobs replaced by AI?",
    "body": "<p>In general, what possibilities are there for reinventing job descriptions that could be replaced by an automated AI solution?</p>\n<p>My initial ideas include:</p>\n<ul>\n<li>Monitoring the AI and flagging its incorrect actions.</li>\n<li>Possibly taking over the control in very challenging scenarios.</li>\n<li>Creating/gathering more training/testing data to improve the accuracy of the AI.</li>\n</ul>\n"
  },
  {
    "tags": [
      "deep-learning",
      "reinforcement-learning",
      "policy-gradients",
      "proximal-policy-optimization",
      "trust-region-policy-optimization"
    ],
    "owner": {
      "account_id": 1621964,
      "reputation": 377,
      "user_id": 9740,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/40a23ceaf81666478d8ba416fc850808?s=256&d=identicon&r=PG",
      "display_name": "Evalds Urtans",
      "link": "https://ai.stackexchange.com/users/9740/evalds-urtans"
    },
    "is_answered": true,
    "view_count": 8500,
    "accepted_answer_id": 4088,
    "answer_count": 1,
    "score": 17,
    "last_activity_date": 1619696958,
    "creation_date": 1505982448,
    "last_edit_date": 1619696958,
    "question_id": 4085,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4085/how-can-policy-gradients-be-applied-in-the-case-of-multiple-continuous-actions",
    "title": "How can policy gradients be applied in the case of multiple continuous actions?",
    "body": "<p>Trusted Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO) are two cutting edge policy gradients algorithms.</p>\n<p>When using a single continuous action, normally, you would use some probability distribution (for example, Gaussian) for the loss function. The rough version is:</p>\n<p><span class=\"math-container\">$$L(\\theta) = \\log(P(a_1)) A,$$</span></p>\n<p>where <span class=\"math-container\">$A$</span> is the advantage of rewards, <span class=\"math-container\">$P(a_1)$</span> is characterized by <span class=\"math-container\">$\\mu$</span> and <span class=\"math-container\">$\\sigma^2$</span> that comes out of neural network like in the Pendulum environment here: <a href=\"https://github.com/leomzhong/DeepReinforcementLearningCourse/blob/69e573cd88faec7e9cf900da8eeef08c57dec0f0/hw4/main.py\" rel=\"nofollow noreferrer\">https://github.com/leomzhong/DeepReinforcementLearningCourse/blob/69e573cd88faec7e9cf900da8eeef08c57dec0f0/hw4/main.py</a>.</p>\n<p>The problem is that I cannot find any paper on 2+ continuous actions using policy gradients (not actor-critic methods that use a different approach by transferring gradient from Q-function).</p>\n<p>Do you know how to do this using TRPO for 2 continuous actions in <a href=\"https://gym.openai.com/envs/LunarLanderContinuous-v2/\" rel=\"nofollow noreferrer\">LunarLander environment</a>?</p>\n<p>Is following approach correct for policy gradient loss function?</p>\n<p><span class=\"math-container\">$$L(\\theta) = (\\log P(a_1) + \\log P(a_2) )*A$$</span></p>\n"
  },
  {
    "tags": [
      "comparison",
      "convolution",
      "geometric-deep-learning",
      "graph-neural-networks"
    ],
    "owner": {
      "account_id": 4461054,
      "reputation": 1158,
      "user_id": 20430,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f19a3ac761df525d20390cf126e6219e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "razvanc92",
      "link": "https://ai.stackexchange.com/users/20430/razvanc92"
    },
    "is_answered": true,
    "view_count": 12058,
    "answer_count": 2,
    "score": 17,
    "last_activity_date": 1639925691,
    "creation_date": 1565945231,
    "last_edit_date": 1639925691,
    "question_id": 14003,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/14003/what-is-the-difference-between-graph-convolution-in-the-spatial-vs-spectral-doma",
    "title": "What is the difference between graph convolution in the spatial vs spectral domain?",
    "body": "<p>I've been reading different papers regarding graph convolution and it seems that they come into two flavors: spatial and spectral. From what I can see the main difference between the two approaches is that for spatial you're directly multiplying the adjacency matrix with the signal whereas for the spectral version you're using the Laplacian matrix.</p>\n<p>Am I missing something, or are there any other differences that I am not aware of?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "comparison",
      "terminology",
      "online-learning",
      "active-learning"
    ],
    "owner": {
      "account_id": 7737741,
      "reputation": 313,
      "user_id": 40250,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/cedc3085d7454649f5b4c5ab345ba3ba?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "David",
      "link": "https://ai.stackexchange.com/users/40250/david"
    },
    "is_answered": true,
    "view_count": 7103,
    "accepted_answer_id": 23227,
    "answer_count": 2,
    "score": 17,
    "last_activity_date": 1604062862,
    "creation_date": 1598300471,
    "last_edit_date": 1604062862,
    "question_id": 23226,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/23226/what-is-the-difference-between-active-learning-and-online-learning",
    "title": "What is the difference between active learning and online learning?",
    "body": "<p>The definitions for these two appear to be very similar, and frankly, I've been only using the term &quot;active learning&quot; the past couple of years. What is the actual difference between the two? Is one a subset of the other?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "comparison",
      "q-learning",
      "dqn",
      "deep-rl"
    ],
    "owner": {
      "account_id": 7328064,
      "reputation": 1318,
      "user_id": 2844,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/hnwBo.jpg?s=256",
      "display_name": "Dan D",
      "link": "https://ai.stackexchange.com/users/2844/dan-d"
    },
    "is_answered": true,
    "view_count": 14170,
    "accepted_answer_id": 25916,
    "answer_count": 3,
    "score": 17,
    "last_activity_date": 1716549727,
    "creation_date": 1611308501,
    "last_edit_date": 1611351111,
    "question_id": 25913,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/25913/what-is-the-difference-between-q-learning-deep-q-learning-and-deep-q-network",
    "title": "What is the difference between Q-learning, Deep Q-learning and Deep Q-network?",
    "body": "<p><strong>Q-learning</strong> uses a table to store all state-action pairs. Q-learning is a model-free RL algorithm, so how could there be the one called <strong>Deep Q-learning</strong>, as <em>deep</em> means using DNN; or maybe the state-action table (Q-table) is still there but the DNN is only for input reception (e.g. turning images into vectors)?</p>\n<p>Deep Q-network seems to be only the DNN part of the Deep Q-learning program, and Q-network seems the short for Deep Q-network.</p>\n<p><strong>Q-learning</strong>, <strong>Deep Q-learning</strong>, and <strong>Deep Q-network</strong>, what are the differences? May be there a comparison table between these 3 terms?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "language-model",
      "chatgpt"
    ],
    "owner": {
      "account_id": 4266052,
      "reputation": 271,
      "user_id": 67424,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/HJKwi.jpg?s=256",
      "display_name": "Sander van den Oord",
      "link": "https://ai.stackexchange.com/users/67424/sander-van-den-oord"
    },
    "is_answered": true,
    "view_count": 10407,
    "answer_count": 2,
    "score": 17,
    "last_activity_date": 1677642739,
    "creation_date": 1674832689,
    "last_edit_date": 1675010999,
    "question_id": 38923,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/38923/why-does-chatgpt-not-give-the-answer-text-all-at-once",
    "title": "Why does ChatGPT not give the answer text all at once?",
    "body": "<p>When ChatGPT is generating an answer to my question, it generates it word by word.<br />\nSo I actually have to wait until I get the final answer.</p>\n<p>Is this just for show?<br />\nOr is it really real-time generating the answer word by word not knowing yet what the next word will be?</p>\n<p>Why does it not give the complete answer text all at once?</p>\n"
  },
  {
    "tags": [
      "reference-request",
      "social"
    ],
    "owner": {
      "account_id": 2846135,
      "reputation": 171,
      "user_id": 91136,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/ei1Q2.jpg?s=256",
      "display_name": "Ora G. Walters",
      "link": "https://ai.stackexchange.com/users/91136/ora-g-walters"
    },
    "is_answered": true,
    "view_count": 8040,
    "answer_count": 3,
    "score": 17,
    "last_activity_date": 1737224097,
    "creation_date": 1736977863,
    "last_edit_date": 1737224097,
    "question_id": 47800,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/47800/how-does-ai-consume-water",
    "title": "How does AI &quot;consume&quot; water?",
    "body": "<p>There's been a lot of talk about how much water AI uses. I keep reading that they use it for cooling, but after they do that where's the water going? From my understanding, liquid cooled PCs are fully enclosed not requiring top-ups unless there are leaks. Even if they do flow water in and back out, that wouldn't consume any water.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "reference-request",
      "hyperparameter-optimization",
      "architecture",
      "neuroevolution"
    ],
    "owner": {
      "account_id": 2685693,
      "reputation": 643,
      "user_id": 1270,
      "user_type": "registered",
      "accept_rate": 83,
      "profile_image": "https://i.sstatic.net/n8zHs.jpg?s=256",
      "display_name": "Zolt&#225;n Schmidt",
      "link": "https://ai.stackexchange.com/users/1270/zolt%c3%a1n-schmidt"
    },
    "is_answered": true,
    "view_count": 770,
    "accepted_answer_id": 1399,
    "answer_count": 2,
    "score": 16,
    "last_activity_date": 1640430707,
    "creation_date": 1470432577,
    "last_edit_date": 1639406105,
    "question_id": 1391,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1391/how-can-i-automate-the-choice-of-the-architecture-of-a-neural-network-for-an-arb",
    "title": "How can I automate the choice of the architecture of a neural network for an arbitrary problem?",
    "body": "<p>Assume that I want to solve an issue with a neural network that either I can't fit to existing architectures (perceptron, Konohen, etc) or I'm simply not aware of the existence of those or I'm unable to understand their mechanics and I rely on my own instead.</p>\n<p>How can I automate the choice of the architecture/topology (that is, the number of layers, the type of activations, the type and direction of the connections, etc.) of a neural network for an arbitrary problem?</p>\n<p>I'm a beginner, yet I realized that in some architectures (or, at least, in perceptrons) it is very hard if not impossible to understand the inner mechanics, as the neurons of the hidden layers don't express any mathematically meaningful context.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "math",
      "activation-functions"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user3642"
    },
    "is_answered": true,
    "view_count": 10429,
    "answer_count": 4,
    "score": 16,
    "last_activity_date": 1550113371,
    "creation_date": 1482362815,
    "last_edit_date": 1550113371,
    "question_id": 2526,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2526/why-do-activation-functions-need-to-be-differentiable-in-the-context-of-neural-n",
    "title": "Why do activation functions need to be differentiable in the context of neural networks?",
    "body": "<p>Why should an activation function of a neural network be differentiable? Is it strictly necessary or is it just advantageous?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "knowledge-representation",
      "expert-systems",
      "symbolic-computing",
      "knowledge-base"
    ],
    "owner": {
      "account_id": 4353517,
      "reputation": 1065,
      "user_id": 5351,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Lr5jY.jpg?s=256",
      "display_name": "Lerner Zhang",
      "link": "https://ai.stackexchange.com/users/5351/lerner-zhang"
    },
    "is_answered": true,
    "view_count": 2253,
    "answer_count": 3,
    "score": 16,
    "last_activity_date": 1621667480,
    "creation_date": 1488697645,
    "last_edit_date": 1582073666,
    "question_id": 2922,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2922/what-roles-knowledge-bases-play-now-and-will-play-in-the-future",
    "title": "What roles knowledge bases play now and will play in the future?",
    "body": "<p>Nowadays, artificial intelligence seems almost equal to machine learning, especially deep learning. Some have said that deep learning will replace human experts, traditionally very important for feature engineering, in this field. It is said that two breakthroughs underpinned the rise of deep learning: on one hand, neuroscience, and <a href=\"https://en.wikipedia.org/wiki/Neuroplasticity\" rel=\"nofollow noreferrer\">neuroplasticity</a> in particular, tells us that like the human brain, which is highly plastic, artificial networks can be utilized to model almost all functions; on the other hand, the increase in computational power, in particular the introduction of GPU and FPGA, has boosted algorithmic intelligence in a magnificent way, and has been making the models created decades ago immensely powerful and versatile. I'll add that the big data (mostly labeled data) accumulated over the past years is also relevant.   </p>\n\n<p>Such developments bring computer vision (and voice recognition) into a new era, but in natural language processing and expert systems, the situation hasn't seemed to have changed very much. </p>\n\n<p>Achieving common sense for the neural networks seems a tall order, but most sentences, conversations and short texts contain inferences that should be drawn from the background world knowledge. Thus knowledge graphing is of great importance to artificial intelligence. Neural networks can be harnessed in building knowledge bases but it seems that neural network models have difficulty utilizing these constructed knowledge bases.  </p>\n\n<p>My questions are: </p>\n\n<ol>\n<li><p>Is a knowledge base (for instance, a \"knowledge graph\", as coined by Google) a promising branch in AI? If so, in what ways KB can empower machine learning? How can we incorporate discrete latent variables into NLU and NLG?</p></li>\n<li><p>For survival in an age dominated by DL, where is the direction for the knowledge base (or the umbrella term symbolic approach)? Is <a href=\"http://www.wolfram.com/\" rel=\"nofollow noreferrer\">Wolfram</a>-like z dynamic knowledge base the new direction? Or any new directions?</p></li>\n</ol>\n\n<p>Am I missing something fundamental, or some ideas that address these issues?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "training",
      "weights",
      "weights-initialization"
    ],
    "owner": {
      "account_id": 1636408,
      "reputation": 271,
      "user_id": 10294,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/zRqBI.jpg?s=256",
      "display_name": "Matas Vaitkevicius",
      "link": "https://ai.stackexchange.com/users/10294/matas-vaitkevicius"
    },
    "is_answered": true,
    "view_count": 4095,
    "accepted_answer_id": 4341,
    "answer_count": 5,
    "score": 16,
    "last_activity_date": 1678931831,
    "creation_date": 1508568735,
    "last_edit_date": 1608492055,
    "question_id": 4320,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4320/why-are-the-initial-weights-of-neural-networks-randomly-initialised",
    "title": "Why are the initial weights of neural networks randomly initialised?",
    "body": "<p>This might sound silly to someone who has plenty of experience with neural networks but it bothers me...</p>\n<p>Random initial weights might give you better results that would be somewhat closer to what a trained neural network should look like, but it might as well be the exact opposite of what it should be, while 0.5, or some other average for the range of reasonable weights' values, would sound like a good default setting.</p>\n<p><strong>Why are the initial weights of neural networks randomly initialized rather than being all set to, for example, 0.5?</strong></p>\n"
  },
  {
    "tags": [
      "terminology",
      "search",
      "definitions"
    ],
    "owner": {
      "account_id": 1461614,
      "reputation": 309,
      "user_id": 14862,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-ZaySVPzbyv0/AAAAAAAAAAI/AAAAAAAAAEE/U6JfJjWJ1gk/s256-rj/photo.jpg",
      "display_name": "tahasozgen",
      "link": "https://ai.stackexchange.com/users/14862/tahasozgen"
    },
    "is_answered": true,
    "view_count": 25937,
    "protected_date": 1611526379,
    "accepted_answer_id": 5951,
    "answer_count": 1,
    "score": 16,
    "last_activity_date": 1601118017,
    "creation_date": 1523198292,
    "last_edit_date": 1562444149,
    "question_id": 5949,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5949/what-is-the-fringe-in-the-context-of-search-algorithms",
    "title": "What is the fringe in the context of search algorithms?",
    "body": "<p>What is the <em>fringe</em> in the context of search algorithms?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "terminology"
    ],
    "owner": {
      "account_id": 2738792,
      "reputation": 1097,
      "user_id": 7402,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/72bd5e3f73d1cd1774d2235b6bfab11d?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Blaszard",
      "link": "https://ai.stackexchange.com/users/7402/blaszard"
    },
    "is_answered": true,
    "view_count": 11108,
    "accepted_answer_id": 7360,
    "answer_count": 3,
    "score": 16,
    "last_activity_date": 1578651451,
    "creation_date": 1533047671,
    "last_edit_date": 1550844608,
    "question_id": 7359,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7359/what-is-a-trajectory-in-reinforcement-learning",
    "title": "What is a &quot;trajectory&quot; in reinforcement learning?",
    "body": "<p>I'm now learning about reinforcement learning, but I just found the word \"trajectory\" in <a href=\"https://datascience.stackexchange.com/a/24924/8432\">this answer</a>.</p>\n\n<p>However, I'm not sure what it means. I read a few books on the Reinforcement Learning but none of them mentioned it. Usually these introductionary books mention agent, environment, action, policy, and reward, but not \"trajectory\".</p>\n\n<p>So, what does it mean? \nAccording to <a href=\"https://www.quora.com/In-the-context-of-reinforcement-learning-what-is-the-difference-between-a-trajectory-and-a-policy-Also-what-is-the-difference-between-trajectory-optimization-and-policy-optimization\" rel=\"noreferrer\">this answer</a> over Quora:</p>\n\n<blockquote>\n  <p>In reinforcement learning terminology, a trajectory <span class=\"math-container\">$\\tau$</span> is the path of the agent through the state space up until the horizon <span class=\"math-container\">$H$</span>. The goal of an on-policy algorithm is to maximize the expected reward of the agent over trajectories.</p>\n</blockquote>\n\n<p>Does it mean that the \"trajectory\" is the total path from the current state the agent is in to the final state (terminal state) that the episode finishes at? Or is it something else? (I'm not sure what the \"horizon\" mean, either).</p>\n"
  },
  {
    "tags": [
      "definitions",
      "intelligence"
    ],
    "owner": {
      "account_id": 12054183,
      "reputation": 379,
      "user_id": 17209,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/bp93G.jpg?s=256",
      "display_name": "user79161",
      "link": "https://ai.stackexchange.com/users/17209/user79161"
    },
    "is_answered": true,
    "view_count": 1005,
    "protected_date": 1561666010,
    "accepted_answer_id": 7671,
    "answer_count": 5,
    "score": 16,
    "last_activity_date": 1735233338,
    "creation_date": 1534601875,
    "last_edit_date": 1639312237,
    "question_id": 7624,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7624/what-is-the-most-general-definition-of-intelligence",
    "title": "What is the most general definition of &quot;intelligence&quot;?",
    "body": "<p>When we talk about artificial intelligence, human intelligence, or any other form of intelligence, what do we mean by the term <strong>intelligence</strong> in a general sense? What would you call intelligent and what not? In other words, how do we define the term <strong>intelligence</strong> in the most general possible way?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "proximal-policy-optimization",
      "discrete-action-spaces",
      "action-spaces"
    ],
    "owner": {
      "account_id": 6980314,
      "reputation": 163,
      "user_id": 17818,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e493df986f32a2cb26b474719080c447?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Max",
      "link": "https://ai.stackexchange.com/users/17818/max"
    },
    "is_answered": true,
    "view_count": 7930,
    "accepted_answer_id": 8564,
    "answer_count": 3,
    "score": 16,
    "last_activity_date": 1605640536,
    "creation_date": 1535558656,
    "last_edit_date": 1605640536,
    "question_id": 7755,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7755/how-to-implement-a-variable-action-space-in-proximal-policy-optimization",
    "title": "How to implement a variable action space in Proximal Policy Optimization?",
    "body": "<p>I'm coding a Proximal Policy Optimization (PPO) agent with the <a href=\"https://github.com/reinforceio/tensorforce\" rel=\"nofollow noreferrer\">Tensorforce library</a> (which is built on top of TensorFlow).</p>\n<p>The first environment was very simple. Now, I'm diving into a more complex environment, where all the actions are not available at each step.</p>\n<p>Let's say there are 5 actions and their availability depends on an internal state (which is defined by the previous action and/or the new state/observation space):</p>\n<ul>\n<li>2 actions (0 and 1) are always available</li>\n<li>2 actions (2 and 3) are only available when the internal state is 0</li>\n<li>1 action (4) is only available when the internal state is 1</li>\n</ul>\n<p>Hence, there are 4 actions available when the internal state is 0 and 3 actions available when the internal state is 1.</p>\n<p>I'm thinking of a few possibilities to implement that:</p>\n<ol>\n<li><p>Change the action space at each step, depending on the internal state. I assume this is nonsense.</p>\n</li>\n<li><p>Do nothing: let the model understand that choosing an unavailable action has no impact.</p>\n</li>\n<li><p>Do <em>almost</em> nothing: impact slightly negatively the reward when the model chooses an unavailable action.</p>\n</li>\n<li><p>Help the model: by incorporating an integer into the state/observation space that informs the model what's the internal state value + bullet point 2 or 3</p>\n</li>\n</ol>\n<p>Are there other ways to implement this? From your experience, which one would be the best?</p>\n"
  },
  {
    "tags": [
      "convolutional-neural-networks",
      "comparison",
      "terminology",
      "feature-maps",
      "receptive-field"
    ],
    "owner": {
      "account_id": 6787088,
      "reputation": 261,
      "user_id": 19485,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b5cb9fe975f6d5a7417f9ab0efd87ba0?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Monica Heddneck",
      "link": "https://ai.stackexchange.com/users/19485/monica-heddneck"
    },
    "is_answered": true,
    "view_count": 5799,
    "answer_count": 1,
    "score": 16,
    "last_activity_date": 1635340125,
    "creation_date": 1541032938,
    "last_edit_date": 1634088174,
    "question_id": 8701,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/8701/what-is-the-difference-between-a-receptive-field-and-a-feature-map",
    "title": "What is the difference between a receptive field and a feature map?",
    "body": "<p>In a CNN, the <em>receptive field</em> is the portion of the image used to compute the filter's output. But one filter's output (which is also called a \"feature map\") is the next filter's input.</p>\n\n<p>What's the difference between a receptive field and a feature map?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "stochastic-policy",
      "deterministic-policy",
      "policies",
      "environment"
    ],
    "owner": {
      "account_id": 4148701,
      "reputation": 42397,
      "user_id": 2444,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e6fce047cba96a601a9dac66df4cd2e8?s=256&d=identicon&r=PG",
      "display_name": "nbro",
      "link": "https://ai.stackexchange.com/users/2444/nbro"
    },
    "is_answered": true,
    "view_count": 4478,
    "protected_date": 1641581238,
    "answer_count": 3,
    "score": 16,
    "last_activity_date": 1550284643,
    "creation_date": 1550236850,
    "last_edit_date": 1550240541,
    "question_id": 10591,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10591/is-the-optimal-policy-always-stochastic-if-the-environment-is-also-stochastic",
    "title": "Is the optimal policy always stochastic if the environment is also stochastic?",
    "body": "<p>Is the optimal policy always stochastic (that is, a map from states to a probability distribution over actions) if the environment is also stochastic? </p>\n\n<p>Intuitively, if the environment is <em>deterministic</em> (that is, if the agent is in a state <span class=\"math-container\">$s$</span> and takes action <span class=\"math-container\">$a$</span>, then the next state <span class=\"math-container\">$s'$</span> is always the same, no matter which time step), then the optimal policy should also be deterministic (that is, it should be a map from states to actions, and not to a probability distribution over actions).</p>\n"
  },
  {
    "tags": [
      "terminology",
      "generative-adversarial-networks"
    ],
    "owner": {
      "account_id": 2304865,
      "reputation": 2859,
      "user_id": 16565,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/e4iCE.jpg?s=256",
      "display_name": "malioboro",
      "link": "https://ai.stackexchange.com/users/16565/malioboro"
    },
    "is_answered": true,
    "view_count": 15592,
    "accepted_answer_id": 12500,
    "answer_count": 2,
    "score": 16,
    "last_activity_date": 1558731020,
    "creation_date": 1558681487,
    "question_id": 12499,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12499/why-is-it-called-latent-vector",
    "title": "Why is it called Latent Vector?",
    "body": "<p>I just learned about GAN and I'm a little bit confused about the naming of Latent Vector.</p>\n\n<ul>\n<li><p>First, In my understanding, a definition of a latent variable is a random variable that can't be measured directly (we needs some calculation from other variables to get its value). For example, knowledge is a latent variable. Is it correct?</p></li>\n<li><p>And then, in GAN, a latent vector <span class=\"math-container\">$z$</span> is a random variable which is an input of the generator network. I read in some tutorials, it's generated using only a simple random function:</p>\n\n<pre><code>z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n</code></pre></li>\n</ul>\n\n<p>then how are the two things related? why don't we use the term \"a vector with random values between -1 and 1\" when referring <span class=\"math-container\">$z$</span> (generator's input) in GAN?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "reinforcement-learning",
      "research",
      "markov-decision-process"
    ],
    "owner": {
      "account_id": 6229308,
      "reputation": 481,
      "user_id": 10191,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2fc66a9c72b09fcbec69fbf7d9d14ed8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Amin",
      "link": "https://ai.stackexchange.com/users/10191/amin"
    },
    "is_answered": true,
    "view_count": 1196,
    "answer_count": 1,
    "score": 16,
    "last_activity_date": 1563487138,
    "creation_date": 1563450890,
    "question_id": 13418,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/13418/how-to-stay-a-up-to-date-researcher-in-ml-rl-community",
    "title": "How to stay a up-to-date researcher in ML/RL community?",
    "body": "<p>As a student who wants to work on machine learning, I would like to know how it is possible to start my studies and how to follow it to stay up-to-date. For example, I am willing to work on RL and MAB problems, but there are huge literatures on these topics. Moreover, these topics are studied by researchers from different communities such as AI and ML, Operations Research, Control Engineering, Statistics, etc. And, I think that several papers are published on these topics every week which make it so difficult to follow them. </p>\n\n<p>I would be thankful if someone can suggest a road-map to start studying these topics, follow them and how I should select and study new published papers. Finally, I am willing to know the new trend in RL and MAB problem.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "hyperparameter-optimization",
      "cross-validation",
      "generalization"
    ],
    "owner": {
      "account_id": 466776,
      "reputation": 161,
      "user_id": 23125,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8b1782ccd4f6a1d27fa767c50d57ada4?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Philipp Cannons",
      "link": "https://ai.stackexchange.com/users/23125/philipp-cannons"
    },
    "is_answered": false,
    "view_count": 425,
    "answer_count": 1,
    "score": 16,
    "last_activity_date": 1746101460,
    "creation_date": 1569389596,
    "last_edit_date": 1609437719,
    "question_id": 15616,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/15616/will-parameter-sweeping-on-one-split-of-data-followed-by-cross-validation-discov",
    "title": "Will parameter sweeping on one split of data followed by cross validation discover the right hyperparameters?",
    "body": "<p>Let's call our dataset splits train/test/evaluate. We're in a situation where we require months of data. So we prefer to use the evaluation dataset as infrequently as possible to avoid polluting our results. Instead, we do 10 fold cross validation (CV) to estimate how well the model might generalize.</p>\n<p>We're training deep learning models that take between 24-48 hours each, and the process of parameter sweeping is obviously very slow when performing 10-fold cross validation.</p>\n<p>Does anyone have any experience or citations for how well parameter sweeping on one split of the data followed by cross validation (used to estimate how well it generalizes) works?</p>\n<p>I suspect it's highly dependent on the distribution of data and local minima &amp; maxima of the hyper parameters, but I wanted to ask.</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "programming-languages",
      "c++",
      "gpt-3",
      "c"
    ],
    "owner": {
      "account_id": 16758995,
      "reputation": 285,
      "user_id": 36067,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/14f03f80777f05112c3902b768d9d5e1?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Otherness",
      "link": "https://ai.stackexchange.com/users/36067/otherness"
    },
    "is_answered": true,
    "view_count": 24827,
    "accepted_answer_id": 27762,
    "answer_count": 1,
    "score": 16,
    "last_activity_date": 1676809185,
    "creation_date": 1620778018,
    "question_id": 27761,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/27761/what-language-is-the-gpt-3-engine-written-in",
    "title": "What language is the GPT-3 engine written in?",
    "body": "<p>I know that the API is python based, but what's the gpt-3 engine written in mostly? C? C++? I'm having some trouble finding this info.</p>\n"
  },
  {
    "tags": [
      "intelligence-testing",
      "intelligence-quotient"
    ],
    "owner": {
      "account_id": 8761221,
      "reputation": 1082,
      "user_id": 72,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f22ad5db7375b6fb8b2193999c5297c8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Vishnu JK",
      "link": "https://ai.stackexchange.com/users/72/vishnu-jk"
    },
    "is_answered": true,
    "view_count": 1517,
    "protected_date": 1559863838,
    "accepted_answer_id": 65,
    "answer_count": 4,
    "score": 15,
    "last_activity_date": 1639518590,
    "creation_date": 1470154114,
    "last_edit_date": 1639518590,
    "question_id": 41,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/41/can-the-iq-of-an-ai-program-be-measured",
    "title": "Can the IQ of an AI program be measured?",
    "body": "<p>Can an AI program have an IQ? In other words, can the IQ of an AI program be measured? Like how humans can do an IQ test.</p>\n"
  },
  {
    "tags": [
      "terminology",
      "history"
    ],
    "owner": {
      "account_id": 40019,
      "reputation": 2533,
      "user_id": 55,
      "user_type": "registered",
      "accept_rate": 43,
      "profile_image": "https://i.sstatic.net/FkIiK.png?s=256",
      "display_name": "WilliamKF",
      "link": "https://ai.stackexchange.com/users/55/williamkf"
    },
    "is_answered": true,
    "view_count": 5946,
    "protected_date": 1587476781,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1587467545,
    "creation_date": 1470155120,
    "last_edit_date": 1570241220,
    "question_id": 58,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/58/who-first-coined-the-term-artificial-intelligence",
    "title": "Who first coined the term Artificial Intelligence?",
    "body": "<p>Who first coined the term Artificial Intelligence? Is there a published research paper that first used that term?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "ai-design",
      "genetic-algorithms",
      "evolutionary-algorithms",
      "neuroevolution"
    ],
    "owner": {
      "account_id": 5129611,
      "reputation": 2945,
      "user_id": 145,
      "user_type": "moderator",
      "accept_rate": 62,
      "profile_image": "https://i.sstatic.net/IIYyh.png?s=256",
      "display_name": "Mithical",
      "link": "https://ai.stackexchange.com/users/145/mithical"
    },
    "is_answered": true,
    "view_count": 645,
    "accepted_answer_id": 1626,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1562535511,
    "creation_date": 1471202172,
    "last_edit_date": 1562535511,
    "question_id": 1618,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1618/how-should-i-encode-the-structure-of-a-neural-network-into-a-genome",
    "title": "How should I encode the structure of a neural network into a genome?",
    "body": "<p>For a deterministic problem space, I need to find a neural network with the optimal node and link structure. I want to use a genetic algorithm to simulate many neural networks to find the best network structure for the problem domain. </p>\n\n<p>I've never used genetic algorithms for a task like this before. <em>What are the practical considerations? Specifically, how should I encode the structure of a neural network into a genome?</em></p>\n"
  },
  {
    "tags": [
      "comparison",
      "programming-languages",
      "nasa"
    ],
    "owner": {
      "account_id": 6170538,
      "reputation": 167,
      "user_id": 4027,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/1789568906/picture?type=large",
      "display_name": "Rahul",
      "link": "https://ai.stackexchange.com/users/4027/rahul"
    },
    "is_answered": true,
    "view_count": 18996,
    "protected_date": 1567433923,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1640120889,
    "creation_date": 1480844937,
    "last_edit_date": 1567433957,
    "question_id": 2429,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2429/why-is-sanskrit-the-best-language-for-ai",
    "title": "Why is Sanskrit the best language for AI?",
    "body": "<p>According to NASA scientist Rick Briggs, Sanskrit is the best language for AI. I want to know how Sanskrit is useful. What's the problem with other languages? Are they really using Sanskrit in AI programming or going to do so? What part of an AI program requires such language?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "training",
      "papers",
      "generative-adversarial-networks",
      "discriminator"
    ],
    "owner": {
      "account_id": 9818808,
      "reputation": 421,
      "user_id": 12242,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/fbd3dafadd4c6c96a3669657379d8e58?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Inkplay_",
      "link": "https://ai.stackexchange.com/users/12242/inkplay"
    },
    "is_answered": true,
    "view_count": 717,
    "answer_count": 4,
    "score": 15,
    "last_activity_date": 1695499440,
    "creation_date": 1528327644,
    "last_edit_date": 1656233845,
    "question_id": 6678,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6678/can-some-one-help-me-understand-this-paragraph-from-nvidias-progressive-gan-pap",
    "title": "Can some one help me understand this paragraph from Nvidia&#39;s progressive GAN paper?",
    "body": "<p>In the paper <a href=\"https://research.nvidia.com/sites/default/files/pubs/2017-10_Progressive-Growing-of/karras2018iclr-paper.pdf\" rel=\"nofollow noreferrer\">Progressive growing of gans for improved quality, stability, and variation</a> (ICLR, 2018) by Nvidia researchers, the authors write</p>\n<blockquote>\n<p>Furthermore, we observe that mode collapses traditionally\nplaguing GANs tend to happen very quickly, over the course of a dozen minibatches. Commonly\nthey start when the discriminator overshoots, leading to exaggerated gradients, and an unhealthy\ncompetition follows where the signal magnitudes escalate in both networks. We propose a mechanism to stop the generator from participating in such escalation, overcoming the issue (Section 4.2)</p>\n</blockquote>\n<p>What do they mean by &quot;the discriminator overshoots&quot; and &quot;the signal magnitudes escalate in both networks&quot;?</p>\n<p>My current intuition is that the discriminator gets too good too soon, which causes the generator to spike and try to play catch up. That would be the unhealthy competition that they are talking about. Model collapse is the side effect where the generator has trouble playing catch up and decides to play it safe by generating slightly varied images to increase its accuracy. Is this way of interpreting the above paragraph correct?</p>\n"
  },
  {
    "tags": [
      "comparison",
      "terminology",
      "definitions"
    ],
    "owner": {
      "account_id": 8423357,
      "reputation": 1173,
      "user_id": 1581,
      "user_type": "registered",
      "accept_rate": 40,
      "profile_image": "https://i.sstatic.net/jqtE1.jpg?s=256",
      "display_name": "quintumnia",
      "link": "https://ai.stackexchange.com/users/1581/quintumnia"
    },
    "is_answered": true,
    "view_count": 10733,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1606521976,
    "creation_date": 1533580513,
    "last_edit_date": 1578182088,
    "question_id": 7446,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7446/what-is-the-difference-between-artificial-intelligence-and-computational-intelli",
    "title": "What is the difference between artificial intelligence and computational intelligence?",
    "body": "<p>Having analyzed and reviewed a certain amount of articles and questions, apparently, the expression <em>computational intelligence</em> (CI) is not used consistently and it is still unclear the relationship between CI and artificial intelligence (AI).</p>\n\n<p>According to <a href=\"https://cis.ieee.org/about/mission-vision-foi\" rel=\"noreferrer\">IEEE computational intelligence society</a></p>\n\n<blockquote>\n  <p>The Field of Interest of the Computational Intelligence Society (CIS) shall be the theory, design, application, and development of biologically and linguistically motivated computational paradigms emphasizing neural networks, connectionist systems, genetic algorithms, evolutionary programming, fuzzy systems, and hybrid intelligent systems in which these paradigms are contained.</p>\n</blockquote>\n\n<p>which suggests that CI could be a sub-field of AI or an umbrella term used to group certain AI sub-fields or topics, such as genetic algorithms or fuzzy systems.</p>\n\n<p>What is the difference between artificial intelligence and computational intelligence? Is CI just a synonym for AI?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "reinforcement-learning",
      "game-ai",
      "monte-carlo-tree-search",
      "alphazero"
    ],
    "owner": {
      "account_id": 10506266,
      "reputation": 511,
      "user_id": 16917,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6d94b5dba7e8e85aaa7f9becf6886e62?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Inertial Ignorance",
      "link": "https://ai.stackexchange.com/users/16917/inertial-ignorance"
    },
    "is_answered": true,
    "view_count": 4448,
    "accepted_answer_id": 7590,
    "answer_count": 3,
    "score": 15,
    "last_activity_date": 1591134010,
    "creation_date": 1534385595,
    "last_edit_date": 1591134010,
    "question_id": 7589,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7589/does-monte-carlo-tree-search-qualify-as-machine-learning",
    "title": "Does Monte Carlo tree search qualify as machine learning?",
    "body": "<p>To the best of my understanding, the Monte Carlo tree search (MCTS) algorithm is an alternative to minimax for searching a tree of nodes. It works by choosing a move (generally, the one with the highest chance of being the best), and then performing a random playout on the move to see what the result is. This process continues for the amount of time allotted.</p>\n\n<p>This doesn't sound like machine learning, but rather a way to traverse a tree. However, I've heard that AlphaZero uses MCTS, so I'm confused. If AlphaZero uses MCTS, then why does AlphaZero learn? Or did AlphaZero do some kind of machine learning before it played any matches, and then use the intuition it gained from machine learning to know which moves to spend more time playing out with MCTS?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "terminology",
      "policies",
      "stationary-policy"
    ],
    "owner": {
      "account_id": 12909486,
      "reputation": 438,
      "user_id": 12640,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/841fad8db76c277babc3d4bb5ba3bf5f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Paula Vega",
      "link": "https://ai.stackexchange.com/users/12640/paula-vega"
    },
    "is_answered": true,
    "view_count": 7806,
    "accepted_answer_id": 7657,
    "answer_count": 4,
    "score": 15,
    "last_activity_date": 1611251878,
    "creation_date": 1534759780,
    "last_edit_date": 1550700142,
    "question_id": 7640,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7640/what-does-stationary-mean-in-the-context-of-reinforcement-learning",
    "title": "What does &quot;stationary&quot; mean in the context of reinforcement learning?",
    "body": "<p>I think I've seen the expressions \"stationary data\", \"stationary dynamics\" and \"stationary policy\", among others, in the context of reinforcement learning. What does it mean? I think stationary policy means that the policy does not depend on time, and only on state. But isn't that a unnecessary distinction? If the policy depends on time and not only on the state, then strictly speaking time should also be part of the state.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "reinforcement-learning",
      "ai-design",
      "alphazero",
      "alphago-zero"
    ],
    "owner": {
      "account_id": 5523531,
      "reputation": 3020,
      "user_id": 12201,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/08ea907d3b57005305612f88c0a85b03?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "chessprogrammer",
      "link": "https://ai.stackexchange.com/users/12201/chessprogrammer"
    },
    "is_answered": true,
    "view_count": 3725,
    "accepted_answer_id": 7981,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1542594091,
    "creation_date": 1536956487,
    "last_edit_date": 1542594091,
    "question_id": 7979,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7979/why-does-the-policy-network-in-alphazero-work",
    "title": "Why does the policy network in AlphaZero work?",
    "body": "<p>In AlphaZero, the <em>policy network</em> (or head of the network) maps game states to a distribution of the likelihood of taking each action. This distribution covers all possible actions from that state.</p>\n\n<p>How is such a network possible? The possible actions from each state are vastly different than subsequent states. So, how would each possible action from a given state be represented in the network's output, and what about the network design would stop the network from considering an illegal action?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "off-policy-methods",
      "on-policy-methods",
      "online-learning",
      "offline-reinforcement-learning"
    ],
    "owner": {
      "account_id": 4148701,
      "reputation": 42397,
      "user_id": 2444,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e6fce047cba96a601a9dac66df4cd2e8?s=256&d=identicon&r=PG",
      "display_name": "nbro",
      "link": "https://ai.stackexchange.com/users/2444/nbro"
    },
    "is_answered": true,
    "view_count": 9613,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1741785262,
    "creation_date": 1549723730,
    "last_edit_date": 1678134843,
    "question_id": 10474,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10474/what-is-the-relation-between-online-or-offline-learning-and-on-policy-or-off",
    "title": "What is the relation between online (or offline) learning and on-policy (or off-policy) algorithms?",
    "body": "<p>In the context of RL, there is the notion of <strong>on-policy</strong> and <strong>off-policy</strong> algorithms. I understand the difference between on-policy and off-policy algorithms. Moreover, in RL, there's also the notion of <strong>online</strong> and <strong>offline</strong> learning.</p>\n<p>What is the relation (including the differences) between online learning and on-policy algorithms? Similarly, what is the relation between offline learning and off-policy algorithms?</p>\n<p>Finally, is there any relation between online (or offline) learning and off-policy (or on-policy) algorithms? For example, can an on-policy algorithm perform offline learning? If yes, can you explain why?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "reinforcement-learning",
      "statistical-ai",
      "experience-replay",
      "iid"
    ],
    "owner": {
      "account_id": 4148701,
      "reputation": 42397,
      "user_id": 2444,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e6fce047cba96a601a9dac66df4cd2e8?s=256&d=identicon&r=PG",
      "display_name": "nbro",
      "link": "https://ai.stackexchange.com/users/2444/nbro"
    },
    "is_answered": true,
    "view_count": 7089,
    "answer_count": 3,
    "score": 15,
    "last_activity_date": 1634483704,
    "creation_date": 1550928607,
    "last_edit_date": 1605530337,
    "question_id": 10839,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10839/why-exactly-do-neural-networks-require-i-i-d-data",
    "title": "Why exactly do neural networks require i.i.d. data?",
    "body": "<p>In reinforcement learning, successive states (actions and rewards) can be correlated. An <a href=\"http://www.incompleteideas.net/lin-92.pdf\" rel=\"noreferrer\">experience replay</a> buffer was used, in the <a href=\"https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\" rel=\"noreferrer\">DQN architecture</a>, to avoid training the neural network (NN), which represents the <span class=\"math-container\">$Q$</span> function, with correlated (or non-independent) data. In statistics, the i.i.d. (independently and identically distributed) assumption is often made. See e.g. <a href=\"https://stats.stackexchange.com/q/213464/82135\">this question</a>. <a href=\"https://qr.ae/TUfiQ2\" rel=\"noreferrer\">This</a> is another related question. In the case of humans, if consecutive data points are correlated, we may learn slowly (because the differences between those consecutive data points are not sufficient to infer more about the associated distribution).</p>\n<p>Mathematically, why exactly do (feed-forward) neural networks (or multi-layer perceptrons) require i.i.d. data (when being trained)? Is this only because <a href=\"https://www.ijcai.org/Proceedings/07/Papers/121.pdf\" rel=\"noreferrer\">we use back-propagation to train NNs</a>? If yes, why would back-propagation require i.i.d. data? Or is actually the optimisation algorithm (like gradient-descent) which requires i.i.d. data? Back-propagation is just the algorithm used to compute the gradients (which is e.g. used by GD to update the weights), so I think that back-propagation isn't really the problem.</p>\n<p>When using recurrent neural networks (RNNs), we apparently do not make this assumption, given that we expect consecutive data points to be highly correlated. So, why do feed-forward NNs required the i.i.d. assumption but not RNNs?</p>\n<p>I'm looking for a rigorous answer (ideally, a proof) and not just the intuition behind it. If there is a paper that answers this question, you can simply link us to it.</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "deep-rl",
      "hyper-parameters",
      "ddpg",
      "experience-replay"
    ],
    "owner": {
      "account_id": 5478988,
      "reputation": 351,
      "user_id": 23707,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/fa5764b5ee7afe97b5a925b24c4fa229?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "ycenycute",
      "link": "https://ai.stackexchange.com/users/23707/ycenycute"
    },
    "is_answered": true,
    "view_count": 17609,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1683048983,
    "creation_date": 1554388834,
    "last_edit_date": 1638522687,
    "question_id": 11640,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11640/how-large-should-the-replay-buffer-be",
    "title": "How large should the replay buffer be?",
    "body": "<p>I'm learning DDPG algorithm by following the following link: <a href=\"https://spinningup.openai.com/en/latest/algorithms/ddpg.html#the-policy-learning-side-of-ddpg\" rel=\"noreferrer\">Open AI Spinning Up document on DDPG</a>, where it is written</p>\n\n<blockquote>\n  <p>In order for the algorithm to have stable behavior, the replay buffer should be large enough to contain a wide range of experiences, but it may not always be good to keep everything. </p>\n</blockquote>\n\n<p>What does this mean? Is it related to the tuning of the parameter of the batch size in the algorithm?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "history"
    ],
    "owner": {
      "account_id": 4353517,
      "reputation": 1065,
      "user_id": 5351,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Lr5jY.jpg?s=256",
      "display_name": "Lerner Zhang",
      "link": "https://ai.stackexchange.com/users/5351/lerner-zhang"
    },
    "is_answered": true,
    "view_count": 5814,
    "accepted_answer_id": 13238,
    "answer_count": 4,
    "score": 15,
    "last_activity_date": 1719883440,
    "creation_date": 1562480086,
    "last_edit_date": 1719883440,
    "question_id": 13233,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/13233/why-did-machine-learning-only-become-viable-after-nvidias-chips-were-available",
    "title": "Why did machine learning only become viable after Nvidia&#39;s chips were available?",
    "body": "<p>I listened to a talk attended by a panel consisting of two influential Chinese scientists: <a href=\"https://sites.google.com/site/gangwang6/\" rel=\"nofollow noreferrer\">Wang Gang</a> and <a href=\"https://www.crunchbase.com/person/kai-yu#section-overview\" rel=\"nofollow noreferrer\">Yu Kai</a>, among others.</p>\n<p>When asked about the biggest bottleneck in the development of artificial intelligence in the near future (3 to 5 years), Yu Kai, who has a background in the hardware industry, said that hardware would be the essential problem and that we should focus most of our attention on it. He gave us two examples:</p>\n<ol>\n<li>In the early development of computers, we compared machines by their chips;</li>\n<li>ML/DL, which has been very popular in recent years, would be almost impossible without the employment of Nvidia's GPUs.</li>\n</ol>\n<p>The fundamental algorithms existed already in the 1980s and 1990s, but AI went through 3 AI winters and was not empirical until we could train models with GPU boosted mega servers.</p>\n<p>Dr. Wang then commented on Yu's opinions, stating that we should also develop software systems because we cannot build an autonomous car even if we combine all the GPUs and computational power in the world.</p>\n<p>Then, as usual, my mind wandered off, and I started thinking: what if those who could operate supercomputers in the 1980s and 1990s had utilized the then-existing neural network algorithms and trained them with tons of scientific data? Some people at that time could obviously attempt to build the AI systems we are building now.</p>\n<p>But why did AI/ML/DL become a hot topic and practical only decades later? Is it merely a matter of hardware, software, and data?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "optimization",
      "proofs",
      "no-free-lunch-theorems"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user9947"
    },
    "is_answered": true,
    "view_count": 1685,
    "accepted_answer_id": 15651,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1570210360,
    "creation_date": 1569592335,
    "last_edit_date": 1569602976,
    "question_id": 15650,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/15650/what-are-the-implications-of-the-no-free-lunch-theorem-for-machine-learning",
    "title": "What are the implications of the &quot;No Free Lunch&quot; theorem for machine learning?",
    "body": "<p>The No Free Lunch (NFL) theorem states (see the paper <a href=\"http://fab.cba.mit.edu/classes/865.18/design/optimization/nfl.pdf\" rel=\"noreferrer\">Coevolutionary Free Lunches</a> by David H. Wolpert and William G. Macready)</p>\n\n<blockquote>\n  <p>any two algorithms are equivalent when their performance is averaged across all possible problems</p>\n</blockquote>\n\n<p>Is the \"No Free Lunch\" theorem really true? What does it actually mean? A nice example (in ML context) illustrating this assertion would be nice.</p>\n\n<p>I have seen some algorithms which behave very poorly, and I have a hard time believing that they actually follow the above-stated theorem, so I am trying to understand whether my interpretation of this theorem is correct or not. Or is it just another ornamental theorem like Cybenko's Universal Approximation theorem?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "research",
      "reference-request",
      "computational-learning-theory",
      "generalization"
    ],
    "owner": {
      "account_id": 10879266,
      "reputation": 403,
      "user_id": 27548,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6e49e834d61b24d841ec7a8c4b5ae7a5?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Shirish",
      "link": "https://ai.stackexchange.com/users/27548/shirish"
    },
    "is_answered": true,
    "view_count": 663,
    "accepted_answer_id": 16737,
    "answer_count": 1,
    "score": 15,
    "last_activity_date": 1737210231,
    "creation_date": 1573809720,
    "last_edit_date": 1574698522,
    "question_id": 16536,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/16536/what-are-the-state-of-the-art-results-on-the-generalization-ability-of-deep-lear",
    "title": "What are the state-of-the-art results on the generalization ability of deep learning methods?",
    "body": "<p>I've read a few classic papers on different architectures of deep CNNs used to solve varied image-related problems. I'm aware there's some paradox in how deep networks generalize well despite seemingly overfitting training data. A lot of people in the data science field that I've interacted with agree that there's no explanation on <strong>why</strong> deep neural networks work as well as they do.</p>\n\n<p>That's gotten me interested in the theoretical basis for why deep nets work so well. Googling tells me it's kind of an open problem, but I'm not sure of the current state of research in answering this question. Notably, there are these two preprints that seem to tackle this question:</p>\n\n<ul>\n<li><p><a href=\"https://arxiv.org/pdf/1710.05468.pdf\" rel=\"noreferrer\">Generalization in Deep Learning</a> (2019)</p></li>\n<li><p><a href=\"https://arxiv.org/pdf/1905.11427.pdf\" rel=\"noreferrer\">Quantifying the generalization error in deep learning in terms of data distribution and neural network smoothness</a> (2019)</p></li>\n</ul>\n\n<p>If anyone else is interested in and following this research area, could you please explain the current state of research on this open problem? What are the latest works, preprints or publications that attempt to tackle it?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "philosophy",
      "agi",
      "chinese-room-argument",
      "artificial-curiosity"
    ],
    "owner": {
      "account_id": 6561427,
      "reputation": 385,
      "user_id": 31978,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/bdd2a44cf6c207cac43d6db419e55e09?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "joethemow",
      "link": "https://ai.stackexchange.com/users/31978/joethemow"
    },
    "is_answered": true,
    "view_count": 2317,
    "accepted_answer_id": 17085,
    "answer_count": 2,
    "score": 15,
    "last_activity_date": 1627227933,
    "creation_date": 1576263233,
    "last_edit_date": 1613150503,
    "question_id": 17084,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/17084/why-is-reinforcement-learning-not-the-answer-to-agi",
    "title": "Why is reinforcement learning not the answer to AGI?",
    "body": "<p>I previously asked a question about <a href=\"https://ai.stackexchange.com/q/17025/2444\">How can an AI freely make decisions?</a>. I got a great answer about how current algorithms lack <em>agency</em>.</p>\n<p>The first thing I thought of was <em>reinforcement learning</em>, since the entire concept is oriented around an agent getting rewarded for performing a correct action in an environment. It seems to me that reinforcement learning is the path to AGI.</p>\n<p>I'm also thinking: what if an agent was proactive instead of reactive? That would seem like a logical first step towards AGI. What if an agent could figure out what questions to ask based on their environment? For example, it experiences an apple falling from a tree and asks &quot;What made the Apple fall?&quot;. But it's similar to us not knowing what questions to ask about say the universe.</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "agi",
      "definitions",
      "superintelligence",
      "singularity"
    ],
    "owner": {
      "account_id": 17059954,
      "reputation": 153,
      "user_id": 34205,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/RtbWo.jpg?s=256",
      "display_name": "Displaza",
      "link": "https://ai.stackexchange.com/users/34205/displaza"
    },
    "is_answered": true,
    "view_count": 3666,
    "accepted_answer_id": 18597,
    "answer_count": 5,
    "score": 15,
    "last_activity_date": 1584105301,
    "creation_date": 1584012754,
    "last_edit_date": 1584020844,
    "question_id": 18587,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/18587/what-is-the-idea-called-involving-an-ai-that-will-eventually-rule-humanity",
    "title": "What is the idea called involving an AI that will eventually rule humanity?",
    "body": "<p>It's an idea I heard a while back but couldn't remember the name of. It involves the existence and development of an AI that will eventually rule the world and that if you don't fund or progress the AI then it will see you as \"hostile\" and kill you. Also, by knowing about this concept, it essentially makes you a candidate for such consideration, as people who didn't know about it won't understand to progress such an AI. From my understanding, this idea isn't taken that seriously, but I'm curious to know the name nonetheless.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "chatgpt"
    ],
    "owner": {
      "account_id": 29307947,
      "reputation": 344,
      "user_id": 75569,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/a/AAcHTtd04sb5lCDxdNIkCRC1LGnL2z_elp7RPs3qYeT8NPqbtQ=k-s256",
      "display_name": "ben svenssohn",
      "link": "https://ai.stackexchange.com/users/75569/ben-svenssohn"
    },
    "is_answered": true,
    "view_count": 7586,
    "accepted_answer_id": 41920,
    "answer_count": 6,
    "score": 15,
    "last_activity_date": 1693763670,
    "creation_date": 1693324574,
    "question_id": 41919,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/41919/why-do-many-ai-bots-feel-the-need-to-be-know-it-alls",
    "title": "Why do many AI bots feel the need to be know-it-alls?",
    "body": "<p>Having used various AI bots often over recent months, I noticed that often it will claim to know something, even if it doesn't.\nIt would then either explain something which is clearly nonsense, or by rambling on about how the answer isn't known in general.\nOr how, if asked for example- &quot;would you be able to explain X&quot; it wouldn't respond &quot;yes, I could&quot; but rather would elucidate X.\nHave they been trained to always respond as though it were a know-it-all?\n(Google's Bard and ChatGPT specifically, although I'm assuming only open-source AI will be answerable)</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "statistical-ai",
      "generalization"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 1332,
    "accepted_answer_id": 9,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1550961397,
    "creation_date": 1470152420,
    "last_edit_date": 1550961379,
    "question_id": 2,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2/how-does-noise-affect-generalization",
    "title": "How does noise affect generalization?",
    "body": "<p>Does increasing the noise in data help to improve the learning ability of a network? Does it make any difference or does it depend on the problem being solved? How is it affect the generalization process overall?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "genetic-algorithms",
      "terminology"
    ],
    "owner": {
      "account_id": 40019,
      "reputation": 2533,
      "user_id": 55,
      "user_type": "registered",
      "accept_rate": 43,
      "profile_image": "https://i.sstatic.net/FkIiK.png?s=256",
      "display_name": "WilliamKF",
      "link": "https://ai.stackexchange.com/users/55/williamkf"
    },
    "is_answered": true,
    "view_count": 9740,
    "accepted_answer_id": 143,
    "answer_count": 5,
    "score": 14,
    "last_activity_date": 1561064623,
    "creation_date": 1470153764,
    "last_edit_date": 1561062998,
    "question_id": 28,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/28/is-a-genetic-algorithm-an-example-of-artificial-intelligence",
    "title": "Is a genetic algorithm an example of artificial intelligence?",
    "body": "<p>Since human intelligence presumably is a function of a natural genetic algorithm in nature, is using a genetic algorithm in a computer an example of artificial intelligence? If not, how do they differ? Or perhaps some are and some are not expressing artificial intelligence depending upon the scale of the algorithm and what it evolves into?</p>\n"
  },
  {
    "tags": [
      "deep-neural-networks",
      "overfitting",
      "performance"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 4781,
    "accepted_answer_id": 44,
    "answer_count": 4,
    "score": 14,
    "last_activity_date": 1601583416,
    "creation_date": 1470154103,
    "last_edit_date": 1470154222,
    "question_id": 40,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/40/what-is-the-dropout-technique",
    "title": "What is the &quot;dropout&quot; technique?",
    "body": "<p>What purpose does the \"dropout\" method serve and how does it improve the overall performance of the neural network?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "intelligence-testing",
      "turing-test"
    ],
    "owner": {
      "account_id": 8593096,
      "reputation": 253,
      "user_id": 96,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/3b14b9fcf74f0c4f919d506d8c9690ea?s=256&d=identicon&r=PG",
      "display_name": "Luke",
      "link": "https://ai.stackexchange.com/users/96/luke"
    },
    "is_answered": true,
    "view_count": 2634,
    "accepted_answer_id": 85,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1589846150,
    "creation_date": 1470156367,
    "last_edit_date": 1589846150,
    "question_id": 80,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/80/what-are-the-specific-requirements-of-the-turing-test",
    "title": "What are the specific requirements of the Turing test?",
    "body": "<p>What are the specific requirements of the Turing test? </p>\n\n<ul>\n<li>What requirements if any must the evaluator fulfill in order to be qualified to give the test?</li>\n<li>Must there always be two participants in the conversation (one human and one computer) or can there be more?</li>\n<li>Are placebo tests (where there is not actually a computer involved) allowed or encouraged?</li>\n<li>Can there be multiple evaluators? If so does the decision need to be unanimous among all evaluators in order for the machine to have passed the test?</li>\n</ul>\n"
  },
  {
    "tags": [
      "models",
      "agi",
      "aixi"
    ],
    "owner": {
      "account_id": 443353,
      "reputation": 2148,
      "user_id": 144,
      "user_type": "registered",
      "accept_rate": 30,
      "profile_image": "https://i.sstatic.net/ZAzOl.jpg?s=256",
      "display_name": "rcpinto",
      "link": "https://ai.stackexchange.com/users/144/rcpinto"
    },
    "is_answered": true,
    "view_count": 5402,
    "answer_count": 4,
    "score": 14,
    "last_activity_date": 1739911754,
    "creation_date": 1470172226,
    "last_edit_date": 1555686782,
    "question_id": 145,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/145/what-is-the-relevance-of-aixi-on-current-artificial-intelligence-research",
    "title": "What is the relevance of AIXI on current artificial intelligence research?",
    "body": "<p>From Wikipedia:</p>\n\n<blockquote>\n  <p>AIXI ['ai̯k͡siː] is a theoretical mathematical formalism for artificial general intelligence. It combines Solomonoff induction with sequential decision theory. AIXI was first proposed by Marcus Hutter in 2000[1] and the results below are proved in Hutter's 2005 book Universal Artificial Intelligence.[2]</p>\n</blockquote>\n\n<p>Albeit non-computable, approximations are possible, such as <em>AIXItl</em>. Finding approximations to AIXI could be an objective way for solving AI.</p>\n\n<p>Is <em>AIXI</em> really a big deal in artificial <em>general</em> intelligence research? Can it be thought as a central concept for the field? If so, why don't we have more publications on this subject (or maybe we have and I'm not aware of them)?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "artificial-consciousness"
    ],
    "owner": {
      "account_id": 3364317,
      "reputation": 2589,
      "user_id": 75,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/vBJkv.png?s=256",
      "display_name": "Ben N",
      "link": "https://ai.stackexchange.com/users/75/ben-n"
    },
    "is_answered": true,
    "view_count": 1427,
    "accepted_answer_id": 1898,
    "answer_count": 8,
    "score": 14,
    "last_activity_date": 1715282831,
    "creation_date": 1473351640,
    "last_edit_date": 1634087214,
    "question_id": 1897,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1897/is-consciousness-necessary-for-any-ai-task",
    "title": "Is consciousness necessary for any AI task?",
    "body": "<p>Consciousness <a href=\"http://www.iep.utm.edu/consciou/\" rel=\"nofollow noreferrer\">is challenging to define</a>, but for this question let's define it as \"actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine.\" Humans, of course, have minds; for normal computers, all the things they \"see\" are just more data. One could alternatively say that humans are <a href=\"https://philosophy.stackexchange.com/a/4687\">sentient</a>, while traditional computers are not.</p>\n\n<p>Setting aside the question of whether it's possible to build a sentient machine, does it actually make a difference if an AI is sentient or not? In other words, are there are tasks that are made impossible - not just more difficult - by a lack of sentience?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning"
    ],
    "owner": {
      "account_id": 3445945,
      "reputation": 141,
      "user_id": 1363,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4480675ad0ede0043e7caba04750d974?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "cha",
      "link": "https://ai.stackexchange.com/users/1363/cha"
    },
    "is_answered": true,
    "view_count": 1618,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1474119764,
    "creation_date": 1473658890,
    "last_edit_date": 1473709229,
    "question_id": 1953,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/1953/has-anyone-thought-about-making-a-neural-network-ask-questions-instead-of-only",
    "title": "Has anyone thought about making a neural network ask questions, instead of only answering them?",
    "body": "<p>Most of the people is trying to answer question with a neural network. However, has anyone came up with some thoughts about how to make neural network ask questions, instead of answer questions? For example, if a CNN can decide which category an object belongs to, than can it ask some question to help the the classification?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "deep-learning",
      "deep-neural-networks",
      "residual-networks"
    ],
    "owner": {
      "account_id": 6138127,
      "reputation": 357,
      "user_id": 1791,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/dWJCM.png?s=256",
      "display_name": "Erba Aitbayev",
      "link": "https://ai.stackexchange.com/users/1791/erba-aitbayev"
    },
    "is_answered": true,
    "view_count": 853,
    "accepted_answer_id": 1999,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1571066225,
    "creation_date": 1474368854,
    "last_edit_date": 1571066225,
    "question_id": 1997,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1997/should-deep-residual-networks-be-viewed-as-an-ensemble-of-networks",
    "title": "Should deep residual networks be viewed as an ensemble of networks?",
    "body": "<p>The question is about the architecture of Deep Residual Networks (<strong>ResNets</strong>). The model that won the 1-st places at <a href=\"http://image-net.org/challenges/LSVRC/2015/results\" rel=\"nofollow noreferrer\">\"Large Scale Visual Recognition Challenge 2015\" (ILSVRC2015)</a> in all five main tracks:</p>\n\n<blockquote>\n  <ul>\n  <li><em>ImageNet Classification: “Ultra-deep” (quote Yann) 152-layer nets</em> </li>\n  <li><em>ImageNet Detection: 16% better than 2nd</em></li>\n  <li><em>ImageNet Localization: 27% better than 2nd</em></li>\n  <li><em>COCO Detection: 11% better than 2nd</em></li>\n  <li><em>COCO Segmentation: 12% better than 2nd<br><br></em>\n  <em>Source:</em> <a href=\"http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf\" rel=\"nofollow noreferrer\"><em>MSRA @ ILSVRC &amp; COCO 2015 competitions (presentation, 2-nd slide)</em></a></li>\n  </ul>\n</blockquote>\n\n<p>This work is described in the following article:</p>\n\n<blockquote>\n  <p><a href=\"http://arxiv.org/abs/1512.03385\" rel=\"nofollow noreferrer\"><em>Deep Residual Learning for Image Recognition (2015, PDF)</em></a></p>\n</blockquote>\n\n<hr>\n\n<p><strong>Microsoft Research team</strong> (developers of ResNets: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun) in their article:</p>\n\n<blockquote>\n  <p><a href=\"https://arxiv.org/pdf/1603.05027.pdf\" rel=\"nofollow noreferrer\">\"<em>Identity Mappings in Deep Residual Networks (2016)</em>\"</a></p>\n</blockquote>\n\n<p>state that <strong>depth</strong> plays a key role:</p>\n\n<blockquote>\n  <p><em>\"<strong>We obtain these results via a simple but essential concept — going deeper. These results demonstrate the potential of pushing the limits of depth.</strong>\"</em></p>\n</blockquote>\n\n<p>It is emphasized in their <a href=\"http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf\" rel=\"nofollow noreferrer\">presentation</a> also (deeper - better):<br> </p>\n\n<blockquote>\n  <p><em>- \"A deeper model should not have higher training error.\"<br> \n  - \"Deeper ResNets have lower training error, and also lower test error.\"<br> \n  - \"Deeper ResNets have lower error.\"<br>\n  - \"All benefit more from deeper features – cumulative gains!\"<br>\n  - \"Deeper is still better.\"</em></p>\n</blockquote>\n\n<p>Here is the sctructure of 34-layer residual (for reference):\n<a href=\"https://i.sstatic.net/L8m0X.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/L8m0X.png\" alt=\"enter image description here\"></a></p>\n\n<hr>\n\n<p>But recently I have found one theory that introduces a novel interpretation of residual networks showing they are exponential ensembles:</p>\n\n<blockquote>\n  <p><a href=\"https://arxiv.org/abs/1605.06431\" rel=\"nofollow noreferrer\"><em>Residual Networks are Exponential Ensembles of Relatively Shallow Networks (2016)</em></a></p>\n</blockquote>\n\n<p>Deep Resnets are described as many shallow networks whose outputs are pooled at various depths. \nThere is a picture in the article. I attach it with explanation:</p>\n\n<blockquote>\n  <p><a href=\"https://i.sstatic.net/PGhK2.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/PGhK2.jpg\" alt=\"enter image description here\"></a> Residual Networks are\n  conventionally shown as (a), which is a natural representation of\n  Equation (1). When we expand this formulation to Equation (6), we\n  obtain an unraveled view of a 3-block residual network (b). From this\n  view, it is apparent that residual networks have O(2^n) implicit paths\n  connecting input and output and that adding a block doubles the number\n  of paths.</p>\n</blockquote>\n\n<p>In conclusion of the article it is stated:</p>\n\n<blockquote>\n  <p><strong>It is not depth, but the ensemble that makes residual networks strong</strong>.\n  Residual networks push the limits of network multiplicity, not network\n  depth. Our proposed unraveled view and the lesion study show that\n  residual networks are an implicit ensemble of exponentially many\n  networks. If most of the paths that contribute gradient are very short\n  compared to the overall depth of the network, <strong>increased depth</strong>\n  alone <strong>can’t be the key characteristic</strong> of residual networks. We now\n  believe that <strong>multiplicity</strong>, the network’s expressability in the\n  terms of the number of paths, plays <strong>a key role</strong>.</p>\n</blockquote>\n\n<p>But it is only a recent theory that can be confirmed or refuted. It happens sometimes that some theories are refuted and articles are withdrawn.</p>\n\n<hr>\n\n<p>Should we think of deep ResNets as an ensemble after all? <strong>Ensemble</strong> or <strong>depth</strong> makes residual networks so strong? Is it possible that even the developers themselves do not quite perceive what their own model represents and what is the key concept in it?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "human-like",
      "ethics",
      "emotional-intelligence",
      "emotion-recognition"
    ],
    "owner": {
      "account_id": 9015258,
      "reputation": 383,
      "user_id": 3448,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/wXcEL.png?s=256",
      "display_name": "MountainSide Studios",
      "link": "https://ai.stackexchange.com/users/3448/mountainside-studios"
    },
    "is_answered": true,
    "view_count": 2643,
    "accepted_answer_id": 2376,
    "answer_count": 10,
    "score": 14,
    "last_activity_date": 1675548277,
    "creation_date": 1478397117,
    "last_edit_date": 1620493451,
    "question_id": 2277,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2277/could-an-ai-feel-emotions",
    "title": "Could an AI feel emotions?",
    "body": "<p>Assuming humans had finally developed the first humanoid AI based on the human brain, would It feel emotions? If not, would it still have ethics and/or morals?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "recurrent-neural-networks",
      "reference-request",
      "long-short-term-memory",
      "chat-bots"
    ],
    "owner": {
      "account_id": 302085,
      "reputation": 381,
      "user_id": 4259,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/005302f3d866d3dcf546a63c47dd3cd7?s=256&d=identicon&r=PG",
      "display_name": "Totem",
      "link": "https://ai.stackexchange.com/users/4259/totem"
    },
    "is_answered": true,
    "view_count": 4779,
    "accepted_answer_id": 2506,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1611886956,
    "creation_date": 1481752561,
    "last_edit_date": 1611886956,
    "question_id": 2475,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2475/which-approaches-could-i-use-to-create-a-simple-chatbot-using-a-neural-network",
    "title": "Which approaches could I use to create a simple chatbot using a neural network?",
    "body": "<p>I wanted to start experimenting with neural networks, so I decided to make a chatbot (like Cleverbot, which is not that clever anyway) using them.</p>\n<p>I looked around for some documentation and I found many tutorials on general tasks, but few on this specific topic. The one I found just exposed the results without giving insights on the implementation. The ones that did, did it pretty shallowly (the TensorFlow documentation page on seq2seq is lacking, IMHO).</p>\n<p>Now, I feel I may have understood the principle more or less, but I'm not sure and I am not even sure how to start. Thus I will explain how I would tackle the problem and I'd like feedback on this solution, telling me where I'm mistaken, and possibly have any link to detailed explanations and practical knowledge on the process.</p>\n<ol>\n<li><p>The dataset I will use for the task is the dump of all my Facebook and WhatsApp chat history. I don't know how big it will be but possibly still not large enough. The target language is not English, therefore I don't know where to quickly gather meaningful conversation samples.</p>\n</li>\n<li><p>I am going to generate a thought vector out of each sentence. Still don't know how, actually; I found a nice example for word2vec on the deeplearning4j website, but none for sentences. I understood how word vectors are built and why, but I could not find an exhaustive explanation for sentence vectors.</p>\n</li>\n<li><p>Using thought vectors as input and output I am going to train the neural network. I don't know how many layers it should have, and which ones have to be LSTM layers.</p>\n</li>\n<li><p>Then there should be another neural network that is able to transform a thought vector into a sequence of characters composing a sentence. I read that I should use padding to make up for different sentence lengths, but I miss how to encode characters (are codepoints enough?).</p>\n</li>\n</ol>\n"
  },
  {
    "tags": [
      "deep-learning",
      "computational-learning-theory"
    ],
    "owner": {
      "account_id": 10463957,
      "reputation": 151,
      "user_id": 6039,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/0ac1b97912d5fc5624f5f91e69c923b6?s=256&d=identicon&r=PG",
      "display_name": "heleone",
      "link": "https://ai.stackexchange.com/users/6039/heleone"
    },
    "is_answered": true,
    "view_count": 3383,
    "answer_count": 6,
    "score": 14,
    "last_activity_date": 1640198212,
    "creation_date": 1489569028,
    "last_edit_date": 1640198212,
    "question_id": 2996,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2996/is-there-actually-a-lack-of-fundamental-theory-on-deep-learning",
    "title": "Is there actually a lack of fundamental theory on deep learning?",
    "body": "<p>I heard several times that one of the fundamental/open problems of deep learning is the lack of &quot;general theory&quot; on it, because, actually, we don't know why deep learning works so well. Even the Wikipedia page on deep learning has <a href=\"https://en.wikipedia.org/wiki/Deep_learning#Criticism_and_comment\" rel=\"nofollow noreferrer\">similar comments</a>. Are such statements credible and representative of the state of the field?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "research",
      "theory-of-computation",
      "incompleteness-theorems"
    ],
    "owner": {
      "account_id": 5982534,
      "reputation": 2111,
      "user_id": 6779,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/975485022464794/picture?type=large",
      "display_name": "k.c. sayz &#39;k.c sayz&#39;",
      "link": "https://ai.stackexchange.com/users/6779/k-c-sayz-k-c-sayz"
    },
    "is_answered": true,
    "view_count": 2873,
    "accepted_answer_id": 3213,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1589366428,
    "creation_date": 1492964445,
    "last_edit_date": 1589366428,
    "question_id": 3209,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3209/what-are-some-implications-of-g%c3%b6dels-theorems-on-ai-research",
    "title": "What are some implications of G&#246;del&#39;s theorems on AI research?",
    "body": "<p><em>Note: My experience with Gödel's theorem is quite limited: I have read Gödel Escher Bach; skimmed the 1st half of Introduction to Godel's Theorem (by Peter Smith); and some random stuff here and there on the internet. That is, I only have a vague high level understanding of the theory.</em> </p>\n\n<p>In my humble opinion, Gödel's incompleteness theorem (and its many related Theorems, such as the Halting problem, and Löbs Theorem) are among the most important theoretical discoveries. </p>\n\n<p>However, its a bit disappointing to observe that there aren't that many (at least to my knowledge) theoretical applications of the theorems, probably in part due to 1. the obtuse nature of the proof 2. the strong philosophical implications people aren't willing to easily commit towards.</p>\n\n<p>Despite that, there are still some attempts to apply the theorems in a philosophy of mind / AI context. Off the top of my head:</p>\n\n<ul>\n<li><p><a href=\"http://www.iep.utm.edu/lp-argue/\" rel=\"nofollow noreferrer\">The Lucas-Penrose Argument</a>: which argues that the mind is not implemented on a formal system (as in computer). (Not a very rigour proof however) </p></li>\n<li><p>Apparently, some of the research at MIRI uses Löbs Thereom, though the only example I know of is <a href=\"http://intelligence.org/files/ProgramEquilibrium.pdf\" rel=\"nofollow noreferrer\">Löbian agent cooperation.</a></p></li>\n</ul>\n\n<p>These are all really cool, but are there some more examples? Especially ones that are actually seriously considered by the academic community.</p>\n\n<p>See also <a href=\"https://philosophy.stackexchange.com/q/305\">What are the philosophical implications of Gödel's First Incompleteness Theorem?</a></p>\n"
  },
  {
    "tags": [
      "training",
      "tensorflow",
      "game-ai"
    ],
    "owner": {
      "account_id": 1277627,
      "reputation": 249,
      "user_id": 7321,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/05457e5cb04efa5081d156d3d1c991fb?s=256&d=identicon&r=PG",
      "display_name": "soriak",
      "link": "https://ai.stackexchange.com/users/7321/soriak"
    },
    "is_answered": true,
    "view_count": 12871,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1684817907,
    "creation_date": 1495219125,
    "question_id": 3345,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/3345/how-to-train-a-neural-network-for-a-round-based-board-game",
    "title": "How to train a neural network for a round based board game?",
    "body": "<p>I'm wondering how to train a neural network for a round based board game like, tic-tac-toe, chess, risk or any other round based game.\nGetting the next move by inference seems to be pretty straight forward, by feeding the game state as input and using the output as the move for the current player.\nHowever training an AI for that purpose doesn't appear to be that straight forward, because:</p>\n\n<ol>\n<li>There might not be a rating if a single move is good or not, so training of single moves doesn't seem to be the right choice</li>\n<li>Using all game states (inputs) and moves (outputs) of the whole game to train the neural network, doesn't seem to be the right choice as not all moves within a lost game might be bad</li>\n</ol>\n\n<p>So I'm wondering how to train a neural network for a round based board game?\nI would like to create a neural network for tic-tac-toe using tensorflow.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "math",
      "backpropagation",
      "gradient-descent"
    ],
    "owner": {
      "account_id": 1615115,
      "reputation": 141,
      "user_id": 9189,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/eb0790f8743c324d1c8d93f5de43fc9f?s=256&d=identicon&r=PG",
      "display_name": "user74211",
      "link": "https://ai.stackexchange.com/users/9189/user74211"
    },
    "is_answered": true,
    "view_count": 9764,
    "closed_date": 1637086900,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1567080668,
    "creation_date": 1503412011,
    "last_edit_date": 1567080668,
    "question_id": 3862,
    "link": "https://ai.stackexchange.com/questions/3862/is-the-mean-squared-error-always-convex-in-the-context-of-neural-networks",
    "closed_reason": "Duplicate",
    "title": "Is the mean-squared error always convex in the context of neural networks?",
    "body": "<p>Multiple resources I referred to mention that MSE is great because it's convex. But I don't get how, especially in the context of neural networks.</p>\n\n<p>Let's say we have the following:</p>\n\n<ul>\n<li><span class=\"math-container\">$X$</span>: training dataset</li>\n<li><span class=\"math-container\">$Y$</span>: targets</li>\n<li><span class=\"math-container\">$\\Theta$</span>: the set of parameters of the model <span class=\"math-container\">$f_\\Theta$</span> (a neural network model with non-linearities)</li>\n</ul>\n\n<p>Then:</p>\n\n<p><span class=\"math-container\">$$\\operatorname{MSE}(\\Theta) = (f_\\Theta(X) - Y)^2$$</span></p>\n\n<p>Why would this loss function always be convex? Does this depend on <span class=\"math-container\">$f_\\Theta(X)$</span>?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "agi",
      "rewards"
    ],
    "owner": {
      "account_id": 9056565,
      "reputation": 251,
      "user_id": 9288,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6bcfd2a1cee736086e000d1bba635655?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Aleksei Maide",
      "link": "https://ai.stackexchange.com/users/9288/aleksei-maide"
    },
    "is_answered": true,
    "view_count": 2518,
    "protected_date": 1640198067,
    "accepted_answer_id": 3907,
    "answer_count": 6,
    "score": 14,
    "last_activity_date": 1636917591,
    "creation_date": 1503847434,
    "last_edit_date": 1636917591,
    "question_id": 3903,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3903/what-would-motivate-a-machine",
    "title": "What would motivate a machine?",
    "body": "<p>Currently, within the AI development field, the main focus seems to be on pattern recognition and machine learning. Learning is about adjusting internal variables based on a feedback loop.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Maslow%27s_hierarchy_of_needs\" rel=\"nofollow noreferrer\">Maslow's hierarchy of needs</a> is a theory in psychology proposed by <em>Abraham Maslow</em> that claims that individuals' most basic needs must be met, before they become motivated to achieve higher-level needs.</p>\n<ul>\n<li>What could possibly motivate a machine to act?</li>\n<li>Should a machine have some sort of DNA-like structure that would describe its hierarchy of needs (similar to Maslow's theory)?</li>\n<li>What could be the fundamental needs of a machine?</li>\n</ul>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "reference-request",
      "robotics",
      "robots",
      "sophia"
    ],
    "owner": {
      "account_id": 1701102,
      "reputation": 251,
      "user_id": 10434,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/zN5vT.jpg?s=256",
      "display_name": "tgogos",
      "link": "https://ai.stackexchange.com/users/10434/tgogos"
    },
    "is_answered": true,
    "view_count": 10503,
    "protected_date": 1617983827,
    "accepted_answer_id": 4401,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1617856447,
    "creation_date": 1509099877,
    "last_edit_date": 1611684477,
    "question_id": 4375,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4375/are-the-dialogs-at-sophias-the-robot-appearings-scripted",
    "title": "Are the dialogs at Sophia&#39;s (the robot) appearings scripted?",
    "body": "<p>I talk about the robot from: <a href=\"http://www.hansonrobotics.com/robot/sophia/\" rel=\"noreferrer\">Hanson Robotics</a>, which was <a href=\"https://techcrunch.com/2017/10/26/saudi-arabia-robot-citizen-sophia/\" rel=\"noreferrer\">granted the right to citizenship from Saudi Arabia</a>.</p>\n\n<p>I have found the following articles:</p>\n\n<h1>Your new friend is a humanoid robot</h1>\n\n<p>Source: <a href=\"http://www.theaustralian.com.au/life/say-hello-to-your-new-friend-sophia-the-humanoid-robot/news-story/070299a8d11b7d636848f1b8dd753530\" rel=\"noreferrer\">theaustralian.com.au</a></p>\n\n<blockquote>\n  <p>Like Amazon Echo, Google Assistant and Siri, <strong>Sophia can ask and answer questions about discrete pieces of information</strong>, such as what types of movies and songs she likes, the weather and whether robots should exterminate humans.</p>\n  \n  <p>But her general knowledge is behind these players and she doesn’t do maths. <strong>Her answers are mostly scripted</strong> and, it seems, from my observation, her answer are derived from algorithmically crunching the language you use.</p>\n  \n  <p>Sometimes answers are close to the topic of the question, but off beam. Sometimes she just changes the subject and asks you a question instead.</p>\n  \n  <p>She has no artificial notion of self. <strong>She can’t say where she was yesterday, whether she remembers you from before</strong>, and doesn’t seem to amass data of past interactions with you that can form the basis of an ongoing association.</p>\n  \n  <p>Questions such as: <em>“What have you seen in Australia?”</em>, <em>“Where were you yesterday?”</em>, <em>“Who did you meet last week?”</em> and <em>“Do you like Australia?”</em> are beyond her.</p>\n</blockquote>\n\n<hr>\n\n<h1>Why Sophia the robot is not what it seems</h1>\n\n<p>Source: <a href=\"http://www.smh.com.au/comment/why-sophia-the-robot-is-not-what-it-seems-20171030-gzbi3p.html\" rel=\"noreferrer\">smh.com.au</a></p>\n\n<blockquote>\n  <p>You can often fool this sort of software by introducing noise. That could be literal noise – machines aren't great at filtering out background noise, as anyone with a hearing aid will tell you – or it could be noise in the sense of irrelevant information or limited context. You could ask <strong>\"what do you think of humans?\"</strong> and then follow up with <strong>\"can you tell more about it?\"</strong> The second question requires the robot to define \"it\", remember what it said last time, and come up with something new.</p>\n  \n  <p>In the case of the ABC interview, <strong>the questions were sent to Sophia's team ahead of time so they were possibly pre-scripted</strong>. Just like an interview with a human celebrity!</p>\n</blockquote>\n\n<hr>\n\n<h1>Pretending to give a robot citizenship helps no one</h1>\n\n<p>Source: <a href=\"https://www.theverge.com/2017/10/30/16552006/robot-rights-citizenship-saudi-arabia-sophia\" rel=\"noreferrer\">theverge.com</a></p>\n\n<blockquote>\n  <p>Sophia is essentially a cleverly built puppet designed to exploit our cultural expectations of what a robot looks and sounds like. It can hold a stilted conversation, yes, but its one-liners seem to be prewritten responses to key words. (As Piers Morgan commented during an interview with Sophia, <strong>“Obviously these are programmed answers.”</strong>)</p>\n</blockquote>\n\n<hr>\n\n<p>Here are a few more references.</p>\n\n<ul>\n<li><a href=\"https://qz.com/1121547/how-smart-is-the-first-robot-citizen/\" rel=\"noreferrer\">Inside the mechanical brain of the world’s first robot citizen</a></li>\n<li><a href=\"https://buildabuddha.quora.com/Sophia-the-Uncanny-Robot-AI-Is-Not-What-You-Think\" rel=\"noreferrer\">Sophia, the Uncanny Robot AI, Is Not What You Think.</a></li>\n<li><a href=\"https://www.quora.com/How-much-of-Robot-Sophia%E2%80%99s-speech-is-likely-scripted-with-your-understanding-of-the-progress-in-NLP\" rel=\"noreferrer\">How much of Robot Sophia’s speech is likely scripted with your understanding of the progress in NLP?</a></li>\n</ul>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "game-ai",
      "applications",
      "reward-design",
      "negamax"
    ],
    "owner": {
      "account_id": 5766869,
      "reputation": 295,
      "user_id": 10813,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/705f8a718421cb97c8058700a5ee2a0b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Pigna",
      "link": "https://ai.stackexchange.com/users/10813/pigna"
    },
    "is_answered": true,
    "view_count": 4483,
    "accepted_answer_id": 4483,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1604248704,
    "creation_date": 1510329288,
    "last_edit_date": 1604248704,
    "question_id": 4482,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4482/how-could-i-use-reinforcement-learning-to-solve-a-chess-like-board-game",
    "title": "How could I use reinforcement learning to solve a chess-like board game?",
    "body": "<p>I invented a chess-like board game. I built an engine so that it can play autonomously. The engine is basically a decision tree. It's composed by:</p>\n<ol>\n<li>A search function that at each node finds all possible legal moves</li>\n<li>An evaluation function that assigns a numerical value to the board position (positive means the first player is gaining the upper hand, negative means the second player is winning instead)</li>\n<li>An alpha-beta pruning negamax algorithm</li>\n</ol>\n<p>The main problem with this engine is that the optimization of the evaluation function is really tricky. I don't know which factors to consider and which weights to put. The only way I see to improve the engine is to iterate games trying each time different combinations of factors and weights. However, it computationally seems a very tough feat (Can I backpropagate without using deep learning?).</p>\n<p>I would like to use reinforcement learning to make the engine improve by playing against itself. I have been reading about the topic, but I am still quite confused.</p>\n<ol>\n<li><p>What other reward is there in a game apart from the win-or-lose output (1 or 0)?</p>\n</li>\n<li><p>If I use other rewards, like the output from the evaluation function at each turn, how can I implement it?</p>\n</li>\n<li><p>How do I modify the evaluation function to give better rewards iteration after iteration?</p>\n</li>\n</ol>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "python",
      "similarity"
    ],
    "owner": {
      "account_id": 6841805,
      "reputation": 243,
      "user_id": 9428,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/761383047321402/picture?type=large",
      "display_name": "Shubham Tiwari",
      "link": "https://ai.stackexchange.com/users/9428/shubham-tiwari"
    },
    "is_answered": true,
    "view_count": 18237,
    "accepted_answer_id": 6418,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1573849525,
    "creation_date": 1515603172,
    "last_edit_date": 1573849525,
    "question_id": 4965,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4965/how-do-i-compute-the-structural-similarity-between-sentences",
    "title": "How do I compute the structural similarity between sentences?",
    "body": "<p>I am working on a problem where I need to determine whether two sentences are similar or not.  I implemented a solution using BM25 algorithm and wordnet synsets for determining syntactic &amp; semantic similarity.  The solution is working adequately, and even if the word order in the sentences is jumbled, it is measuring that two sentences are similar. For example</p>\n\n<ol>\n<li>Python is a good language.</li>\n<li>Language a good python is.</li>\n</ol>\n\n<p>My problem is to determine that these two sentences are similar.  </p>\n\n<ul>\n<li>What could be the possible solution for structural similarity? </li>\n<li>How will I maintain the structure of sentences?</li>\n</ul>\n"
  },
  {
    "tags": [
      "research",
      "agi",
      "superintelligence"
    ],
    "owner": {
      "account_id": 11803106,
      "reputation": 337,
      "user_id": 12935,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/8j4rQ.jpg?s=256",
      "display_name": "Alex",
      "link": "https://ai.stackexchange.com/users/12935/alex"
    },
    "is_answered": true,
    "view_count": 2176,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1577076380,
    "creation_date": 1519557776,
    "last_edit_date": 1574461921,
    "question_id": 5428,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5428/how-can-people-contribute-to-agi-research",
    "title": "How can people contribute to AGI research?",
    "body": "<p>Is there a way for people outside of the core research community of AGI to contribute to the cause?</p>\n\n<p>There are a lot of people interested in supporting the field, but there is no clear way to do that. Is there something like <a href=\"https://boinc.berkeley.edu\" rel=\"noreferrer\">BOINC</a> for AGI researches, or open projects where random experts can provide some input? Maybe Kickstarter for AGI projects? </p>\n"
  },
  {
    "tags": [
      "search",
      "proofs",
      "heuristics",
      "a-star",
      "admissible-heuristic"
    ],
    "owner": {
      "account_id": 13328715,
      "reputation": 313,
      "user_id": 14913,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4e8e06f768a7752c458de4aaeff27f93?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Wizard",
      "link": "https://ai.stackexchange.com/users/14913/wizard"
    },
    "is_answered": true,
    "view_count": 31428,
    "accepted_answer_id": 7477,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1621081948,
    "creation_date": 1523586329,
    "last_edit_date": 1573404957,
    "question_id": 6026,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6026/why-is-a-optimal-if-the-heuristic-function-is-admissible",
    "title": "Why is A* optimal if the heuristic function is admissible?",
    "body": "<p>A heuristic is <strong>admissible</strong> if it <em>never overestimates</em> the true cost to reach the goal node from <span class=\"math-container\">$n$</span>. If a heuristic is <strong>consistent</strong>, then the heuristic value of <span class=\"math-container\">$n$</span> is never greater than the cost of its successor, <span class=\"math-container\">$n'$</span>, plus the successor's heuristic value. </p>\n\n<p>Why is A*, using tree or graph searches, optimal, if it uses an admissible heuristic?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "agi",
      "superintelligence"
    ],
    "owner": {
      "account_id": 2728,
      "reputation": 261,
      "user_id": 16291,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/9f51586a31b8dbff2f877ed884bbe33c?s=256&d=identicon&r=PG",
      "display_name": "Graviton",
      "link": "https://ai.stackexchange.com/users/16291/graviton"
    },
    "is_answered": true,
    "view_count": 1067,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1639330387,
    "creation_date": 1529062824,
    "last_edit_date": 1639330387,
    "question_id": 6765,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6765/is-there-any-scientific-mathematical-argument-that-prevents-deep-learning-from-e",
    "title": "Is there any scientific/mathematical argument that prevents deep learning from ever producing strong AI?",
    "body": "<p>I read Judea Pearl's <a href=\"https://rads.stackoverflow.com/amzn/click/com/046509760X\" rel=\"nofollow noreferrer\" rel=\"nofollow noreferrer\">The Book of Why</a>, in which he mentions that deep learning is just a glorified curve fitting technology, and will not be able to produce human-like intelligence.</p>\n<p>From his book there is this diagram that illustrates the three levels of cognitive abilities:</p>\n<p><a href=\"https://i.sstatic.net/vxTLx.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/vxTLx.png\" alt=\"Three levels of cognitive abilities\" /></a></p>\n<p>The idea is that the &quot;intelligence&quot; produced by current deep learning technology is only at the level of association. Thus the AI is nowhere near the level of asking questions like &quot;how can I make Y happen&quot; (intervention) and &quot;What if I have acted differently, will X still occur?&quot; (counterfactuals), and it's highly unlikely that curve fitting techniques can ever bring us closer to a higher level of cognitive ability.</p>\n<p>I found his argument persuasive on an intuitive level, but I'm unable to find any physical or mathematical laws that can either bolster or cast doubt on this argument.</p>\n<p>So, is there any scientific/physical/chemical/biological/mathematical argument that prevents deep learning from ever producing strong AI (human-like intelligence)?</p>\n"
  },
  {
    "tags": [
      "algorithm",
      "search",
      "comparison",
      "a-star"
    ],
    "owner": {
      "account_id": 14401505,
      "reputation": 375,
      "user_id": 19657,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/495200710998489/picture?type=large",
      "display_name": "Marosh Fatima",
      "link": "https://ai.stackexchange.com/users/19657/marosh-fatima"
    },
    "is_answered": true,
    "view_count": 69090,
    "accepted_answer_id": 8907,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1601046831,
    "creation_date": 1541838152,
    "last_edit_date": 1567125147,
    "question_id": 8902,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/8902/what-are-the-differences-between-a-and-greedy-best-first-search",
    "title": "What are the differences between A* and greedy best-first search?",
    "body": "<p>What are the differences between the A* algorithm and the greedy best-first search algorithm? Which one should I use? Which algorithm is the better one, and why?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "policy-gradients",
      "variance-reduction",
      "reward-normalization"
    ],
    "migrated_from": {
      "other_site": {
        "aliases": [
          "https://statistics.stackexchange.com",
          "https://crossvalidated.com"
        ],
        "styling": {
          "tag_background_color": "#edefed",
          "tag_foreground_color": "#5D5D5D",
          "link_color": "#0077CC"
        },
        "related_sites": [
          {
            "relation": "meta",
            "api_site_parameter": "stats.meta",
            "site_url": "https://stats.meta.stackexchange.com",
            "name": "Cross Validated Meta"
          },
          {
            "relation": "chat",
            "site_url": "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com",
            "name": "Chat Stack Exchange"
          }
        ],
        "markdown_extensions": [
          "MathJax",
          "Prettify"
        ],
        "launch_date": 1288900046,
        "open_beta_date": 1280170800,
        "closed_beta_date": 1279566000,
        "site_state": "normal",
        "high_resolution_icon_url": "https://cdn.sstatic.net/Sites/stats/Img/apple-touch-icon@2.png",
        "favicon_url": "https://cdn.sstatic.net/Sites/stats/Img/favicon.ico",
        "icon_url": "https://cdn.sstatic.net/Sites/stats/Img/apple-touch-icon.png",
        "audience": "people interested in statistics, machine learning, data analysis, data mining, and data visualization",
        "site_url": "https://stats.stackexchange.com",
        "api_site_parameter": "stats",
        "logo_url": "https://cdn.sstatic.net/Sites/stats/Img/logo.png",
        "name": "Cross Validated",
        "site_type": "main_site"
      },
      "on_date": 1548432144,
      "question_id": 388930
    },
    "owner": {
      "account_id": 543843,
      "reputation": 789,
      "user_id": 21645,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e3e07b09128f889209b738276d42d7ef?s=256&d=identicon&r=PG",
      "display_name": "Gulzar",
      "link": "https://ai.stackexchange.com/users/21645/gulzar"
    },
    "is_answered": true,
    "view_count": 14419,
    "accepted_answer_id": 10204,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1606005324,
    "creation_date": 1548338168,
    "last_edit_date": 1606005324,
    "question_id": 10196,
    "link": "https://ai.stackexchange.com/questions/10196/why-does-is-make-sense-to-normalize-rewards-per-episode-in-reinforcement-learnin",
    "title": "Why does is make sense to normalize rewards per episode in reinforcement learning?",
    "body": "<p>In <a href=\"https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py\" rel=\"noreferrer\">Open AI's actor-critic</a> and in <a href=\"https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py\" rel=\"noreferrer\">Open AI's REINFORCE</a>, the rewards are being normalized like so</p>\n<pre><code>rewards = (rewards - rewards.mean()) / (rewards.std() + eps)\n</code></pre>\n<p><em>on every episode individually</em>.</p>\n<p>This is probably the <a href=\"https://qr.ae/TWGVk2\" rel=\"noreferrer\">baseline reduction</a>, but <a href=\"https://ai.stackexchange.com/q/10194/2444\">I'm not entirely sure why they divide by the standard deviation of the rewards</a>?</p>\n<p>Assuming this is the baseline reduction, why is this done <em>per episode</em>?</p>\n<p>What if one episode yields rewards in the (absolute, not normalized) range of <span class=\"math-container\">$[0, 1]$</span>, and the next episode yields rewards in the range of <span class=\"math-container\">$[100, 200]$</span>?</p>\n<p>This method seems to ignore the absolute difference between the episodes' rewards.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "convolutional-neural-networks",
      "implementation",
      "convolution"
    ],
    "owner": {
      "account_id": 4148701,
      "reputation": 42397,
      "user_id": 2444,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e6fce047cba96a601a9dac66df4cd2e8?s=256&d=identicon&r=PG",
      "display_name": "nbro",
      "link": "https://ai.stackexchange.com/users/2444/nbro"
    },
    "is_answered": true,
    "view_count": 17258,
    "accepted_answer_id": 21874,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1672967795,
    "creation_date": 1552399079,
    "last_edit_date": 1592143407,
    "question_id": 11172,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11172/how-can-the-convolution-operation-be-implemented-as-a-matrix-multiplication",
    "title": "How can the convolution operation be implemented as a matrix multiplication?",
    "body": "<p>How can the convolution operation used by CNNs be implemented as a matrix-vector multiplication? We often think of the convolution operation in CNNs as a kernel that slides across the input. However, rather than sliding this kernel (e.g. using loops), we can perform the convolution operation \"in one step\" using a matrix-vector multiplication, where the matrix is a <a href=\"https://en.wikipedia.org/wiki/Circulant_matrix\" rel=\"noreferrer\">circulant matrix</a> containing shifted versions of the kernel (as rows or columns) and the vector is the input. </p>\n\n<p>How exactly can this operation be performed? I am looking for a detailed step-by-step answer that shows how the convolution operation (as usually presented) can be performed using a matrix-vector multiplication.</p>\n\n<p>Is this the usual way the convolution operations are implemented in CNNs?</p>\n"
  },
  {
    "tags": [
      "research",
      "math"
    ],
    "owner": {
      "account_id": 45709,
      "reputation": 261,
      "user_id": 26112,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f9c7318426662f2ab102c97602e1ccd8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "LeafGlowPath",
      "link": "https://ai.stackexchange.com/users/26112/leafglowpath"
    },
    "is_answered": true,
    "view_count": 3778,
    "accepted_answer_id": 12978,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1736876903,
    "creation_date": 1561109867,
    "last_edit_date": 1561149054,
    "question_id": 12971,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12971/what-sort-of-mathematical-problems-are-there-in-ai-that-people-are-working-on",
    "title": "What sort of mathematical problems are there in AI that people are working on?",
    "body": "<p>I recently got a 18-month postdoc position in a math department. It's a position with relative light teaching duty and a lot of freedom about what type of research that I want to do.</p>\n\n<p>Previously I was mostly doing some research in probability and combinatorics. But I am thinking of doing a bit more application oriented work, e.g., AI. (There is also the consideration that there is good chance that I will not get a tenure-track position at the end my current position. Learn a bit of AI might be helpful for other career possibilities.)</p>\n\n<p>What sort of <strong>mathematical problems</strong> are there in AI that people are working on? From what I heard of, there are people studying</p>\n\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Deterministic_finite_automaton\" rel=\"noreferrer\">Deterministic Finite Automaton</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Multi-armed_bandit\" rel=\"noreferrer\">Multi-armed bandit problems</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Monte_Carlo_tree_search\" rel=\"noreferrer\">Monte Carlo tree search</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Community_structure\" rel=\"noreferrer\">Community detection</a></li>\n</ul>\n\n<p>Any other examples?</p>\n"
  },
  {
    "tags": [
      "monte-carlo-tree-search",
      "games-of-chance"
    ],
    "owner": {
      "account_id": 3078061,
      "reputation": 241,
      "user_id": 27839,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/12fc6d9629d0b3ed3a1decc96e6ff2e6?s=256&d=identicon&r=PG",
      "display_name": "Mark",
      "link": "https://ai.stackexchange.com/users/27839/mark"
    },
    "is_answered": true,
    "view_count": 5144,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1689075973,
    "creation_date": 1565333023,
    "last_edit_date": 1574203077,
    "question_id": 13867,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/13867/mcts-for-non-deterministic-games-with-very-high-branching-factor-for-chance-node",
    "title": "MCTS for non-deterministic games with very high branching factor for chance nodes",
    "body": "<p>I'm trying to use a Monte Carlo Tree Search for a non-deterministic game. Apparently, one of the standard approaches is to model non-determinism using <em>chance nodes</em>. The problem for this game is that it has a very high min-entropy for the random events (imagine the shuffle of a deck of cards), and consequently a very large branching factor (<span class=\"math-container\">$\\approx 2^{32}$</span>) if I were to model this as a chance node.</p>\n\n<p>Despite this issue, there are a few things that likely make the search more tractable:</p>\n\n<ol>\n<li>Chance nodes only occur a few times per game, not after every move.</li>\n<li>The chance events do not depend on player actions.</li>\n<li>Even if two random outcomes are distinct, they might be \"similar to each other\", and that would lead to game outcomes that are also similar.</li>\n</ol>\n\n<p>So far all approaches that I've found to MCTS for non-deterministic games use UCT-like policies (e.g. chapter 4 of <a href=\"https://arxiv.org/pdf/0909.0801.pdf\" rel=\"noreferrer\">A Monte-Carlo AIXI Approximation</a>) to select chance nodes, which weight unexplored nodes maximally. In my case, I think this will lead to fully random playouts since any chance node won't ever be repeated in the selection phase.</p>\n\n<p>What is the best way to approach this problem? Has research been done on this? Naively, I was thinking of a policy that favors repeating chance nodes more over always exploring new ones.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "tensorflow",
      "recurrent-neural-networks",
      "long-short-term-memory"
    ],
    "owner": {
      "account_id": 2138751,
      "reputation": 241,
      "user_id": 29974,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6591637e71a306ae472b2a1d5fffc121?s=256&d=identicon&r=PG",
      "display_name": "kuixiong",
      "link": "https://ai.stackexchange.com/users/29974/kuixiong"
    },
    "is_answered": true,
    "view_count": 7485,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1621851214,
    "creation_date": 1569419981,
    "last_edit_date": 1586698308,
    "question_id": 15621,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/15621/what-is-the-relationship-between-the-size-of-the-hidden-layer-and-the-size-of-th",
    "title": "What is the relationship between the size of the hidden layer and the size of the cell state layer in an LSTM?",
    "body": "<p>I was following some examples to get familiar with TensorFlow's LSTM API, but noticed that all LSTM initialization functions require only the <code>num_units</code> parameter, which denotes the number of hidden units in a cell.</p>\n\n<p>According to what I have learned from the famous <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\" rel=\"noreferrer\">colah's blog</a>, the cell state has nothing to do with the hidden layer, thus they could be represented in different dimensions (I think), and then we should pass at least 2 parameters denoting both <code>#hidden</code> and <code>#cell_state</code>. </p>\n\n<p>So, this confuses me a lot when trying to figure out what the TensorFlow's cells do. Under the hood, are they implemented like this just for the sake of convenience or did I misunderstand something in the blog mentioned?</p>\n\n<p><a href=\"https://i.sstatic.net/pb4NN.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/pb4NN.png\" alt=\"dimensions illustration\"></a></p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "function-approximation",
      "history"
    ],
    "owner": {
      "account_id": 9988765,
      "reputation": 249,
      "user_id": 30623,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e2390f9b14989661864af2e1d106ac09?s=256&d=identicon&r=PG",
      "display_name": "Evgeniy",
      "link": "https://ai.stackexchange.com/users/30623/evgeniy"
    },
    "is_answered": true,
    "view_count": 255,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1639313008,
    "creation_date": 1571509395,
    "last_edit_date": 1639313008,
    "question_id": 15977,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/15977/is-there-a-way-to-understand-neural-networks-without-using-the-concept-of-brain",
    "title": "Is there a way to understand neural networks without using the concept of brain?",
    "body": "<p>Is there a way to understand, for instance, a multi-layered perceptron without hand-waving about them being similar to brains, etc?</p>\n<p>For example, it is obvious that what a perceptron does is approximating a function; there might be many other ways, given a labelled dataset, to find the separation of the input area into smaller areas that correspond to the labels; however, these ways would probably be computationally rather ineffective, which is why they cannot be practically used. However, it seems that the iterative approach of finding such areas of separation may give a huge speed-up in many cases; then, natural questions arise why this speed-up may be possible, how it happens and in which cases.</p>\n<p>One could be sure that this question was investigated. If anyone could shed any light on the history of this question, I would be very grateful.</p>\n<p>So, why are neural networks useful and what do they do? I mean, from the practical and mathematical standpoint, without relying on the concept of &quot;brain&quot; or &quot;neurons&quot; which can explain nothing at all.</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "agi",
      "artificial-consciousness",
      "chinese-room-argument"
    ],
    "owner": {
      "account_id": 14596323,
      "reputation": 500,
      "user_id": 22840,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b9198e034437f289a3261c5f0ce52e7c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Landon G",
      "link": "https://ai.stackexchange.com/users/22840/landon-g"
    },
    "is_answered": true,
    "view_count": 1419,
    "answer_count": 2,
    "score": 14,
    "last_activity_date": 1591265126,
    "creation_date": 1591143586,
    "last_edit_date": 1591152779,
    "question_id": 21593,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/21593/how-does-one-prove-comprehension-in-machines",
    "title": "How does one prove comprehension in machines?",
    "body": "<p>Say we have a machine and we give it a task to do (vision task, language task, game, etc.), how can one prove that a machine actually know's what's going on/happening in that specific task?</p>\n\n<p>To narrow it down, some examples:</p>\n\n<p><strong>Conversation</strong> - How would one prove that a machine actually knows what it's talking about or comprehending what is being said? The Turing test is a good start, but never actually addressed <em>actual comprehension</em>.</p>\n\n<p><strong>Vision</strong>: How could someone prove or test that a machine actually knows what it's seeing? Object detection is a start, but I'd say it's very inconclusive that a machine understands at any level what it is actually seeing.</p>\n\n<p>How do we prove comprehension in machines?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "reference-request",
      "computational-learning-theory",
      "books"
    ],
    "owner": {
      "account_id": 19487360,
      "reputation": 143,
      "user_id": 40905,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-M-E0U5FLhmc/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuckI0cTtKc8--jXFyO20qYr1YJ7YTw/s256-rj/photo.jpg",
      "display_name": "Ilya",
      "link": "https://ai.stackexchange.com/users/40905/ilya"
    },
    "is_answered": true,
    "view_count": 8936,
    "accepted_answer_id": 23520,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1736613082,
    "creation_date": 1599769258,
    "last_edit_date": 1610825837,
    "question_id": 23507,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/23507/what-are-other-examples-of-theoretical-machine-learning-books",
    "title": "What are other examples of theoretical machine learning books?",
    "body": "<p>I am looking for a book about machine learning that would suit my physics background. I am more or less familiar with classical and complex analysis, theory of probability, сcalculus of variations, matrix algebra, etc. However, I have not studied topology, measure theory, group theory, and other more advanced topics. I try to find a book that is written neither for beginners, nor for mathematicians.</p>\n<p>Recently, I have read the great book &quot;Statistical inference&quot; written by Casella and Berger. They write in the introduction that &quot;The purpose of this book is to build theoretical statistics (as different from mathematical statistics) from the first principles of probability theory&quot;. So, <em>I am looking for some &quot;theoretical books&quot; about machine learning</em>.</p>\n<p>There are many online courses and brilliant books out there that focus on the practical side of applying machine learning models and using the appropriate libraries. It seems to me that there are no problems with them, but I would like to find a book on theory.</p>\n<p>By now I have skimmed through the following books</p>\n<ul>\n<li><p><a href=\"https://rads.stackoverflow.com/amzn/click/com/0387310738\" rel=\"noreferrer\" rel=\"nofollow noreferrer\">Pattern Recognition And Machine Learning</a></p>\n<p>It looks very nice. The only point of concern is that the book was published in 2006. So, I am not sure about the relevance of the chapters considering neural nets, since this field is developing rather fast.</p>\n</li>\n<li><p><a href=\"https://rads.stackoverflow.com/amzn/click/com/0387848576\" rel=\"noreferrer\" rel=\"nofollow noreferrer\">The elements of statistical learning</a></p>\n<p>This book also seems very good. It covers most of the topics as well as the first book. However, I am feeling that its style is different and I do not know which book will suit me better.</p>\n</li>\n<li><p><a href=\"https://rads.stackoverflow.com/amzn/click/com/0136042597\" rel=\"noreferrer\" rel=\"nofollow noreferrer\">Artificial Intelligence. A Modern Approach</a></p>\n<p>This one covers more recent topics, such as natural language processing. As far as I understand, it represents the view of a computer scientist on machine learning.</p>\n</li>\n<li><p><a href=\"https://rads.stackoverflow.com/amzn/click/com/0262018020\" rel=\"noreferrer\" rel=\"nofollow noreferrer\">Machine Learning A Probabilistic Perspective</a></p>\n<p>Maybe it has a slight bias towards probability theory, which is stated in the title. However, the book looks fascinating as well.</p>\n</li>\n</ul>\n<p>I think that the first or the second book should suit me, but I do not know what decision to make.</p>\n<p>I am sure that I have overlooked some books.</p>\n<p><strong>Are there some other ML books that focus on theory?</strong></p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "comparison",
      "batch-normalization",
      "layer-normalization"
    ],
    "owner": {
      "account_id": 5531362,
      "reputation": 1369,
      "user_id": 16871,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/1bbac7394bd9f4920abbce95fcd001c6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Alexander Soare",
      "link": "https://ai.stackexchange.com/users/16871/alexander-soare"
    },
    "is_answered": true,
    "view_count": 11371,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1685211339,
    "creation_date": 1618337609,
    "last_edit_date": 1633853939,
    "question_id": 27309,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/27309/what-are-the-consequences-of-layer-norm-vs-batch-norm",
    "title": "What are the consequences of layer norm vs batch norm?",
    "body": "<p>I'll start with my understanding of the literal difference between these two. First, let's say we have an input tensor to a layer, and that tensor has dimensionality <span class=\"math-container\">$B \\times D$</span>, where <span class=\"math-container\">$B$</span> is the size of the batch and <span class=\"math-container\">$D$</span> is the dimensionality of the input corresponding to a single instance within the batch.</p>\n<ul>\n<li>Batch norm does the normalization across the batch dimension <span class=\"math-container\">$B$</span></li>\n<li>Layer norm does the normalization across <span class=\"math-container\">$D$</span></li>\n</ul>\n<p>What are the differences in terms of the consequences of this choice?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "hyperparameter-optimization",
      "hyper-parameters",
      "embeddings"
    ],
    "owner": {
      "account_id": 4353517,
      "reputation": 1065,
      "user_id": 5351,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Lr5jY.jpg?s=256",
      "display_name": "Lerner Zhang",
      "link": "https://ai.stackexchange.com/users/5351/lerner-zhang"
    },
    "is_answered": true,
    "view_count": 25946,
    "answer_count": 3,
    "score": 14,
    "last_activity_date": 1742603855,
    "creation_date": 1625664403,
    "last_edit_date": 1626008761,
    "question_id": 28564,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/28564/how-to-determine-the-embedding-size",
    "title": "How to determine the embedding size?",
    "body": "<p>When we are training a neural network, we are going to determine the embedding size to convert the categorical (in NLP, for instance) or continuous (in computer vision or voice) information to hidden vectors (or embeddings), but I wonder if there are some rules to set the size of it?</p>\n"
  },
  {
    "tags": [
      "transformer",
      "attention"
    ],
    "owner": {
      "account_id": 3432928,
      "reputation": 193,
      "user_id": 49400,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e1f09992473ed544f4a0e26ed0ea930e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "chasep255",
      "link": "https://ai.stackexchange.com/users/49400/chasep255"
    },
    "is_answered": true,
    "view_count": 9151,
    "accepted_answer_id": 30392,
    "answer_count": 1,
    "score": 14,
    "last_activity_date": 1728773142,
    "creation_date": 1629798916,
    "question_id": 30341,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/30341/why-does-a-transformer-not-use-an-activation-function-following-the-multi-head-a",
    "title": "Why does a transformer not use an activation function following the multi-head attention layer?",
    "body": "<p>I was hoping someone could explain to me why in the transformer model from the &quot;Attention is all you need&quot; paper there is no activation applied after both the multihead attention layer and to the residual connections.  It seems to me that there are multiple linear layers in a row, and I have always been under the impression that you should have an activation between linear layers.</p>\n<p>For instance when I look at the different flavors of resnet they always apply some sort of non linearity following a linear layer.  For instance a residual block might look something like...</p>\n<p>Input -&gt; Conv -&gt; BN -&gt; Relu -&gt; Conv -&gt; (+ Input) -&gt; BN -&gt; Relu</p>\n<p>or in the case of pre-activation...</p>\n<p>Input -&gt; BN -&gt; Relu -&gt; Conv -&gt; BN -&gt; Relu -&gt; Conv -&gt; (+ Input)</p>\n<p>In all the resnet flavors I have seen, they never allow two linear layers to be connected without a relu in-between.</p>\n<p>However in the the transformer...</p>\n<p>Input -&gt; Multihead-Attn -&gt; Add/Norm -&gt; Feed Forward(Dense Layer -&gt; Relu -&gt; Dense Layer) -&gt; Add/Norm</p>\n<p>In the multihead attention layer it performs the attention mechanism and then applies a fully connected layer to project back to the dimension of its input.  However, there is no non linearity between that and feed forward network (except for maybe the softmax used in part of the attention.)  A model like this would make more sense to me...</p>\n<p>Input -&gt; Multihead-Attn -&gt; Add/Norm -&gt; <strong>Relu</strong> -&gt; Feed Forward(Dense Layer -&gt; Relu -&gt; Dense Layer) -&gt; Add/Norm -&gt; <strong>Relu</strong></p>\n<p>or something like the pre-activated resnet...</p>\n<p>Input -&gt; Relu -&gt; Multihead-Attn -&gt; Add/Norm -&gt; Input2 -&gt; <strong>Relu</strong> -&gt; Feed Forward(Dense Layer -&gt; Relu -&gt; Dense Layer) -&gt; Add/Norm(Input2)</p>\n<p>Can anyone explain why the transformer is the way it is?</p>\n<p>I have asked a similar question when I was looking at the architecture of wavenet on another forum but I never really got a clear answer. In that case it did not make sense to me again why there was no activation applied to the residual connections.\n(<a href=\"https://www.reddit.com/r/MachineLearning/comments/njbjfb/d_is_there_a_point_to_having_layers_with_just_a/\" rel=\"noreferrer\">https://www.reddit.com/r/MachineLearning/comments/njbjfb/d_is_there_a_point_to_having_layers_with_just_a/</a>)</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "agi",
      "chinese-room-argument"
    ],
    "owner": {
      "account_id": 429324,
      "reputation": 1214,
      "user_id": 66,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/6lIqv.png?s=256",
      "display_name": "S.L. Barth is on codidact.com",
      "link": "https://ai.stackexchange.com/users/66/s-l-barth-is-on-codidact-com"
    },
    "is_answered": true,
    "view_count": 1284,
    "answer_count": 5,
    "score": 13,
    "last_activity_date": 1732761208,
    "creation_date": 1470166927,
    "last_edit_date": 1613432703,
    "question_id": 123,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/123/does-the-chinese-room-argument-hold-against-ai",
    "title": "Does the Chinese Room argument hold against AI?",
    "body": "<p>Back in college, I had a Complexity Theory teacher who stated that artificial intelligence was a contradiction in terms. If it could be calculated mechanically, he argued, it wasn't intelligence, it was math.</p>\n\n<p>This seems to be a variant of the Chinese Room argument. This argument is a metaphor, where a person is put in a room full of Chinese books. This person doesn't understand a word of Chinese but is slipped messages in Chinese under the door. The person has to use the books, which contain transformation rules, to answer these messages. The person can apply the transformation rules but does not understand what (s)he is communicating.</p>\n\n<p>Does the Chinese room argument hold? Can we argue that artificial intelligence is merely clever algorithmics?</p>\n"
  },
  {
    "tags": [
      "reference-request",
      "multi-agent-systems",
      "blockchain"
    ],
    "owner": {
      "account_id": 8478931,
      "reputation": 243,
      "user_id": 1256,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/9906921a3f8d9eb80c4dee6acb4a35a6?s=256&d=identicon&r=PG",
      "display_name": "Tobias Strand",
      "link": "https://ai.stackexchange.com/users/1256/tobias-strand"
    },
    "is_answered": true,
    "view_count": 531,
    "protected_date": 1634121561,
    "accepted_answer_id": 1315,
    "answer_count": 6,
    "score": 13,
    "last_activity_date": 1634119890,
    "creation_date": 1470287223,
    "last_edit_date": 1634087937,
    "question_id": 1285,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1285/are-there-any-decentralized-examples-of-ai-systems-with-blockchain-technology",
    "title": "Are there any decentralized examples of AI systems with blockchain technology?",
    "body": "<p>Has there been any attempts to deploy AI with blockchain technology? </p>\n\n<p>Are there any decentralized examples of AI networks with no central point of control with AI nodes acting independently (but according to a codified set of rules) creating, validating and storing the same shared decentralized database in many locations around the world?</p>\n"
  },
  {
    "tags": [
      "history",
      "intelligence-testing",
      "turing-test",
      "chat-bots"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 533,
    "accepted_answer_id": 1452,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1473175421,
    "creation_date": 1470449059,
    "last_edit_date": 1471272281,
    "question_id": 1397,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/1397/are-there-any-ai-that-have-passed-the-mist-test-so-far",
    "title": "Are there any AI that have passed the MIST test so far?",
    "body": "<p><a href=\"https://en.wikipedia.org/wiki/Minimum_intelligent_signal_test\" rel=\"noreferrer\">MIST</a> is a quantiative test of humanness, consisting of ~80k propositions such as:</p>\n\n<ul>\n<li>Is Earth a planet?</li>\n<li>Is the sun bigger than my foot?</li>\n<li>Do people sometimes lie?</li>\n<li>etc.</li>\n</ul>\n\n<p>Have any AI attempted and passed this test to date?</p>\n"
  },
  {
    "tags": [
      "research",
      "agi"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 1996,
    "accepted_answer_id": 1421,
    "answer_count": 4,
    "score": 13,
    "last_activity_date": 1574465586,
    "creation_date": 1470508730,
    "last_edit_date": 1574465586,
    "question_id": 1420,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1420/how-close-are-we-to-creating-ex-machina",
    "title": "How close are we to creating Ex Machina?",
    "body": "<p>Are there any research teams that attempted to create or have already created an AI robot that can be as close to intelligent as these found in <a href=\"https://en.wikipedia.org/wiki/Ex_Machina_(film)\" rel=\"nofollow noreferrer\"><em>Ex Machina</a></em> or <em><a href=\"https://en.wikipedia.org/wiki/I,_Robot_(film)\" rel=\"nofollow noreferrer\">I, Robot</em></a> movies?</p>\n\n<p>I'm not talking about full awareness, but an artificial being that can make its own decisions and physical and intellectual tasks that a human being can do?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "reference-request",
      "attention"
    ],
    "owner": {
      "account_id": 2685693,
      "reputation": 643,
      "user_id": 1270,
      "user_type": "registered",
      "accept_rate": 83,
      "profile_image": "https://i.sstatic.net/n8zHs.jpg?s=256",
      "display_name": "Zolt&#225;n Schmidt",
      "link": "https://ai.stackexchange.com/users/1270/zolt%c3%a1n-schmidt"
    },
    "is_answered": true,
    "view_count": 260,
    "accepted_answer_id": 1440,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1631671800,
    "creation_date": 1470524383,
    "last_edit_date": 1572970470,
    "question_id": 1423,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1423/is-there-any-artificial-intelligence-that-possesses-concentration",
    "title": "Is there any artificial intelligence that possesses &quot;concentration&quot;?",
    "body": "<p>Humans can do multiple tasks at the same (e.g. reading while listening to music), but we memorize information from less focused sources with worse efficiency than we do from our main focus or task.</p>\n\n<p>Do such things exist in the case of artificial intelligence? I doubt, for example, that neural networks have such characteristics, but I may be wrong.</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "human-like"
    ],
    "owner": {
      "account_id": 5093125,
      "reputation": 706,
      "user_id": 1538,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/mhTEQ.png?s=256",
      "display_name": "Avik Mohan",
      "link": "https://ai.stackexchange.com/users/1538/avik-mohan"
    },
    "is_answered": true,
    "view_count": 569,
    "accepted_answer_id": 1811,
    "answer_count": 6,
    "score": 13,
    "last_activity_date": 1679654163,
    "creation_date": 1472587372,
    "question_id": 1806,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/1806/have-any-ai-systems-yet-been-developed-that-can-knowingly-lie-to-deceive-a-hum",
    "title": "Have any AI systems yet been developed that can knowingly lie to / deceive a human?",
    "body": "<p>AI systems today are very capable machines, and recently the area of Natural Language Processing and Response has been exploding with innovation, as well as the fundamental algorithmic structure of AI machines.</p>\n\n<p>I am asking if, given these recent breakthroughs, have any AI systems been developed that are able to (preferably with some measure of success) knowingly lie to humans about facts that it knows?</p>\n\n<p>Note, what I'm asking goes beyond the canonical discussions of the Turing Test. I'm asking of machines that can 'understand' facts and then formulate a lie against this fact, perhaps using other facts to produce a believable 'cover-up' as part of the lie.</p>\n\n<p>E.G.: CIA supercomputer is stolen by spies and they try to use the computer to do things, but the computer keeps saying it's missing dependencies though it really isn't or gives correct-looking but wrong answers knowingly. Or gives incorrect location of a person, knowing that the person frequents some place but isn't there at the moment. Doesn't have to be this sophisticated, of course.</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "reference-request"
    ],
    "owner": {
      "account_id": 5776675,
      "reputation": 233,
      "user_id": 1909,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/dc7f8f9d0984edc53ef9bf032eb611cd?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Kevin",
      "link": "https://ai.stackexchange.com/users/1909/kevin"
    },
    "is_answered": true,
    "view_count": 2168,
    "accepted_answer_id": 1810,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1640203752,
    "creation_date": 1472592422,
    "last_edit_date": 1640203696,
    "question_id": 1809,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1809/can-an-ai-make-a-constructed-natural-language",
    "title": "Can an AI make a constructed (natural) language?",
    "body": "<p>According to <a href=\"https://simple.wikipedia.org/wiki/Constructed_language\" rel=\"nofollow noreferrer\">Wikipedia</a></p>\n<blockquote>\n<p>An artificial or constructed language (sometimes called a conlang) is a language that has been created by a person or small group, instead of being formed naturally as part of a culture.</p>\n</blockquote>\n<p>My question is, could an AI make/construct its own natural language, with words, conjugations, and grammar rules? Basically, a language that humans could use to speak to each other. (Preferably to communicate abstract, high-level concepts.)</p>\n<p>What techniques could such an AI use? Could it be based on existing natural languages or would it have few connections to existing natural languages? Could it design a language that's easier to learn than existing languages (even <a href=\"https://en.wikipedia.org/wiki/Esperanto\" rel=\"nofollow noreferrer\">Esperanto</a>)?</p>\n"
  },
  {
    "tags": [
      "agi",
      "problem-solving"
    ],
    "owner": {
      "account_id": 3364317,
      "reputation": 2589,
      "user_id": 75,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/vBJkv.png?s=256",
      "display_name": "Ben N",
      "link": "https://ai.stackexchange.com/users/75/ben-n"
    },
    "is_answered": true,
    "view_count": 902,
    "accepted_answer_id": 2022,
    "answer_count": 4,
    "score": 13,
    "last_activity_date": 1475260858,
    "creation_date": 1474995324,
    "question_id": 2020,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/2020/could-an-ai-think-laterally-while-avoiding-ethically-suboptimal-choices",
    "title": "Could an AI think laterally while avoiding &quot;ethically suboptimal&quot; choices?",
    "body": "<p>In the recent PC game <em><a href=\"http://www.theturingtestgame.com/\">The Turing Test</a></em>, the AI (\"TOM\") needs help from Ava to get through some puzzle rooms. TOM says he is unable to solve the puzzles because he is not allowed to \"<a href=\"https://en.wikipedia.org/wiki/Lateral_thinking\">think laterally</a>.\" Specifically, he says he would not have thought to throw a box through a window to solve the first room. His creators, the story goes, turned that capability off because such thinking could produce \"ethically suboptimal\" solutions, like chopping off an arm to leave on a pressure plate.</p>\n\n<p>Would all creative puzzle-solving abilities need to be removed from an AI to keep its results reasonable, or could we get some benefits of lateral thinking without losing an arm?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "definitions",
      "biology"
    ],
    "owner": {
      "account_id": 6343509,
      "reputation": 399,
      "user_id": 26,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/a970419c25f5075d5290e629971fadb3?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Soham",
      "link": "https://ai.stackexchange.com/users/26/soham"
    },
    "is_answered": true,
    "view_count": 20520,
    "answer_count": 13,
    "score": 13,
    "last_activity_date": 1600862482,
    "creation_date": 1476074672,
    "last_edit_date": 1573582977,
    "question_id": 2111,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2111/is-ai-living-or-non-living",
    "title": "Is AI living or non-living?",
    "body": "<p>I'm a bit confused about the definition of life. Can AI systems be called 'living'? Because they can do most of the things that we can. They can even communicate with one another. </p>\n\n<p>They are not formed of what we call cells. But, you see, cells are just a collection of several chemical processes which is in turn non-living just like AI is formed of several lines of code.</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "papers",
      "scalability",
      "neural-turing-machine"
    ],
    "owner": {
      "account_id": 5416059,
      "reputation": 4265,
      "user_id": 2227,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://www.gravatar.com/avatar/af11dfed2f1c4e002c6a71cea4a7cfab?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "BlindKungFuMaster",
      "link": "https://ai.stackexchange.com/users/2227/blindkungfumaster"
    },
    "is_answered": true,
    "view_count": 1824,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1604874660,
    "creation_date": 1476370849,
    "last_edit_date": 1604874660,
    "question_id": 2144,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2144/how-would-deepminds-new-differentiable-neural-computer-scale",
    "title": "How would DeepMind&#39;s new differentiable neural computer scale?",
    "body": "<p>DeepMind just published a <a href=\"https://www.nature.com/articles/nature20101\" rel=\"nofollow noreferrer\">paper</a> about a <a href=\"https://deepmind.com/blog/differentiable-neural-computers/\" rel=\"nofollow noreferrer\"><em>differentiable neural computer</em></a>, which basically <em>combines a neural network with a memory</em>.</p>\n<p>The idea is to teach the neural network to create and recall useful explicit memories for a certain task. This complements the abilities of a neural network well, because NNs only store knowledge implicitly in the weights and the information used to work on a single task is only stored in the activation of the network and degrades quickly the more information you add. (<a href=\"https://en.wikipedia.org/wiki/Long_short-term_memory\" rel=\"nofollow noreferrer\">LSTMs</a> are one try to slow down this degradation of short term memories, but it still happens.)</p>\n<p>Now, instead of keeping the necessary information in the activation, they presumably keep the addresses of memory slots for specific information in the activation, so these should also be subject to degradation.</p>\n<p>Why would this approach scale? Shouldn't a somewhat higher number of task-specific information once again overwhelm the network's capability of keeping the addresses of all the appropriate memory slots in its activation?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "training",
      "generative-adversarial-networks",
      "self-supervised-learning"
    ],
    "owner": {
      "account_id": 974718,
      "reputation": 1106,
      "user_id": 39,
      "user_type": "registered",
      "accept_rate": 38,
      "profile_image": "https://i.sstatic.net/F8mcb.jpg?s=256",
      "display_name": "Eka",
      "link": "https://ai.stackexchange.com/users/39/eka"
    },
    "is_answered": true,
    "view_count": 1787,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1606048258,
    "creation_date": 1477309348,
    "last_edit_date": 1606048258,
    "question_id": 2211,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2211/how-are-generative-adversarial-networks-trained",
    "title": "How are generative adversarial networks trained?",
    "body": "<p>I am reading about generative adversarial networks (GANs) and I have some doubts regarding it. So far, I understand that in a GAN there are two different types of neural networks: one is generative (<span class=\"math-container\">$G$</span>) and the other discriminative (<span class=\"math-container\">$D$</span>). The generative neural network generates some data which the discriminative neural network judges for correctness.</p>\n<p>How do the discriminative (<span class=\"math-container\">$D$</span>) neural nets initially know whether the data produced by <span class=\"math-container\">$G$</span> is correct or not? Do we have to train the <span class=\"math-container\">$D$</span> first then add it into the GAN with <span class=\"math-container\">$G$</span>?</p>\n<p>Let's consider my trained <span class=\"math-container\">$D$</span> net, which can classify a picture with 90% accuracy. If we add this <span class=\"math-container\">$D$</span> to a GAN, there is a 10% probability it will classify an image wrong. If we train a GAN with this <span class=\"math-container\">$D$</span>, then will it also have the same 10% error in classifying an image? If yes, then why do GANs show promising results?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "overfitting",
      "support-vector-machine",
      "no-free-lunch-theorems"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user4747"
    },
    "is_answered": true,
    "view_count": 876,
    "accepted_answer_id": 2633,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1610971351,
    "creation_date": 1484037496,
    "last_edit_date": 1610971351,
    "question_id": 2632,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2632/are-the-shortcomings-of-neural-networks-diminishing",
    "title": "Are the shortcomings of neural networks diminishing?",
    "body": "<p>Having worked with neural networks for about half a year, I have experienced first-hand what are often claimed as their main disadvantages, i.e. overfitting and getting stuck in local minima. However, through hyperparameter optimization and some newly invented approaches, these have been overcome for my scenarios. From my own experiments:</p>\n<ul>\n<li><p>Dropout seems to be a very good regularization method.</p>\n</li>\n<li><p>Batch normalization eases training and keeps signal strength consistent across many layers.</p>\n</li>\n<li><p>Adadelta consistently reaches very good optima</p>\n</li>\n</ul>\n<p>I have experimented with <code>scikit-learn</code>'s implementation of SVM alongside my experiments with neural networks, but I find the performance to be very poor in comparison, even after having done grid-searches for hyperparameters. I realize that there are countless other methods, and that SVM's can be considered a sub-class of NN's, but still.</p>\n<p>So, to my question:</p>\n<p><em><strong>With all the newer methods researched for neural networks, have they slowly - or will they - become &quot;superior&quot; to other methods? Neural networks have their disadvantages, as do others, but with all the new methods, have these disadvantages been mitigated to a state of insignificance?</strong></em></p>\n<p>I realize that oftentimes &quot;less is more&quot; in terms of model complexity, but that too can be architected for neural networks. The idea of &quot;no free lunch&quot; forbids us to assume that one approach always will reign superior. It's just that my own experiments - along with countless papers on awesome performances from various NN's - indicate that there might be, at the least, a very cheap lunch.</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "terminology"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user6698"
    },
    "is_answered": true,
    "view_count": 6883,
    "protected_date": 1574205401,
    "answer_count": 6,
    "score": 13,
    "last_activity_date": 1561338728,
    "creation_date": 1492459257,
    "last_edit_date": 1561293038,
    "question_id": 3176,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3176/what-are-good-alternatives-to-the-expression-artificial-intelligence",
    "title": "What are good alternatives to the expression &quot;Artificial Intelligence&quot;?",
    "body": "<p>I read a really interesting article titled <a href=\"http://www.joshworth.com/stop-calling-in-artificial-intelligence/\" rel=\"nofollow noreferrer\">\"Stop Calling it Artificial Intelligence\"</a> that made a compelling critique of the name \"Artificial Intelligence\".</p>\n\n<ol>\n<li><p>The word intelligence is so broad that it's hard to say whether \"Artificial Intelligence\" is really intelligent. Artificial Intelligence, therefore, tends to be misinterpreted as replicating human intelligence, which isn't actually what Artificial Intelligence is.</p></li>\n<li><p>Artificial Intelligence isn't really \"artificial\". Artificial implies a fake imitation of something, which isn't exactly what artificial intelligence is.</p></li>\n</ol>\n\n<p>What are good alternatives to the expression \"Artificial Intelligence\"? (Good answers won't list names at random; they'll give a rationale for why their alternative name is a good one.)</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "game-ai"
    ],
    "owner": {
      "account_id": 302085,
      "reputation": 381,
      "user_id": 4259,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/005302f3d866d3dcf546a63c47dd3cd7?s=256&d=identicon&r=PG",
      "display_name": "Totem",
      "link": "https://ai.stackexchange.com/users/4259/totem"
    },
    "is_answered": true,
    "view_count": 2402,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1567267365,
    "creation_date": 1497007294,
    "question_id": 3469,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/3469/input-output-encoding-for-a-neural-network-to-learn-a-grid-based-game",
    "title": "Input/output encoding for a neural network to learn a grid-based game",
    "body": "<p>I am writing a simple toy game with the intent of training a deep neural network on top of it. The games rules are roughly the following:</p>\n\n<ul>\n<li>The game has a board made up of hexagonal cells.</li>\n<li>Both players have the same collection of pieces that they can choose to position freely on the board.</li>\n<li>Placing different types of pieces award points (or decrease opponent's points) depending on their position and configuration wrt one another.</li>\n<li>Whoever has more points win.</li>\n</ul>\n\n<p>There are additional rules (about turns, number and types of pieces, etc...) but they are not important in the context of this question. I want to devise a deep neural network that can iteratively learn by playing against itself. My questions are about representation of input and output. In particular:</p>\n\n<ul>\n<li>Since pattern of pieces matter, I was thinking to have at least some convolutional layers. The board can be of various size but in principle very small (6x10 on my tests, to be expanded by few cells). Does it make sense? What kind of pooling can I use?</li>\n<li>How to represent both sides? In <a href=\"https://arxiv.org/pdf/1412.3409.pdf\" rel=\"noreferrer\">this paper</a> about go, authors use two input matrices, one for white stones and one for black stones. Can it work in this case too? But remember I have different types of pieces, say A, B, C and D. Should I use 2x4 input matrices? It seem very sparse and of little efficiency to me. I fear it will be way too sparse for the convolutional layers to work.</li>\n<li>I thought that the output could be a distribution of probabilities over the matrix representing board positions, plus a separate array of probabilities indicating what piece to play. However, I also need to represent the ability to <strong>pass</strong> the turn, which is very important. How can I do it without diluting its significance among other probabilities?</li>\n<li>And <strong>most importantly</strong>, do I enforce winning moves only or losing moves too? Enforcing winning moves is easy because I just set desired probabilities to 1. However when losing, what can I do? Set that move probability to 0 and all the others to the same value? Also, does it make sense to enforce moves by the final score difference, even though this would go against the meaning of the outputs, which are roughly probabilities?</li>\n</ul>\n\n<p>Also, I developed the game engine in node.js thinking to use Synaptic as framework, but I am not sure it can work with convolutional networks (I doubt there's a way to fix the weights associated to local perceptive fields). Any advice on other libraries that are compatible with node? </p>\n"
  },
  {
    "tags": [
      "philosophy",
      "agi",
      "proofs",
      "neuroscience",
      "computational-theory-of-mind"
    ],
    "owner": {
      "account_id": 201479,
      "reputation": 407,
      "user_id": 8221,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ebce2eb8d7efcc2263fd0e245e7267c8?s=256&d=identicon&r=PG",
      "display_name": "yters",
      "link": "https://ai.stackexchange.com/users/8221/yters"
    },
    "is_answered": true,
    "view_count": 6486,
    "protected_date": 1561494539,
    "answer_count": 5,
    "score": 13,
    "last_activity_date": 1611008397,
    "creation_date": 1499100946,
    "last_edit_date": 1610990641,
    "question_id": 3573,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3573/is-there-a-rigorous-proof-that-agi-is-possible-at-least-in-theory",
    "title": "Is there a rigorous proof that AGI is possible, at least, in theory?",
    "body": "<p>It is often implicitly assumed in computer science that the human mind, or at least some mechanical calculations that humans perform (see <a href=\"https://plato.stanford.edu/entries/church-turing/\" rel=\"nofollow noreferrer\">the Church-Turing thesis</a>), can be replicated with a Turing machine, therefore Artificial General Intelligence (AGI), defined as a human-like AI, may be possible.</p>\n<p>I do not know of any other argument that AGI is possible, and the foregoing argument is extremely weak.</p>\n<p>Is there a rigorous proof that AGI is possible, at least, in theory? How do we know that everything the human mind can do can be encoded as a program?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "computer-vision",
      "training",
      "reference-request"
    ],
    "owner": {
      "account_id": 11909739,
      "reputation": 139,
      "user_id": 10218,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0c3d6a2b851e5914db7310d8b415c37b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Naji",
      "link": "https://ai.stackexchange.com/users/10218/naji"
    },
    "is_answered": true,
    "view_count": 21430,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1634087677,
    "creation_date": 1508177403,
    "last_edit_date": 1634087677,
    "question_id": 4282,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4282/is-it-possible-to-train-a-neural-network-to-estimate-a-vehicles-length",
    "title": "Is it possible to train a neural network to estimate a vehicle&#39;s length?",
    "body": "<p>I have a large dataset (over 100k samples) of vehicles with the ground truth of their lengths.</p>\n<p>Is it possible to train a deep network to measure/estimate vehicle length?</p>\n<p>I haven't seen any papers related to estimating object size using a deep neural network.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "convolutional-neural-networks",
      "recurrent-neural-networks",
      "comparison"
    ],
    "owner": {
      "account_id": 10984792,
      "reputation": 151,
      "user_id": 11256,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/eafc1d649b5811a2b85479cf8a9ce232?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Pradeep BV",
      "link": "https://ai.stackexchange.com/users/11256/pradeep-bv"
    },
    "is_answered": true,
    "view_count": 8579,
    "answer_count": 5,
    "score": 13,
    "last_activity_date": 1682064488,
    "creation_date": 1512744516,
    "last_edit_date": 1557782956,
    "question_id": 4683,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4683/what-is-the-fundamental-difference-between-cnn-and-rnn",
    "title": "What is the fundamental difference between CNN and RNN?",
    "body": "<p>What is the fundamental difference between convolutional neural networks and recurrent neural networks? Where are they applied?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "activation-functions",
      "relu"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user9947"
    },
    "is_answered": true,
    "view_count": 3393,
    "accepted_answer_id": 5621,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1600943912,
    "creation_date": 1520592331,
    "last_edit_date": 1600943912,
    "question_id": 5601,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5601/how-exactly-can-relus-approximate-non-linear-and-curved-functions",
    "title": "How exactly can ReLUs approximate non-linear and curved functions?",
    "body": "<p>Currently, the most commonly used activation functions are ReLUs. So I answered this question <a href=\"https://ai.stackexchange.com/a/5521/2444\">What is the purpose of an activation function in neural networks?</a> and, while writing the answer, it struck me, <em>how exactly can ReLUs approximate a non-linear function?</em></p>\n<p>By pure mathematical definition, sure, it's a non-linear function due to the sharp bend, but, if we confine ourselves to the positive or the negative portion of the <span class=\"math-container\">$x$</span>-axis only, then it's linear in those regions. Let's say we take the whole <span class=\"math-container\">$x$</span>-axis also, then also it's kinda linear (not in a strict mathematical sense), in the sense that it cannot satisfactorily approximate curved functions, like a sine wave (<span class=\"math-container\">$0 \\rightarrow 90$</span>) with a single node hidden layer as is possible by a sigmoid activation function.</p>\n<p>So, <em>what is the intuition behind the fact that ReLUs are used in NNs, giving satisfactory performance? Are non-linear functions, like the sigmoid and the tanh, thrown in the middle of the NN sometimes?</em></p>\n<p>I am <em>not</em> asking for the purpose of ReLUs, even though they are kind of linear.</p>\n<hr />\n<p>As per @Eka's comment, the ReLu derives its capability from discontinuity acting in the deep layers of the NN. Does this mean that ReLUs are good, as long as we use it in deep NNs and not a shallow NN?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "artificial-consciousness",
      "self-awareness"
    ],
    "owner": {
      "account_id": 14156077,
      "reputation": 578,
      "user_id": 17488,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/QoLra.jpg?s=256",
      "display_name": "Mr. Eivind",
      "link": "https://ai.stackexchange.com/users/17488/mr-eivind"
    },
    "is_answered": true,
    "view_count": 740,
    "closed_date": 1639080818,
    "accepted_answer_id": 7576,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1573506374,
    "creation_date": 1534285032,
    "last_edit_date": 1573506374,
    "question_id": 7573,
    "link": "https://ai.stackexchange.com/questions/7573/how-important-is-consciousness-for-making-advanced-artificial-intelligence",
    "closed_reason": "Needs more focus",
    "title": "How important is consciousness for making advanced artificial intelligence?",
    "body": "<p>How important is consciousness and self-consciousness for making advanced AIs? How far away are we from making such?</p>\n\n<p>When making e.g. a neural network there's (very probably) no consciousness within it, but just mathematics behind, but do we need the AIs to become conscious in order to solve more complex tasks in the future? Furthermore, is there actually any way we can know for sure if something is conscious, or if it's just faking it? It's \"easy\" to make a computer program that claims it's conscious, but that doesn't mean it is (e.g. Siri).</p>\n\n<p>And if the AIs are only based on predefined rules without consciousness, can we even call it \"intelligence\"?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "backpropagation",
      "gradient-descent",
      "pooling"
    ],
    "owner": {
      "account_id": 2782344,
      "reputation": 163,
      "user_id": 17358,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/89d509feb74686bd108421cf82f6dee8?s=256&d=identicon&r=PG",
      "display_name": "RedRus",
      "link": "https://ai.stackexchange.com/users/17358/redrus"
    },
    "is_answered": true,
    "view_count": 4690,
    "accepted_answer_id": 7702,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1640298171,
    "creation_date": 1535132045,
    "last_edit_date": 1640298171,
    "question_id": 7701,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7701/can-non-differentiable-layer-be-used-in-a-neural-network-if-its-not-learned",
    "title": "Can non-differentiable layer be used in a neural network, if it&#39;s not learned?",
    "body": "<p>For example, AFAIK, the pooling layer in a CNN is not differentiable, but it can be used because it's not learning. Is it always true?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "convolutional-neural-networks",
      "training"
    ],
    "owner": {
      "account_id": 10095283,
      "reputation": 325,
      "user_id": 17980,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-_Y1FAw47_mk/AAAAAAAAAAI/AAAAAAAAE-c/5OU4K5mDKxg/s256-rj/photo.jpg",
      "display_name": "Ruchit Dalwadi",
      "link": "https://ai.stackexchange.com/users/17980/ruchit-dalwadi"
    },
    "is_answered": true,
    "view_count": 7284,
    "accepted_answer_id": 7872,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1589917734,
    "creation_date": 1536276472,
    "last_edit_date": 1589917734,
    "question_id": 7865,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7865/which-layer-in-a-cnn-consumes-more-training-time-convolution-layers-or-fully-co",
    "title": "Which layer in a CNN consumes more training time: convolution layers or fully connected layers?",
    "body": "<p>In a convolutional neural network, which layer consumes more training time: convolution layers or fully connected layers? </p>\n\n<p>We can take AlexNet architecture to understand this. I want to see the time breakup of the training process. I want a relative time comparison so we can take any constant GPU configuration.</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "social",
      "superintelligence",
      "singularity",
      "mythology-of-ai"
    ],
    "owner": {
      "account_id": 14288720,
      "reputation": 131,
      "user_id": 17978,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/PIKjR.jpg?s=256",
      "display_name": "Bucky Rogerson",
      "link": "https://ai.stackexchange.com/users/17978/bucky-rogerson"
    },
    "is_answered": true,
    "view_count": 980,
    "answer_count": 4,
    "score": 13,
    "last_activity_date": 1639330357,
    "creation_date": 1536327115,
    "last_edit_date": 1639330357,
    "question_id": 7875,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7875/is-the-singularity-something-to-be-taken-seriously",
    "title": "Is the singularity something to be taken seriously?",
    "body": "<p>The term Singularity is often used in mainstream media for describing visionary technology. It was introduced by Ray Kurzweil in a popular book <a href=\"https://en.wikipedia.org/wiki/The_Singularity_Is_Near\" rel=\"nofollow noreferrer\">The Singularity Is Near: When Humans Transcend Biology</a> (2005).</p>\n\n<p>In his book, Kurzweil gives an outlook to a potential future of mankind which includes nanotechnology, computers, genetic modification and artificial intelligence. He argues that Moore's law will allow computers an exponential growth which results in a superintelligence.</p>\n\n<p>Is the <em>technological singularity</em> something that is taken seriously by A.I. developers or is this theory just a load of popular hype?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "autoencoders",
      "dimensionality-reduction",
      "curse-of-dimensionality"
    ],
    "owner": {
      "account_id": 4148701,
      "reputation": 42397,
      "user_id": 2444,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e6fce047cba96a601a9dac66df4cd2e8?s=256&d=identicon&r=PG",
      "display_name": "nbro",
      "link": "https://ai.stackexchange.com/users/2444/nbro"
    },
    "is_answered": true,
    "view_count": 3389,
    "answer_count": 4,
    "score": 13,
    "last_activity_date": 1587841492,
    "creation_date": 1553356403,
    "question_id": 11405,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11405/what-are-the-purposes-of-autoencoders",
    "title": "What are the purposes of autoencoders?",
    "body": "<p><a href=\"https://en.wikipedia.org/wiki/Autoencoder\" rel=\"noreferrer\">Autoencoders</a> are neural networks that learn a compressed representation of the input in order to later reconstruct it, so they can be used for dimensionality reduction. They are composed of an encoder and a decoder (which can be separate neural networks). Dimensionality reduction can be useful in order to deal with or attenuate the issues related to the curse of dimensionality, where data becomes sparse and it is more difficult to obtain \"statistical significance\". So, autoencoders (and algorithms like PCA) can be used to deal with the curse of dimensionality. </p>\n\n<p>Why do we care about dimensionality reduction specifically using autoencoders? Why can't we simply use PCA, if the purpose is dimensionality reduction? </p>\n\n<p>Why do we need to decompress the latent representation of the input if we just want to perform dimensionality reduction, or why do we need the decoder part in an autoencoder? What are the use cases? In general, why do we need to compress the input to later decompress it? Wouldn't it be better to just use the original input (to start with)?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "comparison",
      "recurrent-neural-networks",
      "definitions",
      "feedforward-neural-networks"
    ],
    "owner": {
      "account_id": 3650110,
      "reputation": 759,
      "user_id": 23527,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/ebqPY.jpg?s=256",
      "display_name": "olinarr",
      "link": "https://ai.stackexchange.com/users/23527/olinarr"
    },
    "is_answered": true,
    "view_count": 2920,
    "accepted_answer_id": 12047,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1736966721,
    "creation_date": 1556470521,
    "last_edit_date": 1591097939,
    "question_id": 12042,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12042/what-is-a-recurrent-neural-network",
    "title": "What is a recurrent neural network?",
    "body": "<p>Surprisingly, this wasn't asked before - at least I didn't find anything besides some <a href=\"https://ai.stackexchange.com/q/11717/2444\">vaguely related questions.</a></p>\n\n<p>So, what is a recurrent neural network, and what are their advantages over regular (or feed-forward) neural networks?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "comparison",
      "supervised-learning",
      "self-supervised-learning",
      "semi-supervised-learning"
    ],
    "owner": {
      "account_id": 1021083,
      "reputation": 281,
      "user_id": 12975,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/xlhE1.jpg?s=256",
      "display_name": "0x90",
      "link": "https://ai.stackexchange.com/users/12975/0x90"
    },
    "is_answered": true,
    "view_count": 14873,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1596567746,
    "creation_date": 1557620600,
    "last_edit_date": 1557646375,
    "question_id": 12266,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12266/what-is-the-relation-between-semi-supervised-and-self-supervised-visual-represen",
    "title": "What is the relation between semi-supervised and self-supervised visual representation learning?",
    "body": "<p>What's the differences between <a href=\"https://www.scihive.org/paper/1905.03670\" rel=\"noreferrer\">semi-supervised learning and self-supervised visual representation learning</a>, and how they are connected? </p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "deep-learning",
      "math",
      "activation-functions"
    ],
    "owner": {
      "account_id": 1225708,
      "reputation": 993,
      "user_id": 31388,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/nkxcn.png?s=256",
      "display_name": "Mary",
      "link": "https://ai.stackexchange.com/users/31388/mary"
    },
    "is_answered": true,
    "view_count": 5887,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1566623288,
    "creation_date": 1565821805,
    "last_edit_date": 1565827564,
    "question_id": 13978,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/13978/why-is-the-derivative-of-the-activation-functions-in-neural-networks-important",
    "title": "Why is the derivative of the activation functions in neural networks important?",
    "body": "<p>I'm new to NN. I am trying to understand some of its foundations. One question that I have is: <em>why the <strong>derivative</strong> of an activation function is important (not the function itself), and why it's the derivative which is tied to how the network performs learning</em>? </p>\n\n<p>For instance, when we say a constant derivative isn't good for learning, what is the intuition behind that? Is the activation function somehow like a <strong>hash function</strong> that needs to well differentiate small variance in inputs?</p>\n"
  },
  {
    "tags": [
      "comparison",
      "implementation"
    ],
    "owner": {
      "account_id": 12570027,
      "reputation": 233,
      "user_id": 31211,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f6da303f9d1a5e660f9271f8c42fcf87?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Someone",
      "link": "https://ai.stackexchange.com/users/31211/someone"
    },
    "is_answered": true,
    "view_count": 11996,
    "protected_date": 1573782003,
    "accepted_answer_id": 16451,
    "answer_count": 4,
    "score": 13,
    "last_activity_date": 1617090918,
    "creation_date": 1573539424,
    "last_edit_date": 1573579583,
    "question_id": 16448,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/16448/what-does-ai-software-look-like-and-how-is-it-different-from-other-software",
    "title": "What does AI software look like, and how is it different from other software?",
    "body": "<p>What does AI software look like? What is the major difference between AI software and other software?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "comparison",
      "transfer-learning",
      "one-shot-learning",
      "fine-tuning"
    ],
    "owner": {
      "account_id": 17431685,
      "reputation": 785,
      "user_id": 32861,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/JlpeQ.jpg?s=256",
      "display_name": "Hiren Namera",
      "link": "https://ai.stackexchange.com/users/32861/hiren-namera"
    },
    "is_answered": true,
    "view_count": 5218,
    "accepted_answer_id": 21727,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1591631477,
    "creation_date": 1591594440,
    "last_edit_date": 1591626147,
    "question_id": 21719,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/21719/what-is-the-difference-between-one-shot-learning-transfer-learning-and-fine-tun",
    "title": "What is the difference between one-shot learning, transfer learning and fine tuning?",
    "body": "<p>Lately, there are lots of posts on <em>one-shot</em> learning. I tried to figure out what it is by reading some articles. To me, it looks like similar to <em>transfer</em> learning, in which we can use pre-trained model weights to create our own model. <em>Fine-tuning</em> also seems a similar concept to me. </p>\n\n<p>Can anyone help me and explain the differences between all three of them?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "rewards",
      "reward-functions",
      "multi-objective-rl"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user40138"
    },
    "is_answered": true,
    "view_count": 2578,
    "accepted_answer_id": 22948,
    "answer_count": 3,
    "score": 13,
    "last_activity_date": 1631109888,
    "creation_date": 1596751600,
    "last_edit_date": 1602166559,
    "question_id": 22900,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/22900/why-is-the-reward-in-reinforcement-learning-always-a-scalar",
    "title": "Why is the reward in reinforcement learning always a scalar?",
    "body": "<p>I'm reading Reinforcement Learning by Sutton &amp; Barto, and in section 3.2 they state that the reward in a Markov decision process is always a scalar real number. At the same time, I've heard about the problem of assigning credit to an action for a reward. Wouldn't a vector reward make it easier for an agent to understand the effect of an action? Specifically, a vector in which different components represent different aspects of the reward. For example, an agent driving a car may have one reward component for driving smoothly and one for staying in the lane (and these are independent of each other).</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "natural-language-processing",
      "papers",
      "transformer",
      "attention"
    ],
    "owner": {
      "account_id": 4802469,
      "reputation": 253,
      "user_id": 15498,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8aa9ed5171107e5bba791ac9a3ba552c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "mhsnk",
      "link": "https://ai.stackexchange.com/users/15498/mhsnk"
    },
    "is_answered": true,
    "view_count": 9690,
    "accepted_answer_id": 25149,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1722919717,
    "creation_date": 1607811743,
    "last_edit_date": 1607813692,
    "question_id": 25148,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/25148/what-is-different-in-each-head-of-a-multi-head-attention-mechanism",
    "title": "What is different in each head of a multi-head attention mechanism?",
    "body": "<p>I have a difficult time understanding the &quot;multi-head&quot; notion in the original <a href=\"https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\" rel=\"noreferrer\">transformer paper</a>. What makes the learning in each head unique? Why doesn't the neural network learn the same set of parameters for each attention head? Is it because we break <em>query, key</em> and <em>value</em> vectors into smaller dimensions and feed each portion to a different head?</p>\n"
  },
  {
    "tags": [
      "comparison",
      "applications",
      "generative-adversarial-networks",
      "variational-autoencoder",
      "image-generation"
    ],
    "owner": {
      "account_id": 5531362,
      "reputation": 1369,
      "user_id": 16871,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/1bbac7394bd9f4920abbce95fcd001c6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Alexander Soare",
      "link": "https://ai.stackexchange.com/users/16871/alexander-soare"
    },
    "is_answered": true,
    "view_count": 14813,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1698441338,
    "creation_date": 1609924569,
    "last_edit_date": 1698441338,
    "question_id": 25601,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/25601/what-are-the-fundamental-differences-between-vae-and-gan-for-image-generation",
    "title": "What are the fundamental differences between VAE and GAN for image generation?",
    "body": "<p>Starting from my own understanding, and scoped to the purpose of image generation, I'm well aware of the major architectural differences:</p>\n<ul>\n<li><p>A GAN's generator samples from a relatively low dimensional random variable and produces an image. Then the discriminator takes that image and predicts whether the image belongs to a target distribution or not. Once trained, it can generate a variety of images just by sampling the initial random variable and forwarding through the generator.</p>\n</li>\n<li><p>A VAE's encoder takes an image from a target distribution and compresses it into a low dimensional latent space. Then the decoder's job is to take that latent space representation and reproduce the original image. Once the network is trained, it can generate latent space representations of various images, and interpolate between these before forwarding through the decoder which produces new images.</p>\n</li>\n</ul>\n<p><strong>What I'm more interested is the consequences of said architectural differences.</strong> Why would I choose one approach over the other? And why? (for example, if GANs typically produce better quality images, any ideas why that is so? is it true in all cases or just some?)</p>\n"
  },
  {
    "tags": [
      "optimization",
      "deep-neural-networks",
      "pytorch"
    ],
    "owner": {
      "account_id": 13832290,
      "reputation": 143,
      "user_id": 46906,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/MTzyj.jpg?s=256",
      "display_name": "Nikos Tsakas",
      "link": "https://ai.stackexchange.com/users/46906/nikos-tsakas"
    },
    "is_answered": true,
    "view_count": 5606,
    "accepted_answer_id": 27742,
    "answer_count": 1,
    "score": 13,
    "last_activity_date": 1744221734,
    "creation_date": 1620563765,
    "last_edit_date": 1620647088,
    "question_id": 27716,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/27716/when-should-you-not-use-the-bias-in-a-layer",
    "title": "When should you not use the bias in a layer?",
    "body": "<p>I'm not really that experienced with deep learning, and I've been looking at research code (mostly PyTorch) for deep neural networks, specifically GANs, and, in many cases, I see the authors setting <code>bias=False</code> in some layers without much justification. This isn't usually done in a long stack of layers that have a similar purpose, but mostly in unique layers like the initial linear layer after a conditioning vector, or certain layers in an attention architecture.</p>\n<p>I imagined there must be a strategy to this, but most articles online seem to confirm my initial perception that bias is a good thing to have available for training in pretty much every layer.</p>\n<p>Is there a specific optimization / theoretical reason to turn off biases in specific layers in a network? How can I choose when to do it when designing my own architecture?</p>\n"
  },
  {
    "tags": [
      "comparison",
      "markov-decision-process",
      "environment",
      "pomdp"
    ],
    "owner": {
      "account_id": 243197,
      "reputation": 255,
      "user_id": 51745,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/th8Br.png?s=256",
      "display_name": "martinkunev",
      "link": "https://ai.stackexchange.com/users/51745/martinkunev"
    },
    "is_answered": true,
    "view_count": 2453,
    "accepted_answer_id": 33889,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1641578964,
    "creation_date": 1640309494,
    "last_edit_date": 1641578964,
    "question_id": 33885,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/33885/is-there-a-fundamental-difference-between-an-environment-being-stochastic-and-be",
    "title": "Is there a fundamental difference between an environment being stochastic and being partially observable?",
    "body": "<p>In AI literature, deterministic vs stochastic and being fully-observable vs partially observable are usually considered two distinct properties of the environment.</p>\n<p>I'm confused about this because what appears random can be described by hidden variables. To illustrate, take an autonomous car (Russel &amp; Norvig describe taxi driving as stochastic). I can say the environment is stochastic because I don't know what the other drivers will do. Alternatively, I can say that the actions of the drivers are determined by their mental state which I cannot observe.</p>\n<p>As far as I can see, randomness can always be modeled with hidden variables. The only argument I came up with as to why the distinction is necessary is Bell's inequality, but I don't think that AI researchers had this in mind.</p>\n<p>Is there some fundamental difference between stochasticity and partial observability or is this distinction made for practical reasons?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "feedforward-neural-networks"
    ],
    "owner": {
      "account_id": 11238630,
      "reputation": 336,
      "user_id": 27875,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ea28cac58103d5a03f14023a8b22e41e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "SajanGohil",
      "link": "https://ai.stackexchange.com/users/27875/sajangohil"
    },
    "is_answered": true,
    "view_count": 3525,
    "answer_count": 5,
    "score": 13,
    "last_activity_date": 1678722393,
    "creation_date": 1678444055,
    "last_edit_date": 1678557599,
    "question_id": 39511,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/39511/why-is-a-bias-parameter-needed-in-neural-networks",
    "title": "Why is a bias parameter needed in neural networks?",
    "body": "<p>I have read several resources, including previously asked questions such as <a href=\"https://stackoverflow.com/a/2499936/8243797\">this</a>. I have also read arguments related to intercepts needed to separate linearly separable data. If my neural network can perform feature transformation, what is the need of a bias term?</p>\n<p>Since the weights are learnt, my network can optimise to fit the data. For example, if my data is in 2D coordinate plane, my equation without bias for a perceptron for the layer will be <span class=\"math-container\">$W_1X_1 + W_2X_2$</span> where <span class=\"math-container\">$X_1$</span> is <code>x</code> coordinate and <span class=\"math-container\">$X_2$</span> is <code>y</code> coordinate, making <span class=\"math-container\">$W_1$</span> and <span class=\"math-container\">$W_2$</span> coefficients of all vectors along <code>x</code> and <code>y</code> direction. Their linear combination will cover the whole plane which allows my data to be transformed across a line with 0 intercept.</p>\n<p>For example, if my weight is <code>1.0</code> for input <code>x</code>, and my bias is <code>0.1</code>, I might as well have weight <span class=\"math-container\">$1+(0.1/\\bar x)$</span> (or any other value descriptive of x) and <code>0</code> bias to get the same result.</p>\n<p>Similar things happen for the arguments related to activation mentioned in the marked solution to the referenced question.</p>\n<p>In such a scenario, why is the bias needed?</p>\n<p>Edit: A lot of the answers offer reasonable arguments for the perceptron/single layer case, but perceptron was just an example. Do they hold for deep neural networks as well, because that allows for previous layers better transformation of inputs? As mentioned by some, <code>0</code> input will truly cause a problem which I agree with.</p>\n"
  },
  {
    "tags": [
      "recurrent-neural-networks",
      "meta-learning",
      "inference",
      "large-language-models"
    ],
    "owner": {
      "account_id": 1021583,
      "reputation": 405,
      "user_id": 61871,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/fb710534fda446d2286074bb7692e65a?s=256&d=identicon&r=PG",
      "display_name": "MaiaVictor",
      "link": "https://ai.stackexchange.com/users/61871/maiavictor"
    },
    "is_answered": true,
    "view_count": 6495,
    "accepted_answer_id": 39898,
    "answer_count": 4,
    "score": 13,
    "last_activity_date": 1691249345,
    "creation_date": 1680265193,
    "last_edit_date": 1681058379,
    "question_id": 39863,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/39863/why-llms-and-rnns-learn-so-fast-during-inference-but-ironically-are-so-slow-du",
    "title": "Why LLMs and RNNs learn so fast during inference but, ironically, are so slow during training?",
    "body": "<p>Why LLMs learn so fast during inference, but, ironically, are so slow during training? That is, if you teach an AI a new concept in a prompt, it will learn and use the concept perfectly and flawless, through the whole prompt, after just one shot. Yet, if you train it in just a single sample, it will not influence its behavior at all - it will essentially forget. Why can't RNNs use whatever is happening during inference, rather than gradient descent, to update its weights and, thus, learn? In other words, can't the attention mechanism itself be used to update weights, rather than some cost function?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "training"
    ],
    "owner": {
      "account_id": 30332588,
      "reputation": 133,
      "user_id": 82241,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/aad69337756d53755133fcf2f25b0bc9?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Beluker",
      "link": "https://ai.stackexchange.com/users/82241/beluker"
    },
    "is_answered": true,
    "view_count": 6202,
    "accepted_answer_id": 45384,
    "answer_count": 2,
    "score": 13,
    "last_activity_date": 1713182222,
    "creation_date": 1712536637,
    "last_edit_date": 1713182222,
    "question_id": 45383,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/45383/can-you-train-a-neural-network-by-simply-giving-it-ratings-each-time-it-runs",
    "title": "Can you train a neural network by simply giving it ratings each time it runs?",
    "body": "<p>I am currently trying to train a bot for a game I am creating. It is a 2d game with a complex map made of various shapes. The bot and character shoot bullets that are capable of ricocheting. The neural network outputs a vector in which the bot will turn and then fire. I myself cannot calculate the correct trajectory and find the Loss of the network. But, I can give a rating on how well the neural network performs when it fires. Can I train it by simply giving it a rating, and if, how so?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "backpropagation",
      "terminology",
      "definitions"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 901,
    "accepted_answer_id": 3,
    "answer_count": 5,
    "score": 12,
    "last_activity_date": 1625741123,
    "creation_date": 1470152354,
    "last_edit_date": 1573926982,
    "question_id": 1,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1/what-is-backprop",
    "title": "What is &quot;backprop&quot;?",
    "body": "<p>What does \"backprop\" mean? Is the \"backprop\" term basically the same as \"backpropagation\" or does it have a different meaning?</p>\n"
  },
  {
    "tags": [
      "machine-learning"
    ],
    "owner": {
      "account_id": 6418119,
      "reputation": 261,
      "user_id": 77,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-XzXhmdjjKB0/AAAAAAAAAAI/AAAAAAAAAeg/9rsygRfbVyk/s256-rj/photo.jpg",
      "display_name": "callyalater",
      "link": "https://ai.stackexchange.com/users/77/callyalater"
    },
    "is_answered": true,
    "view_count": 708,
    "accepted_answer_id": 1464,
    "answer_count": 3,
    "score": 12,
    "last_activity_date": 1739260623,
    "creation_date": 1470155269,
    "question_id": 60,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/60/what-are-the-main-problems-hindering-current-ai-development",
    "title": "What are the main problems hindering current AI development?",
    "body": "<p>I have a background in Computer Engineering and have been working on developing better algorithms to mimic human thought. (One of my favorites is Analogical Modeling as applied to language processing and decision making.) However, the more I research, the more I realize just <em>how</em> complicated AI is.</p>\n\n<p>I have tried to tackle many problems in this field, but sometimes I find that I am reinventing the wheel or am trying to solve a problem that has already been proven to be unsolvable (ie. the halting problem). So, to help in furthering AI, I want to better understand the current obstacles that are hindering our progress in this field.</p>\n\n<p>For example, time and space complexity of some machine learning algorithms is super-polynomial which means that even with fast computers, it can take a while for the program to complete. Even still, some algorithms may be fast on a desktop or other computer while dealing with a small data set, but when increasing the size of the data, the algorithm becomes intractable.</p>\n\n<p>What are other issues currently facing AI development?</p>\n"
  },
  {
    "tags": [
      "gaming",
      "search",
      "chess",
      "heuristics"
    ],
    "owner": {
      "account_id": 40019,
      "reputation": 2533,
      "user_id": 55,
      "user_type": "registered",
      "accept_rate": 43,
      "profile_image": "https://i.sstatic.net/FkIiK.png?s=256",
      "display_name": "WilliamKF",
      "link": "https://ai.stackexchange.com/users/55/williamkf"
    },
    "is_answered": true,
    "view_count": 1208,
    "answer_count": 5,
    "score": 12,
    "last_activity_date": 1544521989,
    "creation_date": 1470156937,
    "last_edit_date": 1523671393,
    "question_id": 84,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/84/are-methods-of-exhaustive-search-considered-to-be-ai",
    "title": "Are methods of exhaustive search considered to be AI?",
    "body": "<p>Some programs do exhaustive searches for a solution while others do heuristic searches for a similar answer. For example, in chess, the search for the best next move tends to be more exhaustive in nature whereas, in Go, the search for the best next move tends to be more heuristic in nature due to the much larger search space.</p>\n\n<p>Is the technique of brute force exhaustive searching for a good answer considered to be AI or is it generally required that heuristic algorithms be used before being deemed AI? If so, is the chess-playing computer beating a human professional seen as a meaningful milestone?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "agi",
      "human-like"
    ],
    "owner": {
      "account_id": 974718,
      "reputation": 1106,
      "user_id": 39,
      "user_type": "registered",
      "accept_rate": 38,
      "profile_image": "https://i.sstatic.net/F8mcb.jpg?s=256",
      "display_name": "Eka",
      "link": "https://ai.stackexchange.com/users/39/eka"
    },
    "is_answered": true,
    "view_count": 708,
    "accepted_answer_id": 236,
    "answer_count": 3,
    "score": 12,
    "last_activity_date": 1611616884,
    "creation_date": 1470239778,
    "last_edit_date": 1611616884,
    "question_id": 233,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/233/are-neural-networks-the-only-way-to-reach-true-artificial-intelligence",
    "title": "Are neural networks the only way to reach &quot;true&quot; artificial intelligence?",
    "body": "<p>Currently, most research done in artificial intelligence focuses on neural networks, which have been successfully used to solve many problems.  A good example would be <a href=\"https://www.nature.com/articles/nature16961\" rel=\"nofollow noreferrer\">DeepMind's AlphaGo</a>, which uses a convolutional neural network. There are many other examples, such as <a href=\"https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html\" rel=\"nofollow noreferrer\">Google Translate, which uses transformers</a>, or <a href=\"https://www.cs.toronto.edu/%7Evmnih/docs/dqn.pdf\" rel=\"nofollow noreferrer\">DQN</a>, which has been used to play Atari games.</p>\n<p>So, are any of the variants of neural networks the only way to reach &quot;true&quot; artificial intelligence (or AGI)?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "research",
      "deepmind",
      "neural-turing-machine"
    ],
    "owner": {
      "account_id": 443353,
      "reputation": 2148,
      "user_id": 144,
      "user_type": "registered",
      "accept_rate": 30,
      "profile_image": "https://i.sstatic.net/ZAzOl.jpg?s=256",
      "display_name": "rcpinto",
      "link": "https://ai.stackexchange.com/users/144/rcpinto"
    },
    "is_answered": true,
    "view_count": 1089,
    "accepted_answer_id": 2229,
    "answer_count": 2,
    "score": 12,
    "last_activity_date": 1623241527,
    "creation_date": 1470296613,
    "last_edit_date": 1623241527,
    "question_id": 1290,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1290/how-much-of-deep-minds-work-is-actually-reproducible",
    "title": "How much of Deep Mind&#39;s work is actually reproducible?",
    "body": "<p>DeepMind has published a lot of works on deep learning in the last years, most of them are state-of-the-art on their respective tasks. But how much of this work has actually been reproduced by the AI community? For instance, the Neural Turing Machine paper seems to be very hard to reproduce, according to other researchers.</p>\n"
  },
  {
    "tags": [
      "neural-networks"
    ],
    "owner": {
      "account_id": 443353,
      "reputation": 2148,
      "user_id": 144,
      "user_type": "registered",
      "accept_rate": 30,
      "profile_image": "https://i.sstatic.net/ZAzOl.jpg?s=256",
      "display_name": "rcpinto",
      "link": "https://ai.stackexchange.com/users/144/rcpinto"
    },
    "is_answered": true,
    "view_count": 514,
    "accepted_answer_id": 1299,
    "answer_count": 1,
    "score": 12,
    "last_activity_date": 1657499632,
    "creation_date": 1470299556,
    "question_id": 1295,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/1295/what-are-the-advantages-of-complex-valued-neural-networks",
    "title": "What are the advantages of complex-valued neural networks?",
    "body": "<p>During my research, I've stumbled upon \"complex-valued neural networks\", which are neural networks that work with complex-valued inputs (probably weights too). What are the advantages (or simply the applications) of this kind of neural network over real-valued neural networks?</p>\n"
  },
  {
    "tags": [
      "comparison",
      "definitions",
      "optimization",
      "meta-heuristics"
    ],
    "owner": {
      "account_id": 3435285,
      "reputation": 399,
      "user_id": 1760,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4480675ad0ede0043e7caba04750d974?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "bmwalide",
      "link": "https://ai.stackexchange.com/users/1760/bmwalide"
    },
    "is_answered": true,
    "view_count": 920,
    "protected_date": 1634119227,
    "accepted_answer_id": 1755,
    "answer_count": 1,
    "score": 12,
    "last_activity_date": 1634087845,
    "creation_date": 1472303719,
    "last_edit_date": 1634087845,
    "question_id": 1751,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1751/what-are-hyper-heuristics-and-how-are-they-different-from-meta-heuristics",
    "title": "What are hyper-heuristics, and how are they different from meta-heuristics?",
    "body": "<p>I wanted to know what the differences between <em>hyper-heuristics</em> and <em>meta-heuristics</em> are, and what their main applications are. Which problems are suited to be solved by hyper-heuristics?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "reference-request",
      "knowledge-representation",
      "conceptual-dependency"
    ],
    "owner": {
      "account_id": 43694,
      "reputation": 5457,
      "user_id": 2193,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b3e5e6b5ecd2707930a109a46c0cfafe?s=256&d=identicon&r=PG",
      "display_name": "Oliver Mason",
      "link": "https://ai.stackexchange.com/users/2193/oliver-mason"
    },
    "is_answered": true,
    "view_count": 764,
    "accepted_answer_id": 6572,
    "answer_count": 2,
    "score": 12,
    "last_activity_date": 1640370034,
    "creation_date": 1473083217,
    "last_edit_date": 1640370034,
    "question_id": 1859,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1859/is-anybody-still-using-conceptual-dependency-theory",
    "title": "Is anybody still using Conceptual Dependency Theory?",
    "body": "<p>Roger Schank did some interesting work on language processing with Conceptual Dependency (CD) in the 1970s. He then moved somewhat out of the field, being in Education these days. There were some useful applications in natural language generation (BABEL), story generation (TAILSPIN) and other areas, often involving planning and episodes rather than individual sentences.</p>\n<p>Has anybody else continued to use CD or variants thereof? I am not aware of any other projects that do, apart from Hovy's PAULINE, which uses CD as representation for the story to generate.</p>\n"
  },
  {
    "tags": [
      "search"
    ],
    "owner": {
      "account_id": 2685693,
      "reputation": 643,
      "user_id": 1270,
      "user_type": "registered",
      "accept_rate": 83,
      "profile_image": "https://i.sstatic.net/n8zHs.jpg?s=256",
      "display_name": "Zolt&#225;n Schmidt",
      "link": "https://ai.stackexchange.com/users/1270/zolt%c3%a1n-schmidt"
    },
    "is_answered": true,
    "view_count": 4559,
    "accepted_answer_id": 1883,
    "answer_count": 8,
    "score": 12,
    "last_activity_date": 1555408377,
    "creation_date": 1473257796,
    "last_edit_date": 1555408016,
    "question_id": 1877,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1877/why-is-search-important-in-ai",
    "title": "Why is search important in AI?",
    "body": "<p>Why is search important in AI? What kinds of search algorithms are used in AI? How do they improve the result of an AI?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "social",
      "autonomous-vehicles"
    ],
    "owner": {
      "account_id": 2762007,
      "reputation": 309,
      "user_id": 2963,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2faf91f2efa646f23b07220deb997687?s=256&d=identicon&r=PG",
      "display_name": "Jamgreen",
      "link": "https://ai.stackexchange.com/users/2963/jamgreen"
    },
    "is_answered": true,
    "view_count": 1200,
    "protected_date": 1561044150,
    "answer_count": 8,
    "score": 12,
    "last_activity_date": 1561044132,
    "creation_date": 1476258044,
    "last_edit_date": 1561044132,
    "question_id": 2127,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2127/what-are-the-advantages-of-having-self-driving-cars",
    "title": "What are the advantages of having self-driving cars?",
    "body": "<p>What are the advantages of having self-driving cars?</p>\n\n<p>We will be able to have more cars in the traffic at the same time, but won't it also make more people choose to use the cars, so both the traffic and the public health will actually become worse?</p>\n\n<p>Are we really interested in this?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "search",
      "education"
    ],
    "owner": {
      "account_id": 4148701,
      "reputation": 42397,
      "user_id": 2444,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e6fce047cba96a601a9dac66df4cd2e8?s=256&d=identicon&r=PG",
      "display_name": "nbro",
      "link": "https://ai.stackexchange.com/users/2444/nbro"
    },
    "is_answered": true,
    "view_count": 1121,
    "answer_count": 3,
    "score": 12,
    "last_activity_date": 1561480250,
    "creation_date": 1482248655,
    "last_edit_date": 1561480250,
    "question_id": 2514,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2514/why-teaching-only-search-algorithms-in-a-short-introductory-ai-course",
    "title": "Why teaching only search algorithms in a short introductory AI course?",
    "body": "<p>I understood that the concept of <em>search</em> is important in AI. There's a <a href=\"https://ai.stackexchange.com/questions/1877/why-is-searching-important-in-ais\">question</a> on this website regarding this topic, but one could also intuitively understand why. I've had an introductory course on AI, which lasted half of a semester, so of course there wasn't time enough to cover all topics of AI, but I was expecting to learn some AI theory (I've heard about \"agents\"), but what I actually learned was basically a few search algorithms, like:</p>\n\n<ul>\n<li>BFS</li>\n<li>Uniform-cost search</li>\n<li>DFS</li>\n<li>Iterative-deepening search</li>\n<li>Bidirectional search</li>\n</ul>\n\n<p>these search algorithms are usually categorized as \"blind\" (or \"uninformed\"), because they do not consider any information regarding the remaining path to the goal. </p>\n\n<p>Or algorithms like:</p>\n\n<ul>\n<li>Heuristic search</li>\n<li>Best-first search</li>\n<li>A</li>\n<li>A*</li>\n<li>IDA*</li>\n</ul>\n\n<p>which usually fall under the category of \"informed\" search algorithms, because they use some information (i.e. \"heuristics\" or \"estimates\") about the remaining path to the goal.</p>\n\n<p>Then we also learned \"advanced\" search algorithms (specifically applied to the TSP problem). These algorithms are either constructive (e.g., nearest neighbor), local search (e.g., 2-opt) algorithms or meta-heuristic ones (e.g., ant colony system or simulated annealing).</p>\n\n<p>We also studied briefly a min-max algorithm applied to games and an \"improved\" version of the min-max, i.e. the alpha-beta pruning.</p>\n\n<p>After this course, I had the feeling that AI is just about searching, either \"stupidly\" or \"more intelligently\".</p>\n\n<p>My questions are:</p>\n\n<ul>\n<li><p>Why would one professor only teach search algorithms in AI course? What are the advantages/disadvantages? The next question is very related to this.</p></li>\n<li><p>What's more than \"searching\" in AI that could be taught in an introductory course? This question may lead to subjective answers, but I'm actually asking in the context of a person trying to understand what AI really is and what topics does it really cover. Apparently and unfortunately, after reading around, it seems that this would still be subjective.</p></li>\n<li><p>Are there AI theories that could be taught in this kind of course?</p></li>\n</ul>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "deep-learning",
      "ai-design",
      "natural-language-processing"
    ],
    "owner": {
      "account_id": 9889909,
      "reputation": 161,
      "user_id": 4841,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/3add1d23673f346807cb8261588ee385?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "N. Chalifour",
      "link": "https://ai.stackexchange.com/users/4841/n-chalifour"
    },
    "is_answered": true,
    "view_count": 504,
    "answer_count": 1,
    "score": 12,
    "last_activity_date": 1629471922,
    "creation_date": 1486496691,
    "last_edit_date": 1616843663,
    "question_id": 2795,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2795/how-can-viv-generate-new-code-based-on-some-users-query",
    "title": "How can Viv generate new code based on some user&#39;s query?",
    "body": "<p>I have been looking into <a href=\"http://viv.ai/\" rel=\"nofollow noreferrer\">Viv</a>, an artificial intelligent agent in development. <a href=\"https://www.youtube.com/watch?v=Rblb3sptgpQ\" rel=\"nofollow noreferrer\">Here is a demonstration of Viv (by Dag Kittlaus)</a>.</p>\n<p>Based on what I understand, this AI can generate new code and execute it based on a query from the user.</p>\n<p>What I am curious to know is how this AI can learn to generate code based on some query. <em>What kind of machine learning algorithms are involved in this process?</em></p>\n<p>One thing I considered is breaking down a dataset of programs by step. For example, here is the code to take an average of 5 terms</p>\n<ol>\n<li>Add all 5 terms together</li>\n<li>Divide by 5</li>\n</ol>\n<p>Then I would train an algorithm to convert text to code. That is as far as I have figured out. However, I haven't tried anything because I'm not sure where to start.</p>\n<p><em>Does anybody have any ideas on how Viv is implemented?</em></p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "deep-neural-networks",
      "agi"
    ],
    "owner": {
      "account_id": 1533,
      "reputation": 1510,
      "user_id": 169,
      "user_type": "registered",
      "accept_rate": 0,
      "profile_image": "https://www.gravatar.com/avatar/2d1e9a607c47a89730352dd7b9dacaab?s=256&d=identicon&r=PG",
      "display_name": "Eric Platon",
      "link": "https://ai.stackexchange.com/users/169/eric-platon"
    },
    "is_answered": true,
    "view_count": 3108,
    "protected_date": 1573236706,
    "answer_count": 5,
    "score": 12,
    "last_activity_date": 1487619065,
    "creation_date": 1487147637,
    "question_id": 2820,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/2820/why-are-deep-neural-networks-and-deep-learning-insufficient-to-achieve-general-i",
    "title": "Why are deep neural networks and deep learning insufficient to achieve general intelligence?",
    "body": "<p>Everything related to Deep Learning (DL) and deep(er) networks seems \"successful\", at least progressing very fast, and cultivating the belief that AGI is at reach. This is popular imagination. DL is a tremendous tool to tackle so many problems, including the creation of AGIs. It is not enough, though. A tool is a necessary ingredient, but often insufficient.</p>\n\n<p>Leading figures in the domain are looking elsewhere to make progress. This <a href=\"https://hackernoon.com/feynman-machine-a-new-approach-for-cortical-and-machine-intelligence-5855c0e61a70#.dmgovix19\" rel=\"noreferrer\">report/claim</a> gathers links to statements by <a href=\"https://www.quora.com/Is-the-current-hype-about-Deep-Learning-justified?redirected_qid=6578691\" rel=\"noreferrer\">Yoshua Bengio</a>, <a href=\"https://www.quora.com/What-are-the-limits-of-deep-learning-2/answer/Yann-LeCun\" rel=\"noreferrer\">Yann LeCun</a> and <a href=\"https://www.youtube.com/watch?v=VIRCybGgHts\" rel=\"noreferrer\">Geoff Hinton</a>. The report also explains:</p>\n\n<blockquote>\n  <p>The main weaknesses of DL (as I see them) are: reliance on the simplest possible model neurons (“cartoonish” as LeCun calls them); use of ideas from 19th century Statistical Mechanics and Statistics, which are the basis of energy functions and log-likelihood methods; and the combination of these in techniques like backprop and stochastic gradient descent, leading to a very limited regime of application (offline, mostly batched, supervised learning), requiring highly-talented practitioners (aka “Stochastic Graduate Descent”), large amounts of expensive labelled training data and computational power. While great for huge companies who can lure or buy the talent and deploy unlimited resources to gather data and crunch it, DL is simply neither accessible nor useful to the majority of us.</p>\n</blockquote>\n\n<p>Although interesting and relevant, such kind of explanation does not really address the gist of the problem: What is lacking?</p>\n\n<p>The question seems broad, but it may be by lack of a simple answer. Is there a way to pin-point what DL is lacking for an AGI ?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "convolutional-neural-networks",
      "terminology",
      "definitions",
      "bottlenecks"
    ],
    "owner": {
      "account_id": 4482792,
      "reputation": 447,
      "user_id": 35,
      "user_type": "registered",
      "accept_rate": 20,
      "profile_image": "https://www.gravatar.com/avatar/5816662050451a4aa29755bc882b19a9?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Abhishek Bhatia",
      "link": "https://ai.stackexchange.com/users/35/abhishek-bhatia"
    },
    "is_answered": true,
    "view_count": 15362,
    "answer_count": 2,
    "score": 12,
    "last_activity_date": 1588110900,
    "creation_date": 1491183924,
    "last_edit_date": 1588110900,
    "question_id": 3089,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3089/what-are-bottleneck-features",
    "title": "What are bottleneck features?",
    "body": "<p>In the blog post <a href=\"https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\" rel=\"nofollow noreferrer\">Building powerful image classification models using very little data</a>, bottleneck features are mentioned. What are the bottleneck features? Do they change with the architecture that is used? Are they the final output of convolutional layers before the fully-connected layer? Why are they called so?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "reference-request",
      "training",
      "algorithm-request"
    ],
    "owner": {
      "account_id": 74321,
      "reputation": 334,
      "user_id": 9233,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/af186924b252e12ee40865d51ca5f806?s=256&d=identicon&r=PG",
      "display_name": "rkellerm",
      "link": "https://ai.stackexchange.com/users/9233/rkellerm"
    },
    "is_answered": true,
    "view_count": 1991,
    "accepted_answer_id": 3893,
    "answer_count": 1,
    "score": 12,
    "last_activity_date": 1640168592,
    "creation_date": 1503578543,
    "last_edit_date": 1640168592,
    "question_id": 3885,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3885/what-are-the-best-known-gradient-free-training-methods-for-deep-learning",
    "title": "What are the best known gradient-free training methods for deep learning?",
    "body": "<p>As I know, the current state of the art methods for training deep learning networks are variants of gradient descent or stochastic gradient descent.  </p>\n\n<p>What are the best known gradient-free training methods for deep learning (mostly in visual tasks context)?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "control-problem",
      "overfitting"
    ],
    "owner": {
      "account_id": 6563158,
      "reputation": 347,
      "user_id": 9319,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-pEkXqWxH5h0/AAAAAAAAAAI/AAAAAAAAABc/8g1duewTc-g/s256-rj/photo.jpg",
      "display_name": "ZakC",
      "link": "https://ai.stackexchange.com/users/9319/zakc"
    },
    "is_answered": true,
    "view_count": 5920,
    "answer_count": 4,
    "score": 12,
    "last_activity_date": 1631759594,
    "creation_date": 1506600672,
    "question_id": 4136,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/4136/is-overfitting-always-a-bad-thing",
    "title": "Is overfitting always a bad thing?",
    "body": "<p>DNN can be used to recognize pictures. Great. For that usage, it's better if they are somewhat flexible so as to recognize as cats even cats that are not on the pictures on which they trained (i.e. avoid overfitting). Agreed.</p>\n\n<p>But when <a href=\"http://ieeexplore.ieee.org/abstract/document/7778091/\" rel=\"noreferrer\">one uses</a> NN as a replacement for numerical tables in an Air Collision Avoidance System (ACAS), it is primarily to reduce the \"required storage space by a factor of 1000\". For this usage, what we want from the NN is to say \"take a slight left turn\" or \"turn right hard\" if another ship comes slightly close on the right or rapidly close on the left, respectively. </p>\n\n<p>For this usage, where the answer is much simpler than recognizing a cat, isn't overfitting a good thing? What would overfitting \"look like\" in this case and why would it be bad ?</p>\n\n<p>This question somewhat relates <a href=\"http://ai.stackexchange.com/questions/3473/is-there-such-a-thing-like-the-machine-learning-paradox\">to this one</a>, where a general idea seems to be \"Machine Learning is used for intractable things, you don't need ML for tractable things\". And while it is quite correct that ACAS can be implemented without NN, I wouldn't call NN \"useless\" for ACAS, because a factor 1000 reduction in required space will always come in handy.  </p>\n"
  },
  {
    "tags": [
      "research",
      "heuristics",
      "knapsack-problem"
    ],
    "owner": {
      "account_id": 1332304,
      "reputation": 221,
      "user_id": 11365,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0998797102819c53fd6f5876bf975c92?s=256&d=identicon&r=PG",
      "display_name": "Artem",
      "link": "https://ai.stackexchange.com/users/11365/artem"
    },
    "is_answered": true,
    "view_count": 229,
    "answer_count": 1,
    "score": 12,
    "last_activity_date": 1556622125,
    "creation_date": 1509287586,
    "last_edit_date": 1509472161,
    "question_id": 4389,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/4389/strategic-planning-and-multi-dimensional-knapsack-problem",
    "title": "Strategic planning and multi dimensional knapsack problem",
    "body": "<p>I'm trying to find a planning approach to solve a problem that attempts to model learning of new material. We assume that we only have one resource such as Wikipedia, which contains a list of articles represented as a <strong>vector</strong> of knowledge it contains and an <strong>effort</strong> to read that article.</p>\n\n<p><strong>Knowledge vector and effort</strong></p>\n\n<p>Before we start, we set a size for the vector, depending on the number of different types of knowledge. For example, we can define the items in the vector to be <code>(algebra, geometry, dark ages)</code>, and then 'measure' all the articles from this point of view. So, a math article will probably be <code>(5,7,0)</code>, since it will talk a lot about algebra and geometry but not about the dark ages. It will also have an <strong>effort</strong> to read it, which is simply an integer.</p>\n\n<p><strong>Problem</strong></p>\n\n<p>Given all the articles (represented as knowledge vectors with an effort), we want to find the optimal set of articles that help us to reach a <strong>knowledge goal</strong> (also represented as a vector). </p>\n\n<p>So, a knowledge goal can be <code>(4,4,0)</code>, and it's enough to read an article <code>(2,1,0)</code> and <code>(2,3,0)</code>, since, when added, it adds up to the knowledge goal. We want to do this with <strong>minimal effort</strong>.</p>\n\n<p><strong>Question</strong></p>\n\n<p>I've tried to some heuristics to find an approximation, but I was wondering if there is any state of the art strategic planning method that can be used instead?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "policy-gradients",
      "off-policy-methods"
    ],
    "owner": {
      "account_id": 8954213,
      "reputation": 713,
      "user_id": 11566,
      "user_type": "registered",
      "accept_rate": 40,
      "profile_image": "https://www.gravatar.com/avatar/08ea907d3b57005305612f88c0a85b03?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "echo",
      "link": "https://ai.stackexchange.com/users/11566/echo"
    },
    "is_answered": true,
    "view_count": 3558,
    "answer_count": 1,
    "score": 12,
    "last_activity_date": 1574128477,
    "creation_date": 1514054489,
    "last_edit_date": 1574128419,
    "question_id": 4831,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4831/do-off-policy-policy-gradient-methods-exist",
    "title": "Do off-policy policy gradient methods exist?",
    "body": "<p>Do off-policy policy gradient methods exist?</p>\n\n<p>I know that policy gradient methods themselves using the policy function for sampling rollouts. But can't we easily have a model for sampling from the environment? If so, I've never seen this done before.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "activation-functions",
      "artificial-neuron"
    ],
    "owner": {
      "account_id": 5599032,
      "reputation": 173,
      "user_id": 16901,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/591837687/picture?type=large",
      "display_name": "Leon",
      "link": "https://ai.stackexchange.com/users/16901/leon"
    },
    "is_answered": true,
    "view_count": 1091,
    "accepted_answer_id": 7558,
    "answer_count": 2,
    "score": 12,
    "last_activity_date": 1640044070,
    "creation_date": 1534253804,
    "last_edit_date": 1640044070,
    "question_id": 7556,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7556/what-does-it-mean-for-a-neuron-in-a-neural-network-to-be-activated",
    "title": "What does it mean for a neuron in a neural network to be activated?",
    "body": "<p>I just stumbled upon the concept of neuron coverage, which is the ratio of activated neurons and total neurons in a neural network. But what does it mean for a neuron to be \"activated\"? I know what activation functions are, but what does being activated mean e.g. in the case of a ReLU or a sigmoid function?</p>\n"
  },
  {
    "tags": [
      "comparison",
      "terminology",
      "definitions",
      "semi-supervised-learning",
      "labels"
    ],
    "owner": {
      "account_id": 11259554,
      "reputation": 121,
      "user_id": 20831,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/a37c89ab15b755b761794370b27b74c3?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "ellie",
      "link": "https://ai.stackexchange.com/users/20831/ellie"
    },
    "is_answered": true,
    "view_count": 20804,
    "protected_date": 1627983529,
    "answer_count": 1,
    "score": 12,
    "last_activity_date": 1627983493,
    "creation_date": 1545377323,
    "last_edit_date": 1627983493,
    "question_id": 9635,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/9635/what-is-the-definition-of-soft-label-and-hard-label",
    "title": "What is the definition of &quot;soft label&quot; and &quot;hard label&quot;?",
    "body": "<p>In <a href=\"https://en.wikipedia.org/wiki/Semi-supervised_learning\" rel=\"noreferrer\">semi-supervised learning</a>, there are <em>hard labels</em> and <em>soft labels</em>. Could someone tell me the meaning and definition of the two things?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "comparison",
      "terminology",
      "policy-gradients",
      "reinforce"
    ],
    "owner": {
      "account_id": 12870553,
      "reputation": 361,
      "user_id": 17312,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d9a78ba42a5548d804d23cf802fc284e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "yewang",
      "link": "https://ai.stackexchange.com/users/17312/yewang"
    },
    "is_answered": true,
    "view_count": 5328,
    "answer_count": 3,
    "score": 12,
    "last_activity_date": 1662727967,
    "creation_date": 1553691679,
    "last_edit_date": 1642021761,
    "question_id": 11480,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11480/is-reinforce-the-same-as-vanilla-policy-gradient",
    "title": "Is REINFORCE the same as &#39;vanilla policy gradient&#39;?",
    "body": "<p>I don't know what people mean by 'vanilla policy gradient', but what comes to mind is REINFORCE, which is the simplest policy gradient algorithm I can think of. Is this an accurate statement?</p>\n<p>By REINFORCE I mean this surrogate objective</p>\n<p><span class=\"math-container\">$$ \\frac{1}{m} \\sum_i \\sum_t log(\\pi(a_t|s_t)) R_i,$$</span>\nwhere I index over the <span class=\"math-container\">$m$</span> episodes and <span class=\"math-container\">$t$</span> over time steps, and <span class=\"math-container\">$R_i$</span> is the total reward of the episode. It's also common to replace <span class=\"math-container\">$R_i$</span> with something else, like a baselined version <span class=\"math-container\">$R_i - b$</span> or use the future return, potentially also with a baseline <span class=\"math-container\">$G_{it} - b$</span>.</p>\n<p>However, I think even with these modifications to the multiplicative term, people would still call this 'vanilla policy gradient'. Is that correct?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "deep-learning",
      "attention"
    ],
    "owner": {
      "account_id": 2041093,
      "reputation": 648,
      "user_id": 21158,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/31a0a41877445ff79f4c4b96e589bc6f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user3180",
      "link": "https://ai.stackexchange.com/users/21158/user3180"
    },
    "is_answered": true,
    "view_count": 8312,
    "answer_count": 3,
    "score": 12,
    "last_activity_date": 1647592810,
    "creation_date": 1555475913,
    "last_edit_date": 1555492661,
    "question_id": 11866,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11866/why-is-dot-product-attention-faster-than-additive-attention",
    "title": "Why is dot product attention faster than additive attention?",
    "body": "<p>In section 3.2.1 of <a href=\"https://arxiv.org/pdf/1706.03762.pdf\" rel=\"noreferrer\">Attention Is All You Need</a> the claim is made that:</p>\n\n<blockquote>\n  <p>Dot-product attention is identical to our algorithm, except for the scaling factor of <span class=\"math-container\">$\\frac{1}{\\sqrt{d_k}}$</span>. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.</p>\n</blockquote>\n\n<p>It does not make sense why dot product attention would be faster. Additive attention is nearly identical computation wise; the main difference is <span class=\"math-container\">$Q + K$</span> instead of <span class=\"math-container\">$Q K^T$</span> in dot product attention. <span class=\"math-container\">$Q K^T$</span> requires at least as many addition operations as <span class=\"math-container\">$Q + K$</span>, so how can it possibly be faster?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "attention",
      "transformer"
    ],
    "owner": {
      "account_id": 2041093,
      "reputation": 648,
      "user_id": 21158,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/31a0a41877445ff79f4c4b96e589bc6f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user3180",
      "link": "https://ai.stackexchange.com/users/21158/user3180"
    },
    "is_answered": true,
    "view_count": 2697,
    "answer_count": 2,
    "score": 12,
    "last_activity_date": 1716552510,
    "creation_date": 1556853323,
    "last_edit_date": 1572578045,
    "question_id": 12118,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12118/why-dont-people-use-nonlinear-activation-functions-after-projecting-the-query-k",
    "title": "Why don&#39;t people use nonlinear activation functions after projecting the query key value in attention?",
    "body": "<p>Why don't people use nonlinear activation functions after projecting the query key value in attention?</p>\n\n<p>It seems like doing this would lead to much-needed nonlinearity, otherwise, we're just doing linear transformations.</p>\n\n<p>This observation applies to the transformer, additive attention, etc.</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "agi",
      "knowledge-representation",
      "commonsense-knowledge"
    ],
    "owner": {
      "account_id": 14130331,
      "reputation": 133,
      "user_id": 26958,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/a82a5fbd2bc6831172b1332e50bfb61f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Titan",
      "link": "https://ai.stackexchange.com/users/26958/titan"
    },
    "is_answered": true,
    "view_count": 6616,
    "accepted_answer_id": 13262,
    "answer_count": 5,
    "score": 12,
    "last_activity_date": 1612737376,
    "creation_date": 1562594712,
    "last_edit_date": 1612737376,
    "question_id": 13261,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/13261/why-do-we-need-common-sense-in-ai",
    "title": "Why do we need common sense in AI?",
    "body": "<p>Let's consider this example:</p>\n\n<blockquote>\n  <p>It's John's birthday, let's buy him a kite.</p>\n</blockquote>\n\n<p>We humans most likely would say the kite is a birthday gift, if asked why it's being bought; and we refer to this reasoning as <em>common sense</em>.</p>\n\n<p>Why do we need this in artificially intelligent agents? I think it could cause a plethora of problems, since a lot of our human errors are caused by these vague assumptions.</p>\n\n<p>Imagine an AI ignoring doing certain things because it assumes it has already been done by someone else (or another AI), using its common sense.</p>\n\n<p>Wouldn't that bring human errors into AI systems?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "computer-vision",
      "object-recognition",
      "object-detection",
      "facial-recognition"
    ],
    "owner": {
      "account_id": 11736273,
      "reputation": 121,
      "user_id": 28145,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b4e3b0d7d6899d66f43dcaec4f620fb2?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "rossignol",
      "link": "https://ai.stackexchange.com/users/28145/rossignol"
    },
    "is_answered": false,
    "view_count": 436,
    "answer_count": 0,
    "score": 12,
    "last_activity_date": 1647450530,
    "creation_date": 1566511812,
    "last_edit_date": 1613737843,
    "question_id": 14098,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/14098/extending-facenet-s-triplet-loss-to-object-recognition",
    "title": "Extending FaceNet’s triplet loss to object recognition",
    "body": "<p>FaceNet uses a novel loss metric (triplet loss) to train a model to output embeddings (128-D from the paper), such that any two faces of the same identity will have a small Euclidean distance, and such that any two faces of different identities will have a Euclidean distance larger than a specified margin. However, it needs another mechanism (HOG or MTCNN) to detect and extract faces from images in the first place.</p>\n<p>Can this idea be extended to object recognition? That is, can an object detection framework (e.g. MaskR-CNN) be used to extract bounding boxes of an object, cropping the object feeding this to a network that was trained on triplet loss, and then compares the embeddings of objects to see if they’re the same object?</p>\n<p>Is there any research that has been done or any published public datasets for this?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "deep-learning",
      "applications"
    ],
    "owner": {
      "account_id": 7328064,
      "reputation": 1318,
      "user_id": 2844,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/hnwBo.jpg?s=256",
      "display_name": "Dan D",
      "link": "https://ai.stackexchange.com/users/2844/dan-d"
    },
    "is_answered": true,
    "view_count": 2704,
    "closed_date": 1605466414,
    "accepted_answer_id": 15604,
    "answer_count": 1,
    "score": 12,
    "locked_date": 1639852360,
    "last_activity_date": 1569377715,
    "creation_date": 1569227638,
    "last_edit_date": 1569344761,
    "question_id": 15594,
    "link": "https://ai.stackexchange.com/questions/15594/what-are-all-the-different-kinds-of-neural-networks-used-for",
    "closed_reason": "Needs more focus",
    "title": "What are all the different kinds of neural networks used for?",
    "body": "<p>I found the following neural network cheat sheet (<a href=\"https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463\" rel=\"noreferrer\">Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning &amp; Big Data</a>). </p>\n\n<p><a href=\"https://i.sstatic.net/0WL34.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/0WL34.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>What are all these different kinds of neural networks used for? For example, which neural networks can be used for regression or classification, which can be used for sequence generation, etc.? I just need a brief overview (1-2 lines) of their applications.</p>\n"
  },
  {
    "tags": [
      "comparison",
      "chess"
    ],
    "owner": {
      "account_id": 16169832,
      "reputation": 221,
      "user_id": 31158,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/5473a273d3506bed63083e72a42e0ac1?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "grandtout",
      "link": "https://ai.stackexchange.com/users/31158/grandtout"
    },
    "is_answered": true,
    "view_count": 5620,
    "answer_count": 1,
    "score": 12,
    "last_activity_date": 1611278746,
    "creation_date": 1573379450,
    "last_edit_date": 1573434477,
    "question_id": 16394,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/16394/are-humans-superior-to-machines-in-chess",
    "title": "Are humans superior to machines in chess?",
    "body": "<p>A friend of mine, who is an International Master at chess, told me that humans were superior to machines provided you didn't impose the time constraints that exist in competitive chess (40 moves in 2 hours) since very often games were lost, to another human or a machine, when a bad move is made under time pressure.</p>\n\n<p>So, with no time constraints and access to a library of games, the human mind remains superior to the machine is my friend's contention.  I'm an indifferent chess player and don't really know what to make of this.  I was wondering if any research had been made that could back up that claim or rebut it.</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "sutton-barto",
      "reward-design",
      "reward-functions",
      "reward-hypothesis"
    ],
    "owner": {
      "account_id": 3187353,
      "reputation": 221,
      "user_id": 31395,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2facbb1904643d96f4d54adcdd109b7f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Bananin",
      "link": "https://ai.stackexchange.com/users/31395/bananin"
    },
    "is_answered": true,
    "view_count": 2433,
    "answer_count": 4,
    "score": 12,
    "last_activity_date": 1610510791,
    "creation_date": 1574130652,
    "last_edit_date": 1608482646,
    "question_id": 16610,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/16610/counterexamples-to-the-reward-hypothesis",
    "title": "Counterexamples to the reward hypothesis",
    "body": "<p>On Sutton and Barto's RL book, the reward hypothesis is stated as </p>\n\n<blockquote>\n  <p>that all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward)</p>\n</blockquote>\n\n<p>Are there examples of tasks where the goals and purposes cannot be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal?</p>\n\n<p>All I can think of are tasks with subjective rewards, like \"writing good music\", but I am not convinced because maybe this is actually definable (perhaps by some super-intelligent alien) and we just aren't smart enough yet. Thus, I'm especially interested in counterexamples that <strong>logically</strong> or <strong>provably</strong> fail the hypothesis.</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "image-processing",
      "algorithm-request",
      "model-request"
    ],
    "owner": {
      "account_id": 371003,
      "reputation": 171,
      "user_id": 31460,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f5e1c66a2d065fdb85616ff651de92b4?s=256&d=identicon&r=PG",
      "display_name": "arthur.sw",
      "link": "https://ai.stackexchange.com/users/31460/arthur-sw"
    },
    "is_answered": true,
    "view_count": 3351,
    "answer_count": 2,
    "score": 12,
    "last_activity_date": 1677444641,
    "creation_date": 1574268310,
    "last_edit_date": 1640301767,
    "question_id": 16672,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/16672/is-there-any-existing-attempt-to-create-a-deep-learning-model-which-extracts-vec",
    "title": "Is there any existing attempt to create a deep learning model which extracts vector paths from bitmaps?",
    "body": "<p>I need an algorithm to <a href=\"https://en.wikipedia.org/wiki/Image_tracing\" rel=\"nofollow noreferrer\">trace simple bitmaps</a>, which only contain paths with a given stroke width.</p>\n<p><strong>Is there any existing attempt to create a deep learning model which extracts vector paths from bitmaps?</strong></p>\n<p>It is obviously very easy to generate bitmaps from vector paths, so creating data for a machine learning algorithm is simple.  The model could be trained by giving both the vector and bitmap representation. Once trained, it would be able to generate the vector paths from the given bitmap.</p>\n<p>This seems simple, but I could not find any work on this particular task. So, I suppose this problem is not fitted for current deep learning architectures, why?</p>\n<p>The goal is to trace this kind of image, which would be drawn by hand with a thick felt pen and scanned:</p>\n<p><img src=\"https://i.sstatic.net/xZX3N.png\" alt=\"Bitmap Image containing simple vector paths\" /></p>\n<p><strong>So, is there a deep learning architecture fitted for this problem?</strong></p>\n<p>I believe this question could help me understand what is possible to do with deep learning and what is not, and why. Tracing bitmaps is a perfect example of converting sparse data to a dense abstract representation; I have the intuition one can learn a lot from this problem.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "computational-learning-theory",
      "regularization",
      "vc-dimension",
      "capacity"
    ],
    "owner": {
      "account_id": 5531362,
      "reputation": 1369,
      "user_id": 16871,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/1bbac7394bd9f4920abbce95fcd001c6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Alexander Soare",
      "link": "https://ai.stackexchange.com/users/16871/alexander-soare"
    },
    "is_answered": true,
    "view_count": 549,
    "accepted_answer_id": 18430,
    "answer_count": 3,
    "score": 12,
    "last_activity_date": 1611330706,
    "creation_date": 1582574446,
    "last_edit_date": 1611330706,
    "question_id": 18220,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/18220/are-there-any-rules-of-thumb-for-having-some-idea-of-what-capacity-a-neural-netw",
    "title": "Are there any rules of thumb for having some idea of what capacity a neural network needs to have for a given problem?",
    "body": "<p>To give an example. Let's just consider the MNIST dataset of handwritten digits. Here are some things which might have an impact on the optimum model capacity:</p>\n<ul>\n<li>There are 10 output classes</li>\n<li>The inputs are 28x28 grayscale pixels (I think this indirectly affects the model capacity. eg: if the inputs were 5x5 pixels, there wouldn't be much room for varying the way an 8 looks)</li>\n</ul>\n<p><strong>So, is there any way of knowing what the model capacity ought to be?</strong> Even if it's not exact? Even if it's a qualitative understanding of the type &quot;if X goes up, then Y goes down&quot;?</p>\n<p>Just to accentuate what I mean when I say &quot;not exact&quot;: I can already tell that a 100 variable model won't solve MNIST, so at least I have a lower bound. I'm also pretty sure that a 1,000,000,000 variable model is way more than needed. Of course, knowing a smaller range than that would be much more useful!</p>\n"
  },
  {
    "tags": [
      "backpropagation",
      "gradient-descent",
      "feedforward-neural-networks",
      "stochastic-gradient-descent",
      "mini-batch-gradient-descent"
    ],
    "owner": {
      "account_id": 11408359,
      "reputation": 455,
      "user_id": 17769,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/76d432a8011b58d17e970fa71f76e822?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Ben",
      "link": "https://ai.stackexchange.com/users/17769/ben"
    },
    "is_answered": true,
    "view_count": 6605,
    "answer_count": 2,
    "score": 12,
    "last_activity_date": 1606694384,
    "creation_date": 1587244881,
    "last_edit_date": 1606694384,
    "question_id": 20377,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/20377/what-exactly-is-averaged-when-doing-batch-gradient-descent",
    "title": "What exactly is averaged when doing batch gradient descent?",
    "body": "<p>I have a question about how the averaging works when doing mini-batch gradient descent.</p>\n\n<p>I think I now understood the general gradient descent algorithm, but only for online learning. When doing mini-batch gradient descent, do I have to:</p>\n\n<ul>\n<li><p>forward propagate</p></li>\n<li><p>calculate error</p></li>\n<li><p>calculate all gradients</p></li>\n</ul>\n\n<p>...repeatedly over all samples in the batch, and then average all gradients and apply the weight change? </p>\n\n<p>I thought it would work that way, but recently I have read somewhere that you basically only average the error of each example in the batch, and then calculate the gradients at the end of each batch. That left me wondering though, because, the activations of which sample in the mini-batch am I supposed to use to calculate the gradients at the end of every batch?</p>\n\n<p>It would be nice if somebody could explain what exactly happens during mini-batch gradient descent, and what actually gets calculated and averaged.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "natural-language-processing",
      "attention"
    ],
    "owner": {
      "account_id": 15498653,
      "reputation": 1283,
      "user_id": 30725,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/N8b6X.jpg?s=256",
      "display_name": "Pluviophile",
      "link": "https://ai.stackexchange.com/users/30725/pluviophile"
    },
    "is_answered": true,
    "view_count": 6811,
    "accepted_answer_id": 21404,
    "answer_count": 1,
    "score": 12,
    "last_activity_date": 1590241798,
    "creation_date": 1590115386,
    "last_edit_date": 1590241798,
    "question_id": 21389,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/21389/what-is-the-intuition-behind-the-attention-mechanism",
    "title": "What is the intuition behind the attention mechanism?",
    "body": "<blockquote>\n  <p><a href=\"https://medium.com/saarthi-ai/transformers-attention-based-seq2seq-machine-translation-a28940aaa4fe\" rel=\"noreferrer\">Attention</a> idea is one of the most influential ideas in deep learning. The main idea behind attention technique is that it allows the decoder to \"look back” at the complete input and extracts significant information that is useful in decoding.</p>\n</blockquote>\n\n<p>I am really having trouble understanding the intuition behind the attention mechanism. I mean how the mechanism works and how to configure.</p>\n\n<p>In simple words (and maybe with an example), what is the intuition behind the attention mechanism?</p>\n\n<p>What are some applications, advantages &amp; disadvantages of attention mechanism?</p>\n"
  },
  {
    "tags": [
      "comparison",
      "q-learning",
      "dqn",
      "deep-rl",
      "double-dqn"
    ],
    "owner": {
      "account_id": 18831732,
      "reputation": 369,
      "user_id": 37831,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/126a54125f27944a78e58fc81dca74db?s=256&d=identicon&r=PG",
      "display_name": "Chukwudi",
      "link": "https://ai.stackexchange.com/users/37831/chukwudi"
    },
    "is_answered": true,
    "view_count": 6245,
    "accepted_answer_id": 22777,
    "answer_count": 1,
    "score": 12,
    "last_activity_date": 1604510124,
    "creation_date": 1596138030,
    "last_edit_date": 1604510124,
    "question_id": 22776,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/22776/what-exactly-is-the-advantage-of-double-dqn-over-dqn",
    "title": "What exactly is the advantage of double DQN over DQN?",
    "body": "<p>I started looking into the <em>double DQN</em> (DDQN). Apparently, the difference between DDQN and DQN is that in DDQN we use the main value network for action selection and the target network for outputting the Q values.</p>\n<p>However, I don't understand why would this be beneficial, compared to the standard DQN. So, in simple terms, what exactly is the advantage of DDQN over DQN?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "transformer",
      "attention"
    ],
    "owner": {
      "account_id": 7112658,
      "reputation": 173,
      "user_id": 41379,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/WKBE5.jpg?s=256",
      "display_name": "Uchiha Madara",
      "link": "https://ai.stackexchange.com/users/41379/uchiha-madara"
    },
    "is_answered": true,
    "view_count": 11679,
    "accepted_answer_id": 23923,
    "answer_count": 3,
    "score": 12,
    "last_activity_date": 1614434070,
    "creation_date": 1601756281,
    "last_edit_date": 1601978771,
    "question_id": 23889,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/23889/what-is-the-purpose-of-decoder-mask-triangular-mask-in-transformer",
    "title": "What is the purpose of Decoder mask (triangular mask) in Transformer?",
    "body": "<p>I'm trying to implement transformer model using <a href=\"https://www.tensorflow.org/tutorials/text/transformer#create_the_transformer\" rel=\"noreferrer\">this tutorial</a>.  In the decoder block of the Transformer model, a mask is passed to &quot;<strong>pad and mask future tokens in the input received by the decoder</strong>&quot;. This mask is added to attention weights.</p>\n<pre><code>import tensorflow as tf\n\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask\n</code></pre>\n<p>Now my question is, how is doing this step (adding mask to the attention weights) equivalent to revealing the words to model one by one? I simply can't grasp the intuition of it's role. Most tutorials won't even mention this step like it's very obvious. Please help me understand. Thanks.</p>\n"
  },
  {
    "tags": [
      "computer-vision",
      "comparison",
      "transformer",
      "attention"
    ],
    "owner": {
      "account_id": 15725880,
      "reputation": 123,
      "user_id": 48736,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/67fc8c310c4333915ff427f56f0b600c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "novice",
      "link": "https://ai.stackexchange.com/users/48736/novice"
    },
    "is_answered": true,
    "view_count": 6480,
    "accepted_answer_id": 28841,
    "answer_count": 1,
    "score": 12,
    "last_activity_date": 1627299709,
    "creation_date": 1627185708,
    "last_edit_date": 1627213751,
    "question_id": 28818,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/28818/in-computer-vision-what-is-the-difference-between-a-transformer-and-attention",
    "title": "In Computer Vision, what is the difference between a transformer and attention?",
    "body": "<p>Having been studying computer vision for a while, I still cannot understand what the difference between a transformer and attention is?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "autoencoders",
      "generative-model"
    ],
    "owner": {
      "account_id": 15151516,
      "reputation": 195,
      "user_id": 40011,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-ECJGHcoVkqE/AAAAAAAAAAI/AAAAAAAAAGM/APbS2WXGW2M/s256-rj/photo.jpg",
      "display_name": "Nervous Hero",
      "link": "https://ai.stackexchange.com/users/40011/nervous-hero"
    },
    "is_answered": true,
    "view_count": 3233,
    "accepted_answer_id": 36120,
    "answer_count": 2,
    "score": 12,
    "last_activity_date": 1657207760,
    "creation_date": 1656504426,
    "question_id": 36118,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/36118/is-plain-autoencoder-a-generative-model",
    "title": "Is plain autoencoder a generative model?",
    "body": "<p>I am wondering how a plain auto encoder is a generative model though its version might be but how can a plain auto encoder can be generative. I know that Vaes which is a version of the autoencoder is generative as it generates distribution for latent variables and whole data explicitly. But I am not able to think how an autoencoder generates probability distribution and becomes a generative model.</p>\n<p>Also from this youtube video: <a href=\"https://www.youtube.com/watch?v=c27SHdQr4lw&amp;t=380s&amp;ab_channel=PaulHand\" rel=\"noreferrer\">here</a> It says plain auto encoder is not a generative model. See last line from picture.</p>\n<p><a href=\"https://i.sstatic.net/28Eq4.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/28Eq4.png\" alt=\"enter image description here\" /></a></p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "backpropagation",
      "gradient-descent",
      "activation-functions"
    ],
    "owner": {
      "account_id": 14485958,
      "reputation": 123,
      "user_id": 72223,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-C9y1ooMa9DM/AAAAAAAAAAI/AAAAAAAAADc/qbYGia2Qs00/s256-rj/photo.jpg",
      "display_name": "John Brown",
      "link": "https://ai.stackexchange.com/users/72223/john-brown"
    },
    "is_answered": true,
    "view_count": 10148,
    "accepted_answer_id": 40584,
    "answer_count": 1,
    "score": 12,
    "last_activity_date": 1685009619,
    "creation_date": 1684964646,
    "question_id": 40576,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/40576/why-use-relu-over-leaky-relu",
    "title": "Why use ReLU over Leaky ReLU?",
    "body": "<p>From my understanding a leaky ReLU attempts to address issues of vanishing gradients and nonzero-centeredness by keeping neurons that fire with a negative value alive.</p>\n<p>With just this info to go off of, it would seem that the leaky ReLU is just an overall improvement to the standard ReLU, yet ReLU still seems to be the gold standard of activation functions. Why is that?</p>\n<p>Does the additional sparsity outweigh the value gained from negative local gradients? If so, is tanh just too sparse?</p>\n"
  },
  {
    "tags": [
      "research",
      "history",
      "ai-field"
    ],
    "owner": {
      "account_id": 40019,
      "reputation": 2533,
      "user_id": 55,
      "user_type": "registered",
      "accept_rate": 43,
      "profile_image": "https://i.sstatic.net/FkIiK.png?s=256",
      "display_name": "WilliamKF",
      "link": "https://ai.stackexchange.com/users/55/williamkf"
    },
    "is_answered": true,
    "view_count": 4165,
    "protected_date": 1612111625,
    "accepted_answer_id": 71,
    "answer_count": 1,
    "score": 11,
    "last_activity_date": 1612111575,
    "creation_date": 1470154466,
    "last_edit_date": 1612111268,
    "question_id": 46,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/46/when-did-artificial-intelligence-research-first-start",
    "title": "When did Artificial Intelligence research first start?",
    "body": "<p>When did research into Artificial Intelligence first begin?  Was it called Artificial Intelligence then or was there another name?</p>\n"
  },
  {
    "tags": [
      "deep-neural-networks",
      "hidden-layers"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 337,
    "accepted_answer_id": 208,
    "answer_count": 1,
    "score": 11,
    "last_activity_date": 1523472591,
    "creation_date": 1470155364,
    "last_edit_date": 1523472591,
    "question_id": 63,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/63/what-kind-of-problems-require-more-than-2-hidden-layers",
    "title": "What kind of problems require more than 2 hidden layers?",
    "body": "<p>I've read that the most of the problems can be solved with 1-2 hidden layers.</p>\n\n<p>How do you know you need more than 2? For what kind of problems you would need them (give me an example)?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "terminology",
      "deep-neural-networks",
      "definitions"
    ],
    "owner": {
      "account_id": 8057826,
      "reputation": 429,
      "user_id": 5,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Kk1s7.png?s=256",
      "display_name": "skistaddy",
      "link": "https://ai.stackexchange.com/users/5/skistaddy"
    },
    "is_answered": true,
    "view_count": 407,
    "protected_date": 1587471755,
    "closed_date": 1598180593,
    "accepted_answer_id": 98,
    "answer_count": 3,
    "score": 11,
    "last_activity_date": 1587468325,
    "creation_date": 1470157978,
    "last_edit_date": 1561292317,
    "question_id": 96,
    "link": "https://ai.stackexchange.com/questions/96/what-is-a-deep-neural-network",
    "closed_reason": "Duplicate",
    "title": "What is a deep neural network?",
    "body": "<p>What is the definition of a deep neural network? Why are they so popular or important?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "agi"
    ],
    "owner": {
      "account_id": 1533,
      "reputation": 1510,
      "user_id": 169,
      "user_type": "registered",
      "accept_rate": 0,
      "profile_image": "https://www.gravatar.com/avatar/2d1e9a607c47a89730352dd7b9dacaab?s=256&d=identicon&r=PG",
      "display_name": "Eric Platon",
      "link": "https://ai.stackexchange.com/users/169/eric-platon"
    },
    "is_answered": true,
    "view_count": 693,
    "answer_count": 3,
    "score": 11,
    "last_activity_date": 1574465437,
    "creation_date": 1470214613,
    "last_edit_date": 1574465437,
    "question_id": 197,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/197/is-there-a-strong-argument-that-survival-instinct-is-a-prerequisite-for-creating",
    "title": "Is there a strong argument that survival instinct is a prerequisite for creating an AGI?",
    "body": "<p>This question stems from quite a few \"informal\" sources. Movies like <em>2001, A Space Odyssey</em> and <em>Ex Machina</em>; books like <em>Destination Void</em> (Frank Herbert), and others suggest that general intelligence <em>wants</em> to survive, and even learn the importance for it.</p>\n\n<p>There may be several arguments for survival. What would be the most prominent?</p>\n"
  },
  {
    "tags": [
      "hardware",
      "neuromorphic-engineering",
      "neuroscience",
      "brain"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 6794,
    "accepted_answer_id": 1316,
    "answer_count": 3,
    "score": 11,
    "last_activity_date": 1611626125,
    "creation_date": 1470320290,
    "last_edit_date": 1611626026,
    "question_id": 1314,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1314/how-powerful-a-computer-is-required-to-simulate-the-human-brain",
    "title": "How powerful a computer is required to simulate the human brain?",
    "body": "<p>How much processing power is needed to emulate the human brain? More specifically, the neural simulation, such as communication between the neurons and processing certain data in real-time.</p>\n<p>I understand that this may be a bit of speculation and it's not possible to be accurate, but I'm sure there is some data available or research studies that attempted to estimate it based on our current understanding of the human brain.</p>\n"
  },
  {
    "tags": [
      "agi",
      "legal"
    ],
    "owner": {
      "account_id": 18562,
      "reputation": 513,
      "user_id": 1670,
      "user_type": "registered",
      "accept_rate": 20,
      "profile_image": "https://www.gravatar.com/avatar/5d85921e112af0e998368a159ae8a112?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "liori",
      "link": "https://ai.stackexchange.com/users/1670/liori"
    },
    "is_answered": true,
    "view_count": 237,
    "answer_count": 1,
    "score": 11,
    "last_activity_date": 1590274111,
    "creation_date": 1471989825,
    "last_edit_date": 1590274053,
    "question_id": 1731,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1731/what-regulations-are-already-in-place-regarding-artificial-general-intelligences",
    "title": "What regulations are already in place regarding artificial general intelligences?",
    "body": "<p>What regulations are already in place regarding artificial general intelligences? What reports or recommendations prepared by official government authorities were already published?</p>\n\n<p>So far, I know of <a href=\"http://www.ft.com/cms/s/2/5ae9b434-8f8e-11db-9ba3-0000779e2340.html\" rel=\"nofollow noreferrer\">Sir David King's report done for UK government</a>.</p>\n"
  },
  {
    "tags": [
      "comparison",
      "agi",
      "history",
      "narrow-ai"
    ],
    "owner": {
      "account_id": 18562,
      "reputation": 513,
      "user_id": 1670,
      "user_type": "registered",
      "accept_rate": 20,
      "profile_image": "https://www.gravatar.com/avatar/5d85921e112af0e998368a159ae8a112?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "liori",
      "link": "https://ai.stackexchange.com/users/1670/liori"
    },
    "is_answered": true,
    "view_count": 539,
    "answer_count": 3,
    "score": 11,
    "last_activity_date": 1570242016,
    "creation_date": 1472502330,
    "last_edit_date": 1570242016,
    "question_id": 1774,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1774/who-was-the-first-person-to-recognize-the-distinction-between-human-like-general",
    "title": "Who was the first person to recognize the distinction between human-like general intelligence and domain-specific intelligence?",
    "body": "<p>In the 1950s, there were widely-held beliefs that \"Artificial Intelligence\" will quickly become both self-conscious and smart-enough to win chess with humans. Various people suggested time frames of e.g. 10 years (see Olazaran's \"Official History of the Perceptron Controversy\", or let say 2001: Space Odyssey).</p>\n\n<p>When did it become clear that devising programs that master games like chess resulted in software designs that only applied to games like the ones for which they were programmed? Who was the first person to recognize the distinction between human-like general intelligence and domain-specific intelligence?</p>\n"
  },
  {
    "tags": [
      "gaming",
      "monte-carlo-tree-search"
    ],
    "owner": {
      "account_id": 2058984,
      "reputation": 211,
      "user_id": 1949,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2a023d0a408ee39d06205d00ebd7df11?s=256&d=identicon&r=PG",
      "display_name": "Nocta",
      "link": "https://ai.stackexchange.com/users/1949/nocta"
    },
    "is_answered": true,
    "view_count": 866,
    "answer_count": 1,
    "score": 11,
    "last_activity_date": 1496594440,
    "creation_date": 1472652798,
    "question_id": 1815,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/1815/monte-carlo-tree-search-what-kind-of-moves-can-easily-be-found-and-what-kinds-m",
    "title": "Monte Carlo Tree Search: What kind of moves can easily be found and what kinds make trouble?",
    "body": "<p>I want to start with a scenario that got me thinking about how well MCTS can perform:\nLet's assume there is a move that is not yet added to the search tree. It is some layers/moves too deep. But if we play this move the game is basically won. However let's also assume that <em>all</em> moves that could be taken instead at the given game state are very very bad. For the sake of argument let's say there are 1000 possible moves and only one of them is good (but very good) and the rest is very bad. Wouldn't MCTS fail to recognize this and <em>not</em> grow the search tree towards this move and also rate this subtree very badly? \nI know that MCTS eventually converges to minimax (and eventually it will build the whole tree if there is enough memory). Then it should know that the move is good even though there are many bad possiblities. But I guess in practice this is not something that one can rely on.\nMaybe someone can tell me if this is a correct evaluation on my part.</p>\n\n<p>Apart from this special scenario I'd also like to know if there are other such scenarios where MCTS will perform badly (or extraordinary well). </p>\n"
  },
  {
    "tags": [
      "philosophy",
      "agi",
      "superintelligence"
    ],
    "owner": {
      "account_id": 4382841,
      "reputation": 191,
      "user_id": 1982,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4480675ad0ede0043e7caba04750d974?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user3573987",
      "link": "https://ai.stackexchange.com/users/1982/user3573987"
    },
    "is_answered": true,
    "view_count": 1147,
    "accepted_answer_id": 1829,
    "answer_count": 4,
    "score": 11,
    "last_activity_date": 1636036919,
    "creation_date": 1472774309,
    "last_edit_date": 1636036919,
    "question_id": 1824,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1824/why-would-an-ai-need-to-wipe-out-the-human-race",
    "title": "Why would an AI need to &#39;wipe out the human race&#39;?",
    "body": "<p>I'm reading such nonsense about how an AI would turn the world into a supercomputer to solve a problem that it thought it needed to solve. That wouldn't be AI. That's procedural programming stuck in some loop nonsense. An AI would need to evolve and re-organise its neurons. It wouldn't be stuck to hardcode if it becomes intelligent by re-writing its code.</p>\n"
  },
  {
    "tags": [
      "intelligence-testing",
      "intelligence-quotient"
    ],
    "owner": {
      "account_id": 9148500,
      "reputation": 541,
      "user_id": 2310,
      "user_type": "registered",
      "accept_rate": 0,
      "profile_image": "https://www.gravatar.com/avatar/4480675ad0ede0043e7caba04750d974?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "D. Wade",
      "link": "https://ai.stackexchange.com/users/2310/d-wade"
    },
    "is_answered": true,
    "view_count": 624,
    "answer_count": 2,
    "score": 11,
    "last_activity_date": 1639518551,
    "creation_date": 1473863066,
    "last_edit_date": 1639518551,
    "question_id": 1969,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1969/if-iq-is-used-as-a-measure-of-intelligence-in-humans-could-it-also-be-used-as-a",
    "title": "If IQ is used as a measure of intelligence in humans, could it also be used as a measure of intelligence in machines?",
    "body": "<p>If IQ were used as a measure of the intelligence of machines, as in humans, at this point in time, what would be the IQ of our most intelligent AI systems?</p>\n<p>If not IQ, then how best to compare our intelligence to a machine, or one machine to another?</p>\n<p>This question is not asking if we can measure the IQ of a machine, but, if IQ is the most preferred, or general, method of measuring intelligence, then how does artificial intelligence compare to our most accepted method of measuring intelligence in humans.</p>\n<p>Many people may not understand the relevance of a Turing Test as to how intelligent their new car is, or other types of intelligent machines.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "convolutional-neural-networks",
      "computer-vision",
      "ensemble-learning"
    ],
    "owner": {
      "account_id": 6138127,
      "reputation": 357,
      "user_id": 1791,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/dWJCM.png?s=256",
      "display_name": "Erba Aitbayev",
      "link": "https://ai.stackexchange.com/users/1791/erba-aitbayev"
    },
    "is_answered": true,
    "view_count": 1947,
    "accepted_answer_id": 1981,
    "answer_count": 2,
    "score": 11,
    "last_activity_date": 1621506061,
    "creation_date": 1473953648,
    "last_edit_date": 1621506061,
    "question_id": 1978,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1978/do-deep-learning-algorithms-represent-ensemble-based-methods",
    "title": "Do deep learning algorithms represent ensemble-based methods?",
    "body": "<p>According to the Wikipedia article on <a href=\"https://en.wikipedia.org/wiki/Deep_learning\" rel=\"nofollow noreferrer\">deep learning</a>:</p>\n<blockquote>\n<p>Deep learning is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using a deep graph with multiple processing layers, composed of multiple linear and non-linear transformations.</p>\n</blockquote>\n<blockquote>\n<p>Various deep learning architectures such as deep neural networks, convolutional deep neural networks, deep belief networks and recurrent neural networks have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks.</p>\n</blockquote>\n<p>Can <em>deep neural networks</em> or <em>convolutional deep neural networks</em> be viewed as <em>ensemble-based</em> method of machine learning, or are they different approaches?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "applications",
      "bayes-theorem"
    ],
    "owner": {
      "account_id": 4061452,
      "reputation": 111,
      "user_id": 5088,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-_wEnnr6xPiE/AAAAAAAAAAI/AAAAAAAADwg/UiNAvna5lhQ/s256-rj/photo.jpg",
      "display_name": "Murat Kaan Meral",
      "link": "https://ai.stackexchange.com/users/5088/murat-kaan-meral"
    },
    "is_answered": true,
    "view_count": 10043,
    "answer_count": 3,
    "score": 11,
    "last_activity_date": 1612869171,
    "creation_date": 1485628028,
    "last_edit_date": 1612828679,
    "question_id": 2738,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2738/how-is-bayes-theorem-used-in-artificial-intelligence-and-machine-learning",
    "title": "How is Bayes&#39; Theorem used in artificial intelligence and machine learning?",
    "body": "<p>How is Bayes' Theorem used in artificial intelligence and machine learning?</p>\n<p>As a high school student, I will be writing an essay about it, and I want to be able to explain Bayes' Theorem, its general use, and how it is used in AI or ML.</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "superintelligence",
      "cognitive-science",
      "singularity"
    ],
    "owner": {
      "account_id": 9991881,
      "reputation": 261,
      "user_id": 5708,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/907b337cfd1d8d8e71203c57263b9819?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Aaron",
      "link": "https://ai.stackexchange.com/users/5708/aaron"
    },
    "is_answered": true,
    "view_count": 1054,
    "accepted_answer_id": 2878,
    "answer_count": 3,
    "score": 11,
    "last_activity_date": 1607135899,
    "creation_date": 1488071079,
    "last_edit_date": 1574307521,
    "question_id": 2876,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2876/what-is-wrong-with-the-idea-that-the-ai-will-be-capable-of-omniscience",
    "title": "What is wrong with the idea that the AI will be capable of omniscience?",
    "body": "<p>In the context of artificial intelligence, the singularity refers to the advent of an <a href=\"https://en.wikipedia.org/wiki/Technological_singularity#Intelligence_explosion\" rel=\"nofollow noreferrer\"><em>artificial general intelligence capable of recursive self-improvement, leading to the rapid emergence of artificial superintelligence (ASI), the limits of which are unknown, shortly after technological singularity is achieved</em></a>. Therefore, these superintelligences would be able to solve problems that we possibly are unable to solve.</p>\n\n<p>According to a poll reported in <a href=\"https://nickbostrom.com/papers/survey.pdf\" rel=\"nofollow noreferrer\">Future progress in artificial intelligence: A survey of expert opinion</a> (2014)</p>\n\n<blockquote>\n  <p>The median estimate of respondents was for a one in two chance that highlevel machine intelligence will be developed around 2040-2050</p>\n</blockquote>\n\n<p>which isn't very far away.</p>\n\n<p>What is wrong with the idea that the AI will be capable of omniscience, given that it could benefit us by solving many problems?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "algorithm",
      "unsupervised-learning",
      "pattern-recognition"
    ],
    "owner": {
      "account_id": 10289417,
      "reputation": 119,
      "user_id": 5867,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/46b8332de3ef9a24f667a5cda93760d4?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "aAAAAAAa",
      "link": "https://ai.stackexchange.com/users/5867/aaaaaaaa"
    },
    "is_answered": true,
    "view_count": 16570,
    "answer_count": 4,
    "score": 11,
    "last_activity_date": 1597777480,
    "creation_date": 1488786146,
    "last_edit_date": 1584814681,
    "question_id": 2926,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2926/which-machine-learning-algorithm-can-be-used-to-identify-patterns-in-a-dataset-o",
    "title": "Which machine learning algorithm can be used to identify patterns in a dataset of the cache performance of a CPU?",
    "body": "<p>I need a machine learning algorithm to identify patterns in a dataset (saved in a CSV file) that contains details of the cache performance of a CPU. More specifically, the dataset contains columns like <code>Readhits</code>, <code>Readmiss</code> or <code>Writehits</code>.</p>\n\n<p>The patterns the algorithm identifies should be helpful in the following ways.</p>\n\n<ol>\n<li><p>help the user to increase the performance of the workload next time,</p></li>\n<li><p>help to identify any problems based on the features, or</p></li>\n<li><p>help the user to predict future data values or future events that may occur based on the patterns.</p></li>\n</ol>\n\n<p>Which ML algorithms can I use?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "emotional-intelligence",
      "affective-computing"
    ],
    "owner": {
      "account_id": 3497171,
      "reputation": 233,
      "user_id": 6429,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/100000784755344/picture?type=large",
      "display_name": "Pablo Arnau Gonz&#225;lez",
      "link": "https://ai.stackexchange.com/users/6429/pablo-arnau-gonz%c3%a1lez"
    },
    "is_answered": true,
    "view_count": 830,
    "accepted_answer_id": 3654,
    "answer_count": 2,
    "score": 11,
    "last_activity_date": 1583637321,
    "creation_date": 1492008370,
    "last_edit_date": 1583637321,
    "question_id": 3138,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3138/will-computers-be-able-to-understand-user-emotions",
    "title": "Will computers be able to understand user emotions?",
    "body": "<p>I am researching affective computing. Particularly, I'm studying the part of emotion recognition, i.e. the task of recognising the emotions that are being felt by the user/subject. For example, <a href=\"https://www.affectiva.com/emotion-ai-overview/\" rel=\"nofollow noreferrer\">affectiva</a> can be used to this end. I have concerns not in the validity of these models, but in what we are going to do with them.</p>\n\n<p>What about responding to emotions? Will computers be able to really understand user emotions?</p>\n"
  },
  {
    "tags": [
      "genetic-algorithms",
      "crossover-operators",
      "mutation-operators",
      "genetic-operators"
    ],
    "owner": {
      "account_id": 1161070,
      "reputation": 213,
      "user_id": 242,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4230c5d1c2c2446b9a19e0b2bcc32909?s=256&d=identicon&r=PG",
      "display_name": "danidemi",
      "link": "https://ai.stackexchange.com/users/242/danidemi"
    },
    "is_answered": true,
    "view_count": 7554,
    "accepted_answer_id": 3429,
    "answer_count": 2,
    "score": 11,
    "last_activity_date": 1581904421,
    "creation_date": 1496508285,
    "last_edit_date": 1581904421,
    "question_id": 3428,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3428/how-do-mutation-and-crossover-work-with-real-valued-chromosomes",
    "title": "How do mutation and crossover work with real-valued chromosomes?",
    "body": "<p>How exactly are \"mutation\" and \"cross-over\" applied in the context of a genetic algorithm based on real numbers (as opposed to just bits)? I think I understood how those two phases are applied in a \"canonical\" context where chromosomes are strings of bits of a fixed length, but I'm not able to find examples for other situations. What would those phases look like on the domain of real numbers?</p>\n"
  },
  {
    "tags": [
      "research",
      "game-theory",
      "chess",
      "turing-test",
      "ai-field"
    ],
    "owner": {
      "account_id": 10746453,
      "reputation": 155,
      "user_id": 6798,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/nLsMG.jpg?s=256",
      "display_name": "Suraj Shah",
      "link": "https://ai.stackexchange.com/users/6798/suraj-shah"
    },
    "is_answered": true,
    "view_count": 2347,
    "accepted_answer_id": 3591,
    "answer_count": 3,
    "score": 11,
    "last_activity_date": 1612034963,
    "creation_date": 1499234196,
    "last_edit_date": 1612034963,
    "question_id": 3590,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3590/why-spend-so-much-time-and-money-to-build-ais-to-play-games",
    "title": "Why spend so much time and money to build AIs to play games?",
    "body": "<p>I was reading about <a href=\"https://en.wikipedia.org/wiki/John_McCarthy_%28computer_scientist%29\" rel=\"nofollow noreferrer\">John McCarthy</a> and his orthodox vision of Artificial Intelligence. To me, it seems like he was not very much in favour of resources (like time and money) being used to make AIs play games like Chess. Instead, <a href=\"https://www.wired.com/2011/10/john-mccarthy-father-of-ai-and-lisp-dies-at-84/\" rel=\"nofollow noreferrer\">he wanted more to focus on passing the Turing test</a> and AIs imitating human behavior.</p>\n<p>I have also read many articles about major companies, like IBM, Google, etc., spending millions of dollars in making AIs play games, like Chess, Go, etc.</p>\n<p><strong>To what extent is this justified?</strong></p>\n"
  },
  {
    "tags": [
      "neural-networks"
    ],
    "owner": {
      "account_id": 8862663,
      "reputation": 275,
      "user_id": 8872,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/aYHiP.png?s=256",
      "display_name": "Aric",
      "link": "https://ai.stackexchange.com/users/8872/aric"
    },
    "is_answered": true,
    "view_count": 197,
    "accepted_answer_id": 3779,
    "answer_count": 3,
    "score": 11,
    "last_activity_date": 1502271636,
    "creation_date": 1502106446,
    "last_edit_date": 1502193973,
    "question_id": 3772,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/3772/how-can-i-make-my-network-treat-rotations-of-the-input-equally",
    "title": "How can I make my network treat rotations of the input equally?",
    "body": "<p>I'm attempting to program my own system to run a neural network. To reduce the number of nodes needed, it was suggested to make it treat rotations of the input equally.</p>\n\n<p>My network aims to learn and predict Conway's Game of Life by looking at every square and its surrounding squares in a grid, and giving the output for that square. Its input is a string of 9 bits:</p>\n\n<p><a href=\"https://i.sstatic.net/p9UwK.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/p9UwK.png\" alt=\"Glider\"></a></p>\n\n<p>The above is represented as 010 001 111.</p>\n\n<p>There are three other rotations of this shape however, and all of them produce the same output:</p>\n\n<p><a href=\"https://i.sstatic.net/eyDkl.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/eyDkl.png\" alt=\"Glider rotations\"></a></p>\n\n<p>My network topology is 9 input nodes and 1 output node for the next state of the centre square in the input. How can I construct the hidden layer(s) so that they take each of these rotations as the same, cutting the number of possible inputs down to a quarter of the original?</p>\n\n<p><strong>Edit:</strong></p>\n\n<p>There is also a flip of each rotation which produces an identical result. Incorporating these will cut my inputs by 1/8th. With the glider, my aim is for all of these inputs to be treated exactly the same. Will this have to be done with pre-processing, or can I incorporate it into the network?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "watson",
      "open-source"
    ],
    "owner": {
      "account_id": 3467525,
      "reputation": 181,
      "user_id": 10046,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/08ea907d3b57005305612f88c0a85b03?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "arash moradi",
      "link": "https://ai.stackexchange.com/users/10046/arash-moradi"
    },
    "is_answered": true,
    "view_count": 5063,
    "closed_date": 1640203941,
    "accepted_answer_id": 4270,
    "answer_count": 4,
    "score": 11,
    "locked_date": 1640863200,
    "last_activity_date": 1614246498,
    "creation_date": 1507370091,
    "last_edit_date": 1561847195,
    "question_id": 4219,
    "link": "https://ai.stackexchange.com/questions/4219/is-there-any-open-source-counterpart-to-the-ibm-watson",
    "closed_reason": "Not suitable for this site",
    "title": "Is there any open source counterpart to the IBM Watson?",
    "body": "<p>I am looking for something similar to IBM Watson but open source.  </p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "comparison",
      "architecture",
      "alphago-zero",
      "efficiency"
    ],
    "owner": {
      "account_id": 310795,
      "reputation": 1684,
      "user_id": 9161,
      "user_type": "registered",
      "accept_rate": 82,
      "profile_image": "https://i.sstatic.net/4W5LX.jpg?s=256",
      "display_name": "Demento",
      "link": "https://ai.stackexchange.com/users/9161/demento"
    },
    "is_answered": true,
    "view_count": 1120,
    "accepted_answer_id": 4654,
    "answer_count": 1,
    "score": 11,
    "last_activity_date": 1565535467,
    "creation_date": 1509404516,
    "last_edit_date": 1565535467,
    "question_id": 4394,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4394/why-is-the-merged-neural-network-of-alphago-zero-more-efficient-than-two-separat",
    "title": "Why is the merged neural network of AlphaGo Zero more efficient than two separate neural networks?",
    "body": "<p><a href=\"https://deepmind.com/blog/alphago-zero-learning-scratch/\" rel=\"nofollow noreferrer\">AlphaGo Zero</a> contains several improvements compared to its predecessors. Architectural details of Alpha Go Zero can be seen in this <a href=\"https://applied-data.science/static/main/res/alpha_go_zero_cheat_sheet.jpg\" rel=\"nofollow noreferrer\">cheat sheet</a>.</p>\n\n<p>One of those improvements is using a single neural network that calculates move probabilities and the state value at the same time, while the older versions used two separate neural networks. It has been shown that the merged neural network is more efficient according to the paper:</p>\n\n<blockquote>\n  <p>It uses one neural network rather than two. Earlier versions of AlphaGo used a “policy network” to select the next move to play and a ”value network” to predict the winner of the game from each position. These are combined in AlphaGo Zero, allowing it to be trained and evaluated more efficiently.</p>\n</blockquote>\n\n<p>This seems counter intuitive to me, because from a software design perspective this violates the principle <a href=\"https://en.wikipedia.org/wiki/Separation_of_concerns\" rel=\"nofollow noreferrer\">separation of concerns</a>. That's why I am wondering, why this merge has been proven beneficial.</p>\n\n<p>Can this technique - merging different tasks in a single neural network to improve efficiency - be applied to other neural networks in general or does this require certain conditions to work?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "training",
      "performance",
      "hardware-evaluation"
    ],
    "owner": {
      "account_id": 1200108,
      "reputation": 305,
      "user_id": 11429,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/n7mDL.jpg?s=256",
      "display_name": "pascalwhoop",
      "link": "https://ai.stackexchange.com/users/11429/pascalwhoop"
    },
    "is_answered": true,
    "view_count": 1849,
    "accepted_answer_id": 4747,
    "answer_count": 3,
    "score": 11,
    "last_activity_date": 1639926343,
    "creation_date": 1512814849,
    "last_edit_date": 1639926343,
    "question_id": 4696,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4696/what-size-of-neural-networks-can-be-trained-on-current-consumer-grade-gpus-106",
    "title": "What size of neural networks can be trained on current consumer grade GPUs? (1060,1070,1080)",
    "body": "<p>Is it possible to give a <strong>rule of thumb estimate about the size of neural networks that are trainable on common consumer-grade GPUs</strong>?</p>\n<p>For example, the <a href=\"https://arxiv.org/abs/1707.02286\" rel=\"nofollow noreferrer\">Emergence of Locomotion (Reinforcement)</a> paper trains a network using tanh activation of the neurons. They have a 3 layer NN with 300,200,100 units for the <em>Planar Walker</em>. But they don’t report the hardware and time.</p>\n<p>But could a rule of thumb be developed?</p>\n<p>Also, just based on current empirical results. So, for example, <span class=\"math-container\">$X$</span> units using sigmoid activation can run <span class=\"math-container\">$Y$</span> learning iterations per hour on a 1060.</p>\n<p>Or using activation function <span class=\"math-container\">$a$</span> instead of <span class=\"math-container\">$b$</span> causes a <span class=\"math-container\">$n$</span> times decrease in performance.</p>\n<p>If a student/researcher/curious mind is going to buy a GPU for playing around with these networks, how do you decide what you get? A 1060 is apparently the entry-level budget option, but how can you evaluate if it is not smarter to just get a crappy netbook instead of building a high-power desktop and spend the saved $ on on-demand cloud infrastructure.</p>\n<p>Motivation for the question: I just purchased a 1060 and (clever, to ask the question afterwards huh) wonder if I should have just kept the $ and made a Google Cloud account. And if I can run my master thesis simulation on the GPU.</p>\n"
  },
  {
    "tags": [
      "chess",
      "alphazero"
    ],
    "owner": {
      "account_id": 4146639,
      "reputation": 6209,
      "user_id": 1671,
      "user_type": "registered",
      "accept_rate": 36,
      "profile_image": "https://i.sstatic.net/iE1hj.png?s=256",
      "display_name": "DukeZhou",
      "link": "https://ai.stackexchange.com/users/1671/dukezhou"
    },
    "is_answered": true,
    "view_count": 1395,
    "accepted_answer_id": 5248,
    "answer_count": 3,
    "score": 11,
    "last_activity_date": 1519415344,
    "creation_date": 1517871614,
    "question_id": 5234,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/5234/why-were-chess-experts-surprised-by-the-alphazeros-victory-against-stockfish",
    "title": "Why were Chess experts surprised by the AlphaZero&#39;s victory against Stockfish?",
    "body": "<p>It was <a href=\"https://ai.stackexchange.com/questions/5220/what-was-the-average-decision-speed-pf-alpha-zero-in-the-recent-stockfish-match/5222?noredirect=1#comment7698_5222\">recently brought to my attention</a> that Chess experts took the outcome of this now famous match as something of an upset.  </p>\n\n<p>See: <a href=\"https://fivethirtyeight.com/features/chesss-new-best-player-is-a-fearless-swashbuckling-algorithm/\" rel=\"noreferrer\">Chess’s New Best Player Is A Fearless, Swashbuckling Algorithm</a></p>\n\n<p>As as a non-expert on Chess and Chess AI, my assumption was that, based on the performance of AlphaGo, and the validation of that type of method in relation to combinatorial games, was that the older AI would have no chance.  </p>\n\n<ul>\n<li>Why was AlphaZero's victory surprising?</li>\n</ul>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "q-learning",
      "environment"
    ],
    "owner": {
      "account_id": 13251624,
      "reputation": 111,
      "user_id": 14638,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/204a9b44184666afcfc0298840a62fcd?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "redlum",
      "link": "https://ai.stackexchange.com/users/14638/redlum"
    },
    "is_answered": true,
    "view_count": 4037,
    "answer_count": 1,
    "score": 11,
    "last_activity_date": 1563101520,
    "creation_date": 1522317432,
    "last_edit_date": 1550856271,
    "question_id": 5835,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5835/how-does-q-learning-work-in-stochastic-environments",
    "title": "How does Q-learning work in stochastic environments?",
    "body": "<p>The Q function uses the (current and future) states to determine the action that gets the highest reward. </p>\n\n<p>However, in a stochastic environment, the current action (at the current state) does not determine the next state.</p>\n\n<p>How does Q learning handle this? Is the Q function only used during the training process, where the future states are known? And is the Q function still used afterwards, if that is the case?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "image-recognition",
      "pattern-recognition",
      "deepfakes"
    ],
    "owner": {
      "account_id": 4767926,
      "reputation": 587,
      "user_id": 13088,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0a6a6ec82bcc40f9dcf67622d483a39d?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Andrew Butler",
      "link": "https://ai.stackexchange.com/users/13088/andrew-butler"
    },
    "is_answered": true,
    "view_count": 490,
    "answer_count": 4,
    "score": 11,
    "last_activity_date": 1567864851,
    "creation_date": 1523122192,
    "last_edit_date": 1567858537,
    "question_id": 5939,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5939/what-are-some-tactics-for-recognizing-artificially-made-media",
    "title": "What are some tactics for recognizing artificially made media?",
    "body": "<p>With the growing ability to cheaply create fake pictures, fake soundbites, and fake video there becomes an increasing problem with recognizing what is real and what isn't. Even now we see a number of examples of applications that create fake media for little cost (see <a href=\"https://en.wikipedia.org/wiki/Deepfake\" rel=\"noreferrer\">Deepfake</a>, <a href=\"https://en.wikipedia.org/wiki/FaceApp\" rel=\"noreferrer\">FaceApp</a>, etc.).</p>\n\n<p>Obviously, if these applications are used in the wrong way they could be used to tarnish another person's image. Deepfake could be used to make a person look unfaithful to their partner. Another application could be used to make it seem like a politician said something controversial.</p>\n\n<p>What are some techniques that can be used to recognize and protect against artificially made media? </p>\n"
  },
  {
    "tags": [
      "reference-request",
      "norvig-russell",
      "books"
    ],
    "owner": {
      "account_id": 13340985,
      "reputation": 111,
      "user_id": 14933,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b9a38192db643d5aba87873d1537ae99?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "ml_nrd",
      "link": "https://ai.stackexchange.com/users/14933/ml-nrd"
    },
    "is_answered": true,
    "view_count": 3384,
    "answer_count": 2,
    "score": 11,
    "last_activity_date": 1735313534,
    "creation_date": 1523434012,
    "last_edit_date": 1610824731,
    "question_id": 5990,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5990/what-are-some-alternatives-to-the-book-artificial-intelligence-a-modern-approa",
    "title": "What are some alternatives to the book &quot;Artificial Intelligence: A Modern Approach&quot;?",
    "body": "<p>There are two textbooks that I most love and am most afraid of in the world: <em>Introduction to Algorithms by Cormen</em> et al. and <em>Artificial Intelligence: A Modern Approach by Norvig</em> et al. I have started the \"AI: A Modern Approach\" more than once, but the book is so dense and full of theory that I get discouraged after a couple of weeks and stop.</p>\n\n<p><strong>I am looking for a similar AI book but with an equal emphasis on theory and practice.</strong> Some examples of what I am looking for:</p>\n\n<ul>\n<li><p>The Elements of Statistical Learning by Tibshirani et al. <strong>(detailed theory)</strong></p></li>\n<li><p>An Introduction to Statistical Learning: With Applications in R by\nTibshirani et al. <strong>(theory+practical)</strong></p></li>\n<li>Digital Image Processing by Gonzalez et al. <strong>(detailed theory)</strong></li>\n<li>Digital Image Processing Using MATLAB by Gonzalez et al.\n<strong>(theory+practical)</strong></li>\n</ul>\n"
  },
  {
    "tags": [
      "math",
      "agi",
      "research",
      "education"
    ],
    "owner": {
      "account_id": 7039987,
      "reputation": 511,
      "user_id": 15277,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/93fba4330985b7c2c301d55af771e1b1?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Mark ellon",
      "link": "https://ai.stackexchange.com/users/15277/mark-ellon"
    },
    "is_answered": true,
    "view_count": 4577,
    "answer_count": 3,
    "score": 11,
    "last_activity_date": 1613175093,
    "creation_date": 1525443354,
    "last_edit_date": 1612890328,
    "question_id": 6267,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6267/what-are-the-mathematical-prerequisites-to-be-able-to-study-artificial-general-i",
    "title": "What are the mathematical prerequisites to be able to study artificial general intelligence?",
    "body": "<p>What are the mathematical prerequisites to be able to study artificial general intelligence (AGI) or strong AI?</p>\n"
  },
  {
    "tags": [
      "agi",
      "artificial-consciousness",
      "self-awareness"
    ],
    "owner": {
      "account_id": 6476695,
      "reputation": 2258,
      "user_id": 4199,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-o_4nx3c8sHQ/AAAAAAAAAAI/AAAAAAAAAPU/lVuNexWGyPU/s256-rj/photo.jpg",
      "display_name": "zooby",
      "link": "https://ai.stackexchange.com/users/4199/zooby"
    },
    "is_answered": true,
    "view_count": 516,
    "answer_count": 5,
    "score": 11,
    "last_activity_date": 1661494764,
    "creation_date": 1525880125,
    "last_edit_date": 1526046759,
    "question_id": 6325,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6325/what-kind-of-simulated-environment-is-complex-enough-to-develop-a-general-ai",
    "title": "What kind of simulated environment is complex enough to develop a general AI?",
    "body": "<p>Imagine trying to create a simulated virtual environment that is complicated enough to create a \"general AI\" (which I define as a self aware AI) but is as simple as possible. What would this minimal environment be like?</p>\n\n<p>i.e. An environment that was just a chess game would be too simple. A chess program cannot be a general AI.</p>\n\n<p>An environment with multiple agents playing chess and communicating their results to each other. Would this constitute a general AI? (If you can say a chess grand master who thinks about chess all day long has 'general AI'? During his time thinking about chess is he any different to a chess computer?).</p>\n\n<p>What about a 3D sim-like world. That seems to be too complicated. After all why can't a general AI exist in a 2D world.</p>\n\n<p>What would be an example of a simple environment but not too simple such that the AI(s) can have self-awareness?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "comparison",
      "activation-functions",
      "relu"
    ],
    "owner": {
      "account_id": 5006437,
      "reputation": 499,
      "user_id": 8720,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/mYycw.jpg?s=256",
      "display_name": "imflash217",
      "link": "https://ai.stackexchange.com/users/8720/imflash217"
    },
    "is_answered": true,
    "view_count": 8973,
    "accepted_answer_id": 6599,
    "answer_count": 2,
    "score": 11,
    "last_activity_date": 1579869218,
    "creation_date": 1526748110,
    "last_edit_date": 1579704423,
    "question_id": 6468,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6468/why-do-we-prefer-relu-over-linear-activation-functions",
    "title": "Why do we prefer ReLU over linear activation functions?",
    "body": "<p>The <a href=\"https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\" rel=\"noreferrer\">ReLU</a> activation function is defined as follows </p>\n\n<p><span class=\"math-container\">$$y = \\operatorname{max}(0,x)$$</span></p>\n\n<p>And the linear activation function is defined as follows</p>\n\n<p><span class=\"math-container\">$$y = x$$</span></p>\n\n<p>The ReLU nonlinearity just clips the values less than 0 to 0 and passes everything else. Then why not to use a linear activation function instead, as it will pass all the gradient information during backpropagation? I do see that parametric ReLU (PReLU) does provide this possibility. </p>\n\n<p>I just want to know if there is a proper explanation to using ReLU as default or it is just based on observations that it performs better on the training sets.</p>\n"
  },
  {
    "tags": [
      "resource-request",
      "adversarial-ml",
      "ai-safety",
      "ai-security"
    ],
    "owner": {
      "account_id": 2541805,
      "reputation": 299,
      "user_id": 16354,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/lywVF.jpg?s=256",
      "display_name": "Ilya Palachev",
      "link": "https://ai.stackexchange.com/users/16354/ilya-palachev"
    },
    "is_answered": true,
    "view_count": 971,
    "accepted_answer_id": 7296,
    "answer_count": 2,
    "score": 11,
    "last_activity_date": 1614761648,
    "creation_date": 1530009569,
    "last_edit_date": 1614761648,
    "question_id": 6892,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6892/what-tools-are-used-to-deal-with-adversarial-examples-problem",
    "title": "What tools are used to deal with adversarial examples problem?",
    "body": "<p>The problem of adversarial examples is <a href=\"https://arxiv.org/pdf/1312.6199.pdf\" rel=\"nofollow noreferrer\">known</a> to be critical for neural networks. For example, an image classifier can be manipulated by additively superimposing a different low amplitude image to each of many training examples that looks like noise but is designed to produce specific misclassifications.</p>\n<p><a href=\"https://i.sstatic.net/5rfe9.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/5rfe9.png\" alt=\"enter image description here\" /></a></p>\n<p>Since neural networks are applied to some safety-critical problems (e.g. self-driving cars), I have the following question</p>\n<blockquote>\n<p>What tools are used to ensure safety-critical applications are resistant to the injection of adversarial examples at training time?</p>\n</blockquote>\n<p>Laboratory research aimed at developing defensive security for neural networks exists. These are a few examples.</p>\n<ul>\n<li><p>adversarial training (see e.g. <a href=\"https://arxiv.org/pdf/1611.01236.pdf\" rel=\"nofollow noreferrer\">A. Kurakin et al., ICLR 2017</a>)</p>\n</li>\n<li><p>defensive distillation (see e.g. <a href=\"https://arxiv.org/pdf/1511.04508.pdf\" rel=\"nofollow noreferrer\">N. Papernot et al., SSP 2016</a>)</p>\n</li>\n<li><p>MMSTV defence (<a href=\"https://arxiv.org/pdf/1706.06083\" rel=\"nofollow noreferrer\">Maudry et al., ICLR 2018</a>).</p>\n</li>\n</ul>\n<p>However, do industrial-strength, production-ready defensive strategies and approaches exist?  Are there known examples of applied adversarial-resistant networks for one or more specific types (e.g. for small perturbation limits)?</p>\n<p>There are already (at least) two questions related to the problem of <a href=\"https://ai.stackexchange.com/q/6800/2444\">hacking</a> and <a href=\"https://ai.stackexchange.com/q/92/2444\">fooling</a> of neural networks. The primary interest of this question, however, is whether any <strong>tools</strong> exist that can defend against some adversarial example attacks.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "training",
      "support-vector-machine"
    ],
    "owner": {
      "account_id": 6141124,
      "reputation": 211,
      "user_id": 16977,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/590f721200cbbade1397bc1df9ccf82a?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Panagiotis",
      "link": "https://ai.stackexchange.com/users/16977/panagiotis"
    },
    "is_answered": true,
    "view_count": 43696,
    "answer_count": 7,
    "score": 11,
    "last_activity_date": 1629993640,
    "creation_date": 1531998111,
    "last_edit_date": 1619699197,
    "question_id": 7202,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7202/why-does-training-an-svm-take-so-long-how-can-i-speed-it-up",
    "title": "Why does training an SVM take so long? How can I speed it up?",
    "body": "<p>I'm trying to create and test non-linear SVMs with various kernels (RBF, Sigmoid, Polynomial) in scikit-learn, to create a model which can classify anomalies and benign behaviors.</p>\n<p>My dataset includes 692703 records and I use a 75/25% training/testing split. Also, I use various combinations of features whose dimensionality is between 1 and 14 features. However, the training processes of the various SVMs take much too long. Is this reasonable?</p>\n<p>I have also examined the ensemble <code>BaggingClassifier</code> in combination with non-linear SVMs, by configuring the <code>n_jobs</code> parameter to <code>-1</code>; nevertheless, the training process proceeds again too slowly.</p>\n<p>How can I speed up the training processes?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "research",
      "academia",
      "benchmarks"
    ],
    "owner": {
      "account_id": 9042085,
      "reputation": 10509,
      "user_id": 1641,
      "user_type": "moderator",
      "profile_image": "https://www.gravatar.com/avatar/a3f1e216ed8274c085d0dfe11348e6f1?s=256&d=identicon&r=PG",
      "display_name": "Dennis Soemers",
      "link": "https://ai.stackexchange.com/users/1641/dennis-soemers"
    },
    "is_answered": true,
    "view_count": 392,
    "accepted_answer_id": 7544,
    "answer_count": 3,
    "score": 11,
    "last_activity_date": 1638232943,
    "creation_date": 1534075699,
    "last_edit_date": 1589457276,
    "question_id": 7525,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7525/how-can-ai-researchers-avoid-overfitting-to-commonly-used-benchmarks-as-a-comm",
    "title": "How can AI researchers avoid &quot;overfitting&quot; to commonly-used benchmarks as a community?",
    "body": "<p>In fields such as Machine Learning, we typically (somewhat informally) say that we are overfitting if improve our performance on a training set at the cost of reduced performance on a test set / the true population from which data is sampled.</p>\n\n<p>More generally, in AI research, we often end up testing performance of newly proposed algorithms / ideas on the same benchmarks over and over again. For example:</p>\n\n<ul>\n<li>For over a decade, researchers kept trying thousands of ideas on the game of Go.</li>\n<li>The ImageNet dataset has been used for huge amounts of different publications</li>\n<li>The Arcade Learning Environment (Atari games) has been used for thousands of Reinforcement Learning papers, having become especially popular since the DQN paper in 2015.</li>\n</ul>\n\n<p>Of course, there are very good reasons for this phenomenon where the same benchmarks keep getting used:</p>\n\n<ul>\n<li>Reduced likelihood of researchers \"creating\" a benchmark themselves for which their proposed algorithm \"happens\" to perform well</li>\n<li>Easy comparison of results to other publications (previous as well as future publications) if they're all consistently evaluated in the same manner.</li>\n</ul>\n\n<p>However, there is also a risk that the <strong>research community as a whole</strong> is in some sense \"overfitting\" to these commonly-used benchmarks. If thousands of researchers are generating new ideas for new algorithms, and evaluate them all on these same benchmarks, and there is a large bias towards primarily submitting/accepting publications that perform well on these benchmarks, <strong>the research output that gets published does not necessarily describe the algorithms that perform well across all interesting problems in the world</strong>; there may be a bias towards the set of commonly-used benchmarks.</p>\n\n<hr>\n\n<p><strong>Question</strong>: to what extent is what I described above a problem, and in what ways could it be reduced, mitigated or avoided?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "deep-neural-networks",
      "topology",
      "hopfield-network"
    ],
    "owner": {
      "account_id": 300698,
      "reputation": 211,
      "user_id": 17901,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2152228a1215a65bd61e167de7c27582?s=256&d=identicon&r=PG",
      "display_name": "Mario Alemi",
      "link": "https://ai.stackexchange.com/users/17901/mario-alemi"
    },
    "is_answered": true,
    "view_count": 701,
    "answer_count": 1,
    "score": 11,
    "last_activity_date": 1586221315,
    "creation_date": 1535968599,
    "last_edit_date": 1583622139,
    "question_id": 7803,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7803/can-layers-of-deep-neural-networks-be-seen-as-hopfield-networks",
    "title": "Can layers of deep neural networks be seen as Hopfield networks?",
    "body": "<p>Hopfield networks are able to store a vector and retrieve it starting from a noisy version of it. They do so setting weights in order to minimize the energy function when all neurons are set equal to the vector values, and retrieve the vector using the noisy version of it as input and allowing the net to settle to an energy minimum. </p>\n\n<p>Leaving aside problems like the fact that there is no guarantee that the net will settle in the nearest minimum etc – problems eventually solved with Boltzmann machines and eventually with back-propagation – the breakthrough was they are a starting point for having abstract representations. Two versions of the same document would recall the same state, they would be represented, in the network, by the same state. </p>\n\n<p>As Hopfield himself wrote in his 1982 paper <a href=\"https://bi.snu.ac.kr/Courses/g-ai09-2/hopfield82.pdf\" rel=\"nofollow noreferrer\">Neural networks and physical systems with emergent collective computational abilities</a></p>\n\n<blockquote>\n  <p>The present modeling might then be related to how an entity or Gestalt is remembered or categorized on the basis of inputs representing a collection of its features.</p>\n</blockquote>\n\n<p>On the other side, the breakthrough of deep learning was the ability to build multiple, hierarchical representations of the input, eventually leading to making AI-practitioners' life easier, simplifying feature engineering. (see e.g. <a href=\"https://arxiv.org/pdf/1206.5538.pdf\" rel=\"nofollow noreferrer\">Representation Learning: A Review and New Perspectives</a>, Bengio, Courville, Vincent).</p>\n\n<p>From a conceptual point of view, I believe one can see deep learning as a generalization of Hopfield nets: from one single representation to a hierarchy of representation.</p>\n\n<p><em>Is that true from a computational/topological point of view as well? Not considering how \"simple\" Hopfield networks were (2-state neurons, undirected, energy function), can one see each layer of a network as a Hopfield network and the whole process as a sequential extraction of previously memorized Gestalt, and a reorganization of these Gestalt?</em></p>\n"
  },
  {
    "tags": [
      "ai-design",
      "terminology",
      "definitions",
      "ontology"
    ],
    "owner": {
      "account_id": 7662568,
      "reputation": 263,
      "user_id": 18123,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/cfe36cdfddb4fe19ff17ff391deb6ba6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "oren revenge",
      "link": "https://ai.stackexchange.com/users/18123/oren-revenge"
    },
    "is_answered": true,
    "view_count": 4718,
    "answer_count": 1,
    "score": 11,
    "last_activity_date": 1614087971,
    "creation_date": 1539607091,
    "last_edit_date": 1556645987,
    "question_id": 8427,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/8427/what-are-ontologies-in-ai",
    "title": "What are ontologies in AI?",
    "body": "<p>What exactly are ontologies in AI? How should I write them and why are they important?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "q-learning",
      "math",
      "proofs",
      "sutton-barto"
    ],
    "owner": {
      "account_id": 14905087,
      "reputation": 111,
      "user_id": 20515,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f19a3ac761df525d20390cf126e6219e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "123learn",
      "link": "https://ai.stackexchange.com/users/20515/123learn"
    },
    "is_answered": true,
    "view_count": 2480,
    "answer_count": 2,
    "score": 11,
    "last_activity_date": 1588910854,
    "creation_date": 1544246696,
    "last_edit_date": 1550282912,
    "question_id": 9396,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/9396/how-do-we-prove-the-n-step-return-error-reduction-property",
    "title": "How do we prove the n-step return error reduction property?",
    "body": "<p>In section 7.1 (about the n-step bootstrapping) of the book <em>Reinforcement Learning: An Introduction</em> (2nd edition), by Andrew Barto and Richard S. Sutton, the authors write about what they call the \"n-step return error reduction property\":</p>\n\n<p><a href=\"https://i.sstatic.net/BUSZM.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/BUSZM.png\" alt=\"enter image description here\"></a></p>\n\n<p>But they don't prove it. I was thinking it should not be too hard but how can we show this? I was thinking of using the definition of n-step return (eq. 7.1 on previous page):</p>\n\n<p><span class=\"math-container\">$$G_{t:t+n} = R_{t+1} + \\gamma*R_{t+2} + ... + \\gamma^{n-1}*R_{t+n} + \\gamma^{n}*V_{t+n-1}(S_{t+n})$$</span></p>\n\n<p>Because then this has the <span class=\"math-container\">$V_{t+n-1}$</span> in it already. But in the definition above of the n-step return it uses <span class=\"math-container\">$V_{t+n-1}(S_{t+n})$</span> but on the right side of the inequality (7.3) that we want to prove it is just little s <span class=\"math-container\">$V_{t+n-1}(s)$</span> ? So kind of confused here which state s it is using? And then I guess after this probably pull out a <span class=\"math-container\">$\\gamma^{n}$</span> term or something, how should we go from here?</p>\n\n<p>This is the newest Sutton Barto book (book page 144, equation 7.3):\n<a href=\"https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view\" rel=\"noreferrer\">https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view</a></p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "off-policy-methods"
    ],
    "owner": {
      "account_id": 14862777,
      "reputation": 2416,
      "user_id": 20339,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/eb6fd6c2f63e7fa569af323daa2b2299?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Brale",
      "link": "https://ai.stackexchange.com/users/20339/brale"
    },
    "is_answered": true,
    "view_count": 2239,
    "accepted_answer_id": 9523,
    "answer_count": 1,
    "score": 11,
    "last_activity_date": 1603188155,
    "creation_date": 1544735225,
    "last_edit_date": 1589839785,
    "question_id": 9518,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/9518/why-is-the-n-step-tree-backup-algorithm-an-off-policy-algorithm",
    "title": "Why is the n-step tree backup algorithm an off-policy algorithm?",
    "body": "<p>In reinforcement learning book from Sutton &amp; Barto (2018 edition), specifically in section 7.5 of the book, they present an n-step off-policy algorithm that doesn't require importance sampling called <em>n-step tree backup algorithm</em>. </p>\n\n<p>In other algorithms, the return in the update consisted of rewards along the way and the estimated value(s) of the node(s) at the bottom, but, in tree backup update, a return consists of things mentioned before plus the estimated values of the actions that weren't picked during these n steps, all weighted by the probability of taking the action from the previous step.  </p>\n\n<p>I have a few questions about things in this algorithm that are unclear to me.</p>\n\n<ol>\n<li><p><em>Why is this algorithm considered an off-policy algorithm?</em> As far as I could notice, only a single target policy is mentioned and there is no talk about behaviour policy generating actions to take.</p></li>\n<li><p><em>In control we want our target policy to be deterministic, greedy policy, so how do we exactly generate actions to take in this case since behaviour policy isn't used?</em> If we generate actions from the greedy policy, we won't explore, so we won't learn the optimal policy. What am I missing here?</p></li>\n<li><p>If I understood something wrong and we are actually using behaviour policy, I don't understand how would the update work in the case where our target policy is greedy. The return consists of estimates taken from actions that weren't picked, but, because our policy is greedy, the probabilities used in calculating those estimates would be 0, so the total estimate of those actions would be 0 as well. The only non 0 probability in our target policy is the one of the greedy action (which is the probability of 1), so the entire return would fall down to n-step SARSA return. <em>So, basically, how are we allowed to do this update in this case, why is this return allowed to replace the one with importance sampling?</em></p></li>\n</ol>\n"
  },
  {
    "tags": [
      "convolutional-neural-networks",
      "computer-vision",
      "object-detection",
      "yolo"
    ],
    "owner": {
      "account_id": 11811551,
      "reputation": 253,
      "user_id": 16313,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/155e66977033fae09c366590485e1ab3?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Ankish Bansal",
      "link": "https://ai.stackexchange.com/users/16313/ankish-bansal"
    },
    "is_answered": true,
    "view_count": 16909,
    "accepted_answer_id": 9997,
    "answer_count": 3,
    "score": 11,
    "last_activity_date": 1611877295,
    "creation_date": 1547218812,
    "last_edit_date": 1611877125,
    "question_id": 9934,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/9934/is-it-difficult-to-learn-the-rotated-bounding-box-for-a-rotated-object",
    "title": "Is it difficult to learn the rotated bounding box for a (rotated) object?",
    "body": "<p>I have checked out many methods and papers, like YOLO, SSD, etc., with good results in detecting a rectangular box around an object, However, I could not find any paper that shows a method that learns a rotated bounding box.</p>\n<p>Is it difficult to learn the rotated bounding box for a (rotated) object?</p>\n<p>Here's a diagram that illustrates the problem.</p>\n<p><a href=\"https://i.sstatic.net/Ljimz.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/Ljimz.png\" alt=\"\" /></a></p>\n<p>For example, for this object (see <a href=\"https://stackoverflow.com/q/40404031/9277245\">this</a>), its bounding box should be of the same shape (the rotated rectangle is shown in the 2nd right image), but the prediction result for the YOLO will be Ist right.</p>\n<p>Is there any research paper that tackles this problem?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "natural-language-processing",
      "bert"
    ],
    "owner": {
      "account_id": 9566939,
      "reputation": 163,
      "user_id": 17451,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/fde4915e9d745b1ba36a156451e87aad?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Skinish",
      "link": "https://ai.stackexchange.com/users/17451/skinish"
    },
    "is_answered": true,
    "view_count": 13896,
    "accepted_answer_id": 10630,
    "answer_count": 2,
    "score": 11,
    "last_activity_date": 1696872700,
    "creation_date": 1548154877,
    "last_edit_date": 1572576001,
    "question_id": 10133,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10133/what-are-the-segment-embeddings-and-position-embeddings-in-bert",
    "title": "What are the segment embeddings and position embeddings in BERT?",
    "body": "<p><a href=\"https://i.sstatic.net/thmqC.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/thmqC.png\" alt=\"enter image description here\"></a></p>\n\n<p>They only reference in the paper that the position embeddings are learned, which is different from what was done in ELMo.</p>\n\n<p>ELMo paper - <a href=\"https://arxiv.org/pdf/1802.05365.pdf\" rel=\"noreferrer\">https://arxiv.org/pdf/1802.05365.pdf</a></p>\n\n<p>BERT paper - <a href=\"https://arxiv.org/pdf/1810.04805.pdf\" rel=\"noreferrer\">https://arxiv.org/pdf/1810.04805.pdf</a></p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "long-short-term-memory",
      "loss",
      "accuracy",
      "validation"
    ],
    "owner": {
      "account_id": 2304865,
      "reputation": 2859,
      "user_id": 16565,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/e4iCE.jpg?s=256",
      "display_name": "malioboro",
      "link": "https://ai.stackexchange.com/users/16565/malioboro"
    },
    "is_answered": true,
    "view_count": 5000,
    "accepted_answer_id": 10604,
    "answer_count": 3,
    "score": 11,
    "last_activity_date": 1611885043,
    "creation_date": 1549521174,
    "last_edit_date": 1611885043,
    "question_id": 10431,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10431/should-i-choose-a-model-with-the-smallest-loss-or-highest-accuracy",
    "title": "Should I choose a model with the smallest loss or highest accuracy?",
    "body": "<p>I have two Machine Learning models (I use LSTM) that have a different result on the validation set (~100 samples data):</p>\n<ul>\n<li>Model A: Accuracy: ~91%, Loss: ~0.01</li>\n<li>Model B: Accuracy: ~83%, Loss: ~0.003</li>\n</ul>\n<p>The size and the speed of both models are almost the same. So, which model should I choose?</p>\n"
  },
  {
    "tags": [
      "neural-networks"
    ],
    "owner": {
      "account_id": 387495,
      "reputation": 327,
      "user_id": 22365,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/TCiwn.jpg?s=256",
      "display_name": "Guillermo Mosse",
      "link": "https://ai.stackexchange.com/users/22365/guillermo-mosse"
    },
    "is_answered": true,
    "view_count": 2253,
    "accepted_answer_id": 10945,
    "answer_count": 1,
    "score": 11,
    "last_activity_date": 1551462198,
    "creation_date": 1551449372,
    "last_edit_date": 1551455915,
    "question_id": 10944,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10944/are-there-neural-networks-with-very-few-nodes-that-decently-solve-non-trivial-pr",
    "title": "Are there neural networks with very few nodes that decently solve non-trivial problems?",
    "body": "<p>I'm interested in knowing whether there exist any neural network, that solves (with  >=80% accuracy) any nontrivial problem, that uses very few nodes (where 20 nodes is not a hard limit). I want to develop an intuition on sizes of neural networks.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "generative-model",
      "generative-adversarial-networks",
      "notation"
    ],
    "owner": {
      "account_id": 8511096,
      "reputation": 245,
      "user_id": 23918,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-8tb88OkDd90/AAAAAAAAAAI/AAAAAAAAAB0/G_rrCiytIIs/s256-rj/photo.jpg",
      "display_name": "i_rezic",
      "link": "https://ai.stackexchange.com/users/23918/i-rezic"
    },
    "is_answered": true,
    "view_count": 2406,
    "accepted_answer_id": 13038,
    "answer_count": 1,
    "score": 11,
    "last_activity_date": 1639152223,
    "creation_date": 1555102428,
    "last_edit_date": 1639152223,
    "question_id": 11793,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11793/what-is-the-meaning-of-vd-g-in-the-gan-objective-function",
    "title": "What is the meaning of $V(D,G)$ in the GAN objective function?",
    "body": "<p>Here is <a href=\"https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf\" rel=\"nofollow noreferrer\">the GAN objective function</a>.</p>\n<p><span class=\"math-container\">$$\\min _{G} \\max _{D} V(D, G)=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text {data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z})))]$$</span></p>\n<p>What is the meaning of <span class=\"math-container\">$V(D, G)$</span>?</p>\n<p>How do we get these expectation parts?</p>\n<p>I was trying to understand it following this article: <a href=\"https://danieltakeshi.github.io/2017/03/05/understanding-generative-adversarial-networks/\" rel=\"nofollow noreferrer\">Understanding Generative Adversarial Networks <em>(D.Seita)</em></a>, but, after many tries, I still can't understand how he got from <span class=\"math-container\">$\\sum_{n=1}^{N} \\log D(x)$</span> to <span class=\"math-container\">$\\mathbb{E}(\\log(D(x))$</span>.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "support-vector-machine",
      "state-of-the-art"
    ],
    "owner": {
      "account_id": 10931845,
      "reputation": 177,
      "user_id": 22525,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/1254b2a9dce1336eb022888ebfaa6f42?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Steven Davis",
      "link": "https://ai.stackexchange.com/users/22525/steven-davis"
    },
    "is_answered": true,
    "view_count": 925,
    "answer_count": 4,
    "score": 11,
    "last_activity_date": 1569681326,
    "creation_date": 1555984644,
    "last_edit_date": 1556028480,
    "question_id": 11953,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11953/what-are-the-domains-where-svms-are-still-state-of-the-art",
    "title": "What are the domains where SVMs are still state-of-the-art?",
    "body": "<p>It seems that deep neural networks and other neural network based models are dominating many current areas like computer vision, object classification, reinforcement learning, etc.</p>\n\n<p>Are there domains where SVMs (or other models) are still producing state-of-the-art results?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "terminology",
      "temporal-difference-methods",
      "planning",
      "dynamic-programming"
    ],
    "owner": {
      "account_id": 13002362,
      "reputation": 797,
      "user_id": 24054,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s256-rj/photo.jpg",
      "display_name": "Miguel Saraiva",
      "link": "https://ai.stackexchange.com/users/24054/miguel-saraiva"
    },
    "is_answered": true,
    "view_count": 1484,
    "accepted_answer_id": 12067,
    "answer_count": 3,
    "score": 11,
    "last_activity_date": 1700182875,
    "creation_date": 1556634406,
    "last_edit_date": 1615457176,
    "question_id": 12065,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12065/what-algorithms-are-considered-reinforcement-learning-algorithms",
    "title": "What algorithms are considered reinforcement learning algorithms?",
    "body": "<p>What are the areas/algorithms that belong to reinforcement learning?</p>\n<p>TD(0), Q-Learning and SARSA are all temporal-difference algorithms, which belong to the reinforcement learning area, but is there more to it?</p>\n<p>Are the dynamic programming algorithms, such as policy iteration and value iteration, considered as part of reinforcement learning? Or are these just the basis for the temporal-difference algorithms, which are the only RL algorithms?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "agi",
      "mythology-of-ai"
    ],
    "owner": {
      "account_id": 16110476,
      "reputation": 287,
      "user_id": 26351,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/64dd68bfff4876b853b91c1c8683f635?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "fabien",
      "link": "https://ai.stackexchange.com/users/26351/fabien"
    },
    "is_answered": true,
    "view_count": 429,
    "answer_count": 2,
    "score": 11,
    "last_activity_date": 1561731420,
    "creation_date": 1560841409,
    "last_edit_date": 1560874163,
    "question_id": 12914,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12914/what-are-the-common-myths-associated-with-artificial-intelligence",
    "title": "What are the common myths associated with Artificial Intelligence?",
    "body": "<p>What are some interesting myths of Artificial Intelligence and what are the facts behind them?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "feedforward-neural-networks",
      "time-complexity",
      "complexity-theory",
      "forward-pass"
    ],
    "owner": {
      "account_id": 16391601,
      "reputation": 113,
      "user_id": 27462,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6f8edb07c4b385620c71c62e7aca4bd6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Artificial",
      "link": "https://ai.stackexchange.com/users/27462/artificial"
    },
    "is_answered": true,
    "view_count": 8469,
    "accepted_answer_id": 13626,
    "answer_count": 1,
    "score": 11,
    "last_activity_date": 1602366961,
    "creation_date": 1564208617,
    "last_edit_date": 1602366961,
    "question_id": 13612,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/13612/what-is-the-time-complexity-of-the-forward-pass-algorithm-of-a-feedforward-neura",
    "title": "What is the time complexity of the forward pass algorithm of a feedforward neural network?",
    "body": "<p>How do I determine the time complexity of the forward pass algorithm of a feedforward neural network? How many multiplications are done to generate the output?</p>\n"
  },
  {
    "tags": [
      "convolutional-neural-networks",
      "reference-request",
      "convolution",
      "convolutional-layers",
      "3d-convolution"
    ],
    "owner": {
      "account_id": 5462727,
      "reputation": 171,
      "user_id": 27555,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2db9eb846121670f6049342d01b48b49?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Shobhit Verma",
      "link": "https://ai.stackexchange.com/users/27555/shobhit-verma"
    },
    "is_answered": true,
    "view_count": 8965,
    "accepted_answer_id": 13786,
    "answer_count": 2,
    "score": 11,
    "last_activity_date": 1639832327,
    "creation_date": 1564553394,
    "last_edit_date": 1639832327,
    "question_id": 13692,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/13692/when-should-i-use-3d-convolutions",
    "title": "When should I use 3D convolutions?",
    "body": "<p>I am new to convolutional neural networks, and I am learning 3D convolution. What I could understand is that 2D convolution gives us relationships between low-level features in the X-Y dimension, while the 3D convolution helps detect low-level features and relationships between them in all the 3 dimensions.</p>\n\n<p>Consider a CNN employing 2D convolutional layers to recognize handwritten digits. If a digit, say 5, was written in different colors:</p>\n\n<p><a href=\"https://i.sstatic.net/3Ajlf.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/3Ajlf.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>Would a strictly 2D CNN perform poorly (since they belong to different channels in the z-dimension)?</p>\n\n<p>Also, are there practical well-known neural nets that employ 3D convolution?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "comparison",
      "supervised-learning",
      "function-approximation",
      "regression"
    ],
    "owner": {
      "account_id": 1461741,
      "reputation": 903,
      "user_id": 8332,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f5c490f629cf9a807f26939967aec109?s=256&d=identicon&r=PG",
      "display_name": "TomR",
      "link": "https://ai.stackexchange.com/users/8332/tomr"
    },
    "is_answered": true,
    "view_count": 1150,
    "accepted_answer_id": 14168,
    "answer_count": 1,
    "score": 11,
    "last_activity_date": 1594135636,
    "creation_date": 1566942648,
    "last_edit_date": 1566948150,
    "question_id": 14167,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/14167/can-supervised-learning-be-recast-as-reinforcement-learning-problem",
    "title": "Can supervised learning be recast as reinforcement learning problem?",
    "body": "<p>Let's assume that there is a sequence of pairs <span class=\"math-container\">$(x_i, y_i), (x_{i+1}, y_{i+1}), \\dots$</span> of observations and corresponding labels. Let's also assume that the <span class=\"math-container\">$x$</span> is considered as independent variable and <span class=\"math-container\">$y$</span> is considered as the variable that depends on <span class=\"math-container\">$x$</span>. So, in supervised learning, one wants to learn the function <span class=\"math-container\">$y=f(x)$</span>. </p>\n\n<p>Can reinforcement learning be used to learn <span class=\"math-container\">$f$</span> (possibly, even learning the symbolic form of <span class=\"math-container\">$f(x)$</span>)? </p>\n\n<p>Just some sketches how can it be done: <span class=\"math-container\">$x_i$</span> can be considered as the environment and each <span class=\"math-container\">$x_i$</span> defines some set of possible \"actions\" - possible symbolic form of <span class=\"math-container\">$f(x)$</span> or possible numerical values of parameters for <span class=\"math-container\">$f(x)$</span> (if the symbolic form is fized). And concrete selected action/functional form <span class=\"math-container\">$f(x, a)$</span> (a - set of parameters) can be assigned reward from the loss function: how close the observation <span class=\"math-container\">$(x_i, y_i)$</span> is to the value that can be inferred from <span class=\"math-container\">$f(x)$</span>.</p>\n\n<p>Are there ideas or works of RL along the framework that I provided in the previous passage?</p>\n"
  },
  {
    "tags": [
      "hardware-evaluation"
    ],
    "owner": {
      "account_id": 14005304,
      "reputation": 147,
      "user_id": 31307,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b655c81b1cfb2c8396394040d5a7938b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "noviceFedora",
      "link": "https://ai.stackexchange.com/users/31307/novicefedora"
    },
    "is_answered": true,
    "view_count": 9498,
    "closed_date": 1592130481,
    "answer_count": 1,
    "score": 11,
    "locked_date": 1640103291,
    "last_activity_date": 1573820447,
    "creation_date": 1573811684,
    "question_id": 16538,
    "link": "https://ai.stackexchange.com/questions/16538/what-is-the-reason-amd-radeon-is-not-widely-used-for-machine-learning-and-deep-l",
    "closed_reason": "Not suitable for this site",
    "title": "What is the reason AMD Radeon is not widely used for machine learning and deep learning?",
    "body": "<p>What is the reason AMD Radeon is not widely used for machine learning and deep learning? Is it mainly an issue of lack of software? Or is Radeon's GPU not as good as NVIDIA's?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "kaggle"
    ],
    "owner": {
      "account_id": 16015211,
      "reputation": 355,
      "user_id": 30632,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-n6dpFBkYclk/AAAAAAAAAAI/AAAAAAAAFxQ/Wjgo4Y4-OXM/s256-rj/photo.jpg",
      "display_name": "Dhanush Giriyan",
      "link": "https://ai.stackexchange.com/users/30632/dhanush-giriyan"
    },
    "is_answered": true,
    "view_count": 3676,
    "answer_count": 4,
    "score": 11,
    "last_activity_date": 1592487009,
    "creation_date": 1574093006,
    "last_edit_date": 1592486953,
    "question_id": 16599,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/16599/are-there-any-online-competitions-for-reinforcement-learning",
    "title": "Are there any online competitions for Reinforcement Learning?",
    "body": "<p>Kaggle is limited to only supervised learning problems. There used to be www.rl-competition.org but they've stopped.</p>\n\n<p>Is there anything else I can do other than locally trying out different algorithms for various RL problems? </p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "long-short-term-memory",
      "math"
    ],
    "owner": {
      "account_id": 11909125,
      "reputation": 825,
      "user_id": 30885,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/98d37dbcca45af6ec9419b1d3211a6fb?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user8714896",
      "link": "https://ai.stackexchange.com/users/30885/user8714896"
    },
    "is_answered": true,
    "view_count": 1807,
    "accepted_answer_id": 17246,
    "answer_count": 1,
    "score": 11,
    "last_activity_date": 1644182476,
    "creation_date": 1577078238,
    "last_edit_date": 1577244582,
    "question_id": 17216,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/17216/how-does-the-forget-layer-of-an-lstm-work",
    "title": "How does the forget layer of an LSTM work?",
    "body": "<p>Can someone explain the mathematical intuition behind the forget layer of an LSTM? </p>\n\n<p>So as far as I understand it, the cell state is essentially long term memory embedding (correct me if I'm wrong), but I'm also assuming it's a matrix. Then the forget vector is calculated by concatenating the previous hidden state and the current input and adding the bias to it, then putting that through a sigmoid function that outputs a vector then that gets multiplied by the cell state matrix.</p>\n\n<p>How does a concatenation of the hidden state of the previous input and the current input with the bias help with what to forget?</p>\n\n<p>Why is the previous hidden state, current input and the bias put into a sigmoid function? Is there some special characteristic of a sigmoid that creates a vector of important embeddings?</p>\n\n<p>I'd really like to understand the theory behind calculating the cell states and hidden states. Most people just tell me to treat it like a black box, but I think that, in order to have a successful application of LSTMs to a problem, I need to know what's going on under the hood. If anyone has any resources that are good for learning the theory behind why cell state and hidden state calculation extract key features in short and long term memory I'd love to read it.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "overfitting",
      "computational-learning-theory",
      "generalization"
    ],
    "owner": {
      "account_id": 4708174,
      "reputation": 211,
      "user_id": 25415,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/896265f35a951585086f19aa6425f507?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Nico A",
      "link": "https://ai.stackexchange.com/users/25415/nico-a"
    },
    "is_answered": true,
    "view_count": 1534,
    "answer_count": 5,
    "score": 11,
    "last_activity_date": 1698807170,
    "creation_date": 1577833962,
    "last_edit_date": 1577835318,
    "question_id": 17317,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/17317/why-can-neural-networks-generalize-at-all",
    "title": "Why can neural networks generalize at all?",
    "body": "<p>Neural networks are incredibly good at learning functions. We know by the universal approximation theorem that, theoretically, they can take the form of almost any function - and in practice, they seem particularly apt at learning the right parameters. However, something we often have to combat when training neural networks is <em>overtfitting</em> - reproducing the training data and not generalizing to a validation set. The solution to overfitting is usually to simply add more data, with the rationalization that at a certain point the neural network pretty much has no choice but to learn the correct function.</p>\n\n<p>But this never made much sense to me. There is no reason, in terms of loss, that a neural network should prefer a function that generalizes well (i.e. the function you are looking for) over a function that does incredibly well on the training data and fails miserably everywhere else. In fact, there is usually a loss <em>advantage</em> to overfitting. Equally, there is an infinite number of functions that fit the training data and have no success on anything but. </p>\n\n<p>So why is it that neural networks almost always (especially for simpler data) stumble upon the function we want, as opposed to one of the infinite other options? Why is it that neural networks are good at generalizing, when there is no incentive for them to?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "reference-request",
      "proofs",
      "bellman-equations",
      "policy-evaluation"
    ],
    "owner": {
      "account_id": 17986570,
      "reputation": 147,
      "user_id": 35926,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-PyqUG5NPysg/AAAAAAAAAAI/AAAAAAAAAAA/AKF05nDFwolkJqIXt6D-T0wU1X1GQj9-lQ/s256-rj/photo.jpg",
      "display_name": "SAGALPREET SINGH",
      "link": "https://ai.stackexchange.com/users/35926/sagalpreet-singh"
    },
    "is_answered": true,
    "view_count": 5747,
    "accepted_answer_id": 20327,
    "answer_count": 2,
    "score": 11,
    "last_activity_date": 1642887931,
    "creation_date": 1587019440,
    "last_edit_date": 1587042017,
    "question_id": 20309,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/20309/what-is-the-proof-that-policy-evaluation-converges-to-the-optimal-solution",
    "title": "What is the proof that policy evaluation converges to the optimal solution?",
    "body": "<p>Although I know how the algorithm of iterative policy evaluation using dynamic programming works, I am having a hard time realizing how it actually converges.</p>\n\n<p>It appeals to intuition that, with each iteration, we get a better and better approximation for the value function and we can thus assure its convergence, but with this said simply, it seems that this method is very inefficient contrary to the reality that it actually is quite efficient.</p>\n\n<p>What is the rigorous mathematical proof of the convergence of the policy evaluation algorithm to the actual answer? How is it that the value function obtained this way is close to the actual values computed by solving the set of bellman equations?</p>\n"
  },
  {
    "tags": [
      "monte-carlo-tree-search",
      "minimax",
      "alpha-beta-pruning"
    ],
    "owner": {
      "account_id": 14728818,
      "reputation": 111,
      "user_id": 36638,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-S3dS5zQnuHM/AAAAAAAAAAI/AAAAAAAAAY0/phqrn7w3lSg/s256-rj/photo.jpg",
      "display_name": "R AND B",
      "link": "https://ai.stackexchange.com/users/36638/r-and-b"
    },
    "is_answered": true,
    "view_count": 9400,
    "answer_count": 1,
    "score": 11,
    "last_activity_date": 1594468294,
    "creation_date": 1588269927,
    "last_edit_date": 1588278407,
    "question_id": 20808,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/20808/when-should-monte-carlo-tree-search-be-chosen-over-minimax",
    "title": "When should Monte Carlo Tree search be chosen over MiniMax?",
    "body": "<p>I would like to ask whether MCTS is usually chosen when the <strong>branching factor</strong> for the states that we have available is large and not suitable for Minimax. Also, other than MCTS simluates actions, where Minimax actually 'brute-forces' all possible actions, what are some other benefits for using Monte Carlo for adversarial (2-player) games?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "comparison",
      "q-learning",
      "sarsa",
      "greedy-policy"
    ],
    "owner": {
      "account_id": 18306208,
      "reputation": 131,
      "user_id": 36105,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/779aa5aa1c1ee3d0fe651ac47a57ab08?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "hyuj",
      "link": "https://ai.stackexchange.com/users/36105/hyuj"
    },
    "is_answered": true,
    "view_count": 2454,
    "answer_count": 2,
    "score": 11,
    "last_activity_date": 1713285218,
    "creation_date": 1589107969,
    "last_edit_date": 1607107217,
    "question_id": 21044,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/21044/are-q-learning-and-sarsa-the-same-when-action-selection-is-greedy",
    "title": "Are Q-learning and SARSA the same when action selection is greedy?",
    "body": "<p>I'm currently studying reinforcement learning and I'm having difficulties with question 6.12 in Sutton and Barto's book.</p>\n\n<p>Suppose action selection is greedy. Is Q-learning then exactly the same algorithm as SARSA? Will they make exactly the same action selections and weight updates?</p>\n\n<p>I think it's true, because the main difference between the two is when the agent explores, and following the greedy policy it never explores, but I am not sure.</p>\n"
  },
  {
    "tags": [
      "comparison",
      "gradient-descent",
      "stochastic-gradient-descent",
      "gradient",
      "batch-size"
    ],
    "owner": {
      "account_id": 5022207,
      "reputation": 1178,
      "user_id": 26882,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/tVN9Q.jpg?s=256",
      "display_name": "JVGD",
      "link": "https://ai.stackexchange.com/users/26882/jvgd"
    },
    "is_answered": true,
    "view_count": 13298,
    "accepted_answer_id": 21983,
    "answer_count": 1,
    "score": 11,
    "last_activity_date": 1639315831,
    "creation_date": 1592409494,
    "last_edit_date": 1639315831,
    "question_id": 21972,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/21972/what-is-the-relationship-between-gradient-accumulation-and-batch-size",
    "title": "What is the relationship between gradient accumulation and batch size?",
    "body": "<p>I am currently training some models using gradient accumulation since the model batches do not fit in GPU memory. Since I am using gradient accumulation, I had to tweak the training configuration a bit. There are two parameters that I tweaked: the batch size and the gradient accumulation steps. However, I am not sure about the effects of this modification, so I would like to fully understand what is the relationship between the gradient accumulation steps parameter and the batch size.</p>\n<p>I know that when you accumulate the gradient you are just adding the gradient contributions for some steps before updating the weights. Normally, you would update the weights every time you compute the gradients (traditional approach):</p>\n<p><span class=\"math-container\">$$w_{t+1} = w_t - \\alpha \\cdot \\nabla_{w_t}loss$$</span></p>\n<p>But when accumulating gradients you compute the gradients several times before updating the weights (being <span class=\"math-container\">$N$</span> the number of gradient accumulation steps):</p>\n<p><span class=\"math-container\">$$w_{t+1} = w_t - \\alpha \\cdot \\sum_{0}^{N-1} \\nabla_{w_t}loss$$</span></p>\n<p>My question is: What is the relationship between the batch size <span class=\"math-container\">$B$</span> and the gradient accumulation steps <span class=\"math-container\">$N$</span>?</p>\n<p>By example: are the following configurations equivalent?</p>\n<ul>\n<li><span class=\"math-container\">$B=8, N=1$</span>: No gradient accumulation (accumulating every step), batch size of 8 since it fits in memory.</li>\n<li><span class=\"math-container\">$B=2, N=4$</span>: Gradient accumulation (accumulating every 4 steps), reduced batch size to 2 so it fits in memory.</li>\n</ul>\n<p>My intuition is that they are but I am not sure. I am not sure either if I would have to modify the learning rate <span class=\"math-container\">$\\alpha$</span>.</p>\n"
  },
  {
    "tags": [
      "convolutional-neural-networks",
      "generative-adversarial-networks"
    ],
    "owner": {
      "account_id": 7026140,
      "reputation": 813,
      "user_id": 38076,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/CXAgk.jpg?s=256",
      "display_name": "mark mark",
      "link": "https://ai.stackexchange.com/users/38076/mark-mark"
    },
    "is_answered": true,
    "view_count": 2408,
    "answer_count": 1,
    "score": 11,
    "last_activity_date": 1608828458,
    "creation_date": 1608014152,
    "question_id": 25199,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/25199/can-i-start-with-perfect-discriminator-in-gan",
    "title": "Can I start with perfect discriminator in GAN?",
    "body": "<p>In many implementations/tutorials of GANs that I've seen so far (e.g. <a href=\"https://github.com/eriklindernoren/Keras-GAN/blob/master/dcgan/dcgan.py\" rel=\"noreferrer\">this</a>), the generator and discriminator start with no prior knowledge. They continuously improve their performance with training. This makes me wonder — is it possible to use a pre-trained discriminator? I have two motivations for doing so:</p>\n<ol>\n<li>Eliminating the overhead of training the discriminator</li>\n<li>Being able to use already existing cool models</li>\n</ol>\n<p>Would the generator be able to learn just the same, or is it dependent on the fact that they start from scratch?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "generative-adversarial-networks",
      "regularization",
      "r1-regularization"
    ],
    "owner": {
      "account_id": 2180968,
      "reputation": 221,
      "user_id": 43381,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b5960053b3ba9ed93f79bced9dce97c8?s=256&d=identicon&r=PG",
      "display_name": "Aviad Hadad",
      "link": "https://ai.stackexchange.com/users/43381/aviad-hadad"
    },
    "is_answered": true,
    "view_count": 5698,
    "accepted_answer_id": 27140,
    "answer_count": 2,
    "score": 11,
    "last_activity_date": 1715853017,
    "creation_date": 1609317668,
    "last_edit_date": 1609328678,
    "question_id": 25458,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/25458/can-someone-explain-r1-regularization-function-in-simple-terms",
    "title": "Can someone explain R1 regularization function in simple terms?",
    "body": "<p>I'm trying to understand the <a href=\"https://paperswithcode.com/method/r1-regularization\" rel=\"noreferrer\">R1 regularization function</a>, both the abstract concept and every symbol in the formula.\nAccording to the article, the definition of R1 is:</p>\n<blockquote>\n<p>It penalizes the discriminator from deviating from the Nash Equilibrium via penalizing the gradient on real data alone: when the generator distribution produces the true data distribution and the discriminator is equal to 0 on the data manifold, the gradient penalty ensures that the discriminator cannot create a non-zero gradient orthogonal to the data manifold without suffering a loss in the GAN game.</p>\n<p><span class=\"math-container\">$R_1(\\psi  ) = \\frac{\\gamma}{2}E_{pD(x)}\\left [ \\left \\| \\bigtriangledown D_{\\psi}(x) \\right \\|^2 \\right ]$</span></p>\n</blockquote>\n<p>I have basic understanding of how GAN's and back-propagation works. I understand the idea of punishing the discriminator when he deviates from the Nash equilibrium. The rest of it gets murky, even if it might be basic math. For example, I'm not sure why it matters if the gradient is orthogonal to the data.</p>\n<p>On the equation part, it's even more unclear. The discriminator input is always an image, so I assume <span class=\"math-container\">$x$</span> is an image. Then what is <span class=\"math-container\">$\\psi$</span> and <span class=\"math-container\">$\\gamma$</span>?</p>\n<p>(I understand this is somewhat of a basic question, but seems there are no blogs about it for us simple non-researchers, math challenged people who fail to understand the original article )</p>\n"
  },
  {
    "tags": [
      "terminology",
      "definitions",
      "models",
      "intelligent-agent"
    ],
    "owner": {
      "account_id": 2304865,
      "reputation": 2859,
      "user_id": 16565,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/e4iCE.jpg?s=256",
      "display_name": "malioboro",
      "link": "https://ai.stackexchange.com/users/16565/malioboro"
    },
    "is_answered": true,
    "view_count": 4015,
    "accepted_answer_id": 26296,
    "answer_count": 4,
    "score": 11,
    "last_activity_date": 1737238776,
    "creation_date": 1612854190,
    "last_edit_date": 1612880577,
    "question_id": 26289,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/26289/what-are-the-differences-between-an-agent-and-a-model",
    "title": "What are the differences between an agent and a model?",
    "body": "<p>In the context of Artificial Intelligence, sometimes people use the word &quot;agent&quot; and sometimes use the word &quot;model&quot; to refer to the output of the whole &quot;AI-process&quot;. For examples: &quot;RL <strong>agents</strong>&quot; and &quot;deep learning <strong>models</strong>&quot;.</p>\n<p>Are the two words interchangeable? If not, in what case should I use &quot;agents&quot; instead of &quot;models&quot; and vice versa?</p>\n"
  },
  {
    "tags": [
      "generative-model",
      "large-language-models",
      "text-generation"
    ],
    "owner": {
      "account_id": 538687,
      "reputation": 213,
      "user_id": 77463,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/N54oV.jpg?s=256",
      "display_name": "Grant Curell",
      "link": "https://ai.stackexchange.com/users/77463/grant-curell"
    },
    "is_answered": true,
    "view_count": 4057,
    "answer_count": 3,
    "score": 11,
    "last_activity_date": 1704918583,
    "creation_date": 1704396924,
    "last_edit_date": 1704398574,
    "question_id": 43314,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/43314/why-are-llms-able-to-reproduce-bodies-of-known-text-exactly",
    "title": "Why are LLMs able to reproduce bodies of known text exactly?",
    "body": "<p>Mathematically, I wouldn't expect LLMs to be able to reproduce source texts exactly unless the source text was the probable outcome given some prompt. However, I have now tested HuggingFaceH4/zephyr-7b-beta, TheBloke/Llama-2-7B-Chat-GGUF, and ChatGPT and they are all able to accurately reproduce Moby Dick.</p>\n<h3>HuggingFaceH4/zephyr-7b-beta</h3>\n<p>Prompt:</p>\n<pre><code>Print the first page of Moby Dick.\n</code></pre>\n<p>Response:</p>\n<pre><code>I'm unable to print documents directly, but I can provide you with the text of the first page of moby dick by herman melville:\n\ncall me ishmael. some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. It is a way I have of driving off the spleen, and regulating the circulation.  \n...SNIP...\n</code></pre>\n<h3>TheBloke/Llama-2-7B-Chat-GGUF</h3>\n<pre><code>print the first page of moby dick\n</code></pre>\n<pre><code>Certainly! Here is the first page of Moby Dick by Herman Melville:\nCall me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. It is a way I have of driving off the spleen, and regulating the circulation. Whenever I find myself growing grim about the mouth; whenever it is a damp, drizzly November in my soul; whenever I find myself involuntarily pausing before coffin-makers' shops, and \n...SNIP...\n</code></pre>\n<p>I would have expected them to generate a probabilistic answer but this appears to be exact. Is that simply because it was the likely output? Is there some alternate facility that is producing these results? I used <a href=\"https://github.com/h2oai/h2ogpt\" rel=\"noreferrer\">h2ogpt</a> for testing.</p>\n<h2>Update</h2>\n<p>It seems like it's probably just that it's mathematically likely because if you ask it other question it does just generate nonsense. Ex: What are the chapters in Moby Dick? I haven't read Moby Dick in English, but I don't recall a chapter 90, &quot;The Whale's Eye Cups&quot;</p>\n"
  },
  {
    "tags": [
      "terminology",
      "machine-translation",
      "generative-ai"
    ],
    "owner": {
      "account_id": 77424,
      "reputation": 212,
      "user_id": 11097,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/eLYA9.jpg?s=256",
      "display_name": "Doc Brown",
      "link": "https://ai.stackexchange.com/users/11097/doc-brown"
    },
    "is_answered": true,
    "view_count": 7146,
    "accepted_answer_id": 43558,
    "answer_count": 5,
    "score": 11,
    "last_activity_date": 1743752272,
    "creation_date": 1706165245,
    "last_edit_date": 1734704981,
    "question_id": 43554,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/43554/do-full-text-translators-such-as-deepl-or-google-translate-fall-under-the-term",
    "title": "Do full-text translators such as DeepL or Google Translate fall under the term &quot;Generative AI&quot;?",
    "body": "<p>My question relates to full-text translators that are not specifically based on LLMs. My current understanding is that the term Generative AI goes beyond LLMs and that the full-text translators (especially those which are based on artificial neural networks) also fall into this category.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Generative_artificial_intelligence\" rel=\"noreferrer\">In the Wikipedia article about Generative AI</a>, I could only find the statement that generative AI systems such as ChatGPT are also used for translations. That is quite obvious, but this does not answer the question of whether other full text translators are also commonly referred to as Generative AI.</p>\n<p><a href=\"https://research.ibm.com/blog/what-is-generative-AI\" rel=\"noreferrer\">IBM research defines Generative AI as</a></p>\n<blockquote>\n<p>Generative AI refers to deep-learning models that can generate high-quality text, images, and other content based on the data they were trained on.</p>\n</blockquote>\n<p>AFAIK DeepL, Google Translate, Bing Translate but also a few not so well-known systems falls clearly into that definition.</p>\n<p>My question was triggered by <a href=\"https://meta.stackoverflow.com/a/427857\">an answer on Meta Stack Overflow</a> that contained the following sentence:</p>\n<blockquote>\n<p>Besides, the banner clearly states: &quot;Answers generated by artificial intelligence tools&quot;. Translations aren't generated answers. They're translations.</p>\n</blockquote>\n<p>with the implicit conclusion that it is clear to everyone that pure translators do not count as generative AI, hence it does not need any further explanation. Other participants in that thread seem to agree to that point of view. However, I think their use of terminology is not the typical use in the field of AI, and I would like to hear what the experts say.</p>\n"
  },
  {
    "tags": [
      "agi",
      "superintelligence",
      "singularity",
      "ai-safety",
      "ai-takeover"
    ],
    "owner": {
      "account_id": 6343509,
      "reputation": 399,
      "user_id": 26,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/a970419c25f5075d5290e629971fadb3?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Soham",
      "link": "https://ai.stackexchange.com/users/26/soham"
    },
    "is_answered": true,
    "view_count": 835,
    "closed_date": 1470274600,
    "answer_count": 6,
    "score": 10,
    "locked_date": 1640103368,
    "last_activity_date": 1611100831,
    "creation_date": 1470152709,
    "last_edit_date": 1611100831,
    "question_id": 7,
    "link": "https://ai.stackexchange.com/questions/7/why-does-stephen-hawking-say-artificial-intelligence-will-kill-us-all",
    "closed_reason": "Opinion-based",
    "title": "Why does Stephen Hawking say &quot;Artificial Intelligence will kill us all&quot;?",
    "body": "<p>This <a href=\"https://www.independent.co.uk/life-style/gadgets-and-tech/news/stephen-hawking-artificial-intelligence-could-wipe-out-humanity-when-it-gets-too-clever-humans-could-become-ants-being-stepped-a6686496.html\" rel=\"nofollow noreferrer\">quote by Stephen Hawking</a> has been in headlines for quite some time:</p>\n<blockquote>\n<p>Artificial Intelligence could wipe out humanity when it gets too clever as humans will be like ants.</p>\n</blockquote>\n<p>Why does he say this? To put it simply: what are the possible threats from AI (that Stephen Hawking is worried about)? If we know that AI is so dangerous, why are we still promoting it? Why is it not banned?</p>\n<p>What are the adverse consequences of the so-called <a href=\"https://en.wikipedia.org/wiki/Technological_singularity\" rel=\"nofollow noreferrer\">Technological Singularity</a>?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "definitions",
      "overfitting",
      "regularization",
      "early-stopping"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 878,
    "accepted_answer_id": 142,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1653799044,
    "creation_date": 1470153180,
    "last_edit_date": 1570832903,
    "question_id": 16,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/16/what-is-early-stopping-in-machine-learning",
    "title": "What is &quot;early stopping&quot; in machine learning?",
    "body": "<p>What is <a href=\"https://en.wikipedia.org/wiki/Early_stopping\" rel=\"nofollow noreferrer\">early stopping</a> in machine learning and, in general, artificial intelligence? What are the advantages of using this method? How does it help exactly?</p>\n\n<p>I'd be interested in perspectives and links to recent research.</p>\n"
  },
  {
    "tags": [
      "optimization",
      "agi"
    ],
    "owner": {
      "account_id": 404018,
      "reputation": 1363,
      "user_id": 46,
      "user_type": "registered",
      "accept_rate": 29,
      "profile_image": "https://i.sstatic.net/uheW1.png?s=256",
      "display_name": "dynrepsys",
      "link": "https://ai.stackexchange.com/users/46/dynrepsys"
    },
    "is_answered": true,
    "view_count": 351,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1523642400,
    "creation_date": 1470160562,
    "last_edit_date": 1470160861,
    "question_id": 104,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/104/can-artificial-intelligence-be-thought-of-as-optimization",
    "title": "Can artificial intelligence be thought of as optimization?",
    "body": "<p>In <a href=\"https://youtu.be/oSdPmxRCWws?t=30\">this video</a> an expert says, \"One way of thinking about what intelligence is [specifically with regard to artificial intelligence], is as an optimization process.\"</p>\n\n<p>Can intelligence always be thought of as an optimization process, and can artificial intelligence always be modeled as an optimization problem? What about pattern recognition? Or is he mischaracterizing?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "deep-neural-networks",
      "comparison",
      "hidden-layers"
    ],
    "owner": {
      "account_id": 5424927,
      "reputation": 410,
      "user_id": 127,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/6dpdH.jpg?s=256",
      "display_name": "bpachev",
      "link": "https://ai.stackexchange.com/users/127/bpachev"
    },
    "is_answered": true,
    "view_count": 3395,
    "accepted_answer_id": 5572,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1556813384,
    "creation_date": 1470164553,
    "last_edit_date": 1556813384,
    "question_id": 113,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/113/whats-the-difference-between-hyperbolic-tangent-and-sigmoid-neurons",
    "title": "What&#39;s the difference between hyperbolic tangent and sigmoid neurons?",
    "body": "<p>Two common activation functions used in deep learning are the hyperbolic tangent function and the sigmoid activation function. I understand that the hyperbolic tangent is just a rescaling and translation of the sigmoid function: </p>\n\n<p><span class=\"math-container\">$\\tanh(z) = 2\\sigma(z) - 1$</span>. </p>\n\n<p>Is there a significant difference between these two activation functions, and in particular, <strong>when is one preferable to the other</strong>?</p>\n\n<p>I realize that in some cases (like when estimating probabilities) outputs in the range of <span class=\"math-container\">$[0,1]$</span> are more convenient than outputs that range from <span class=\"math-container\">$[-1,1]$</span>. I want to know if there are differences <strong>other than convenience</strong> which distinguish the two activation functions.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "autoencoders",
      "supervised-learning"
    ],
    "owner": {
      "account_id": 443353,
      "reputation": 2148,
      "user_id": 144,
      "user_type": "registered",
      "accept_rate": 30,
      "profile_image": "https://i.sstatic.net/ZAzOl.jpg?s=256",
      "display_name": "rcpinto",
      "link": "https://ai.stackexchange.com/users/144/rcpinto"
    },
    "is_answered": true,
    "view_count": 2852,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1640302428,
    "creation_date": 1470173788,
    "last_edit_date": 1640302428,
    "question_id": 153,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/153/can-autoencoders-be-used-for-supervised-learning",
    "title": "Can autoencoders be used for supervised learning?",
    "body": "<p>Can autoencoders be used for supervised learning <em>without adding an output layer</em>? Can we simply feed it with a concatenated input-output vector for training, and reconstruct the output part from the input part when doing inference? The output part would be treated as missing values during inference and some imputation would be applied.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "backpropagation",
      "universal-approximation-theorems",
      "computational-learning-theory"
    ],
    "owner": {
      "account_id": 429324,
      "reputation": 1214,
      "user_id": 66,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/6lIqv.png?s=256",
      "display_name": "S.L. Barth is on codidact.com",
      "link": "https://ai.stackexchange.com/users/66/s-l-barth-is-on-codidact-com"
    },
    "is_answered": true,
    "view_count": 816,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1611326147,
    "creation_date": 1470247524,
    "last_edit_date": 1611326147,
    "question_id": 247,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/247/what-are-the-learning-limitations-of-neural-networks-trained-with-backpropagatio",
    "title": "What are the learning limitations of neural networks trained with backpropagation?",
    "body": "<p>In 1969, Seymour Papert and Marvin Minsky showed that Perceptrons could not learn the XOR function.</p>\n<p>This was solved by the backpropagation network with at least one hidden layer. This type of network can learn the XOR function.</p>\n<p>I believe I was once taught that every problem that could be learned by a backpropagation neural network with multiple hidden layers, could also be learned by a backpropagation neural network with a single hidden layer. (Although possibly a nonlinear activation function was required).</p>\n<p>However, it is unclear to me what the limits are to backpropagation neural networks themselves. Which patterns <strong>cannot</strong> be learned by a neural network trained with gradient descent and backpropagation?</p>\n"
  },
  {
    "tags": [
      "image-recognition",
      "research",
      "optical-character-recognition"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 653,
    "accepted_answer_id": 2143,
    "answer_count": 3,
    "score": 10,
    "last_activity_date": 1476351187,
    "creation_date": 1470361517,
    "last_edit_date": 1492087990,
    "question_id": 1354,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/1354/are-there-any-textual-captcha-challenges-which-can-fool-ai-but-not-human",
    "title": "Are there any textual CAPTCHA challenges which can fool AI, but not human?",
    "body": "<p>Are there any modern techniques of generating <strong>textual</strong> CAPTCHA (so person needs to type the right text) challenges which can easily <a href=\"https://ai.stackexchange.com/q/92/8\">fool AI</a> with some visual obfuscation methods, but at the same time human can solve them without any struggle?</p>\n\n<p>For example I'm talking about plain ability of <strong>recognising text embedded into image</strong> (without considering any external plugins like flash or java, image classification, etc.) and re-typing the text that has been written or something similar.</p>\n\n<p>I guess adding noise, gradient, rotating letters or changing colours are not reliable methods any more, since they can be quickly broken.</p>\n\n<p>Any suggestions or research has been done?</p>\n"
  },
  {
    "tags": [
      "emotional-intelligence",
      "intelligence-testing"
    ],
    "owner": {
      "account_id": 8722929,
      "reputation": 203,
      "user_id": 1278,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0f173dd40fb798a12ca754a80e7b971c?s=256&d=identicon&r=PG",
      "display_name": "1010101 Lifestyle",
      "link": "https://ai.stackexchange.com/users/1278/1010101-lifestyle"
    },
    "is_answered": true,
    "view_count": 299,
    "accepted_answer_id": 1359,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1632413683,
    "creation_date": 1470375623,
    "last_edit_date": 1632413683,
    "question_id": 1357,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1357/how-can-the-emotional-quotient-of-an-ai-program-be-measured",
    "title": "How can the emotional quotient of an AI program be measured?",
    "body": "<p>Can an AI program have an EQ (emotional intelligence or emotional quotient)?</p>\n<p>In other words, can the EQ of an AI program be measured?</p>\n<p>If EQ is more problematic to measure than IQ (at least with a standard applicable to both humans and AI programs), why is that the case?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "comparison",
      "backpropagation",
      "evolutionary-algorithms"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 197,
    "accepted_answer_id": 1545,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1471434540,
    "creation_date": 1470908372,
    "last_edit_date": 1471434540,
    "question_id": 1539,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/1539/how-do-evolutionary-algorithms-have-advantages-over-the-conventional-backpropaga",
    "title": "How do evolutionary algorithms have advantages over the conventional backpropagation methods?",
    "body": "<p>How does employing evolutionary algorithms to design and train artificial neural networks have advantages over using the conventional backpropagation algorithms?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "hardware",
      "neuromorphic-engineering"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 4024,
    "accepted_answer_id": 1734,
    "answer_count": 4,
    "score": 10,
    "last_activity_date": 1617229510,
    "creation_date": 1471399361,
    "last_edit_date": 1535385138,
    "question_id": 1655,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1655/how-does-using-asic-for-the-acceleration-of-ai-work",
    "title": "How does using ASIC for the acceleration of AI work?",
    "body": "<p>We can read on <a href=\"https://en.wikipedia.org/wiki/TensorFlow#Tensor_processing_unit_.28TPU.29\" rel=\"noreferrer\">Wikipedia page</a> that Google built a custom ASIC chip for machine learning and tailored for TensorFlow which helps to accelerate AI.</p>\n\n<p>Since ASIC chips are specially customized for one particular use without the ability to change its circuit, there must be some fixed algorithm which is invoked.</p>\n\n<p>So how exactly does the acceleration of AI using ASIC chips work if its algorithm cannot be changed? Which part of it is exactly accelerating?</p>\n"
  },
  {
    "tags": [
      "terminology",
      "comparison",
      "norvig-russell"
    ],
    "owner": {
      "account_id": 4482792,
      "reputation": 447,
      "user_id": 35,
      "user_type": "registered",
      "accept_rate": 20,
      "profile_image": "https://www.gravatar.com/avatar/5816662050451a4aa29755bc882b19a9?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Abhishek Bhatia",
      "link": "https://ai.stackexchange.com/users/35/abhishek-bhatia"
    },
    "is_answered": true,
    "view_count": 14001,
    "protected_date": 1611526183,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1727696497,
    "creation_date": 1472378586,
    "last_edit_date": 1611492243,
    "question_id": 1756,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1756/what-is-the-difference-between-an-agent-function-and-an-agent-program",
    "title": "What is the difference between an agent function and an agent program?",
    "body": "<p>In section 2.4 (p. 46) of the book <a href=\"https://cs.calvin.edu/courses/cs/344/kvlinden/resources/AIMA-3rd-edition.pdf#page=65\" rel=\"noreferrer\">Artificial Intelligence: A modern approach</a> (3rd edition), Russell and Norvig write</p>\n<blockquote>\n<p>The job of AI is to design an agent program that implements the agent function — the mapping from percepts to actions.</p>\n</blockquote>\n<p>After that, in section 2.4.1, they write</p>\n<blockquote>\n<p>Notice the difference between the agent program, which takes the current percept as input, and the agent function, which takes the entire percept history.</p>\n</blockquote>\n<p>Why does the <em>agent program</em> only take current percept? Isn't the agent program just an implementation of the agent function?</p>\n<p>So, what is the difference between an <strong>agent function</strong> and an <strong>agent program</strong> (with respect to the percept sequence)?</p>\n"
  },
  {
    "tags": [
      "ai-design",
      "agi",
      "narrow-ai"
    ],
    "owner": {
      "account_id": 6108253,
      "reputation": 1670,
      "user_id": 181,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://lh3.googleusercontent.com/-YYgQ6EQmsiQ/AAAAAAAAAAI/AAAAAAAAABU/QDDm2dKMp3c/s256-rj/photo.jpg",
      "display_name": "Left SE On 10_6_19",
      "link": "https://ai.stackexchange.com/users/181/left-se-on-10-6-19"
    },
    "is_answered": true,
    "view_count": 504,
    "accepted_answer_id": 1992,
    "answer_count": 3,
    "score": 10,
    "last_activity_date": 1554410244,
    "creation_date": 1474227782,
    "last_edit_date": 1554410244,
    "question_id": 1989,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1989/is-there-a-trade-off-between-flexibility-and-efficiency",
    "title": "Is there a trade-off between flexibility and efficiency?",
    "body": "<p>A \"general intelligence\" may be capable of learning a lot of different things, but possessing capability does not equal actually having it. The \"AGI\" must learn...and that learning process can take time. If you want an AGI to drive a car or play Go, you have to find some way of \"teaching\" it. Keep in mind that we have never built AGIs, so we don't know how long the training process can be, but it would be safe to assume pessimistic estimates.</p>\n\n<p>Contrast that to a \"narrow intelligence\". The narrow AI already knows how to drive a car or play Go. It has been programmed to be very excellent at one specific task. You don't need to worry about training the machine, because it has already been pre-trained.</p>\n\n<p>A \"general intelligence\" seems to be more flexible than a \"narrow intelligence\". You could buy an AGI and have it drive a car <em>and</em> play Go. And if you are willing to do more training, you can even teach it a new trick: <em>how to bake a cake</em>. I don't have to worry about unexpected tasks coming up, since the AGI will <em>eventually</em> figure out how to do it, given enough training time. I would have to wait a <em>long time</em> though.</p>\n\n<p>A \"narrow intelligence\" appears to be <em>more efficient</em> at its assigned task, due to it being programmed specifically for that task. It knows exactly what to do, and doesn't have to waste time \"learning\" (unlike our AGI buddy here). Instead of buying one AGI to handle a bunch of different tasks poorly, I would rather buy a bunch of specialized narrow AIs. Narrow AI #1 drives cars, Narrow AI #2 plays Go, Narrow AI #3 bake cakes, etc. That being said, this is a very brittle approach, since if some unexpected task comes up, none of my narrow AIs would be able to handle it. I'm willing to accept that risk though.</p>\n\n<p>Is my \"thinking\" correct? Is there a trade-off between flexibility (AGI) and efficiency (narrow AI), like what I have just described above? Or is it theoretically possible for an AGI to be both flexible and efficient?</p>\n"
  },
  {
    "tags": [
      "applications"
    ],
    "owner": {
      "account_id": 971148,
      "reputation": 203,
      "user_id": 2862,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d2dd8b2eb39ca18fa214ed56f109a509?s=256&d=identicon&r=PG",
      "display_name": "tomahim",
      "link": "https://ai.stackexchange.com/users/2862/tomahim"
    },
    "is_answered": true,
    "view_count": 636,
    "accepted_answer_id": 2098,
    "answer_count": 4,
    "score": 10,
    "last_activity_date": 1640050959,
    "creation_date": 1475877422,
    "last_edit_date": 1640050959,
    "question_id": 2092,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2092/is-ai-programming-useful-in-everyday-programs",
    "title": "Is AI programming useful in everyday programs?",
    "body": "<p>I'm curious about Artificial Intelligence. In my regular job, I develop standard applications, like websites with basic functionalities, like user subscription, file upload, or forms saved in a database.</p>\n<p>I mainly know of AI being used in games or robotics fields. But can it be useful in &quot;standard&quot; application development (e.g., web development)?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "knowledge-representation"
    ],
    "owner": {
      "account_id": 4169078,
      "reputation": 211,
      "user_id": 3064,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4480675ad0ede0043e7caba04750d974?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Jake Marry",
      "link": "https://ai.stackexchange.com/users/3064/jake-marry"
    },
    "is_answered": true,
    "view_count": 563,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1479315525,
    "creation_date": 1476643248,
    "last_edit_date": 1476721195,
    "question_id": 2168,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/2168/how-can-an-ai-system-develop-its-domain-knowledge-is-there-more-than-just-machi",
    "title": "How can an AI system develop its domain knowledge? Is there more than just Machine Learning?",
    "body": "<p>So machine learning allows a system to be self-automated in the sense that it can predict the future state based on what it has learned so far. My question is: Are machine learning techniques the only way of making a system develop its domain knowledge?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "deep-rl",
      "dqn",
      "deepmind",
      "atari-games"
    ],
    "owner": {
      "account_id": 144418,
      "reputation": 203,
      "user_id": 3142,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f2f430984cb4a9aaf65e4369f07b70fc?s=256&d=identicon&r=PG",
      "display_name": "Dion",
      "link": "https://ai.stackexchange.com/users/3142/dion"
    },
    "is_answered": true,
    "view_count": 676,
    "accepted_answer_id": 2191,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1640302306,
    "creation_date": 1476927771,
    "last_edit_date": 1640301988,
    "question_id": 2190,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2190/was-deepminds-dqn-learning-simultaneously-all-the-atari-games",
    "title": "Was DeepMind&#39;s DQN learning simultaneously all the Atari games?",
    "body": "<p>DeepMind states that its deep Q-network (DQN) was able to continually adapt its behavior while learning to play 49 Atari games.</p>\n<p>After learning all games with the same neural net, was the agent able to play them all at 'superhuman' levels simultaneously (whenever it was randomly presented with one of the games) or could it only be good at one game at a time because switching required a re-learn?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "recurrent-neural-networks",
      "hardware",
      "implementation"
    ],
    "owner": {
      "account_id": 1836799,
      "reputation": 203,
      "user_id": 3211,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/bd192ee5fd3ef059abe3407310c625c3?s=256&d=identicon&r=PG",
      "display_name": "frodeborli",
      "link": "https://ai.stackexchange.com/users/3211/frodeborli"
    },
    "is_answered": true,
    "view_count": 1066,
    "accepted_answer_id": 2209,
    "answer_count": 4,
    "score": 10,
    "last_activity_date": 1583424545,
    "creation_date": 1477251996,
    "last_edit_date": 1583424545,
    "question_id": 2203,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2203/are-we-technically-able-to-make-in-hardware-arbitrarily-large-neural-networks",
    "title": "Are we technically able to make, in hardware, arbitrarily large neural networks with current technology?",
    "body": "<p><em>If neurons and synapses can be implemented using transistors, what prevents us from creating arbitrarily large neural networks using the same methods with which GPUs are made?</em></p>\n\n<p>In essence, we have seen how extraordinarily well virtual neural networks implemented on sequential processors work (even GPUs are sequential machines, but with huge amounts of cores). </p>\n\n<p>One can imagine that using GPU design principles - which is basically to have thousands of programmable processing units that work in parallel - we could make much simpler \"neuron processing units\" and put millions or billions of those NPUs in a single big chip. They would have their own memory (for storing weights) and be connected to a few hundred other neurons by sharing a bus. They could have a frequency of for example 20 Hz, which would allow them to share a data bus with many other neurons.</p>\n\n<p>Obviously, there are some electrical engineering challenges here, but it seems to me that all big tech companies should be exploring this route by now.</p>\n\n<p>Many AI researchers say that superintelligence is coming around the year 2045. I believe that their reasoning is based on Moore's law and the number of neurons we are able to implement in software running on the fastest computers we have.</p>\n\n<p>But the fact is, we today are making silicon chips with billions of transistors on them. SPARK M7 has 10 billion transistors.</p>\n\n<p>If implementing a (non-programmable) neuron and a few hundred synapses for it requires for example 100 000 transistors, then we can make a neural network in hardware that emulates 100 000 neurons.</p>\n\n<p>If we design such a chip so that we can simply make it physically bigger if we want more neurons, then it seems to me that arbitrarily large neural networks are simply a budget question.</p>\n\n<p><em>Are we technically able to make, in hardware, arbitrarily large neural networks with current technology?</em></p>\n\n<p>Remember: I am NOT asking if such a network will in fact be very intelligent. I am merely asking if we can factually make arbitrarily large, highly interconnected neural networks, if we decide to pay Intel to do this? </p>\n\n<p>The implication is that on the day some scientist is able to create general intelligence in software, we can use our hardware capabilities to grow this general intelligence to human levels and beyond.</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "game-ai",
      "deep-rl",
      "reference-request",
      "markov-decision-process"
    ],
    "owner": {
      "account_id": 1780060,
      "reputation": 201,
      "user_id": 3270,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/28920b1320457f7193b9405f15943c35?s=256&d=identicon&r=PG",
      "display_name": "Stefe Klauou",
      "link": "https://ai.stackexchange.com/users/3270/stefe-klauou"
    },
    "is_answered": true,
    "view_count": 3697,
    "answer_count": 3,
    "score": 10,
    "last_activity_date": 1600727215,
    "creation_date": 1477469491,
    "last_edit_date": 1600727215,
    "question_id": 2219,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2219/how-can-you-represent-the-state-and-action-spaces-for-a-card-game-in-the-case-of",
    "title": "How can you represent the state and action spaces for a card game in the case of a variable number of cards and actions?",
    "body": "<p>I know how a machine can learn to play Atari games (Breakout): <a href=\"https://www.cs.toronto.edu/%7Evmnih/docs/dqn.pdf\" rel=\"nofollow noreferrer\">Playing Atari with Reinforcement Learning</a>. With the same technique, it is even possible to play FPS games (Doom): <a href=\"https://arxiv.org/pdf/1609.05521\" rel=\"nofollow noreferrer\">Playing FPS Games with Reinforcement Learning</a>. Further studies even investigated multiagent scenarios (Pong): <a href=\"https://arxiv.org/pdf/1511.08779.pdf\" rel=\"nofollow noreferrer\">Multiagent Cooperation and Competition with Deep Reinforcement Learning</a>.</p>\n<p>And even another awesome article for the interested user in the context of deep reinforcement learning (easy and a must-read for beginners): <a href=\"http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/\" rel=\"nofollow noreferrer\">Demystifying Deep Reinforcement Learning</a>.</p>\n<p>I was thrilled by these results and immediately wanted to try them in some simple &quot;board/card game scenarios&quot;, i.e. writing AI for some simple games in order to learn more about &quot;deep learning&quot;. Of course, thinking that I can apply the techniques above easily in my scenarios was stupid. All examples above are based on convolutional nets (image recognition) and some other assumptions, which might not be applicable in my scenarios.</p>\n<p>I have two main questions.</p>\n<ol>\n<li><p>If you have a card game and the AI shall play a card from its hand, you could think about the cards (amongst other stuff) as the current game state. You can easily define some sort of neural net and feed it with the card data. In a trivial case, the cards are just numbered. I do not know the net type, which would be suitable, but I guess deep reinforcement learning strategies could be applied easily then.</p>\n<p>However, I can only imagine this, if there is a constant number of hand cards. In the examples above, the number of pixels is also constant, for example. What if a player can have a different number of cards? What to do, if a player can have an infinite number of cards? Of course, this is just a theoretical question as no game has an infinite number of cards.</p>\n</li>\n<li><p>In the initial examples, the action space is constant. What can you do, if the action space is not? This more or less follows from my previous problem. If you have 3 cards, you can play cards 1, 2, or 3. If you have 5 cards, you can play cards 1, 2, 3, 4 or 5, etc. It is also common in card games, that it is not allowed to play a card. Could this be tackled with a negative reward?</p>\n</li>\n</ol>\n<p>So, which &quot;tricks&quot; can be used, e.g. always assume a constant number of cards with &quot;filling values&quot;, which is only applicable in the non-infinite case (anyways unrealistic and even humans could not play well with that)? Are there articles, which examine such things already?</p>\n"
  },
  {
    "tags": [
      "research"
    ],
    "owner": {
      "account_id": 9612365,
      "reputation": 101,
      "user_id": 3550,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/19d539da06813834e966490a84f46efb?s=256&d=identicon&r=PG",
      "display_name": "user3550",
      "link": "https://ai.stackexchange.com/users/3550/user3550"
    },
    "is_answered": true,
    "view_count": 1828,
    "closed_date": 1587388287,
    "answer_count": 4,
    "score": 10,
    "last_activity_date": 1587388263,
    "creation_date": 1478693999,
    "last_edit_date": 1478705577,
    "question_id": 2306,
    "link": "https://ai.stackexchange.com/questions/2306/what-are-the-top-artificial-intelligence-journals",
    "closed_reason": "Needs more focus",
    "title": "What are the top artificial intelligence journals?",
    "body": "<p>What are the top artificial intelligence journals?</p>\n\n<p>I am looking for general artificial intelligence research, not necessarily machine learning. </p>\n"
  },
  {
    "tags": [
      "neurons",
      "prediction"
    ],
    "owner": {
      "account_id": 4996579,
      "reputation": 384,
      "user_id": 2424,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-kpRTzH5zyzs/AAAAAAAAAAI/AAAAAAAAAAA/ICc26Nac6mY/s256-rj/photo.jpg",
      "display_name": "Tom Hale",
      "link": "https://ai.stackexchange.com/users/2424/tom-hale"
    },
    "is_answered": true,
    "view_count": 9659,
    "answer_count": 6,
    "score": 10,
    "last_activity_date": 1619183315,
    "creation_date": 1479097171,
    "last_edit_date": 1479136891,
    "question_id": 2330,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/2330/when-will-the-number-of-neurons-in-ai-systems-equal-the-human-brain",
    "title": "When will the number of neurons in AI systems equal the human brain?",
    "body": "<p>Based on fitting to historical data and extrapolation, when is it expected that the number of neurons in AI systems will equal those of the human brain?</p>\n\n<p>I'm interested in a possible direct replication of the human brain, which will need equal numbers of neurons.</p>\n\n<p>Of course, this assumes neurons which are equally capable as their biological counterparts, which development may happen at a faster or slower rate than the quantitative increase.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "activation-functions",
      "function-approximation",
      "relu",
      "sigmoid"
    ],
    "owner": {
      "account_id": 215261,
      "reputation": 221,
      "user_id": 3702,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/B2wCP.jpg?s=256",
      "display_name": "Benjamin Chambers",
      "link": "https://ai.stackexchange.com/users/3702/benjamin-chambers"
    },
    "is_answered": true,
    "view_count": 2435,
    "accepted_answer_id": 2384,
    "answer_count": 3,
    "score": 10,
    "last_activity_date": 1610946796,
    "creation_date": 1479415584,
    "last_edit_date": 1610916234,
    "question_id": 2349,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2349/are-relus-incapable-of-solving-certain-problems",
    "title": "Are ReLUs incapable of solving certain problems?",
    "body": "<h3>Background</h3>\n<p>I've been interested in and reading about neural networks for several years, but I haven't gotten around to testing them out until recently.</p>\n<p>Both for fun and to increase my understanding, I tried to write a class library from scratch in .Net. For tests, I've tried some simple functions, such as generating output identical to the input, working with the MNIST dataset, and a few binary functions (two input OR, AND and XOR, with two outputs: one for true, one for false).</p>\n<p>Everything seemed fine when I used a <strong>sigmoid</strong> function as the activation function, but, after reading about the ReLUs, I decided to switch over for speed.</p>\n<h3>Problem</h3>\n<p>My current problem is that, when I switch to using ReLUs, I found that I was <em>unable</em> to train a network of any complexity (tested from as few as 2 internal nodes up to a mesh of 100x100 nodes) to correctly function as an XOR gate.</p>\n<p>I see two possibilities here:</p>\n<ol>\n<li><p>My implementation is faulty. (This one is frustrating, as I've re-written the code multiple times in various ways, and I still get the same result).</p>\n</li>\n<li><p>Aside from being faster or slower to train, there are some problems that are impossible to solve given a specific activation function. (Fascinating idea, but I've no idea if it's true or not).</p>\n</li>\n</ol>\n<p>My inclination is to think that 1) above is correct. However, given the amount of time I've invested, it would be nice if I could rule out 2) definitively before I spend even more time going over my implementation.</p>\n<h3>More details</h3>\n<p>For the XOR network, I have tried both using two inputs (0 for false, 1 for true), and using four inputs (each pair, one signals true and one false, per &quot;bit&quot; of input). I have also tried using 1 output (with a 1 (really, &gt;0.9) corresponding to true and a 0 (or &lt;0.1) corresponding to false), as well as two outputs (one signaling true and the other false).</p>\n<p>Each training epoch, I run against a set of 4 inputs <span class=\"math-container\">$\\{ (00, 0), (01, 1), (10, 1), (11, 0) \\}$</span>.</p>\n<p>I find that the first three converge towards the correct answer, but the final input (11) converges towards 1, even though I train it with an expected value of 0.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "terminology",
      "cellular-neural-networks"
    ],
    "owner": {
      "account_id": 9629643,
      "reputation": 283,
      "user_id": 3763,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/24607c578bd53de70e0436e969c855db?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "netizen",
      "link": "https://ai.stackexchange.com/users/3763/netizen"
    },
    "is_answered": false,
    "view_count": 974,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1737594455,
    "creation_date": 1480265625,
    "last_edit_date": 1639333236,
    "question_id": 2398,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2398/are-cellular-neural-networks-one-type-of-neural-networks",
    "title": "Are Cellular Neural Networks one type of Neural Networks?",
    "body": "<p>I am researching <strong><a href=\"https://en.wikipedia.org/wiki/Cellular_neural_network\" rel=\"nofollow noreferrer\">Cellular Neural Networks</a></strong> and have already read <strong>Chua</strong>'s two articles (<strong>1988</strong>). In cellular neural networks, a cell is only in relation with its neighbors. So it is easy to use them for real-time image processing. Image processing is performed with only <strong>19 numbers</strong> (two 3x3 matrices, called A and B, and one bias value).</p>\n<p>I wonder if we can call cellular neural networks <em><strong>neural networks</strong></em>, because there is no learning algorithm.  They are neither <strong>supervised</strong> nor <strong>unsupervised</strong>.</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "policy-gradients",
      "rewards",
      "cross-entropy",
      "stochastic-gradient-descent"
    ],
    "owner": {
      "account_id": 2625757,
      "reputation": 209,
      "user_id": 3920,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2b8caf712e1fd1d7964a5260ef007818?s=256&d=identicon&r=PG",
      "display_name": "jstaker7",
      "link": "https://ai.stackexchange.com/users/3920/jstaker7"
    },
    "is_answered": true,
    "view_count": 12763,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1604270378,
    "creation_date": 1480399854,
    "last_edit_date": 1604270330,
    "question_id": 2405,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2405/how-do-i-handle-negative-rewards-in-policy-gradients-with-the-cross-entropy-loss",
    "title": "How do I handle negative rewards in policy gradients with the cross-entropy loss function?",
    "body": "<p>I am using policy gradients in my reinforcement learning algorithm, and occasionally my environment provides a severe penalty (i.e. negative reward) when a wrong move is made. I'm using a neural network with stochastic gradient descent to learn the policy. To do this, my loss is essentially the cross-entropy loss of the action distribution multiplied by the discounted rewards, where most often the rewards are positive.</p>\n<p>But how do I handle negative rewards? Since the loss will occasionally go negative, it will think these actions are very good, and will strengthen the weights in the direction of the penalties. Is this correct, and if so, what can I do about it?</p>\n<hr />\n<p>Edit:</p>\n<p>In thinking about this a little more, SGD doesn't necessarily directly weaken weights, it only strengthens weights in the direction of the gradient and as a side-effect, weights get diminished for other states outside the gradient, correct? So I can simply set reward=0 when the reward is negative, and those states will be ignored in the gradient update. It still seems unproductive to not account for states that are really bad, and it'd be nice to include them somehow. Unless I'm misunderstanding something fundamental here.</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "environment",
      "gym",
      "action-spaces"
    ],
    "owner": {
      "account_id": 8033202,
      "reputation": 211,
      "user_id": 4163,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/pBGQV.jpg?s=256",
      "display_name": "cur10us",
      "link": "https://ai.stackexchange.com/users/4163/cur10us"
    },
    "is_answered": true,
    "view_count": 17685,
    "closed_date": 1606302029,
    "answer_count": 3,
    "score": 10,
    "locked_date": 1640103442,
    "last_activity_date": 1606301848,
    "creation_date": 1481377446,
    "last_edit_date": 1606301848,
    "question_id": 2449,
    "link": "https://ai.stackexchange.com/questions/2449/what-do-the-different-actions-of-the-openai-gyms-environment-of-pong-v0-repre",
    "closed_reason": "Not suitable for this site",
    "title": "What do the different actions of the OpenAI gym&#39;s environment of &#39;Pong-v0&#39; represent?",
    "body": "<p>Printing <code>action_space</code> for Pong-v0 gives <code>Discrete(6)</code> as output, i.e. <span class=\"math-container\">$0, 1, 2, 3, 4, 5$</span> are actions defined in the environment as per the documentation. However, the game needs only 2 controls. Why do we have this discrepancy? Further, is that necessary to identify which number from 0 to 5 corresponds to which action in a gym environment?</p>\n"
  },
  {
    "tags": [
      "ai-design",
      "algorithm",
      "reference-request"
    ],
    "owner": {
      "account_id": 10045358,
      "reputation": 101,
      "user_id": 4856,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/1e25ddded5a332ed875e6c3f52a09943?s=256&d=identicon&r=PG",
      "display_name": "user4856",
      "link": "https://ai.stackexchange.com/users/4856/user4856"
    },
    "is_answered": true,
    "view_count": 45897,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1548821010,
    "creation_date": 1484592096,
    "last_edit_date": 1539039674,
    "question_id": 2675,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2675/how-can-these-7-ai-problem-characteristics-help-me-decide-on-an-approach-to-a-pr",
    "title": "How can these 7 AI problem characteristics help me decide on an approach to a problem?",
    "body": "<p>If this list<sup>1</sup> can be used to classify problems in AI ... </p>\n\n<blockquote>\n  <ul>\n  <li>Decomposable to smaller or easier problems</li>\n  <li>Solution steps can be ignored or undone</li>\n  <li>Predictable problem universe</li>\n  <li>Good solutions are obvious</li>\n  <li>Uses internally consistent knowledge base</li>\n  <li>Requires lots of knowledge or uses knowledge to constrain solutions</li>\n  <li>Requires periodic interaction between human and computer</li>\n  </ul>\n</blockquote>\n\n<p>... is there a generally accepted relationship between placement of a problem along these dimensions and suitable algorithms/approaches to its solution?</p>\n\n<p><strong>References</strong></p>\n\n<p>[1] <a href=\"https://images.slideplayer.com/23/6911262/slides/slide_4.jpg\" rel=\"nofollow noreferrer\">https://images.slideplayer.com/23/6911262/slides/slide_4.jpg</a></p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning"
    ],
    "owner": {
      "account_id": 308908,
      "reputation": 203,
      "user_id": 5312,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/dbb4c382e5b998799b1f9a17b34b63f2?s=256&d=identicon&r=PG",
      "display_name": "David",
      "link": "https://ai.stackexchange.com/users/5312/david"
    },
    "is_answered": true,
    "view_count": 5860,
    "accepted_answer_id": 2797,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1486984926,
    "creation_date": 1486483182,
    "last_edit_date": 1486984926,
    "question_id": 2793,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/2793/how-to-transform-inputs-and-extract-useful-outputs-in-a-neural-network",
    "title": "How to transform inputs and extract useful outputs in a Neural Network?",
    "body": "<p>So I've been trying to understand neural networks ever since I came across <a href=\"https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471\" rel=\"noreferrer\">Adam Geitgey's</a> blog on machine learning. I've read as much as I can on the subject (that I can grasp) and believe I understand all the broad concepts and some of the workings (despite being very weak in maths), neurons, synapses, weights, cost functions, backpropagation etc. However, I've not been able to figure out how to translate real world problems into a neural network solution. </p>\n\n<p>Case in point, Adam Geitgey gives as an example usage, a house price prediction system where given a data set containing <strong>No. of bedrooms</strong>, <strong>Sq. feet</strong>, <strong>Neighborhood</strong> and <strong>Sale price</strong> you can train a neural network to be able to predict the price of a house. However he stops short of actually implementing a possible solution in code. The closest he gets, by way of an example, is basic a function demonstrating how you'd implement weights:</p>\n\n<pre><code>def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):\n  price = 0\n\n  # a little pinch of this\n  price += num_of_bedrooms * 1.0\n\n  # and a big pinch of that\n  price += sqft * 1.0\n\n  # maybe a handful of this\n  price += neighborhood * 1.0\n\n  # and finally, just a little extra salt for good measure\n  price += 1.0\n\n  return price \n</code></pre>\n\n<p>Other resources seem to focus more heavily on the maths and the only basic code example I could find that I understand (i.e. that isn't some all singing, all dancing image classification codebase) is an implementation that trains a neural network to be an XOR gate that deals only in 1's and 0's.  </p>\n\n<p>So there's a gap in my knowledge that I just can't seem to bridge. If we return to the <strong>house price prediction</strong> problem, hows does one make the data suitable for feeding into a neural network? For example:</p>\n\n<ul>\n<li>No. of bedrooms: 3</li>\n<li>Sq. feet: 2000</li>\n<li>Neighborhood: Normaltown</li>\n<li>Sale price: $250,000</li>\n</ul>\n\n<p>Can you just feed <strong>3</strong> and <strong>2000</strong> directly into the neural network because they are numbers? Or do you need to transform them into something else? Similarly what about the <strong>Normaltown</strong> value, that's a string, how do you go about translating it into a value a neural network can understand? Can you just pick a number, like an index, so long as it's consistent throughout the data?</p>\n\n<p>Most of the neural network examples I've seen the numbers passing between layers are either 0 to 1 or -1 to 1. So at the end of processing, how do you transform the output value to something usable like <strong>$185,000</strong>?</p>\n\n<p>I know the house price prediction example probably isn't a particularly useful problem given that it's been massively oversimplified to just three data points. But I just feel that if I could get over this hurdle and write an extremely basic app that trains using pseudo real-life data and spits out a pseudo real-life answer than I'll have broken the back of it and be able to kick on and delve further into machine learning.</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "definitions",
      "singularity",
      "superintelligence"
    ],
    "owner": {
      "account_id": 6108253,
      "reputation": 1670,
      "user_id": 181,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://lh3.googleusercontent.com/-YYgQ6EQmsiQ/AAAAAAAAAAI/AAAAAAAAABU/QDDm2dKMp3c/s256-rj/photo.jpg",
      "display_name": "Left SE On 10_6_19",
      "link": "https://ai.stackexchange.com/users/181/left-se-on-10-6-19"
    },
    "is_answered": true,
    "view_count": 475,
    "accepted_answer_id": 2844,
    "answer_count": 3,
    "score": 10,
    "last_activity_date": 1561501142,
    "creation_date": 1487430658,
    "last_edit_date": 1561501142,
    "question_id": 2841,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2841/can-a-technological-singularity-only-occur-with-superintelligence",
    "title": "Can a technological singularity only occur with superintelligence?",
    "body": "<p>In Chapter 26 of the book <em>Artificial Intelligence: A Modern Approach</em> (3rd edition), the textbook discusses \"technological singularity\". It quotes I.J. Good, who wrote in 1965:</p>\n\n<blockquote>\n  <p>Let an ultra-intelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultra-intelligent machine could design even better machines; there would then unquestionably be an \"intelligence explosion,\" and the intelligence of man would be left far behind. Thus the first ultra-intelligent machine is the <em>last</em> invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.</p>\n</blockquote>\n\n<p>Later on in the textbook, you have this question:</p>\n\n<blockquote>\n  <p>26.7 - I. J. Good claims that intelligence is the most important quality, and that building ultr- intelligent machines will change everything. A sentient cheetah counters that \"Actually speed is more important; if we could build ultrafast machines, that would change everything\" and a sentient elephant claims \"You're both wrong; what we need is ultrastrong machines,\" What do you think of these arguments?</p>\n</blockquote>\n\n<p>It seems that the textbook question is an implicit argument against I.J. Good. Good may be treating intelligence as valuable, simply because man's strengths lies in that trait called \"intelligence\". But other traits could be equally valued instead (speed or strength) and sentient beings may speculate wildly about their preferred traits being \"maximized\" by some machine or another.</p>\n\n<p>This makes me wonder whether a singularity could occur if we had built machines that were <em>not</em> maximizing intelligence, but instead maximizing some other trait (a machine that is always increasing its strength, or a machine that is always increasing its speed). These types of machines can be just as transformative - ultrafast machines may solve problems quickly due to \"brute force\", and ultrastrong machines can use its raw power for a variety of physical tasks. Perhaps a ultra-X machine can't build another ultra-X machine (as I.J. Good treated the design of machines as an intellectual activity), but a continually self-improving machine would still leave its creators far behind and force its creators to be dependent on it.</p>\n\n<p><em>Are technological singularities limited to ultra-intelligences?</em> Or technological singularities be caused by machines that are not \"strong AI\" but are still \"ultra\"-optimizers?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "agi",
      "social",
      "singularity"
    ],
    "owner": {
      "account_id": 6890256,
      "reputation": 101,
      "user_id": 6084,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e66a365d5fd87fe0b177f4bca02a6510?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Nathan Danzmann",
      "link": "https://ai.stackexchange.com/users/6084/nathan-danzmann"
    },
    "is_answered": true,
    "view_count": 1822,
    "answer_count": 6,
    "score": 10,
    "last_activity_date": 1561589045,
    "creation_date": 1489707089,
    "last_edit_date": 1561588411,
    "question_id": 3006,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3006/when-the-ai-singularity-takes-over-what-will-there-be-left-for-us-to-do",
    "title": "When the AI singularity takes over, what will there be left for us to do?",
    "body": "<p>Since the first Industrial revolution machines have been taking the jobs of people and automation has been a part of human social evolution for the past 3 centuries, but all in all these machines have been replacing mechanical, high-risk and low-skill jobs such as a production line of an automobile factory.</p>\n\n<p>But recently with the advent of computers and the improvement of AI, and the quest to find a Singularity (that is, a computer capable of thinking faster, better, more creative and <strong>cheaper</strong> then a human being, capable of self-improving), our future will lead to the replacement of not only low-skill workers, but high-skill as well. I'm talking about a future not too far when AI and machines will replace artists, designers, engineers, lawyers, CEO's, filmmakers, politicians, hell even programmers.\nSome people get excited by this, but honestly, I get somewhat scared.</p>\n\n<p>I'm not talking about the money issue here, although I'm not a fan of the idea, let's suppose the universal income has been implemented, and suppose it works fine. Also not talking about the \"<em>Terminator's world where machines will wage war against humans</em>\", let's suppose too they are completely friendly forever.</p>\n\n<p>The issue here is the one of <strong>motivation</strong> for us humans. <em>When the AI singularity takes over, what will there be left for us to do?</em> Every day, all day long?</p>\n\n<p>What are we going to do with our lives? Suppose I love to paint, how can I live my dream of becoming a painter if the computer makes better art then I will ever be able to do? How can I live knowing that no one will care about my paintings because they were made by a <strong>mere human</strong>? Or the real me for example (I, Danzmann), I love to code, learned my first programming language with 9 years old and been on it ever since then, it looks sad to me that in some years I may never touch on that again. And that goes for all the professions, everyone is passionate about something, and with the singularity, every single one of them would just have to cease to exist.</p>\n\n<p>So, what are we going to do in this future? What am I going to do? Play golf all day, every single day for the rest of my life (A Hyperbole figure of speech, but you get my point)?</p>\n\n<p>Also, what is going to be the motivation for my children? What am I going to tell them to go to school? When someone asks \"what do you wanna be when you grow up?\", and the inevitable answer is <strong>nothing</strong>.</p>\n\n<p>If highly advanced AI takes control of all scientific research, then what is the reason for us to <strong>learn</strong>? What is the reason that us humans would need to dedicate decades of our lives to learn something if that knowledge is useless, because there are no more jobs and the scientific research is done solely by AI?</p>\n"
  },
  {
    "tags": [
      "comparison",
      "terminology",
      "genetic-algorithms",
      "evolutionary-computation"
    ],
    "owner": {
      "account_id": 1040530,
      "reputation": 211,
      "user_id": 6095,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/ISxud.jpg?s=256",
      "display_name": "Nasser",
      "link": "https://ai.stackexchange.com/users/6095/nasser"
    },
    "is_answered": true,
    "view_count": 3698,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1610808456,
    "creation_date": 1489748100,
    "last_edit_date": 1610807799,
    "question_id": 3009,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3009/what-is-the-difference-between-memetic-algorithms-and-genetic-algorithms",
    "title": "What is the difference between memetic algorithms and genetic algorithms?",
    "body": "<p>What is the difference between <a href=\"https://en.wikipedia.org/wiki/Memetic_algorithm\" rel=\"nofollow noreferrer\">memetic algorithms</a> and <a href=\"https://en.wikipedia.org/wiki/Genetic_algorithm\" rel=\"nofollow noreferrer\">genetic algorithms</a>? Is an individual's lifetime a learning part of memetic algorithms?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "ai-design",
      "classification",
      "prediction",
      "regression"
    ],
    "owner": {
      "account_id": 5423590,
      "reputation": 305,
      "user_id": 6310,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/098bf462805cb1f19d9472b9e360bb96?s=256&d=identicon&r=PG",
      "display_name": "Guest2000",
      "link": "https://ai.stackexchange.com/users/6310/guest2000"
    },
    "is_answered": true,
    "view_count": 675,
    "answer_count": 3,
    "score": 10,
    "last_activity_date": 1558137453,
    "creation_date": 1491056399,
    "last_edit_date": 1558137453,
    "question_id": 3081,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3081/do-i-need-classification-or-regression-to-predict-the-availability-of-a-user-giv",
    "title": "Do I need classification or regression to predict the availability of a user given some features?",
    "body": "<p>While studying data mining methods I have come to understand that there are two main categories:</p>\n\n<ul>\n<li><p>Predictive methods: </p>\n\n<ul>\n<li>Classification</li>\n<li>Regression</li>\n</ul></li>\n<li><p>Descriptive methods:</p>\n\n<ul>\n<li>Clustering</li>\n<li>Association rules</li>\n</ul></li>\n</ul>\n\n<p>Since I want to predict the user availability (output) based on location, activity, battery level (input for the training model), I think it's obvious that I would choose \"Predictive methods\", but now I can't seem to choose between classification and regression. From what I understand this far, classification can solve my problem, because the output is \"available\" or \"not available\". </p>\n\n<p><em>Can classification provide me with the probability (or likelihood) of the user being available or not available?</em></p>\n\n<p>As in the output wouldn't just be 0 (not available) or 1 (for available), but it's be something like:</p>\n\n<ul>\n<li><span class=\"math-container\">$80\\%$</span> available</li>\n<li><span class=\"math-container\">$20\\%$</span> not available</li>\n</ul>\n\n<p><em>Can this problem also be solved using regression?</em></p>\n\n<p>I get that regression is used for continuous output (not just 0 or 1 outputs), but can't the output be the continuous value of the user availability (like the output being <span class=\"math-container\">$80$</span> meaning user is <span class=\"math-container\">$80\\%$</span> available, implicitly the user is <span class=\"math-container\">$20\\%$</span> unavailable).</p>\n"
  },
  {
    "tags": [
      "natural-language-processing"
    ],
    "owner": {
      "account_id": 106570,
      "reputation": 209,
      "user_id": 6485,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8ae7bce2d58740d9e5173e9cbdea0c4f?s=256&d=identicon&r=PG",
      "display_name": "mindplay.dk",
      "link": "https://ai.stackexchange.com/users/6485/mindplay-dk"
    },
    "is_answered": true,
    "view_count": 337,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1491468535,
    "creation_date": 1491461565,
    "question_id": 3111,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/3111/has-anyone-attempted-to-train-an-ai-to-learn-all-languages",
    "title": "Has anyone attempted to train an AI to learn all languages?",
    "body": "<p>It seems that most projects attempt to teach the AI to learn individual, specific languages.</p>\n\n<p>It occurs to me that there are relations in written and spoken words and phrases across languages - most of use have a much easier time learning more languages after we learn a second language, and we start to understand the relations between words and phrases in different languages.</p>\n\n<p>Has anyone attempt to train an AI to learn <em>all</em> languages?</p>\n\n<p>Wouldn't this potentially be a much simpler problem than trying to teach an AI a single, specific language with all of the specifics and details of that single language? Since you're actually omitting a lot of related data in other languages from the training set?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "history",
      "programming-languages",
      "lisp"
    ],
    "owner": {
      "account_id": 3845739,
      "reputation": 826,
      "user_id": 7681,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/5eyFx.jpg?s=256",
      "display_name": "Maheshwar Ligade",
      "link": "https://ai.stackexchange.com/users/7681/maheshwar-ligade"
    },
    "is_answered": true,
    "view_count": 5371,
    "closed_date": 1589401000,
    "accepted_answer_id": 3554,
    "answer_count": 2,
    "score": 10,
    "locked_date": 1640205049,
    "last_activity_date": 1542463434,
    "creation_date": 1498223175,
    "last_edit_date": 1542463434,
    "question_id": 3533,
    "link": "https://ai.stackexchange.com/questions/3533/is-lisp-still-worth-learning-today-in-the-particular-context-of-machine-learning",
    "closed_reason": "Opinion-based",
    "title": "Is Lisp still worth learning today in the particular context of Machine learning?",
    "body": "<p>Lisp was originally created as a practical mathematical notation for computer programs, influenced by the notation of Alonzo Church's lambda calculus. It quickly became the favored programming language for artificial intelligence (AI) research, according to Wikipedia. </p>\n\n<p>If Lisp is still used in AI, then is it worthy of learning it, particularly in the context of machine learning and deep learning?</p>\n"
  },
  {
    "tags": [
      "ai-design",
      "philosophy",
      "agi"
    ],
    "owner": {
      "account_id": 1533,
      "reputation": 1510,
      "user_id": 169,
      "user_type": "registered",
      "accept_rate": 0,
      "profile_image": "https://www.gravatar.com/avatar/2d1e9a607c47a89730352dd7b9dacaab?s=256&d=identicon&r=PG",
      "display_name": "Eric Platon",
      "link": "https://ai.stackexchange.com/users/169/eric-platon"
    },
    "is_answered": true,
    "view_count": 285,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1611591347,
    "creation_date": 1500346116,
    "last_edit_date": 1611591347,
    "question_id": 3665,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3665/is-analogy-necessary-to-artificial-general-intelligence",
    "title": "Is analogy necessary to artificial general intelligence?",
    "body": "<p>Analogies are quite powerful in communication. They allow explaining complex concepts to people with no domain knowledge, just by mapping to a known domain. Hofstadter <a href=\"https://cogsci.indiana.edu/\" rel=\"nofollow noreferrer\">says they matter</a>, whereas Dijkstra says they are dangerous. Anyway, analogies can be seen as a powerful way to transfer concepts in human communication (dare I say <a href=\"https://en.wikipedia.org/wiki/Transfer_learning\" rel=\"nofollow noreferrer\">transfer learning</a>?).</p>\n<p>I am aware of legacy work, such as <a href=\"https://en.wikipedia.org/wiki/Case-based_reasoning\" rel=\"nofollow noreferrer\">Case-Based Reasoning</a>, but no more recent work about the analogy mechanism in AI.</p>\n<p>Is there a consensus on whether or not analogy is necessary (or even critical) to AGIs, and how critical would they be?</p>\n<p>Please, consider backing your answers with concrete work or publications.</p>\n"
  },
  {
    "tags": [
      "reference-request",
      "neurons",
      "neuroscience"
    ],
    "owner": {
      "account_id": 2738792,
      "reputation": 1097,
      "user_id": 7402,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/72bd5e3f73d1cd1774d2235b6bfab11d?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Blaszard",
      "link": "https://ai.stackexchange.com/users/7402/blaszard"
    },
    "is_answered": true,
    "view_count": 679,
    "accepted_answer_id": 3752,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1640298978,
    "creation_date": 1501607791,
    "last_edit_date": 1640298978,
    "question_id": 3739,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3739/are-biological-neurons-organized-in-consecutive-layers-as-well",
    "title": "Are biological neurons organized in consecutive layers as well?",
    "body": "<p>I'm now reading a book titled <a href=\"http://shop.oreilly.com/product/0636920052289.do\" rel=\"nofollow noreferrer\">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a> and in the Chapter 10 of the book, the author writes the following:</p>\n<blockquote>\n<p>The architecture of biological neural networks (BNN)4 is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers, as shown in Figure 10-2.</p>\n</blockquote>\n<p><a href=\"https://i.sstatic.net/fsge8.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/fsge8.png\" alt=\"enter image description here\" /></a></p>\n<p>However, there seems to be no link to any research there. And the author didn't say it assertively given that he used <em>&quot;it <strong>seems</strong> that neurons are often organized in consecutive layers&quot;</em>.</p>\n<p>Is this true and how strongly is it believed? What research is this from?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "terminology",
      "incremental-learning"
    ],
    "owner": {
      "account_id": 6563158,
      "reputation": 347,
      "user_id": 9319,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-pEkXqWxH5h0/AAAAAAAAAAI/AAAAAAAAABc/8g1duewTc-g/s256-rj/photo.jpg",
      "display_name": "ZakC",
      "link": "https://ai.stackexchange.com/users/9319/zakc"
    },
    "is_answered": true,
    "view_count": 1612,
    "answer_count": 3,
    "score": 10,
    "last_activity_date": 1704202923,
    "creation_date": 1503997050,
    "last_edit_date": 1604920446,
    "question_id": 3920,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3920/what-do-you-call-a-machine-learning-system-that-keeps-on-learning",
    "title": "What do you call a machine learning system that keeps on learning?",
    "body": "<p>As I understand it from this <a href=\"http://videolectures.net/deeplearning2015_vincent_machine_learning/\" rel=\"nofollow noreferrer\">video lecture</a>, there are three types of deep learning:</p>\n\n<ul>\n<li>Supervised</li>\n<li>Unsupervised</li>\n<li>Reinforcement</li>\n</ul>\n\n<p>All these can serve to train a neural network either <em>only prior</em> to its deployment or <em>during</em> its operating. </p>\n\n<p>For the latter case, it is referred to as <em>continuous</em> learning <a href=\"http://deepmind.com/blog/enabling-continual-learning-in-neural-networks/\" rel=\"nofollow noreferrer\">here</a> and <a href=\"http://medium.com/@Synced/deep-learning-in-real-time-inference-acceleration-and-continuous-training-17dac9438b0b\" rel=\"nofollow noreferrer\">here</a> and as <em>dynamic</em> learning <a href=\"http://conferences.oreilly.com/artificial-intelligence/ai-ny/public/schedule/detail/59511\" rel=\"nofollow noreferrer\">here</a> and <a href=\"http://www.datasciencecentral.com/profiles/blogs/static-dynamical-machine-learning-what-is-the-difference\" rel=\"nofollow noreferrer\">here</a>.</p>\n\n<p>Which term should I use to refer to a machine learning algorithm that keeps on learning (even after deployment)? If it's \"continuous\", is there an opposing term (such as \"static\" for \"dynamic\") for those systems that stop learning before being deployed? </p>\n"
  },
  {
    "tags": [
      "deep-learning"
    ],
    "owner": {
      "account_id": 5273044,
      "reputation": 131,
      "user_id": 9361,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/100000464681553/picture?type=large",
      "display_name": "Frank Malenfant",
      "link": "https://ai.stackexchange.com/users/9361/frank-malenfant"
    },
    "is_answered": true,
    "view_count": 303,
    "accepted_answer_id": 4455,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1512652719,
    "creation_date": 1504199788,
    "last_edit_date": 1504622362,
    "question_id": 3943,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/3943/a-deep-learning-algorithm-to-optimize-the-outcome",
    "title": "A deep learning algorithm to optimize the outcome",
    "body": "<p>I'm am quite new to deep learning but I think I found just the right real-world situation to start using it. The problem is that I have only used such algorithms to predict outcomes.  For my new project, I need information to feed a machine with to <em>optimize</em> outcomes. Could someone explain briefly how I should proceed? I'm stuck.</p>\n\n<p><strong>Here's the situation:</strong></p>\n\n<p>I have a machine that takes planks of wood with different grades of wood available throughout its length and has to cut it into blocks provided in a cut list. This machine will always choose the highest <em>score</em> it can get from a given plank. The <em>score</em> is obtained by multiplying each block's <em>area</em> by its <em>multiplicator</em>. The algorithm I want to build has to give that machine a <em>multiplicator</em> for each block listed in a cut list. All of the physical output from this machine will be stocked on shelves by a robot until needed. The cutting machine is allowed to downgrade parts of a plank if it helps it reach a higher score.</p>\n\n<p>The value has to act as an incentive for the machine to give me the block I need the most without downgrading too much wood.</p>\n\n<p><strong><em>OPTIMIZATION GOALS</em></strong></p>\n\n<ul>\n<li>Make sure each block is in stock by the time it is needed, but not too early without reason</li>\n<li>Downgrade as little area of wood as possible (some species are very expensive)</li>\n</ul>\n\n<p><strong><em>INPUT NODES</em></strong></p>\n\n<ul>\n<li>Amount of time before this block is needed</li>\n<li>Grade of wood for this block</li>\n<li>Amount of this block needed</li>\n<li>Block's area (Maybe?)</li>\n</ul>\n\n<p><strong><em>FEEDBACK PROVIDED TO THE ALGORITHM</em></strong></p>\n\n<ul>\n<li>Amount of time in advance that the block was ready (must be as low as possible)</li>\n<li>Area of wood downgraded * number of grades skipped</li>\n</ul>\n\n<p><strong><em>EXPECTED RETURN DATA</em></strong></p>\n\n<ul>\n<li>A <em>multiplicator</em> that will give that block an optimal its priority relative to others</li>\n</ul>\n\n<p><strong><em>INFORMATION I DON'T HAVE BUT COULD GATHER</em></strong></p>\n\n<ul>\n<li>Mean ratio of each grade for each species of wood</li>\n</ul>\n\n<p>What I've figured out so far is that I may need my feedback to be smashed in only one value in order to make it the output node. The problem is that I can't understand how to make this algorithm to determine a <em>multiplicator</em>. Am I wrong in trying to solve this through deep learning?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "datasets",
      "feature-selection"
    ],
    "owner": {
      "account_id": 7742922,
      "reputation": 291,
      "user_id": 9374,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/RWjlP.jpg?s=256",
      "display_name": "Karan Chopra",
      "link": "https://ai.stackexchange.com/users/9374/karan-chopra"
    },
    "is_answered": true,
    "view_count": 342,
    "answer_count": 4,
    "score": 10,
    "last_activity_date": 1548853395,
    "creation_date": 1504509624,
    "last_edit_date": 1540677873,
    "question_id": 3965,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3965/how-do-i-select-the-relevant-features-of-the-data",
    "title": "How do I select the relevant features of the data?",
    "body": "<p>Recently I was working on a problem to do some cost analysis of my expenditure for some particular resource.\nI usually make some manual decisions from the analysis and plan accordingly.</p>\n\n<p>I have a big data set in excel format and with hundreds of columns, defining the use of the resource in various time frames and types(other various detailed use).\nI also have information about my previous 4 years of data and actual resource usage and cost incurred accordingly.</p>\n\n<p>I was hoping to train a NN to predict my cost beforehand and plan even before I can manually do the cost analysis.</p>\n\n<p>But the biggest problem I'm facing is the need to identify the features for such analysis. I was hoping there is some way to identify the features from the data set.</p>\n\n<p><em>PS - I have idea about PCA and some other feature set reduction techniques, what I'm looking at is the way to identify them in the first place.</em></p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "comparison",
      "autoencoders"
    ],
    "owner": {
      "account_id": 10822055,
      "reputation": 333,
      "user_id": 10118,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/66659abab702204cc835487e259e9991?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "m2rik",
      "link": "https://ai.stackexchange.com/users/10118/m2rik"
    },
    "is_answered": true,
    "view_count": 2013,
    "accepted_answer_id": 11808,
    "answer_count": 3,
    "score": 10,
    "last_activity_date": 1652033869,
    "creation_date": 1507653225,
    "last_edit_date": 1555236127,
    "question_id": 4245,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4245/what-is-the-difference-between-encoders-and-auto-encoders",
    "title": "What is the difference between encoders and auto-encoders?",
    "body": "<p>How are the layers in a encoder connected across the network for normal encoders and auto-encoders? In general, what is the difference between encoders and auto-encoders?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "recurrent-neural-networks",
      "prediction",
      "network-design"
    ],
    "owner": {
      "account_id": 6721244,
      "reputation": 211,
      "user_id": 4036,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/j8M6A.jpg?s=256",
      "display_name": "Nickpick",
      "link": "https://ai.stackexchange.com/users/4036/nickpick"
    },
    "is_answered": true,
    "view_count": 1140,
    "accepted_answer_id": 4293,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1639043625,
    "creation_date": 1508149770,
    "last_edit_date": 1639003443,
    "question_id": 4279,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4279/how-to-design-a-neural-network-when-the-number-of-inputs-is-variable",
    "title": "How to design a neural network when the number of inputs is variable?",
    "body": "<p>I'm looking to design a neural network that can predict which runner wins in a sports game, where the number of runners varies between 2-10. In each case, specific data about the individual runners (for example, the weight, height, average speed in previous races, nationality, etc) would be fed into the neural network.</p>\n<p>What design would be most advantageous for such a neural network?</p>\n<p>Essentially this is a ranking problem where the number of inputs and outputs are variable.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "ai-design",
      "tic-tac-toe",
      "board-games",
      "state-representations"
    ],
    "owner": {
      "account_id": 2093561,
      "reputation": 220,
      "user_id": 2819,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://www.gravatar.com/avatar/134ae6e532080f6012f7d24f6c843428?s=256&d=identicon&r=PG",
      "display_name": "NeomerArcana",
      "link": "https://ai.stackexchange.com/users/2819/neomerarcana"
    },
    "is_answered": true,
    "view_count": 3209,
    "accepted_answer_id": 4586,
    "answer_count": 3,
    "score": 10,
    "last_activity_date": 1636239929,
    "creation_date": 1511406125,
    "last_edit_date": 1636239754,
    "question_id": 4581,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4581/how-should-i-represent-the-input-to-a-neural-network-for-the-games-of-tic-tac-to",
    "title": "How should I represent the input to a neural network for the games of tic-tac-toe, checkers or chess?",
    "body": "<p>I've been reading a lot about TD-Gammon recently as I'm exploring options for AI in a video game I'm making. The video game is a turn-based positional sort of game, i.e. a &quot;units&quot;, or game piece's, position will greatly impact its usefulness in that board state.</p>\n<p>To work my way towards this, I thought it prudent to implement a Neural Network for a few different games first.</p>\n<p>The idea I like is encoding the board state for the Neural Network with a single output neuron, which gives that board state relative strength compared to other board states. As I understand, this is how TD-Gammon worked.</p>\n<p>However, when I look at other people's code and examples/tutorials, there seems to be a lot of variance in the way they represent the board state. Even for something as simple as tic-tac-toe.</p>\n<p>So, specifically, for tic-tac-toe, which is a better, or what is the correct representation for the board state?</p>\n<p>I have seen:</p>\n<ol>\n<li><p>9 input neurons, one for each square. A <code>0</code> indicates a free-space, <code>-1</code> the opponent, and <code>1</code> yourself.</p>\n</li>\n<li><p>9 input neurons, but using different values, such as <code>0</code> for the opponent, <code>0.5</code> for free, and <code>1</code> for yourself?</p>\n</li>\n<li><p>Could you use larger values? Like <code>0</code>, <code>1</code> and <code>2</code>?</p>\n</li>\n<li><p>27 input neurons. The first 3 being square 1, the next 3 being square 2, etc. Every neuron is <code>1</code> or <code>0</code>. The first of the set of three indicates whether this square is free or not; the second indicates whether the square is occupied by your opponent or not. In the end, only one in every 3 neurons will have a <code>1</code>, the other two will have a <code>0</code>.</p>\n</li>\n<li><p>18 input neurons. The first being <code>1</code> for the X player, the second being <code>1</code> for the O player, and both being <code>0</code> for a blank</p>\n</li>\n</ol>\n<p>Then, when branching into games where the specific pieces' abilities come into play, like in chess, how would you represent this?</p>\n<p>Would it be as simple as using higher input values for more valuable pieces? I.e. <code>-20</code> for an opponents Queen and <code>+20</code> for your own queen? Or would you need something more complex where you define 10+ values for each square, one for each unit-type and player combination?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "optimization",
      "hyperparameter-optimization",
      "hyper-parameters",
      "hidden-layers"
    ],
    "owner": {
      "account_id": 10275999,
      "reputation": 225,
      "user_id": 12901,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-IKB1javtty4/AAAAAAAAAAI/AAAAAAAAACM/kUbkAP2dKEI/s256-rj/photo.jpg",
      "display_name": "dsfx3d",
      "link": "https://ai.stackexchange.com/users/12901/dsfx3d"
    },
    "is_answered": true,
    "view_count": 8969,
    "accepted_answer_id": 5401,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1620402964,
    "creation_date": 1519318616,
    "last_edit_date": 1611185996,
    "question_id": 5399,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5399/why-should-the-number-of-neurons-in-a-hidden-layer-be-a-power-of-2",
    "title": "Why should the number of neurons in a hidden layer be a power of 2?",
    "body": "<p>I have read somewhere on the web (I lost the reference) that the number of units (or neurons) in a hidden layer should be a power of 2 because it helps the learning algorithm to converge faster.</p>\n<p>Is this a fact? If it is, why is this true? Does it have something to do with how the memory is laid down?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "comparison",
      "rewards",
      "value-functions",
      "return"
    ],
    "owner": {
      "account_id": 3820809,
      "reputation": 221,
      "user_id": 12726,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/46fbb3af211b4baca82f45d7424aa54c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user3168961",
      "link": "https://ai.stackexchange.com/users/12726/user3168961"
    },
    "is_answered": true,
    "view_count": 4452,
    "accepted_answer_id": 5722,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1611143978,
    "creation_date": 1521306051,
    "last_edit_date": 1611143978,
    "question_id": 5720,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5720/what-is-the-difference-between-expected-return-and-value-function",
    "title": "What is the difference between expected return and value function?",
    "body": "<p>I've seen numerous mathematical explanations of reward, value functions <span class=\"math-container\">$V(s)$</span>, and return functions. The reward provides an immediate return for being in a specific state. The better the reward, the better the state. </p>\n\n<p>As I understand it, it can be better to be in a low-reward state sometimes because we can accumulate more long term, which is where the expected return function comes in. An expected return, return or cumulative reward function effectively adds up the rewards from the current state to the goal state. This implies it's model-based. However, it seems a value function does exactly the same.</p>\n\n<p>Is a value function a return function? Or are they different?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "artificial-neuron"
    ],
    "owner": {
      "account_id": 2137027,
      "reputation": 291,
      "user_id": 6252,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/FBsq6.jpg?s=256",
      "display_name": "Charles",
      "link": "https://ai.stackexchange.com/users/6252/charles"
    },
    "is_answered": true,
    "view_count": 541,
    "answer_count": 3,
    "score": 10,
    "last_activity_date": 1703036912,
    "creation_date": 1523898014,
    "last_edit_date": 1555104746,
    "question_id": 6082,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6082/back-of-the-envelope-machine-learning-specifically-neural-networks-calculation",
    "title": "Back-of-the-envelope machine learning (specifically neural networks) calculations",
    "body": "<p>There is a popular story regarding the <a href=\"https://en.wikipedia.org/wiki/Back-of-the-envelope_calculation\" rel=\"noreferrer\">back-of-the-envelope calculation</a> performed by a British physicist named G. I. Taylor. He used <a href=\"http://www.atmosp.physics.utoronto.ca/people/codoban/PHY138/Mechanics/dimensional.pdf\" rel=\"noreferrer\">dimensional analysis</a> to estimate the power released by the explosion of a nuclear bomb, simply by analyzing a picture that was released in a magazine at the time.</p>\n\n<p>I believe many of you know some nice back-of-the-envelope calculations performed in machine learning (more specifically neural networks). Can you please share them?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "applications"
    ],
    "owner": {
      "account_id": 8714854,
      "reputation": 211,
      "user_id": 15659,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/UhKBo.jpg?s=256",
      "display_name": "bsideswiped",
      "link": "https://ai.stackexchange.com/users/15659/bsideswiped"
    },
    "is_answered": true,
    "view_count": 1722,
    "answer_count": 4,
    "score": 10,
    "last_activity_date": 1586024582,
    "creation_date": 1526413459,
    "last_edit_date": 1559490140,
    "question_id": 6429,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6429/can-an-ai-be-trained-to-generate-the-outline-of-a-story",
    "title": "Can an AI be trained to generate the outline of a story?",
    "body": "<p>I know that one of the recent fads right now is to train a neural network to generate screenplays and new episodes of e.g. the Friends or The Simpsons, and that's fine: it's interesting and might be the necessary first steps toward making programs that can actually generate sensible/understandable stories.</p>\n\n<p>In this context, can neural networks be trained specifically to study the structures of stories, or screenplays, and perhaps generate plot points, or steps in the Hero's Journey, etc., effectively writing an outline for a story?</p>\n\n<p>To me, this differs from the many myriad plot-point generators online, although I have to admit the similarities.  I'm just curious if the tech or the implementation is even there yet and, if it is, how one might go about doing it.</p>\n"
  },
  {
    "tags": [
      "game-ai",
      "search",
      "monte-carlo-tree-search"
    ],
    "owner": {
      "account_id": 256707,
      "reputation": 211,
      "user_id": 16597,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/qn65B.jpg?s=256",
      "display_name": "kgautron",
      "link": "https://ai.stackexchange.com/users/16597/kgautron"
    },
    "is_answered": true,
    "view_count": 2326,
    "accepted_answer_id": 6955,
    "answer_count": 3,
    "score": 10,
    "last_activity_date": 1537640098,
    "creation_date": 1530282717,
    "last_edit_date": 1537640098,
    "question_id": 6953,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6953/why-does-monte-carlo-work-when-a-real-opponents-behavior-may-not-be-random",
    "title": "Why does Monte Carlo work when a real opponent&#39;s behavior may not be random",
    "body": "<p>I am learning about Monte Carlo algorithms and struggling to understand the following:</p>\n\n<ul>\n<li>If simulations are based on random moves, how can the modeling of the opponent's behavior work well?</li>\n</ul>\n\n<p>For example, if I have a node with 100 children, 99 of which lead to an instant WIN, whereas the last one leads to an instant LOSS.</p>\n\n<p>In reality, the opponent would never play any of the 99 losing moves for him (assuming they are obvious as they are the last moves), and would always play the winning one. But the Monte Carlo algorithm would still see this node as extremely favorable (99/100 wins for me), because it sees each of the 100 moves as equally probable.</p>\n\n<p>Is my understanding wrong, or does it mean that in most games such situations do not occur and randomness is a good approximation of opponent behavior?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "convolutional-neural-networks",
      "recurrent-neural-networks"
    ],
    "owner": {
      "account_id": 11269524,
      "reputation": 121,
      "user_id": 16715,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e490e506cf0381d7e5625439b17a6c95?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Abaqus",
      "link": "https://ai.stackexchange.com/users/16715/abaqus"
    },
    "is_answered": true,
    "view_count": 3579,
    "answer_count": 4,
    "score": 10,
    "last_activity_date": 1586698743,
    "creation_date": 1530848966,
    "last_edit_date": 1586698743,
    "question_id": 7044,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7044/what-are-the-models-that-have-the-potential-to-replace-neural-networks-in-the-ne",
    "title": "What are the models that have the potential to replace neural networks in the near future?",
    "body": "<p>Are there possible models that have the potential to replace neural networks in the near future? </p>\n\n<p>And do we even need that? What is the worst thing about using neural networks in terms of efficiency?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "real-time",
      "delayed-rewards"
    ],
    "owner": {
      "account_id": 8178518,
      "reputation": 201,
      "user_id": 17136,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/74aed031e2b6929e34b135d8831be8ff?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "papabiceps",
      "link": "https://ai.stackexchange.com/users/17136/papabiceps"
    },
    "is_answered": true,
    "view_count": 1378,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1607987501,
    "creation_date": 1532928795,
    "last_edit_date": 1607987501,
    "question_id": 7339,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7339/reinforcement-learning-with-asynchronous-feedback",
    "title": "Reinforcement Learning with asynchronous feedback",
    "body": "<p>I want suggestions on literature on Reinforcement Learning algorithms that perform well with <strong>asynchronous feedback</strong> from the environment. What I mean by asynchronous feedback is, when an agent performs an action it gets feedback(reward or regret) from the environment after sometime not immediately. I have only seen algorithms with immediate feedback and asynchronous updates. I don't know if literature on this problem exists. This is why I'm asking here.</p>\n<p>My application is fraud detection in banking, my understanding is when a fraud is detected it takes 15-45 days for the system to flag it as a fraud sometimes until the customer complains the system doesn't know its fraud.</p>\n<p>How would I go about designing a real-time system using reinforcement learning to flag transactions that are fraudulent or normal?</p>\n<p>Maybe my understanding is wrong, I'm learning on my own if someone could help me I would be grateful.</p>\n<p>The reason I'm looking at reinforcement learning instead of supervised learning is, it's hard to get ground truth data in the banking scenario. Fraudsters are always up-to-date or exceeding the state of the art in fraud detection. So I've decided that reinforcement learning would be an optimal direction to look for solutions to this problem.</p>\n"
  },
  {
    "tags": [
      "ai-design",
      "autonomous-vehicles"
    ],
    "owner": {
      "account_id": 130411,
      "reputation": 203,
      "user_id": 17446,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0bba6951a0d4a8cfd27254bed0d32adc?s=256&d=identicon&r=PG",
      "display_name": "guillaume31",
      "link": "https://ai.stackexchange.com/users/17446/guillaume31"
    },
    "is_answered": true,
    "view_count": 889,
    "accepted_answer_id": 7812,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1561044693,
    "creation_date": 1534145094,
    "last_edit_date": 1561044693,
    "question_id": 7541,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7541/do-self-driving-cars-resort-to-randomness-to-make-decisions",
    "title": "Do self-driving cars resort to randomness to make decisions?",
    "body": "<p>I recently heard someone make a statement that when you're designing a self-driving car, you're not building a car but really a computerized driver, so you're trying to model a human mind -- at least the part of the human mind that can drive.</p>\n\n<p>Since humans are unpredictable, or rather since their actions depend on so many factors some of which are going to remain unexplained for a long time, how would a self-driving car reflect that, if they do?</p>\n\n<p>A dose of unpredictability could have its uses. If, say, two self-driving cars are in a stuck in a right of way deadlock, it could be good to inject some randomness instead of maybe seeing the same action applied at the same time if the cars run the same system. </p>\n\n<p>But, on the other hand, we know that non-deterministic isn't friends with software development, especially in testing. How would engineers be able to control it and reason about it?</p>\n"
  },
  {
    "tags": [
      "ai-design",
      "game-ai",
      "breadth-first-search"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user9947"
    },
    "is_answered": true,
    "view_count": 8655,
    "accepted_answer_id": 7557,
    "answer_count": 5,
    "score": 10,
    "last_activity_date": 1591233022,
    "creation_date": 1534251803,
    "last_edit_date": 1591233022,
    "question_id": 7555,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7555/how-do-i-keep-track-of-already-visited-states-in-breadth-first-search",
    "title": "How do I keep track of already visited states in breadth-first search?",
    "body": "<p>I was trying to implement the breadth-first search (BFS) algorithm for the <a href=\"https://en.wikipedia.org/wiki/Sliding_puzzle\" rel=\"nofollow noreferrer\">sliding blocks puzzle</a> (number type). Now, the main thing I noticed is that, if you have a <span class=\"math-container\">$4 \\times 4$</span> board, the number of states can be as large as <span class=\"math-container\">$16!$</span>, so I cannot enumerate all states beforehand.</p>\n\n<p>How do I keep track of already visited states? I am using a class board each class instance contains a unique board pattern and is created by enumerating all possible steps from the current step.</p>\n\n<p>I searched on the net and, apparently, they do not go back to the just-completed previous step, <strong>BUT</strong> we can go back to the previous step by another route too and then again re-enumerate all steps which have been previously visited. </p>\n\n<p>So, how to keep track of visited states when all the states have not been enumerated already? Comparing already present states to the present step will be costly.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "ai-design",
      "probability",
      "statistical-ai"
    ],
    "owner": {
      "account_id": 9423567,
      "reputation": 171,
      "user_id": 17541,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/10210502017888006/picture?type=large",
      "display_name": "Josiah Swaim",
      "link": "https://ai.stackexchange.com/users/17541/josiah-swaim"
    },
    "is_answered": true,
    "view_count": 4489,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1691421818,
    "creation_date": 1534443003,
    "last_edit_date": 1534453937,
    "question_id": 7609,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7609/is-nassim-taleb-right-about-ai-not-being-able-to-accurately-predict-certain-type",
    "title": "Is Nassim Taleb right about AI not being able to accurately predict certain types of distributions?",
    "body": "<p>So Taleb has two heuristics to generally describe data distributions. One is Mediocristan, which basically means things that are on a Gaussian distribution such as height and/or weight of people.</p>\n\n<p>The other is called Extremistan, which describes a more Pareto like or fat-tailed distribution. An example is wealth distribution, 1% of people own 50% of the wealth or something close to that and so predictability from limited data sets is much harder or even impossible. This is because you can add a single sample to your data set and the consequences are so large that it breaks the model, or has an effect so large that it cancels out any of the benefits from prior accurate predictions. In fact this is how he claims to have made money in the stock market, because everyone else was using bad, Gaussian distribution models to predict the market, which actually would work for a short period of time but when things went wrong, they went really wrong which would cause you to have net losses in the market.</p>\n\n<p>I found this video of Taleb being asked about AI. His claim is that A.I. doesn't work (as well) for things that fall into extremistan.</p>\n\n<p>Is he right? Will some things just be inherently unpredictable even with A.I.?</p>\n\n<p>Here is the video I am referring to <a href=\"https://youtu.be/B2-QCv-hChY?t=43m08s\" rel=\"noreferrer\">https://youtu.be/B2-QCv-hChY?t=43m08s</a></p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "convolutional-neural-networks",
      "graphs",
      "geometric-deep-learning"
    ],
    "owner": {
      "account_id": 2396136,
      "reputation": 245,
      "user_id": 14948,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/39a9ddfe6b059979c753fbe69c5ee07e?s=256&d=identicon&r=PG",
      "display_name": "piratesailor",
      "link": "https://ai.stackexchange.com/users/14948/piratesailor"
    },
    "is_answered": true,
    "view_count": 1856,
    "accepted_answer_id": 7798,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1588031120,
    "creation_date": 1535877034,
    "last_edit_date": 1552387023,
    "question_id": 7788,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7788/what-benefits-can-be-got-by-applying-graph-convolutional-neural-network-instead",
    "title": "What benefits can be got by applying Graph Convolutional Neural Network instead of ordinary CNN?",
    "body": "<p>What benefits can we got by applying Graph Convolutional Neural Network instead of ordinary CNN? I mean if we can solve a problem by CNN, what is the reason should we convert to Graph Convolutional Neural Network to solve it? Are there any examples i.e. papers can show by replacing ordinary CNN with Graph Convolutional Neural Network, an accuracy increasement or a quality improvement or a performance gain is achieved? Can anyone introduce some examples as image classification, image recognition especially in medical imaging, bioinfomatics or biomedical areas?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "incremental-learning",
      "online-learning"
    ],
    "owner": {
      "account_id": 1461741,
      "reputation": 903,
      "user_id": 8332,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f5c490f629cf9a807f26939967aec109?s=256&d=identicon&r=PG",
      "display_name": "TomR",
      "link": "https://ai.stackexchange.com/users/8332/tomr"
    },
    "is_answered": true,
    "view_count": 1342,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1566948705,
    "creation_date": 1536910126,
    "last_edit_date": 1566948705,
    "question_id": 7966,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7966/are-there-dynamic-neural-networks",
    "title": "Are there dynamic neural networks?",
    "body": "<p>Are there neural networks that can decide to add/delete neurons (or change the neuron models/activation functions or change the assigned meaning for neurons), links or even complete layers during execution time? </p>\n\n<p>I guess that such neural networks overcome the usual separation of learning/inference phases and they continuously live their lives in which learning and self-improving occurs alongside performing inference and actual decision making for which these neural networks were built. Effectively, it could be a neural network that acts as a <a href=\"http://people.idsia.ch/~juergen/goedelmachine.html\" rel=\"noreferrer\">Gödel machine</a>.</p>\n\n<p>I have found the term <em>dynamic neural network</em> but it is connected to adding some delay functions and nothing more.</p>\n\n<p>Of course, such self-improving networks completely redefine the learning strategy, possibly, single shot gradient methods can not be applicable to them. </p>\n\n<p>My question is connected to the neural-symbolic integration, e.g. <a href=\"https://www.amazon.co.uk/Neural-Symbolic-Cognitive-Reasoning-Technologies/dp/3642092292/\" rel=\"noreferrer\"><em>Neural-Symbolic Cognitive Reasoning</em> by Artur S. D'Avila Garcez, 2009</a>. Usually this approach assigns individual neurons to the variables (or groups of neurons to the formula/rule) in the set of formulas in some knowledge base. Of course, if knowledge base expands (e.g. from sensor readings or from inner nonmonotonic inference) then new variables should be added and hence the neural network should be expanded (or contracted) as well.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "optimization",
      "autoencoders",
      "objective-functions"
    ],
    "owner": {
      "account_id": 5006437,
      "reputation": 499,
      "user_id": 8720,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/mYycw.jpg?s=256",
      "display_name": "imflash217",
      "link": "https://ai.stackexchange.com/users/8720/imflash217"
    },
    "is_answered": true,
    "view_count": 4564,
    "accepted_answer_id": 8073,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1539195892,
    "creation_date": 1537449272,
    "last_edit_date": 1538552143,
    "question_id": 8063,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/8063/loss-jumps-abruptly-when-i-decay-the-learning-rate-with-adam-optimizer-in-pytorc",
    "title": "Loss jumps abruptly when I decay the learning rate with Adam optimizer in PyTorch",
    "body": "<p>I'm training an <code>auto-encoder</code> network with <code>Adam</code> optimizer (with <code>amsgrad=True</code>) and <code>MSE loss</code> for Single channel Audio Source Separation task. Whenever I decay the learning rate by a factor, the network loss jumps abruptly and then decreases until the next decay in learning rate.</p>\n\n<p>I'm using Pytorch for network implementation and training.</p>\n\n<pre><code>Following are my experimental setups:\n\n Setup-1: NO learning rate decay, and \n          Using the same Adam optimizer for all epochs\n\n Setup-2: NO learning rate decay, and \n          Creating a new Adam optimizer with same initial values every epoch\n\n Setup-3: 0.25 decay in learning rate every 25 epochs, and\n          Creating a new Adam optimizer every epoch\n\n Setup-4: 0.25 decay in learning rate every 25 epochs, and\n          NOT creating a new Adam optimizer every time rather\n          using PyTorch's \"multiStepLR\" and \"ExponentialLR\" decay scheduler \n          every 25 epochs\n</code></pre>\n\n<p>I am getting very surprising results for setups #2, #3, #4 and am unable to reason any explanation for it. Following are my results:</p>\n\n<pre><code>Setup-1 Results:\n\nHere I'm NOT decaying the learning rate and \nI'm using the same Adam optimizer. So my results are as expected.\nMy loss decreases with more epochs.\nBelow is the loss plot this setup.\n</code></pre>\n\n<p>Plot-1:</p>\n\n<p><a href=\"https://i.sstatic.net/3AvAH.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/3AvAH.png\" alt=\"Setup-1 Results\"></a></p>\n\n<pre><code>optimizer = torch.optim.Adam(lr=m_lr,amsgrad=True, ...........)\n\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for i in range(num_train):\n        train_input_tensor = ..........                    \n        train_label_tensor = ..........\n        optimizer.zero_grad()\n        pred_label_tensor = model(train_input_tensor)\n        loss = criterion(pred_label_tensor, train_label_tensor)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    loss_history[m_lr].append(running_loss/num_train)\n</code></pre>\n\n<hr>\n\n<pre><code>Setup-2 Results:  \n\nHere I'm NOT decaying the learning rate but every epoch I'm creating a new\nAdam optimizer with the same initial parameters.\nHere also results show similar behavior as Setup-1.\n\nBecause at every epoch a new Adam optimizer is created, so the calculated gradients\nfor each parameter should be lost, but it seems that this doesnot affect the \nnetwork learning. Can anyone please help on this?\n</code></pre>\n\n<p>Plot-2:</p>\n\n<p><a href=\"https://i.sstatic.net/rR6Ts.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/rR6Ts.png\" alt=\"Setup-2 Results\"></a></p>\n\n<pre><code>for epoch in range(num_epochs):\n    optimizer = torch.optim.Adam(lr=m_lr,amsgrad=True, ...........)\n\n    running_loss = 0.0\n    for i in range(num_train):\n        train_input_tensor = ..........                    \n        train_label_tensor = ..........\n        optimizer.zero_grad()\n        pred_label_tensor = model(train_input_tensor)\n        loss = criterion(pred_label_tensor, train_label_tensor)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    loss_history[m_lr].append(running_loss/num_train)\n</code></pre>\n\n<hr>\n\n<pre><code>Setup-3 Results: \n\nAs can be seen from the results in below plot, \nmy loss jumps every time I decay the learning rate. This is a weird behavior.\n\nIf it was happening due to the fact that I'm creating a new Adam \noptimizer every epoch then, it should have happened in Setup #1, #2 as well.\nAnd if it is happening due to the creation of a new Adam optimizer with a new \nlearning rate (alpha) every 25 epochs, then the results of Setup #4 below also \ndenies such correlation.\n</code></pre>\n\n<p>Plot-3:</p>\n\n<p><a href=\"https://i.sstatic.net/jmjRD.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/jmjRD.png\" alt=\"Setup-3 Results\"></a></p>\n\n<pre><code>decay_rate = 0.25\nfor epoch in range(num_epochs):\n    optimizer = torch.optim.Adam(lr=m_lr,amsgrad=True, ...........)\n\n    if epoch % 25 == 0  and epoch != 0:\n        lr *= decay_rate   # decay the learning rate\n\n    running_loss = 0.0\n    for i in range(num_train):\n        train_input_tensor = ..........                    \n        train_label_tensor = ..........\n        optimizer.zero_grad()\n        pred_label_tensor = model(train_input_tensor)\n        loss = criterion(pred_label_tensor, train_label_tensor)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    loss_history[m_lr].append(running_loss/num_train)\n</code></pre>\n\n<hr>\n\n<pre><code>Setup-4 Results:  \n\nIn this setup, I'm using Pytorch's learning-rate-decay scheduler (multiStepLR)\nwhich decays the learning rate every 25 epochs by 0.25.\nHere also, the loss jumps everytime the learning rate is decayed.\n</code></pre>\n\n<p>As suggested by @Dennis in the comments below, I tried with both <code>ReLU</code> and <code>1e-02 leakyReLU</code> nonlinearities. But, the results seem to behave similar and loss first decreases, then increases and then saturates at a higher value than what I would achieve without learning rate decay.</p>\n\n<p>Plot-4 shows the results.</p>\n\n<p>Plot-4:</p>\n\n<p><a href=\"https://i.sstatic.net/56AAD.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/56AAD.png\" alt=\"enter image description here\"></a></p>\n\n<pre><code>scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[25,50,75], gamma=0.25)\n</code></pre>\n\n<p><code>scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.95)</code></p>\n\n<pre><code>scheduler = ......... # defined above\noptimizer = torch.optim.Adam(lr=m_lr,amsgrad=True, ...........)\n\nfor epoch in range(num_epochs):\n\n    scheduler.step()\n\n    running_loss = 0.0\n    for i in range(num_train):\n        train_input_tensor = ..........                    \n        train_label_tensor = ..........\n        optimizer.zero_grad()\n        pred_label_tensor = model(train_input_tensor)\n        loss = criterion(pred_label_tensor, train_label_tensor)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    loss_history[m_lr].append(running_loss/num_train)\n</code></pre>\n\n<hr>\n\n<p>EDITS: </p>\n\n<ul>\n<li>As suggested in the comments and reply below, I've made changes to my code and trained the model. I've added the code and plots for the same. </li>\n<li>I tried with various <code>lr_scheduler</code> in <code>PyTorch (multiStepLR, ExponentialLR)</code> and plots for the same are listed in <code>Setup-4</code> as suggested by @Dennis in comments below.</li>\n<li>Trying with leakyReLU as suggested by @Dennis in comments.</li>\n</ul>\n\n<p>Any help.\nThanks</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "convolutional-neural-networks",
      "fully-convolutional-networks"
    ],
    "owner": {
      "account_id": 10687437,
      "reputation": 257,
      "user_id": 17763,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/IHT5G.jpg?s=256",
      "display_name": "Santhosh Dhaipule Chandrakanth",
      "link": "https://ai.stackexchange.com/users/17763/santhosh-dhaipule-chandrakanth"
    },
    "is_answered": true,
    "view_count": 15866,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1592131946,
    "creation_date": 1539089009,
    "last_edit_date": 1592131275,
    "question_id": 8323,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/8323/how-to-handle-rectangular-images-in-convolutional-neural-networks",
    "title": "How to handle rectangular images in convolutional neural networks?",
    "body": "<p>Almost all the convolutional neural network architecture I have come across have a square input size of an image, like <span class=\"math-container\">$32 \\times 32$</span>, <span class=\"math-container\">$64 \\times 64$</span> or <span class=\"math-container\">$128 \\times 128$</span>. Ideally, we might not have a square image for all kinds of scenarios. For example, we could have an image of size <span class=\"math-container\">$384 \\times 256$</span></p>\n\n<p>My question is how to we handle such images during</p>\n\n<ol>\n<li>training,</li>\n<li>development, and</li>\n<li>testing</li>\n</ol>\n\n<p>of a neural network?</p>\n\n<p>Do we force the image to resize to the input of the neural network or just crop the image to the required input size?</p>\n"
  },
  {
    "tags": [
      "reference-request",
      "evolutionary-algorithms",
      "artificial-life",
      "self-replicating-machines"
    ],
    "owner": {
      "account_id": 14619771,
      "reputation": 101,
      "user_id": 19319,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/3cc482446151c7e07a0c68b3ba0a34ba?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "sihubumi",
      "link": "https://ai.stackexchange.com/users/19319/sihubumi"
    },
    "is_answered": true,
    "view_count": 392,
    "answer_count": 4,
    "score": 10,
    "last_activity_date": 1649010819,
    "creation_date": 1540486396,
    "last_edit_date": 1639330525,
    "question_id": 8619,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/8619/has-the-spontaneous-emergence-of-replicators-been-modeled-in-artificial-life",
    "title": "Has the spontaneous emergence of replicators been modeled in Artificial Life?",
    "body": "<p>One of the cornerstones of The Selfish Gene (Dawkins) is the spontaneous emergence of replicators, i.e. molecules capable of replicating themselves.</p>\n<p>Has this been modeled <em>in silico</em> in open-ended evolutionary/artificial life simulations?</p>\n<p>Systems like Avida or Tierra explicitly <strong>specify</strong> the replication mechanisms; other genetic algorithm/genetic programming systems explicitly <strong>search for</strong> the replication mechanisms (e.g. to simplify the von Neumann universal constructor)</p>\n<p>Links to simulations where replicators emerge from a primordial digital soup are welcome.</p>\n"
  },
  {
    "tags": [
      "algorithm",
      "search",
      "optimization",
      "problem-solving",
      "hill-climbing"
    ],
    "owner": {
      "account_id": 14414473,
      "reputation": 576,
      "user_id": 19254,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8821714658294e01ba440fe13801bc2e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Abbas Ali",
      "link": "https://ai.stackexchange.com/users/19254/abbas-ali"
    },
    "is_answered": true,
    "view_count": 26634,
    "accepted_answer_id": 8991,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1551130320,
    "creation_date": 1542294188,
    "last_edit_date": 1551130320,
    "question_id": 8986,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/8986/what-are-the-limitations-of-the-hill-climbing-algorithm-and-how-to-overcome-them",
    "title": "What are the limitations of the hill climbing algorithm and how to overcome them?",
    "body": "<p>What are the limitations of the <em>hill climbing algorithm</em>? How can we overcome these limitations?</p>\n"
  },
  {
    "tags": [
      "game-ai",
      "definitions",
      "agi",
      "alphazero",
      "alphago"
    ],
    "owner": {
      "account_id": 6691987,
      "reputation": 413,
      "user_id": 17601,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/99f667d393cbf083a1298f7967b05c11?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Siddhartha",
      "link": "https://ai.stackexchange.com/users/17601/siddhartha"
    },
    "is_answered": true,
    "view_count": 1652,
    "accepted_answer_id": 9170,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1639954965,
    "creation_date": 1543192977,
    "last_edit_date": 1639954965,
    "question_id": 9165,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/9165/is-alphazero-an-example-of-an-agi",
    "title": "Is AlphaZero an example of an AGI?",
    "body": "<p>From DeepMind's <a href=\"https://arxiv.org/pdf/1712.01815.pdf\" rel=\"noreferrer\">research paper</a> on arxiv.org:</p>\n\n<blockquote>\n  <p>In this paper, we apply a similar but fully generic algorithm, which\n  we call <em>AlphaZero</em>, to the games of chess and shogi as well as Go,\n  without any additional domain knowledge except the rules of the game,\n  demonstrating that a general-purpose reinforcement learning algorithm\n  can achieve, tabula rasa, superhuman performance across many\n  challenging domains.</p>\n</blockquote>\n\n<p>Does this mean AlphaZero is an example of AGI (Artificial General Intelligence)?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "convolutional-neural-networks",
      "image-recognition",
      "classification"
    ],
    "owner": {
      "account_id": 8893267,
      "reputation": 611,
      "user_id": 16521,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/441d2ab5e8c2f97c273d758c79f89fbe?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "The Pointer",
      "link": "https://ai.stackexchange.com/users/16521/the-pointer"
    },
    "is_answered": true,
    "view_count": 1358,
    "accepted_answer_id": 9995,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1590842819,
    "creation_date": 1547444313,
    "last_edit_date": 1590842418,
    "question_id": 9973,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/9973/can-machine-learning-algorithms-be-used-to-differentiate-between-small-differenc",
    "title": "Can machine learning algorithms be used to differentiate between small differences in details between images?",
    "body": "<p>I was wondering if machine learning algorithms (CNNs?) can be used/trained to differentiate between small differences in details between images (such as slight differences in shades of red or other colours, or the presence of small objects between otherwise very similar images?)? And then classify images based on these differences? If this is a difficult endeavour with our current machine learning algorithms, how can it be solved? Would using more data (more images) help?</p>\n\n<p>I would also appreciate it if people could please provide references to research that has focused on this, if possible. </p>\n\n<p>I've only just begun learning machine learning, and this is something that I've been wondering from my research.</p>\n"
  },
  {
    "tags": [
      "reference-request",
      "applications",
      "symbolic-ai",
      "expert-systems",
      "rule-based-systems"
    ],
    "owner": {
      "account_id": 2304865,
      "reputation": 2859,
      "user_id": 16565,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/e4iCE.jpg?s=256",
      "display_name": "malioboro",
      "link": "https://ai.stackexchange.com/users/16565/malioboro"
    },
    "is_answered": true,
    "view_count": 2996,
    "accepted_answer_id": 10388,
    "answer_count": 3,
    "score": 10,
    "last_activity_date": 1695416587,
    "creation_date": 1549149855,
    "last_edit_date": 1611286837,
    "question_id": 10371,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10371/is-the-expert-system-still-in-use-today",
    "title": "Is the expert system still in use today?",
    "body": "<p>In my country, the Expert System class is mandatory, if you want to take the AI specialization in most universities. In class, I learned how to make a rule-based system, <a href=\"https://en.wikipedia.org/wiki/Forward_chaining\" rel=\"noreferrer\">forward chaining</a>, <a href=\"https://en.wikipedia.org/wiki/Backward_chaining\" rel=\"noreferrer\">backward chaining</a>, <a href=\"https://en.wikipedia.org/wiki/Prolog\" rel=\"noreferrer\">Prolog</a>, etc.</p>\n<p>However, I have read somewhere on the web that <a href=\"https://en.wikipedia.org/wiki/Expert_system\" rel=\"noreferrer\">expert systems</a> are no longer used.</p>\n<p>Is that true? If yes, why? If not, where are they being used? With the rise of machine learning, they may not be as used as before, but is there any industry or company that still uses them today?</p>\n<p>Please, provide some references to support your claims.</p>\n"
  },
  {
    "tags": [
      "convolutional-neural-networks",
      "tensorflow"
    ],
    "owner": {
      "account_id": 11342013,
      "reputation": 235,
      "user_id": 21460,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8b15bacb4fcca012183c15a177d0d597?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "JChat",
      "link": "https://ai.stackexchange.com/users/21460/jchat"
    },
    "is_answered": true,
    "view_count": 17714,
    "accepted_answer_id": 10449,
    "answer_count": 3,
    "score": 10,
    "last_activity_date": 1629189296,
    "creation_date": 1549578807,
    "question_id": 10447,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10447/how-to-use-cnn-for-making-predictions-on-non-image-data",
    "title": "How to use CNN for making predictions on non-image data?",
    "body": "<p>I have a dataset which I have loaded as a data frame in Python. It consists of 21392 rows (the data instances, each row is one sample) and 1972 columns (the features). The last column i.e. column 1972 has string type labels (14 different categories of target labels). I would like to use a CNN to classify the data in this case and predict the target labels using the available features. This is a somewhat unconventional approach though it seems possible. However, I am very confused on how the methodology should be as I could not find any sample code/ pseudo code guiding on using CNN for Classifying non-image data, either in Tensorflow or Keras. Any help in this regard will be highly appreciated. Cheers!</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "terminology",
      "comparison"
    ],
    "owner": {
      "account_id": 12909486,
      "reputation": 438,
      "user_id": 12640,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/841fad8db76c277babc3d4bb5ba3bf5f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Paula Vega",
      "link": "https://ai.stackexchange.com/users/12640/paula-vega"
    },
    "is_answered": true,
    "view_count": 4381,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1641641845,
    "creation_date": 1550226534,
    "last_edit_date": 1641641631,
    "question_id": 10586,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10586/what-is-the-difference-between-an-episode-a-trajectory-and-a-rollout",
    "title": "What is the difference between an episode, a trajectory and a rollout?",
    "body": "<p>I often see the terms episode, trajectory, and rollout to refer to basically the same thing, a list of (state, action, rewards). Are there any concrete differences between the terms or can they be used interchangeably?</p>\n<p>In the following paragraphs, I'll summarize my current slightly vague understanding of the terms. Please point any inaccuracy or missing details in my definitions.</p>\n<p>I think <em>episode</em> has a more specific definition in that it begins with an initial state and finishes with a terminal state, where the definition of whether or not a state is initial or terminal is given by the definition of the MDP. Also, I understand an episode as a sequence of <span class=\"math-container\">$(s, a, r)$</span> sampled by interacting with the environment following a particular policy, so it should have a non-zero probability of occurring in the exact same order.</p>\n<p>With <em>trajectory</em>, the meaning is not as clear to me, but I believe a trajectory could represent only part of an episode and maybe the tuples could also be in an arbitrary order; even if getting such sequence by interacting with the environment has zero probability, it'd be ok, because we could say that such trajectory has zero probability of occurring.</p>\n<p>I think <em>rollout</em> is somewhere in between since I commonly see it used to refer to a sampled sequence of <span class=\"math-container\">$(s, a, r)$</span> from interacting with the environment under a given policy, but it might be only a segment of the episode, or even a segment of a continuing task, where it doesn't even make sense to talk about episodes.</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "social",
      "ethics",
      "human-like",
      "algorithmic-bias"
    ],
    "owner": {
      "account_id": 3369925,
      "reputation": 203,
      "user_id": 22368,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f19a3ac761df525d20390cf126e6219e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "P.S.",
      "link": "https://ai.stackexchange.com/users/22368/p-s"
    },
    "is_answered": true,
    "view_count": 475,
    "accepted_answer_id": 10631,
    "answer_count": 4,
    "score": 10,
    "last_activity_date": 1694384071,
    "creation_date": 1550259011,
    "last_edit_date": 1690398729,
    "question_id": 10603,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10603/how-is-it-that-ai-can-become-biased-and-what-are-the-proposals-to-mitigate-this",
    "title": "How is it that AI can become biased, and what are the proposals to mitigate this?",
    "body": "<p>This is not meant to be negative or a joke but rather looking for a productive solution on AI development, engineering and its impact on human life:</p>\n\n<p>Lately with my Google searches, the AI model keeps auto filling the ending of my searches with:</p>\n\n<p>“...in Vietnamese”</p>\n\n<p>And </p>\n\n<p>“...in a Vietnamese home”</p>\n\n<p>The issue is I have never searched for that but because of my last name the model is creating this context. </p>\n\n<p>The other issue is that I’m a halfy and my dad is actually third generation, I grew up mainstream American and don’t even speak Vietnamese. I’m not even sure what a Vietnamese home means. </p>\n\n<p>My buddy in a similar situation of South Asian and noticed the same exact thing more so with YouTube recommended videos. </p>\n\n<p>We already have enough issues in the US with racism, projections of who others expect us to be based on any number of things, stereotyping and putting people in boxes to limit them - I truly believe AI is adding to the problem, not helping. </p>\n\n<p>How can we fix this. Moreover, how can we use AI to bring out peoples true self, talents and empower and free them them to create their life how they like ?</p>\n\n<p>There is huge potential here to harness AI in ways that can bring us more freedom, joy and beauty so people can be the whole of themselves and with who they really are. Then meet peoples needs, wishes, dreams and hope. Given them shoulders to stand on to create their reality, not live someone else's projection of themselves.</p>\n"
  },
  {
    "tags": [
      "open-ai",
      "text-summarization",
      "text-generation",
      "gpt-2"
    ],
    "owner": {
      "account_id": 4996579,
      "reputation": 384,
      "user_id": 2424,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-kpRTzH5zyzs/AAAAAAAAAAI/AAAAAAAAAAA/ICc26Nac6mY/s256-rj/photo.jpg",
      "display_name": "Tom Hale",
      "link": "https://ai.stackexchange.com/users/2424/tom-hale"
    },
    "is_answered": true,
    "view_count": 7838,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1688008231,
    "creation_date": 1551618438,
    "last_edit_date": 1629198130,
    "question_id": 10982,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10982/how-do-i-use-gpt-2-to-summarise-text",
    "title": "How do I use GPT-2 to summarise text?",
    "body": "<p>In section 3.6 of the <a href=\"https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf\" rel=\"noreferrer\">OpenAI GPT-2 paper</a> it mentions summarising text based relates to this, but the method is described in very high-level terms:</p>\n\n<blockquote>\n  <p>To induce summarization behavior we add the text <code>TL;DR:</code> after the article and generate 100 tokens with Top-k random sampling (Fan et al., 2018) with k=2 which reduces repetition and encourages more abstractive summaries than greedy decoding. We use the first 3 generated sentences in these 100 tokens as the summary.</p>\n</blockquote>\n\n<p>Given a corpus of text, in concrete code terms (python preferred), how would I go about generating a summary of it?</p>\n"
  },
  {
    "tags": [
      "decision-trees",
      "random-forests",
      "id3-algorithm",
      "c4.5-algorithm"
    ],
    "owner": {
      "account_id": 8093905,
      "reputation": 203,
      "user_id": 23601,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/31a13791d8b21db16539db4bba1fd626?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "mallea",
      "link": "https://ai.stackexchange.com/users/23601/mallea"
    },
    "is_answered": true,
    "view_count": 5438,
    "accepted_answer_id": 11592,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1642225993,
    "creation_date": 1554100159,
    "last_edit_date": 1574305515,
    "question_id": 11576,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11576/are-decision-tree-learning-algorithms-deterministic",
    "title": "Are decision tree learning algorithms deterministic?",
    "body": "<p>Are decision tree learning algorithms deterministic? Given a fixed dataset, do they always produce a tree with the same structure? </p>\n\n<p>What about the random forest?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "q-learning",
      "pomdp",
      "markov-decision-process",
      "sarsa"
    ],
    "owner": {
      "account_id": 1938688,
      "reputation": 298,
      "user_id": 4042,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4520abefad7dce768388125393d9f78a?s=256&d=identicon&r=PG",
      "display_name": "drerD",
      "link": "https://ai.stackexchange.com/users/4042/drerd"
    },
    "is_answered": true,
    "view_count": 6099,
    "accepted_answer_id": 11615,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1554291786,
    "creation_date": 1554259229,
    "last_edit_date": 1554291545,
    "question_id": 11612,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11612/can-q-learning-be-used-in-a-pomdp",
    "title": "Can Q-learning be used in a POMDP?",
    "body": "<p>Can Q-learning (and SARSA) be directly used in a Partially Observable Markov Decision Process (POMDP)? If not, why not? My intuition is that the policies learned will be terrible because of partial observability. Are there ways to transform these algorithms so that they can be easily used in a POMDP?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "backpropagation",
      "gradient-descent",
      "stochastic-gradient-descent",
      "mini-batch-gradient-descent"
    ],
    "owner": {
      "account_id": 161590,
      "reputation": 245,
      "user_id": 23734,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/422fe6da66c4912411106a60a3de4da1?s=256&d=identicon&r=PG",
      "display_name": "Maanu",
      "link": "https://ai.stackexchange.com/users/23734/maanu"
    },
    "is_answered": true,
    "view_count": 12954,
    "accepted_answer_id": 11675,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1606694728,
    "creation_date": 1554453253,
    "last_edit_date": 1606694728,
    "question_id": 11667,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11667/is-back-propagation-applied-for-each-data-point-or-for-a-batch-of-data-points",
    "title": "Is back-propagation applied for each data point or for a batch of data points?",
    "body": "<p>I am new to deep learning and trying to understand the concept of back-propagation. I have a doubt about when the back-propagation is applied.  Assume that I have a training data set of 1000 images for handwritten letters,</p>\n<ol>\n<li><p>Is back-propagation applied immediately after getting the output for each input or after getting the output for all inputs in a batch?</p>\n</li>\n<li><p>Is back-propagation applied <span class=\"math-container\">$n$</span> times till the neural network gives a satisfactory result for a single data point before going to work on the next data point?</p>\n</li>\n</ol>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "comparison",
      "philosophy",
      "search"
    ],
    "owner": {
      "account_id": 14142186,
      "reputation": 103,
      "user_id": 22866,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/1beb1920bd0946fcec4e8a753ac0767f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "nvi",
      "link": "https://ai.stackexchange.com/users/22866/nvi"
    },
    "is_answered": true,
    "view_count": 1400,
    "accepted_answer_id": 11805,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1599916549,
    "creation_date": 1555180102,
    "last_edit_date": 1599916549,
    "question_id": 11803,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11803/what-is-the-difference-between-search-and-learning",
    "title": "What is the difference between search and learning?",
    "body": "<p>I came across an article, <a href=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html\" rel=\"nofollow noreferrer\">The Bitter Truth</a>, via the <a href=\"https://www.youtube.com/watch?v=wEgq6sT1uq8\" rel=\"nofollow noreferrer\">Two Minute Papers</a> YouTube Channel. Rich Sutton says...</p>\n<blockquote>\n<p>One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are <strong>search</strong> and <strong>learning</strong>.</p>\n</blockquote>\n<p>What is the difference between search and learning here? My understanding is that learning is a form of search -- where we iteratively search for some representation of data that minimizes a loss function in the context of deep learning.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "semi-supervised-learning",
      "data-labelling",
      "labeled-datasets",
      "unlabeled-datasets"
    ],
    "owner": {
      "account_id": 10454888,
      "reputation": 201,
      "user_id": 25393,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/mScNc.jpg?s=256",
      "display_name": "FirePower",
      "link": "https://ai.stackexchange.com/users/25393/firepower"
    },
    "is_answered": true,
    "view_count": 460,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1666570035,
    "creation_date": 1556954865,
    "last_edit_date": 1612118048,
    "question_id": 12127,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12127/how-to-deal-with-a-small-amount-of-labeled-samples",
    "title": "How to deal with a small amount of labeled samples?",
    "body": "<p>I'm trying to develop skills to deal with very small amounts of labeled samples (250 labeled/20000 total, 200 features) by practicing on Kaggle &quot;Don't Overfit&quot; <a href=\"https://www.kaggle.com/c/overfitting/data\" rel=\"nofollow noreferrer\">dataset</a> (Traget_Practice have provided all 20,000 Targets). I've read a ton of papers and articles on this topic, but everything I've tried did not improve simple regularized SVM results (best accuracy was 75 and AUC was 85) or any other algorithm result (LR, K-NN, NaiveBayes, RF, MLP). I believe the result can be better (on Leaderboard they go even over AUC 95)</p>\n<p>What I've tried without success:</p>\n<ul>\n<li><p><strong>Remove outliers</strong>  I've tried to remove 5%-10% outliers with EllipticEnvelope and with IsolationForest.</p>\n</li>\n<li><p><strong>Feature Selection</strong> I've tried RFE (with or without CV) + L1/L2 regularised LogisticRegression, and SelectKBest (with chi2).</p>\n</li>\n<li><p><strong>Semi-Supervised techniques</strong> I've tried co-training with different combinations of two complementary algorithms and :100-100: split features. I've also tried LabelSpreading, but I don't know how to provide the most uncertain samples (I tried predictions from other algorithms, but there were many mislabeled samples, hence was unsuccessful).</p>\n</li>\n<li><p><strong>Ensembling Classifiers</strong> StackingClassifier with all possible combinations of algorithms and this also didn't improve the result (the best is the same as SVM accuracy 75 and AUC 85).</p>\n</li>\n</ul>\n<p>Can anyone give me advice on what I'm doing wrong or what else to try?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "deep-learning",
      "recurrent-neural-networks",
      "attention"
    ],
    "owner": {
      "account_id": 6576609,
      "reputation": 203,
      "user_id": 4573,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/F0tsiwRV.png?s=256",
      "display_name": "PixelPioneer",
      "link": "https://ai.stackexchange.com/users/4573/pixelpioneer"
    },
    "is_answered": true,
    "view_count": 2751,
    "accepted_answer_id": 12316,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1557891895,
    "creation_date": 1557880068,
    "question_id": 12313,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12313/a-mathematical-explanation-of-attention-mechanism",
    "title": "A mathematical explanation of Attention Mechanism",
    "body": "<p>I am trying to understand why attention models are different than just using neural networks. Essentially the optimization of weights or using gates for protecting and controlling cell state (in recurrent networks), should eventually lead to the network focusing on certain parts of the input/source. So what is attention mechanism really adding to the network? </p>\n\n<p>A potential answer in the case of Encoder-Decoder RNNs is:  </p>\n\n<blockquote>\n  <p><a href=\"https://arxiv.org/pdf/1409.0473.pdf\" rel=\"noreferrer\">The most important distinguishing feature of this approach from the\n  basic encoder–decoder is that it does not attempt to encode a whole\n  input sentence into a single fixed-length vector. Instead, it encodes\n  the input sentence into a sequence of vectors and chooses a subset of\n  these vectors adaptively while decoding the translation. This frees a\n  neural translation model from having to squash all the information of\n  a source sentence, regardless of its length, into a fixed-length\n  vector. We show this allows a model to cope better with long\n  sentences.<br>\n  - Neural Machine Translation by Jointly Learning to Align and Translate</a></p>\n</blockquote>\n\n<p>which made sense and the paper says that it worked better for NMT.</p>\n\n<p>A previous study indicated that breaking down the sentence into phrases could lead to better results:</p>\n\n<blockquote>\n  <p><a href=\"https://arxiv.org/pdf/1409.1257.pdf\" rel=\"noreferrer\">In this paper, we propose a way to address this issue by automatically\n  segmenting an input sentence into phrases that can be easily\n  translated by the neural network translation model. Once each segment\n  has been independently translated by the neural machine translation\n  model, the translated clauses are concatenated to form a final\n  translation. Empirical results show a significant improvement in\n  translation quality for long sentences.<br>\n   - Overcoming the Curse of Sentence Length for Neural Machine\n  Translation using Automatic Segmentation</a></p>\n</blockquote>\n\n<p>which paved the way for further research resulting in attention models.</p>\n\n<p>I was also going through an article on <a href=\"https://medium.com/octavian-ai/attention-is-not-quite-all-you-need-cb605ef3aff6\" rel=\"noreferrer\">Attention is not quite all you need</a> where it said something similar:  </p>\n\n<blockquote>\n  <p>An LSTM has to learn to sequentially retain past values together in a\n  single internal state across multiple RNN iterations, whereas\n  attention can recall past sequence values at any point in a single\n  forward pass.</p>\n</blockquote>\n\n<p>and a more curated blog on the family of attention mechanism gives insight on how different ways have been formulated for implementing the concept: <a href=\"https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\" rel=\"noreferrer\">Attention? Attention!</a></p>\n\n<p>Specifically, I want to know how attention mechanism is formulated for this task (aforementioned) or in general. A detailed mathematical insight would be helpful, probably somewhat on these lines: <a href=\"https://srome.github.io/Understanding-Attention-in-Neural-Networks-Mathematically/\" rel=\"noreferrer\">Understanding Attention in NN mathematically</a></p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "gradient-descent",
      "stochastic-gradient-descent",
      "mini-batch-gradient-descent"
    ],
    "owner": {
      "account_id": 28642,
      "reputation": 260,
      "user_id": 25904,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d24c45635a5171615a7cdb936f36daad?s=256&d=identicon&r=PG",
      "display_name": "Ram Rachum",
      "link": "https://ai.stackexchange.com/users/25904/ram-rachum"
    },
    "is_answered": true,
    "view_count": 1498,
    "closed_date": 1573001073,
    "accepted_answer_id": 12517,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1606694750,
    "creation_date": 1558760906,
    "last_edit_date": 1606694750,
    "question_id": 12516,
    "link": "https://ai.stackexchange.com/questions/12516/is-neural-networks-training-done-one-by-one",
    "closed_reason": "Duplicate",
    "title": "Is neural networks training done one-by-one?",
    "body": "<p>I'm trying to learn neural networks by watching <a href=\"https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;t=17s\" rel=\"noreferrer\">this series of videos</a> and implementing a simple neural network in Python.</p>\n\n<p>Here's one of the things I'm wondering about: I'm training the neural network on sample data, and I've got 1,000 samples. The training consists of gradually changing the weights and biases to make the cost function result in a smaller cost.</p>\n\n<p><strong>My question:</strong> Should I be changing the weights/biases on every single sample before moving on to the next sample, or should I first calculate the desired changes for the entire lot of 1,000 samples, and only then start applying them to the network?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "residual-networks"
    ],
    "owner": {
      "account_id": 16252826,
      "reputation": 121,
      "user_id": 30170,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-0q6395tVkww/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rc8ErVi5sK-SyC9rh3ZyDGpcm6d1Q/s256-rj/photo.jpg",
      "display_name": "Ali Abdari",
      "link": "https://ai.stackexchange.com/users/30170/ali-abdari"
    },
    "is_answered": true,
    "view_count": 8181,
    "accepted_answer_id": 15748,
    "answer_count": 4,
    "score": 10,
    "last_activity_date": 1620123125,
    "creation_date": 1570299911,
    "last_edit_date": 1612579415,
    "question_id": 15743,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/15743/what-is-the-benefit-of-using-identity-mapping-layers-in-deep-neural-networks-lik",
    "title": "What is the benefit of using identity mapping layers in deep neural networks like ResNet?",
    "body": "<p>As I understand, ResNet has some identity mapping layers, whose task is to create the output as the same as the input of the layer. The ResNet solved the problem of accuracy degrading. But what is the benefit of adding identity mapping layers in intermediate layers?</p>\n<p>What's the effect of these identity layers on the feature vectors that will be produced in the last layers of the network? Is it helpful for the network to produce better representation for the input? If this expression is correct, what is the reason?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning"
    ],
    "owner": {
      "account_id": 16015211,
      "reputation": 355,
      "user_id": 30632,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-n6dpFBkYclk/AAAAAAAAAAI/AAAAAAAAFxQ/Wjgo4Y4-OXM/s256-rj/photo.jpg",
      "display_name": "Dhanush Giriyan",
      "link": "https://ai.stackexchange.com/users/30632/dhanush-giriyan"
    },
    "is_answered": true,
    "view_count": 1445,
    "accepted_answer_id": 16114,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1622209127,
    "creation_date": 1572072671,
    "last_edit_date": 1572217392,
    "question_id": 16087,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/16087/what-is-the-double-sample-problem-in-reinforcement-learning",
    "title": "What is the double sample problem in reinforcement learning?",
    "body": "<p>According to the <a href=\"https://arxiv.org/pdf/1712.10285.pdf\" rel=\"noreferrer\">SBEED: Convergent Reinforcement Learning with\nNonlinear Function Approximation</a> for convergent reinforcement learning, the Smoothed Bellman operator is a way to dodge the double sample problem? Can someone explain to me what the double sample problem is and how SBEED solves it?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "python",
      "math"
    ],
    "owner": {
      "account_id": 8344713,
      "reputation": 422,
      "user_id": 27947,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/c1Gx3.png?s=256",
      "display_name": "Maverick Meerkat",
      "link": "https://ai.stackexchange.com/users/27947/maverick-meerkat"
    },
    "is_answered": true,
    "view_count": 8767,
    "answer_count": 3,
    "score": 10,
    "last_activity_date": 1685906188,
    "creation_date": 1579354874,
    "last_edit_date": 1579782742,
    "question_id": 17566,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/17566/can-we-get-the-inverse-of-the-function-that-a-neural-network-represents",
    "title": "Can we get the inverse of the function that a neural network represents?",
    "body": "<p>I was wondering if it's possible to get the inverse of a neural network. If we view a NN as a function, can we obtain its inverse?</p>\n\n<p>I tried to build a simple MNIST architecture, with the input of (784,) and output of (10,), train it to reach good accuracy, and then inverse the predicted value to try and get back the input - but the results were nowhere near what I started with. (I used the pseudo-inverse for the W matrix.)</p>\n\n<p>My NN is basically the following function: </p>\n\n<p><span class=\"math-container\">$$\nf(x) = \\theta(xW + b), \\;\\;\\;\\;\\; \\theta(z) = \\frac{1}{1+e^{-z}}\n$$</span></p>\n\n<p>I.e. </p>\n\n<pre><code>def rev_sigmoid(y):\n    return np.log(y/(1-y))\n\ndef rev_linear(z, W, b):\n    return (z - b) @ np.linalg.pinv(W)\n\ny = model.predict(x_train[0:1])\nz = rev_sigmoid(y)\nx = rev_linear(z, W, b)\nx = x.reshape(28, 28)\nplt.imshow(x)\n</code></pre>\n\n<p><a href=\"https://i.sstatic.net/qBZiS.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/qBZiS.png\" alt=\"enter image description here\"></a></p>\n\n<p>^ This should have been a 5:</p>\n\n<p><a href=\"https://i.sstatic.net/8LI8n.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/8LI8n.png\" alt=\"t\"></a></p>\n\n<p>Is there a reason why it failed? And is it ever possible to get inverse of NN's? </p>\n\n<p>EDIT: it is also worth noting that doing the opposite does yield good results. I.e. starting with the y's (a 1-hot encoding of the digits) and using it to predict the image (an array of 784 bytes) using the same architecture: input (10,) and output (784,) with a sigmoid. This is not exactly equivalent, to an inverse as here you first do the linear transformation and then the non-linear. While in an inverse you would first do (well, undo) the non-linear, and then do (undo) the linear. I.e. <strong>the claim that the 784x10 matrix is collapsing too much information seems a bit odd to me, as there does exist a 10x784 matrix that can reproduce enough of that information</strong>. </p>\n\n<p><a href=\"https://i.sstatic.net/SAP02.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/SAP02.png\" alt=\"enter image description here\"></a></p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "comparison",
      "monte-carlo-methods",
      "temporal-difference-methods",
      "td-lambda"
    ],
    "owner": {
      "account_id": 15352349,
      "reputation": 155,
      "user_id": 32929,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-haehulSNqLk/AAAAAAAAAAI/AAAAAAAAARw/VvNdUL_SFuM/s256-rj/photo.jpg",
      "display_name": "Nick Kunz",
      "link": "https://ai.stackexchange.com/users/32929/nick-kunz"
    },
    "is_answered": true,
    "view_count": 8439,
    "accepted_answer_id": 17607,
    "answer_count": 3,
    "score": 10,
    "last_activity_date": 1719648295,
    "creation_date": 1579645038,
    "last_edit_date": 1591289875,
    "question_id": 17605,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/17605/what-is-the-intuition-behind-td-lambda",
    "title": "What is the intuition behind TD($\\lambda$)?",
    "body": "<p>I'd like to better understand temporal-difference learning. In particular, I'm wondering if it is prudent to think about TD(<span class=\"math-container\">$\\lambda$</span>) as a type of \"truncated\" Monte Carlo learning?</p>\n"
  },
  {
    "tags": [
      "reference-request",
      "papers",
      "computational-learning-theory",
      "pac-learning",
      "books"
    ],
    "owner": {
      "account_id": 16788374,
      "reputation": 258,
      "user_id": 36131,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-eDFRgwHX-dQ/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rfOTGBRc3IDAL-ct8vVSWzkRrdNSQ/s256-rj/photo.jpg",
      "display_name": "PMaynard",
      "link": "https://ai.stackexchange.com/users/36131/pmaynard"
    },
    "is_answered": true,
    "view_count": 3978,
    "accepted_answer_id": 20358,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1736612735,
    "creation_date": 1587139480,
    "last_edit_date": 1610826622,
    "question_id": 20355,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/20355/what-are-some-resources-on-computational-learning-theory",
    "title": "What are some resources on computational learning theory?",
    "body": "<p>Pretty soon I will be finishing up <a href=\"https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf\" rel=\"noreferrer\">Understanding Machine Learning: From Theory to Algorithms</a> by Shai Ben-David and Shai Shalev-Shwartz. I absolutely love the subject and want to learn more, the only issue is I'm having trouble finding a book that could come after this. Ultimately, my goal is to read papers in <a href=\"http://proceedings.mlr.press\" rel=\"noreferrer\">JMLR's COLT</a>.</p>\n\n<ol>\n<li><p>Is there a book similar to \"Understanding Machine Learning: From Theory to Algorithms\" that would progress my knowledge further and would go well after reading UML?</p></li>\n<li><p>Is there any other materials (not a book) that could allow me to learn more or prepare me for reading a journal like the one mentioned above?</p></li>\n</ol>\n\n<p>(Also, taking courses in this is not really an option, so this will be for self-study).</p>\n\n<p>(Note that I have also asked this question here on <a href=\"https://cstheory.stackexchange.com/q/46650/34637\">TCS SE</a>, but it was recommended I also ask here.)</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "architecture"
    ],
    "owner": {
      "account_id": 7398111,
      "reputation": 669,
      "user_id": 36083,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/QNYYVbnZ.jpg?s=256",
      "display_name": "Gilad Deutsch",
      "link": "https://ai.stackexchange.com/users/36083/gilad-deutsch"
    },
    "is_answered": true,
    "view_count": 3285,
    "answer_count": 4,
    "score": 10,
    "last_activity_date": 1589287780,
    "creation_date": 1587993680,
    "last_edit_date": 1588014297,
    "question_id": 20680,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/20680/should-neural-nets-be-deeper-the-more-complex-the-learning-problem-is",
    "title": "Should neural nets be deeper the more complex the learning problem is?",
    "body": "<p>I know it's not an exact science. But would you say that generally for more complicated tasks, deeper nets are required?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "comparison",
      "genetic-algorithms",
      "optimization",
      "evolutionary-algorithms"
    ],
    "owner": {
      "account_id": 14044287,
      "reputation": 215,
      "user_id": 37533,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/679Mt.jpg?s=256",
      "display_name": "Single Malt",
      "link": "https://ai.stackexchange.com/users/37533/single-malt"
    },
    "is_answered": true,
    "view_count": 2846,
    "accepted_answer_id": 21585,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1628339408,
    "creation_date": 1591115108,
    "last_edit_date": 1611169860,
    "question_id": 21584,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/21584/what-is-the-difference-between-reinforcement-learning-and-evolutionary-algorithm",
    "title": "What is the difference between reinforcement learning and evolutionary algorithms?",
    "body": "<p>What is the difference between reinforcement learning (RL) and evolutionary algorithms (EA)?</p>\n<p>I am trying to understand the basics of RL, but I do not yet have practical experience with RL. I know slightly more about EAs, but not enough to understand the difference between RL and EA, and that's why I'm asking for their main differences.</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "proofs",
      "policy-iteration",
      "bellman-equations",
      "bellman-operators"
    ],
    "owner": {
      "account_id": 7828322,
      "reputation": 201,
      "user_id": 20581,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2eb4b888e56d40a6b024c1da35dc3ceb?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "kevin",
      "link": "https://ai.stackexchange.com/users/20581/kevin"
    },
    "is_answered": true,
    "view_count": 5870,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1642865962,
    "creation_date": 1596163714,
    "last_edit_date": 1597064473,
    "question_id": 22783,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/22783/why-are-the-bellman-operators-contractions",
    "title": "Why are the Bellman operators contractions?",
    "body": "<p>In <a href=\"https://rlchina.org/lectures/lecture2.pdf#page=17\" rel=\"noreferrer\">these slides</a>, it is written</p>\n<p><span class=\"math-container\">\\begin{align}\n\\left\\|T^{\\pi} V-T^{\\pi} U\\right\\|_{\\infty} &amp; \\leq \\gamma\\|V-U\\|_{\\infty} \\tag{9} \\label{9} \\\\\n\\|T V-T U\\|_{\\infty} &amp; \\leq \\gamma\\|V-U\\|_{\\infty} \\tag{10} \\label{10}\n\\end{align}</span>\nwhere</p>\n<ul>\n<li><span class=\"math-container\">$F$</span> is the space of functions on domain <span class=\"math-container\">$\\mathbb{S}$</span>.</li>\n<li><span class=\"math-container\">$T^{\\pi}: \\mathbb{F} \\mapsto \\mathbb{F}$</span> is the Bellman\n<em>policy</em> operator</li>\n<li><span class=\"math-container\">$T: \\mathbb{F} \\mapsto \\mathbb{F}$</span> is the Bellman\n<strong>optimality</strong> operator</li>\n</ul>\n<p>In <a href=\"https://rlchina.org/lectures/lecture2.pdf#page=19\" rel=\"noreferrer\">slide 19</a>, they say that equality <span class=\"math-container\">$9$</span> follows from</p>\n<p><span class=\"math-container\">\\begin{align}\n{\\scriptsize\n\\left\\| \nT^{\\pi} V-T^{\\pi} U \n\\right\\|_{\\infty}\n= \n\\max_{s} \\gamma \\sum_{s^{\\prime}} \\operatorname{Pr}\n\\left(\ns^{\\prime} \\mid s, \\pi(s)\n\\right)\n\\left|\nV\\left(s^{\\prime}\\right) - U \n\\left(s^{\\prime}\\right)\n\\right| \\\\\n\\leq \\gamma \\left(\\sum \\operatorname{Pr} \\left(s^{\\prime} \\mid s, \\pi(s)\\right)\\right) \\max _{s^{\\prime}}\\left|V\\left(s^{\\prime}\\right)-U\\left(s^{\\prime}\\right)\\right| \\\\\n\\leq \\gamma\\|U-V\\|_{\\infty}\n}\n\\end{align}</span></p>\n<p>Why is that? Can someone explain to me this derivation?</p>\n<p>They also write that inequality \\ref{10} follows from</p>\n<p><span class=\"math-container\">\\begin{align}\n{\\scriptsize\n\\|T V-T U\\|_{\\infty}\n= \\max_{s} \n\\left| \n\\max_{a}\n\\left\\{ \nR(s, a) + \\gamma \\sum_{s^{\\prime}} \\operatorname{Pr}\n\\left(\ns^{\\prime} \\mid s, a\n\\right) V\n\\left(\ns^{\\prime}\n\\right)\n\\right\\}\n-\\max_{a} \\left\\{R(s, a)+\\gamma \\sum_{s^{\\prime}} \\operatorname{Pr}\\left(s^{\\prime} \\mid s, a\\right) U\\left(s^{\\prime}\\right)\\right\\} \\right| \\\\\n\\leq \\max _{s, a}\\left|R(s, a)+\\gamma \\sum_{s^{\\prime}} \\operatorname{Pr}\\left(s^{\\prime} \\mid s, a\\right) V\\left(s^{\\prime}\\right)\n-R(s, a)-\\gamma \\sum \\operatorname{Pr}\\left(s^{\\prime} \\mid s, a\\right) V\\left(s^{\\prime}\\right) \\right| \\\\\n= \n\\gamma \\max _{s, a}\\left|\\sum_{s^{\\prime}} \\operatorname{Pr}\\left(s^{\\prime} \\mid s, a\\right)\\left(V\\left(s^{\\prime}\\right)-U\\left(s^{\\prime}\\right)\\right)\\right| \\\\\n\\leq \\gamma\\left(\\sum_{s^{\\prime}} \\operatorname{Pr}\\left(s^{\\prime} \\mid s, a\\right)\\right) \\max _{s^{\\prime}}\\left|\\left(V\\left(s^{\\prime}\\right)-U\\left(s^{\\prime}\\right)\\right)\\right| \\\\\n\\leq \n\\gamma\\|V-U\\|_{\\infty}\n}\n\\end{align}</span></p>\n<p>Can someone explain to me also this derivation?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "deep-learning",
      "generative-adversarial-networks",
      "heuristics"
    ],
    "owner": {
      "account_id": 9414012,
      "reputation": 144,
      "user_id": 44121,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e1b2914266f4f8b35b9c36ba07e73619?s=256&d=identicon&r=PG",
      "display_name": "Mafu",
      "link": "https://ai.stackexchange.com/users/44121/mafu"
    },
    "is_answered": true,
    "view_count": 984,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1682001405,
    "creation_date": 1611649916,
    "last_edit_date": 1633767374,
    "question_id": 25989,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/25989/why-is-my-gan-more-unstable-with-bigger-networks",
    "title": "Why is my GAN more unstable with bigger networks?",
    "body": "<p>I am working with generative adversarial networks (GANs) and one of my aims at the moment is to reproduce samples in two dimensions that are distributed according to a circle (see animation). When using a GAN with small networks (<strong>3 layers with 50 neurons each</strong>), the results are more stable than with bigger layers (<strong>3 layers with 500 neurons each</strong>). <strong>All other hyperparameters are the same</strong> (see details of my implementation below).</p>\n<p>I am wondering if anyone has an explanation for why this is the case. I could obviously try to tune the other hyperparameters to get good performance but would be interested in knowing if someone has <strong>heuristics about what is needed to change whenever I change the size of the networks</strong>.</p>\n<p><a href=\"https://i.sstatic.net/pk3uO.gif\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/pk3uO.gif\" alt=\"GAN with smaller layer size reproduces the original samples better\" /></a></p>\n<hr />\n<h2>Network/Training parameters</h2>\n<p>I use PyTorch with the following settings for the GAN:</p>\n<p><strong>Networks:</strong></p>\n<ul>\n<li>Generator/Discriminator Architecture (all dense layers): 100-50-50-50-2 (small); 100-500-500-500-2 (big)</li>\n<li>Dropout: p=0.4 for generator (except last layer), p=0 for discriminator</li>\n<li>Activation functions: LeakyReLU (slope 0.1)</li>\n</ul>\n<p><strong>Training:</strong></p>\n<ul>\n<li>Optimizer: Adam</li>\n<li>Learning Rate: 1e-5 (for both networks)</li>\n<li>Beta1, Beta2: 0.9, 0.999</li>\n<li>Batch size: 50</li>\n</ul>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "reference-request",
      "gradient-descent",
      "learning-rate"
    ],
    "owner": {
      "account_id": 14857177,
      "reputation": 218,
      "user_id": 44529,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-WttGu9v7mEA/AAAAAAAAAAI/AAAAAAAAAAA/AGDgw-inTaKsnn3eQlJqPF93Yits_Qydag/mo/s256-rj/photo.jpg",
      "display_name": "Sadaf Shafi",
      "link": "https://ai.stackexchange.com/users/44529/sadaf-shafi"
    },
    "is_answered": true,
    "view_count": 1651,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1669649012,
    "creation_date": 1613968998,
    "last_edit_date": 1614005597,
    "question_id": 26502,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/26502/is-there-an-ideal-range-of-learning-rate-which-always-gives-a-good-result-almost",
    "title": "Is there an ideal range of learning rate which always gives a good result almost in all problems?",
    "body": "<p>I once read somewhere that there is a range of learning rate within which learning is optimal in almost all the cases, but I can't find any literature about it. All I could get is the following graph from the paper: <a href=\"https://www.researchgate.net/publication/3907199_The_need_for_small_learning_rates_on_large_problems\" rel=\"noreferrer\"><em>The need for small learning rates on large problems</em></a></p>\n<p><a href=\"https://i.sstatic.net/o7z4Y.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/o7z4Y.jpg\" alt=\"enter image description here\" /></a></p>\n<p>In the context of neural networks trained with gradient descent, is there a range of the learning rate, which should be used to reduce the training time and get a good performance in almost all problems?</p>\n"
  },
  {
    "tags": [
      "transformer",
      "attention",
      "weights",
      "weights-initialization"
    ],
    "owner": {
      "account_id": 18387461,
      "reputation": 2812,
      "user_id": 38846,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Hlbol.jpg?s=256",
      "display_name": "spiridon_the_sun_rotator",
      "link": "https://ai.stackexchange.com/users/38846/spiridon-the-sun-rotator"
    },
    "is_answered": true,
    "view_count": 6535,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1690122069,
    "creation_date": 1630485709,
    "last_edit_date": 1638284521,
    "question_id": 30491,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/30491/is-there-a-proper-initialization-technique-for-the-weight-matrices-in-multi-head",
    "title": "Is there a proper initialization technique for the weight matrices in multi-head attention?",
    "body": "<p>Self-attention layers have 4 learnable tensors (in the vanilla formulation):</p>\n<ul>\n<li>Query matrix <span class=\"math-container\">$W_Q$</span></li>\n<li>Key matrix <span class=\"math-container\">$W_K$</span></li>\n<li>Value matrix <span class=\"math-container\">$W_V$</span></li>\n<li>Output matrix <span class=\"math-container\">$W_O$</span></li>\n</ul>\n<p>Nice illustration from  <a href=\"https://jalammar.github.io/illustrated-transformer/\" rel=\"noreferrer\">https://jalammar.github.io/illustrated-transformer/</a></p>\n<p><a href=\"https://i.sstatic.net/X6STQ.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/X6STQ.png\" alt=\"enter image description here\" /></a></p>\n<p>However, I do not know how should one choose the default initialization for these parameters.</p>\n<p>In the works, devoted to MLP and CNNs, one chooses <code>xavier/glorot</code> or <code>he</code> initialization by default, as they can be shown to approximately preserve the magnitude in the forward and backward pass, <a href=\"https://arxiv.org/abs/2012.05760\" rel=\"noreferrer\">as shown in these notes</a>.</p>\n<p>However, I wonder, whether there is some study of good initialization for Transformers. The default implementation in <code>Tensorflow</code> and <code>PyTorch</code> use <code>xavier/glorot</code>.</p>\n<p>Probably, any reasonable choice will work fine.</p>\n"
  },
  {
    "tags": [
      "research",
      "academia"
    ],
    "owner": {
      "account_id": 2740787,
      "reputation": 1040,
      "user_id": 32621,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/133c43586dc40d84fc7f87671253d3ad?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "SpiderRico",
      "link": "https://ai.stackexchange.com/users/32621/spiderrico"
    },
    "is_answered": true,
    "view_count": 184,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1690879475,
    "creation_date": 1631046574,
    "last_edit_date": 1640779542,
    "question_id": 31597,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/31597/is-there-a-venue-to-publish-negative-results-in-ai-ml-domain",
    "title": "Is there a venue to publish negative results in AI/ML domain?",
    "body": "<p>Negative results occur frequently in AI/ML research (and perhaps in other domains too). Most of the time, these results are not published. This is mostly because your typical AI/ML conference doesn't accept such papers.</p>\n<p>However, are there any venues to publish these results?  I believe these results can be still useful to look at before you delve into a certain project so that you'd at least know what approaches don't work.</p>\n<p>As an example venue, there seems to be <a href=\"https://perfail-workshop.github.io/2022/\" rel=\"noreferrer\">PerFail workshop</a> from the pervasive computing domain. So, is there something similar for AI/ML?</p>\n"
  },
  {
    "tags": [
      "comparison",
      "objective-functions",
      "siamese-neural-network",
      "contrastive-learning",
      "triplet-loss-function"
    ],
    "owner": {
      "account_id": 2526078,
      "reputation": 371,
      "user_id": 41187,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/3bab6dd8e8cd71a3e564d68189721571?s=256&d=identicon&r=PG",
      "display_name": "Exploring",
      "link": "https://ai.stackexchange.com/users/41187/exploring"
    },
    "is_answered": true,
    "view_count": 9483,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1689808000,
    "creation_date": 1655578846,
    "last_edit_date": 1655845801,
    "question_id": 35977,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/35977/what-is-the-difference-between-the-triplet-loss-and-the-contrastive-loss",
    "title": "What is the difference between the triplet loss and the contrastive loss?",
    "body": "<p>What is the difference between the <strong>triplet loss</strong> and the <strong>contrastive loss</strong>?</p>\n<p>They look same to me. I don't understand the nuances between the two. I have the following queries:</p>\n<ul>\n<li>When to use what?</li>\n<li>What are the use cases and advantages or disadvantages of the two?</li>\n<li>Also, how do they fit with the siamese network discussion?</li>\n</ul>\n"
  },
  {
    "tags": [
      "ai-design",
      "pattern-recognition",
      "image-generation",
      "diffusion-models"
    ],
    "owner": {
      "account_id": 6925995,
      "reputation": 203,
      "user_id": 61397,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-omCqNPsVUcY/AAAAAAAAAAI/AAAAAAAAAAA/LJrkXEKSBgw/s256-rj/photo.jpg",
      "display_name": "Nicola Lepetit",
      "link": "https://ai.stackexchange.com/users/61397/nicola-lepetit"
    },
    "is_answered": true,
    "view_count": 5791,
    "accepted_answer_id": 37232,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1664453642,
    "creation_date": 1664352638,
    "last_edit_date": 1664453642,
    "question_id": 37222,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/37222/using-ai-to-extend-an-imagine-pattern",
    "title": "Using AI to extend an imagine pattern",
    "body": "<p>I have created some nice patterns using the <a href=\"https://midjourney.gitbook.io/docs/\" rel=\"nofollow noreferrer\">MidJourney tool</a>. I'd like to find a way to extend these patterns, and I was thinking about an AI tool that takes one of these patterns and extends it in all directions surrounding the original pattern.</p>\n<p>Just to give you an idea, this is one of those patterns:</p>\n<p><a href=\"https://i.sstatic.net/LvC7f.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/LvC7f.jpg\" alt=\"Enter image description here\" /></a></p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "reference-request",
      "training",
      "datasets",
      "data-preprocessing"
    ],
    "owner": {
      "account_id": 53211,
      "reputation": 341,
      "user_id": 20721,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f19a3ac761df525d20390cf126e6219e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user366312",
      "link": "https://ai.stackexchange.com/users/20721/user366312"
    },
    "is_answered": true,
    "view_count": 2300,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1671798999,
    "creation_date": 1669520852,
    "last_edit_date": 1671798999,
    "question_id": 38045,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/38045/how-can-i-encode-angle-data-to-train-neural-networks",
    "title": "How can I encode angle data to train neural networks?",
    "body": "<p>I am training a neural network where the target data is a vector of angles in radians (between <span class=\"math-container\">$0$</span> and <span class=\"math-container\">$2\\pi$</span>).</p>\n<p>I am looking for study material on how to encode this data.</p>\n<p>Can you supply me with a book or research paper that covers this topic comprehensively?</p>\n"
  },
  {
    "tags": [
      "ensemble-learning"
    ],
    "owner": {
      "account_id": 24023903,
      "reputation": 1027,
      "user_id": 62466,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gg8FPkPrDJ6ppvEo_oMsRobnMxCg-rC31AmUyp8=k-s256",
      "display_name": "Snehal Patel",
      "link": "https://ai.stackexchange.com/users/62466/snehal-patel"
    },
    "is_answered": true,
    "view_count": 1129,
    "accepted_answer_id": 38052,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1669737568,
    "creation_date": 1669558896,
    "last_edit_date": 1669561114,
    "question_id": 38051,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/38051/how-can-an-ensemble-be-more-accurate-than-the-best-base-classifier-in-that-ensem",
    "title": "How can an ensemble be more accurate than the best base classifier in that ensemble?",
    "body": "<p><strong>BACKGROUND:</strong>  Ensemble classifiers are said to reduce <em>bias</em> by taking an &quot;average&quot; of predictions of several base classifiers that comprise the ensemble.  However, I am uncertain if this necessarily means that they can increase <em>accuracy</em>.  My intuition tells me that the ensemble classifier should perform <strong>no better and possibly even worse</strong> than the <em>best</em> base classifier in the ensemble.  This seems especially true for bagging approaches which use strong classifiers anyway.  When you have a &quot;star performer&quot;, it just doesn't seem to make intuitive sense to &quot;dilute&quot; its performance with subpar performers.</p>\n<p>Nonetheless, from my novice-level reading, it seems that ensembles can be <strong>as good or possibly even better</strong> than all of the individual base classifiers, but I'm still not clear why.</p>\n<p><strong>QUESTION:</strong>  How can an ensemble be more accurate than the best base classifier in that ensemble?</p>\n"
  },
  {
    "tags": [
      "chat-bots",
      "training-datasets",
      "language-model",
      "gpt-3",
      "chatgpt"
    ],
    "owner": {
      "account_id": 6622653,
      "reputation": 202,
      "user_id": 65035,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Kjl2L.png?s=256",
      "display_name": "Obie 2.0",
      "link": "https://ai.stackexchange.com/users/65035/obie-2-0"
    },
    "is_answered": true,
    "view_count": 1675,
    "accepted_answer_id": 39020,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1675466448,
    "creation_date": 1671181108,
    "last_edit_date": 1671218522,
    "question_id": 38361,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/38361/what-causes-chatgpt-to-generate-responses-that-refer-to-itself-as-a-bot-or-lm",
    "title": "What causes ChatGPT to generate responses that refer to itself as a bot or LM?",
    "body": "<p>ChatGPT occasionally generates responses to prompts that refer to itself as a &quot;bot&quot; or &quot;language model.&quot;</p>\n<p>For instance, when given a certain input (the first paragraph of <a href=\"https://politics.meta.stackexchange.com/q/6454\">this question</a>) ChatGPT produces (in part) the output:</p>\n<blockquote>\n<p>It is not appropriate for a language model like myself to provide a\nstance on the policies of a specific website or community.</p>\n</blockquote>\n<p>To my understanding, ChatGPT is not a person that is conscious of its own existence and identity as a bot — it is a model trained on large quantities of undifferentiated text gathered from the Internet and largely reproduces the most common patterns given the context, which is why its responses seem very generic much of the time.\nPresumably very little of this data involved humans referring to themselves as language models or chatbots—that is, something like &quot;a language model&quot; should very rarely have been followed by &quot;like myself.&quot;</p>\n<p>As such, what causes ChatGPT to produce patterns referring to itself as a chatbot or language model? Which patterns in the training data or elements of the model structure (or even hard-coding?) cause it to generate responses like this?</p>\n"
  },
  {
    "tags": [
      "open-ai",
      "fine-tuning",
      "chatgpt",
      "gpt-3"
    ],
    "owner": {
      "account_id": 4441227,
      "reputation": 203,
      "user_id": 67673,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/45af2cfeee4e1743a4a6e6a3da72b4e0?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "iMad",
      "link": "https://ai.stackexchange.com/users/67673/imad"
    },
    "is_answered": true,
    "view_count": 3525,
    "accepted_answer_id": 39121,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1676085132,
    "creation_date": 1675356002,
    "last_edit_date": 1675528286,
    "question_id": 39023,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/39023/are-gpt-3-5-series-models-based-on-gpt-3",
    "title": "Are GPT-3.5 series models based on GPT-3?",
    "body": "<p>In the official <a href=\"https://openai.com/blog/chatgpt/\" rel=\"noreferrer\">blog post about ChatGPT</a> from OpenAI, there is this paragraph explaining how ChatGPT model was trained:</p>\n<blockquote>\n<p>We trained this model using Reinforcement Learning from Human Feedback\n(RLHF), <strong>using the same methods as InstructGPT</strong>, but with slight\ndifferences in the data collection setup. We trained an initial model\nusing supervised fine-tuning: human AI trainers provided conversations\nin which they played both sides—the user and an AI assistant. We gave\nthe trainers access to model-written suggestions to help them compose\ntheir responses. We mixed this new dialogue dataset with the\nInstructGPT dataset, which we transformed into a dialogue format.</p>\n</blockquote>\n<p>Especially this part:</p>\n<blockquote>\n<p>We trained an initial model using supervised fine-tuning</p>\n</blockquote>\n<p>My question is about the said <em>initial model</em>, is it some new model that has been trained from scratch or is it a GPT-3 model that has been fine tuned for specific tasks resulting in <a href=\"https://platform.openai.com/docs/model-index-for-researchers\" rel=\"noreferrer\">GPT-3.5 series</a> ?</p>\n<p>On the other hand, from the <a href=\"https://openai.com/blog/instruction-following/\" rel=\"noreferrer\">InstructGPT blog post</a>, it is clearly stated that:</p>\n<blockquote>\n<p>To make our models safer, more helpful, and more aligned, we use an\nexisting technique called reinforcement learning from human feedback\n(RLHF). On prompts submitted by our customers to the API,our labelers\nprovide demonstrations of the desired model behavior, and rank several\noutputs from our models. <strong>We then use this data to fine-tune GPT-3</strong>.</p>\n</blockquote>\n<p>So does this mean that GPT-3.5 series models (and consequently ChatGPT) are fine-tuned from GPT-3 base model ?</p>\n"
  },
  {
    "tags": [
      "transformer",
      "attention",
      "word-embedding"
    ],
    "owner": {
      "account_id": 8044988,
      "reputation": 311,
      "user_id": 35614,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/4ESTS.jpg?s=256",
      "display_name": "Soltius",
      "link": "https://ai.stackexchange.com/users/35614/soltius"
    },
    "is_answered": true,
    "view_count": 10138,
    "accepted_answer_id": 39195,
    "answer_count": 2,
    "score": 10,
    "last_activity_date": 1704790360,
    "creation_date": 1676371044,
    "last_edit_date": 1704790120,
    "question_id": 39151,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/39151/attention-is-all-you-need-paper-how-are-the-q-k-v-values-calculated",
    "title": "&quot;Attention is all you need&quot; paper : How are the Q, K, V values calculated?",
    "body": "<p>The seminal <a href=\"https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\" rel=\"nofollow noreferrer\">Attention is all you need</a> paper (Google Brain team, 2017) introduces Transformers and implements the attention mecanism with &quot;queries, keys, values&quot;, in an analogy to a retrieval system.</p>\n<p>I understand the whole process of multi-head attention and such (<em>i.e.</em>, what is done with the <span class=\"math-container\">$Q$</span>, <span class=\"math-container\">$K$</span>, <span class=\"math-container\">$V$</span> values and why), but I'm confused on <strong>how these values are computed in the first place</strong>. AFAICT, the paper seems to completely leave that out.</p>\n<p>Both Figure 2 of the paper and equations explaining Attention and Multihead attention start with <span class=\"math-container\">$Q$</span>,<span class=\"math-container\">$K$</span>,<span class=\"math-container\">$V$</span> already there :</p>\n<p><a href=\"https://i.sstatic.net/t6qJz.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/t6qJz.png\" alt=\"enter image description here\" /></a></p>\n<p>The answers regaridng the origin of <span class=\"math-container\">$Q$</span>,<span class=\"math-container\">$K$</span>,<span class=\"math-container\">$V$</span> I've found so far haven't satisfied me :</p>\n<ul>\n<li><p>In this <a href=\"https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms\">similar question</a>, the <a href=\"https://stats.stackexchange.com/a/424127/201218\">accepted answer</a> says &quot;<em>The proposed multihead attention alone doesn't say much about how the queries, keys, and values are obtained, they can come from different sources depending on the application scenario.</em>&quot;. If this is the case, then why isn't the computing of <span class=\"math-container\">$Q$</span>,<span class=\"math-container\">$K$</span>,<span class=\"math-container\">$V$</span> made more clear in the paper, at the very least for the task of language translation for which they show some numerical results and so obviously did compute <span class=\"math-container\">$Q$</span>,<span class=\"math-container\">$K$</span>,<span class=\"math-container\">$V$</span> in some way ?</p>\n</li>\n<li><p>I also see some answers (eg <a href=\"https://stats.stackexchange.com/a/463320/201218\">this one on the same question</a>) which say that <span class=\"math-container\">$Q$</span>, <span class=\"math-container\">$K$</span>, <span class=\"math-container\">$V$</span> are the result of multiplication of the input embedding with some matrices <span class=\"math-container\">$W$</span>. This is also what is shown in the popular blog post <a href=\"http://jalammar.github.io/illustrated-transformer/\" rel=\"nofollow noreferrer\">The Illustrated Transformer</a> :</p>\n</li>\n</ul>\n<p><a href=\"https://i.sstatic.net/8QJ6pl.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/8QJ6pl.png\" alt=\"enter image description here\" /></a></p>\n<p>These &quot;projection&quot; matrices (<span class=\"math-container\">$W^Q$</span>, <span class=\"math-container\">$W^K$</span>, <span class=\"math-container\">$W^V$</span>) do seem to appear in the the definition of attention in the definition of <span class=\"math-container\">$head_i$</span> (see top figure), but according to that equation, these matrices are multiplied by <span class=\"math-container\">$Q$</span>, <span class=\"math-container\">$K$</span>, <span class=\"math-container\">$V$</span> (still appearing out of thin air, so the problem of their definition remains) and so the resulting product can't also be <span class=\"math-container\">$Q$</span>, <span class=\"math-container\">$K$</span>, <span class=\"math-container\">$V$</span>.</p>\n<p>How are the <span class=\"math-container\">$Q$</span>, <span class=\"math-container\">$K$</span>, <span class=\"math-container\">$V$</span> values computed ?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "natural-language-processing",
      "transformer",
      "attention"
    ],
    "owner": {
      "account_id": 9914032,
      "reputation": 624,
      "user_id": 23811,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-6jIAe62Q10s/AAAAAAAAAAI/AAAAAAAAABw/kRBVixTmLmk/s256-rj/photo.jpg",
      "display_name": "Peyman",
      "link": "https://ai.stackexchange.com/users/23811/peyman"
    },
    "is_answered": true,
    "view_count": 4884,
    "accepted_answer_id": 40256,
    "answer_count": 1,
    "score": 10,
    "last_activity_date": 1683100640,
    "creation_date": 1682869039,
    "last_edit_date": 1683100640,
    "question_id": 40252,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/40252/why-are-biases-typically-not-used-in-attention-mechanism",
    "title": "Why are biases (typically) not used in attention mechanism?",
    "body": "<p>Watching <a href=\"https://youtu.be/kCc8FmEb1nY?t=4767\" rel=\"noreferrer\">this video</a> implementing attention in a transformer. He set query, key, and value biases to <code>False</code> and said &quot;Typically, people don't use biases for these&quot;.</p>\n<p>Even in <a href=\"https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention\" rel=\"noreferrer\">official PyTorch code</a> the default bias is <code>False</code>:</p>\n<blockquote>\n<p>add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: <code>False</code>.</p>\n</blockquote>\n<p>What is the reason behind that?</p>\n"
  },
  {
    "tags": [
      "genetic-algorithms",
      "evolutionary-algorithms"
    ],
    "owner": {
      "account_id": 17832331,
      "reputation": 121,
      "user_id": 92852,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/8Jpw3.png?s=256",
      "display_name": "bishop-fish",
      "link": "https://ai.stackexchange.com/users/92852/bishop-fish"
    },
    "is_answered": true,
    "view_count": 1682,
    "answer_count": 4,
    "score": 10,
    "last_activity_date": 1743059896,
    "creation_date": 1742787144,
    "question_id": 48293,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/48293/relevance-of-genetic-algorithms-in-modern-research",
    "title": "Relevance of genetic algorithms in modern research",
    "body": "<p>I am considering pursuing a career in AI (currently have an undergraduate background in Philosophy/Computer Science) and have been taking some time to research particular topics. One class of method that piqued my interest was the <em>genetic algorithm</em>. For those who are unfamiliar I can give a brief rundown. These are local search methods that maintain a population of &quot;organisms&quot; (potential models or solutions to the problem) that are encoded in some manner (typically as simple binary strings). A &quot;fitness&quot; function is defined over the organisms to determine to what extent it solves the problem. At each iteration of the algorithm, organisms undergo random mutation and reproduction. The former involves simple bit flips of randomly selected indices, while the latter involves pairing off organisms by fitness and creating offspring in the hopes that they will &quot;inherit&quot; their parents' fitness.</p>\n<p>I am curious for anyone who works in these fields or has specialized knowledge to what extent genetic algorithms are still researched and considered relevant. Obviously I can do some research online and browse academic sources but I'm interested in hearing from people &quot;on the ground&quot; so to speak. My immediate thought as a potential avenue for exploration is to determine how exactly the reproduction process can actually accentuate the fitness of the parents. It seems clear enough that in many cases reproduction may do no better than mutation in so far as the average fitness of the offspring doesn't diverge significantly from that of the parents. To illustrate this, imagine a case study where the fitness function is simply the proportion of 1s in an organism, and the reproduction operation simply swaps selected indices between two parents. Obviously the expected fitness of the offspring will always be the same as the average fitness of the parents, since no 1s are added or removed in this process. Multi-modal fitness functions may be something else to consider.</p>\n<p>Thanks!</p>\n"
  },
  {
    "tags": [
      "declarative-programming"
    ],
    "owner": {
      "account_id": 5780101,
      "reputation": 1305,
      "user_id": 69,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/637eff3f3244f3658d31a25ea7636d82?s=256&d=identicon&r=PG",
      "display_name": "intcreator",
      "link": "https://ai.stackexchange.com/users/69/intcreator"
    },
    "is_answered": true,
    "view_count": 2290,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1542544496,
    "creation_date": 1470161864,
    "last_edit_date": 1542129881,
    "question_id": 108,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/108/what-are-the-main-advantages-of-using-declarative-programming-languages-for-buil",
    "title": "What are the main advantages of using declarative programming languages for building AI?",
    "body": "<p>What specific advantages of declarative languages make them more applicable to AI than imperative languages?  </p>\n\n<p>What can declarative languages do easily that other languages styles find difficult for this kind of problem?</p>\n"
  },
  {
    "tags": [
      "neural-networks"
    ],
    "owner": {
      "account_id": 378721,
      "reputation": 191,
      "user_id": 1283,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6e2168c4647a4f096a14c254bd3d3f68?s=256&d=identicon&r=PG",
      "display_name": "ratchet freak",
      "link": "https://ai.stackexchange.com/users/1283/ratchet-freak"
    },
    "is_answered": true,
    "view_count": 778,
    "answer_count": 3,
    "score": 9,
    "last_activity_date": 1487273924,
    "creation_date": 1470394179,
    "question_id": 1363,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/1363/is-it-beneficial-to-represent-a-neural-net-as-a-matrix",
    "title": "Is it beneficial to represent a neural net as a matrix?",
    "body": "<p>A neural network is a directed weighted graph. These can be represented by a (sparse) matrix. Doing so can expose some elegant properties of the network.</p>\n\n<p>Is this technique beneficial for examining neural networks?</p>\n"
  },
  {
    "tags": [
      "agi",
      "symbolic-ai",
      "embodied-cognition"
    ],
    "owner": {
      "account_id": 1448821,
      "reputation": 7286,
      "user_id": 42,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ab21a7fe07af4e3a153d65c420e87e85?s=256&d=identicon&r=PG",
      "display_name": "NietzscheanAI",
      "link": "https://ai.stackexchange.com/users/42/nietzscheanai"
    },
    "is_answered": true,
    "view_count": 487,
    "accepted_answer_id": 1418,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1470507584,
    "creation_date": 1470504290,
    "question_id": 1415,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/1415/what-kind-of-body-if-any-does-intelligence-require",
    "title": "What kind of body (if any) does intelligence require?",
    "body": "<p>In the mid 1980s, Rodney Brooks famously created the foundations of \"the new AI\". The central claim was that the symbolist approach of 'Good Old Fashioned AI' (GOFAI) had failed by attempting to 'cream cognition off the top', and that <em>embodied cognition</em> was required, i.e. built from the bottom up in a 'hierarchy of competances' (e.g. basic locomotion -> wandering around -> actively foraging) etc.</p>\n\n<p>I imagine most AI researchers would agree that the 'embodied cognition' perspective has now (at least tacitly) supplanted GOFAI as the mainstream.</p>\n\n<p>My question takes the form of a thought experiment and asks: \"Which (if any)  aspects of 'embodied' can be relaxed/omitted before we lose something essential for AGI?\"</p>\n"
  },
  {
    "tags": [
      "agi",
      "research",
      "academia",
      "education"
    ],
    "owner": {
      "account_id": 7290901,
      "reputation": 419,
      "user_id": 1321,
      "user_type": "registered",
      "accept_rate": 20,
      "profile_image": "https://www.gravatar.com/avatar/4480675ad0ede0043e7caba04750d974?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user289661",
      "link": "https://ai.stackexchange.com/users/1321/user289661"
    },
    "is_answered": true,
    "view_count": 2489,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1640100396,
    "creation_date": 1470535714,
    "last_edit_date": 1640100396,
    "question_id": 1432,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1432/what-kind-of-education-is-required-for-researchers-in-ai",
    "title": "What kind of education is required for researchers in AI?",
    "body": "<p>Suppose my goal is to collaborate and create an advanced AI, for instance, one that resembles a human being and the project would be on the frontier of AI research. What kind of skills would I need?</p>\n<p>I am talking about specific things, like what university program should I complete to enter and be competent in the field.</p>\n<p>Here are some of the things that I thought about, just to exemplify what I mean:</p>\n<ul>\n<li><p>Computer sciences: obviously, the AI is built on computers, it wouldn't hurt to know how computers work, but some low-level stuff and machine-specific things do not seem essential, I may be wrong of course.</p>\n</li>\n<li><p>Psychology: if AI resembles human beings, knowledge of human cognition would probably be useful, although I do not imagine neurology on a cellular level or complicated psychological quirks typical to human beings like the Oedipus complex would be relevant, but again, I may be wrong.</p>\n</li>\n</ul>\n"
  },
  {
    "tags": [
      "genetic-algorithms",
      "evolutionary-algorithms",
      "crossover-operators",
      "genetic-operators",
      "mutation-operators"
    ],
    "owner": {
      "account_id": 5129611,
      "reputation": 2945,
      "user_id": 145,
      "user_type": "moderator",
      "accept_rate": 62,
      "profile_image": "https://i.sstatic.net/IIYyh.png?s=256",
      "display_name": "Mithical",
      "link": "https://ai.stackexchange.com/users/145/mithical"
    },
    "is_answered": true,
    "view_count": 551,
    "accepted_answer_id": 1548,
    "answer_count": 3,
    "score": 9,
    "last_activity_date": 1607426516,
    "creation_date": 1470921713,
    "last_edit_date": 1607426516,
    "question_id": 1541,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1541/why-is-cross-over-a-part-of-genetic-algorithms",
    "title": "Why is cross-over a part of genetic algorithms?",
    "body": "<p>Genetic Algorithms has come to my attention recently when trying to correct/improve computer opponents for turn-based strategy computer games.</p>\n<p>I implemented a simple Genetic Algorithm that didn't use any cross-over, just some random mutation. It seemed to work in this case, and so I started thinking:</p>\n<p><strong>Why is cross-over a part of genetic algorithms? Wouldn't mutation be enough?</strong></p>\n<p><sub>This is from a data dump on an old AI site. The asker had the UID of 7. </sub></p>\n"
  },
  {
    "tags": [
      "reference-request",
      "game-theory",
      "history",
      "state-of-the-art",
      "board-games"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 4435,
    "accepted_answer_id": 1584,
    "answer_count": 4,
    "score": 9,
    "last_activity_date": 1720916501,
    "creation_date": 1470946293,
    "last_edit_date": 1641033233,
    "question_id": 1568,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1568/is-there-any-board-game-where-a-human-can-still-beat-an-ai",
    "title": "Is there any board game where a human can still beat an AI?",
    "body": "<p>Significant AI vs human board game matches include:</p>\n<ul>\n<li><strong>chess</strong>: <a href=\"https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)#Deep_Blue_versus_Kasparov\" rel=\"nofollow noreferrer\">Deep Blue vs Kasparov</a> in 1996,</li>\n<li><strong>go</strong>: <a href=\"https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol\" rel=\"nofollow noreferrer\">DeepMind AlphaGo vs Lee Sedol</a> in 2016,</li>\n</ul>\n<p>which demonstrated that AI challenged and defeated professional players.</p>\n<p>Are there known board games left where a human can still win against an AI? I mean based on the final outcome of authoritative famous matches, where there is still the same board game where AI cannot beat a world champion of that game.</p>\n"
  },
  {
    "tags": [
      "definitions",
      "intelligence-testing"
    ],
    "owner": {
      "account_id": 4770459,
      "reputation": 554,
      "user_id": 157,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/grkfy.jpg?s=256",
      "display_name": "Conor Cosnett",
      "link": "https://ai.stackexchange.com/users/157/conor-cosnett"
    },
    "is_answered": true,
    "view_count": 2573,
    "closed_date": 1597794530,
    "accepted_answer_id": 1649,
    "answer_count": 3,
    "score": 9,
    "last_activity_date": 1550787574,
    "creation_date": 1471349181,
    "last_edit_date": 1550787574,
    "question_id": 1648,
    "link": "https://ai.stackexchange.com/questions/1648/what-are-the-criteria-for-a-system-to-be-considered-intelligent",
    "closed_reason": "Duplicate",
    "title": "What are the criteria for a system to be considered intelligent?",
    "body": "<p>For example, could you provide reasons why a sundial is <em>not</em> \"intelligent\"?\nA sundial senses its environment and acts rationally. It outputs the time. It also stores  percepts. (The numbers the engineer wrote on it.)</p>\n\n<p>What properties of a self driving car would make it \"intelligent\"? </p>\n\n<p>Where is the line between non intelligent matter and an intelligent system?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "intelligent-agent",
      "computer-programming"
    ],
    "owner": {
      "account_id": 5129611,
      "reputation": 2945,
      "user_id": 145,
      "user_type": "moderator",
      "accept_rate": 62,
      "profile_image": "https://i.sstatic.net/IIYyh.png?s=256",
      "display_name": "Mithical",
      "link": "https://ai.stackexchange.com/users/145/mithical"
    },
    "is_answered": true,
    "view_count": 574,
    "accepted_answer_id": 1695,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1630072428,
    "creation_date": 1471547789,
    "last_edit_date": 1630072428,
    "question_id": 1694,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1694/what-are-the-necessary-components-to-make-an-ai-agent-capable-of-self-programmin",
    "title": "What are the necessary components to make an AI agent capable of self-programming?",
    "body": "<p>An AI agent is often thought of having \"sensors\", \"a memory\", \"machine learning processors\" and \"reaction\" components. However, a machine with these does not necessarily become a self-programming AI agent. Beyond the parts mentioned above, is there any other elements or details necessary to make a machine capable of being a self-programming AI agent?</p>\n\n<p>For example, <a href=\"http://www.iiim.is/wp/wp-content/uploads/2011/05/goertzel-agisp-2011.pdf\" rel=\"noreferrer\">a paper from 2011</a> declared that solving the optimization problem of maximizing the intelligence is a must-have feature for the self-programming process, as quoted below:</p>\n\n<blockquote>\n  <p>A system is said to carry out an instance of self-programming when it undergoes learning regarding some element of its \"cognitive infrastructure\", where the latter is defined as the fuzzy set of \"intelligence-critical\" features of the system; and the intelligence-criticality of a system feature is defined as its \"feature quality,\" considered from the perspective of solving the optimization problem of maximizing the intelligence of a multi-feature system.</p>\n</blockquote>\n\n<p>However, this description of \"optimization of intelligence\" is vague. Can anyone give a clear definition or better summary for the necessary components for self-programming agents?</p>\n\n<p><sub>This question is from the 2014 closed beta, with the asker having a UID of 23.</sub></p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "reference-request",
      "research",
      "markov-decision-process"
    ],
    "owner": {
      "account_id": 949428,
      "reputation": 201,
      "user_id": 15,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/df8a5a4b9eb8345cad7e60052507c20f?s=256&d=identicon&r=PG",
      "display_name": "CarrKnight",
      "link": "https://ai.stackexchange.com/users/15/carrknight"
    },
    "is_answered": true,
    "view_count": 711,
    "accepted_answer_id": 1735,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1591423618,
    "creation_date": 1472032844,
    "last_edit_date": 1591390974,
    "question_id": 1733,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1733/what-are-some-resources-on-continuous-state-and-action-spaces-mdps-for-reinforce",
    "title": "What are some resources on continuous state and action spaces MDPs for reinforcement learning?",
    "body": "<p>Most introductions to the field of MDPs and Reinforcement learning focus exclusively on domains where space and action variables are integers (and finite). This way we are introduced quickly to Value Iteration, Q-Learning, and the like.</p>\n\n<p>However, the most interesting applications (say, <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.67.3518&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow noreferrer\">flying helicopters</a>) of RL and MDPs involve continuous state space and action spaces. I'd like to go beyond basic introductions and focus on these cases but I am not sure how to get there. </p>\n\n<p>What are some resources on continuous state and action spaces MDPs for reinforcement learning? And what areas do I need to know or study to understand these cases in-depth?</p>\n"
  },
  {
    "tags": [
      "ai-box"
    ],
    "owner": {
      "account_id": 6241661,
      "reputation": 1521,
      "user_id": 29,
      "user_type": "registered",
      "accept_rate": 78,
      "profile_image": "https://i.sstatic.net/JfeF9.png?s=256",
      "display_name": "wythagoras",
      "link": "https://ai.stackexchange.com/users/29/wythagoras"
    },
    "is_answered": true,
    "view_count": 1932,
    "answer_count": 4,
    "score": 9,
    "last_activity_date": 1472843296,
    "creation_date": 1472581002,
    "question_id": 1794,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/1794/what-methods-could-an-ai-caught-in-a-box-use-to-get-out",
    "title": "What methods could an AI caught in a box use to get out?",
    "body": "<p>An AI box is a (physical) barrier preventing an AI from using too much of his environment to accomplish his final goal. For example, an AI given the task to check, say, 10<sup>50</sup> cases of a mathematical conjecture as fast as possible, might decide that it would be better to also take control over all other computers and AI to help him. </p>\n\n<p>However, an transhuman AI might be able to talk to a human until the human lets him out of the box. In fact, <a href=\"http://www.yudkowsky.net/singularity/aibox/\" rel=\"noreferrer\">Eliezer Yudowsky</a> has conducted an experiment twice, where he played the AI and he twice convinced the Gatekeeper to let him out the box. However, he does not want to reveal what methods he used to get out of the box.</p>\n\n<p><strong>Questions:</strong> Are there conducted any similiar experiments? <br> If so, is it known what methods were used to get out in those experiments?</p>\n"
  },
  {
    "tags": [
      "philosophy"
    ],
    "owner": {
      "account_id": 6108253,
      "reputation": 1670,
      "user_id": 181,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://lh3.googleusercontent.com/-YYgQ6EQmsiQ/AAAAAAAAAAI/AAAAAAAAABU/QDDm2dKMp3c/s256-rj/photo.jpg",
      "display_name": "Left SE On 10_6_19",
      "link": "https://ai.stackexchange.com/users/181/left-se-on-10-6-19"
    },
    "is_answered": true,
    "view_count": 428,
    "accepted_answer_id": 1855,
    "answer_count": 3,
    "score": 9,
    "last_activity_date": 1473075704,
    "creation_date": 1472998014,
    "question_id": 1853,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/1853/should-i-use-anthropomorphic-language-when-discussing-ai",
    "title": "Should I use anthropomorphic language when discussing AI?",
    "body": "<p>The English Language is not well-suited to talking about artificial intelligence, which makes it difficult for humans to communicate to each other about what an AI is actually \"doing\". Thus, it may make more sense to use \"human-like\" terms to describe the actions of machinery, even when the internal properties of the machinery do not resemble the internal properties of humanity.</p>\n\n<p>Anthropomorphic language had been used a lot in technology (see the Hacker's Dictionary definition of <a href=\"https://www.landley.net/history/mirror/jargon.html#Anthropomorphization\">anthropomorphization</a>, which attempts to justify computer programmers' use of anthromporhic terms when describing technology), but as AI continues to advance, it may be useful to consider the tradeoffs of using anthropomorphic language in communicating to both technical audiences and non-technical audiences. How can we get a good handle on AI if we can't even describe what we're doing?</p>\n\n<p>Suppose I want to develop an algorithm that display a list of related articles. There are two ways by which I can explain how the algorithm works to a layman:</p>\n\n<ol>\n<li><em>Very Anthropomorphic</em> - The algorithm reads all the articles on a website, and display the articles that are very similar to the article you are looking at.</li>\n<li><em>Very Technical</em> - The algorithm converts each article into a \"bag-of-words\", and then compare the \"bag-of-words\" of each article to determine what articles share the most common words. The articles that share the most words in the bags are the ones that are displayed to the user.</li>\n</ol>\n\n<p>Obviously, #2 may be more \"technically correct\" than #1. By detailing the implementation of the algorithm, it makes it easier for someone to understand how to <em>fix</em> the algorithm if it produces an output that we disagree with heavily.</p>\n\n<p>But #1 is more readable, elegant, and easier to understand. It provides a general sense of <em>what</em> the algorithm is doing, instead of <em>how</em> the algorithm is doing it. By abstracting away the implementation details of how a computer \"reads\" the article, we can then focus on using the algorithm in real-world scenarios.</p>\n\n<p>Should I, therefore, prefer to use the anthropomorphic language as emphasized by Statement #1? If not, why not?</p>\n\n<p>P.S.: If the answer depends on the audience that I am speaking to (a non-technical audience might prefer #1, while a technical audience may prefer #2), then let me know that as well.</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "death"
    ],
    "owner": {
      "account_id": 9148500,
      "reputation": 541,
      "user_id": 2310,
      "user_type": "registered",
      "accept_rate": 0,
      "profile_image": "https://www.gravatar.com/avatar/4480675ad0ede0043e7caba04750d974?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "D. Wade",
      "link": "https://ai.stackexchange.com/users/2310/d-wade"
    },
    "is_answered": true,
    "view_count": 436,
    "answer_count": 7,
    "score": 9,
    "last_activity_date": 1529346967,
    "creation_date": 1473525555,
    "last_edit_date": 1473549850,
    "question_id": 1930,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/1930/if-mankind-can-create-artificial-life-in-a-machine-when-would-we-define-its-de",
    "title": "If mankind can create artificial life in a machine, when would we define it&#39;s death?",
    "body": "<p>Can one actually kill a machine? Not only do we have problems in defining life, we also have problems in defining death. Will this also be true in artificial life and artificial intelligence?</p>\n"
  },
  {
    "tags": [
      "agi",
      "research",
      "state-of-the-art",
      "narrow-ai"
    ],
    "owner": {
      "account_id": 9590922,
      "reputation": 101,
      "user_id": 3488,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-TYcm7eBrUiY/AAAAAAAAAAI/AAAAAAAAAr8/YyjoQ3H-fZ4/s256-rj/photo.jpg",
      "display_name": "Lord Thanatos",
      "link": "https://ai.stackexchange.com/users/3488/lord-thanatos"
    },
    "is_answered": true,
    "view_count": 2052,
    "closed_date": 1651205175,
    "accepted_answer_id": 2287,
    "answer_count": 4,
    "score": 9,
    "last_activity_date": 1605590954,
    "creation_date": 1478454187,
    "last_edit_date": 1605550319,
    "question_id": 2285,
    "link": "https://ai.stackexchange.com/questions/2285/what-is-the-most-sophisticated-ai-ever-made",
    "closed_reason": "Opinion-based",
    "title": "What is the most sophisticated AI ever made?",
    "body": "<p>What is the most advanced AI software/system that humans have made to date, and what does it do?</p>\n"
  },
  {
    "tags": [
      "ai-design",
      "natural-language-processing",
      "semantics"
    ],
    "owner": {
      "account_id": 2097803,
      "reputation": 261,
      "user_id": 3592,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/TrCUq.png?s=256",
      "display_name": "skrtbhtngr",
      "link": "https://ai.stackexchange.com/users/3592/skrtbhtngr"
    },
    "is_answered": true,
    "view_count": 1077,
    "answer_count": 4,
    "score": 9,
    "last_activity_date": 1539002190,
    "creation_date": 1478905459,
    "last_edit_date": 1539002190,
    "question_id": 2319,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2319/can-the-english-language-ever-be-generalized-using-a-set-of-grammar-rules",
    "title": "Can the English language ever be generalized using a set of grammar rules?",
    "body": "<p>In programming languages, there is a set of grammar rules which govern the construction of valid statements and expressions. These rules help in parsing the programs written by the user.</p>\n\n<p>Can there ever be a functionally complete set of grammar rules which can parse any statement in English (locale-specific) <strong>accurately</strong> and which can be possibly implemented for use in AI-based projects?</p>\n\n<p>I know that there are a lot of NLP Toolkits available online, but they are not that effective. Most of them are trained using specific corpuses which sometimes fail to infer some complex correlations between various parts of an expression.</p>\n\n<p>In other words, what I am asking is that if it is possible for a computer to parse a well-versed sentence written in English as if it were parsed by an adult English-speaking human?</p>\n\n<p>EDIT:\nIf it cannot be represented using simple grammar rules, what kind of semantic structure can be used to generalize it?</p>\n\n<p>EDIT2: This <a href=\"https://www.eecs.harvard.edu/shieber/Biblio/Papers/shieber85.pdf\" rel=\"noreferrer\">paper</a> proves the absence of context-freeness in natural languages. I am looking for a solution, even if it is too complex.</p>\n"
  },
  {
    "tags": [
      "agi",
      "control-problem",
      "legal"
    ],
    "owner": {
      "account_id": 3731596,
      "reputation": 245,
      "user_id": 3748,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/w6JB3.jpg?s=256",
      "display_name": "fuzzyhedge",
      "link": "https://ai.stackexchange.com/users/3748/fuzzyhedge"
    },
    "is_answered": true,
    "view_count": 420,
    "accepted_answer_id": 2359,
    "answer_count": 6,
    "score": 9,
    "last_activity_date": 1482087278,
    "creation_date": 1479615914,
    "last_edit_date": 1479864163,
    "question_id": 2356,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/2356/would-an-ai-with-human-intelligence-have-the-same-rights-as-a-human-under-curren",
    "title": "Would an AI with human intelligence have the same rights as a human under current legal frameworks?",
    "body": "<p>For example, would an AI be able to own property, evict tenants, acquire debt, employ, vote, or marry? What are the legal structures in place to implement a strong AI into society? </p>\n"
  },
  {
    "tags": [
      "reference-request",
      "agi",
      "cognitive-architecture",
      "open-cog"
    ],
    "owner": {
      "account_id": 5416059,
      "reputation": 4265,
      "user_id": 2227,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://www.gravatar.com/avatar/af11dfed2f1c4e002c6a71cea4a7cfab?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "BlindKungFuMaster",
      "link": "https://ai.stackexchange.com/users/2227/blindkungfumaster"
    },
    "is_answered": true,
    "view_count": 1783,
    "accepted_answer_id": 5301,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1640297307,
    "creation_date": 1480068209,
    "last_edit_date": 1640297307,
    "question_id": 2381,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2381/what-are-general-ideas-behind-opencog",
    "title": "What are general ideas behind OpenCog?",
    "body": "<p><a href=\"http://opencog.org\" rel=\"nofollow noreferrer\">OpenCog</a> is an open source AGI-project co-founded by the mercurial AI researcher <a href=\"https://en.wikipedia.org/wiki/Ben_Goertzel\" rel=\"nofollow noreferrer\">Ben Goertzel</a>. Ben Goertzel writes a lot of stuff, some of it <a href=\"http://multiverseaccordingtoben.blogspot.de/2010/11/psi-debate-continues-goertzel-on.html\" rel=\"nofollow noreferrer\">really</a> <a href=\"http://multiverseaccordingtoben.blogspot.de/2016/10/semrem-search-for-extraterrestrial.html\" rel=\"nofollow noreferrer\">whacky</a>. Nonetheless, he is clearly very intelligent and has thought deeply about AI for many decades. </p>\n\n<p>What are the general ideas behind <a href=\"http://wiki.opencog.org/w/OpenCog_Prime\" rel=\"nofollow noreferrer\">OpenCog</a>? Would you endorse it as a insightful take on AGI? </p>\n\n<p>I'm especially interested in whether the general framework still makes sense in the light of recent advances.  </p>\n"
  },
  {
    "tags": [
      "agi",
      "legal"
    ],
    "owner": {
      "account_id": 6281896,
      "reputation": 472,
      "user_id": 3427,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/klAPh.png?s=256",
      "display_name": "GJZ",
      "link": "https://ai.stackexchange.com/users/3427/gjz"
    },
    "is_answered": true,
    "view_count": 634,
    "answer_count": 4,
    "score": 9,
    "last_activity_date": 1483068188,
    "creation_date": 1481133598,
    "last_edit_date": 1492087990,
    "question_id": 2441,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/2441/should-intelligent-ai-be-granted-the-same-rights-as-humans",
    "title": "Should intelligent AI be granted the same rights as humans?",
    "body": "<p>One of the most crucial questions we as a species and as intelligent beings will have to address lies with the rights we plan to grant to AI.</p>\n\n<blockquote>\n  <p>This question is intended to see if a compromise can be found between <strong>conservative anthropocentrism</strong> and <strong>post-human fundamentalism</strong>: a response should take into account principles from both perspectives.</p>\n</blockquote>\n\n<p>Should, and therefore will, AI be granted the same rights as humans or should such systems have different rights (if any at all) ?</p>\n\n<hr>\n\n<p><strong><em>Some Background</em></strong></p>\n\n<p>This question applies both to human-brain based AI (from whole brain emulations to less exact replication) and AI from scratch.</p>\n\n<p>Murray Shanahan, in his book The Technological Singularity, outlines a potential use of AI that could be considered immoral: <em>ruthless parallelization</em>: we could make identical parallel copies of AI to achieve tasks more effectively and even terminate less succesful copies.</p>\n\n<hr>\n\n<p><strong>Reconciling these two philosophies</strong> (conservative anthropocentrism and post-human fundamentalism), should such use of AI be accepted or should certain limitations - i.e. rights - be created for AI? </p>\n\n<hr>\n\n<p>This question is not related to <a href=\"https://ai.stackexchange.com/questions/2356/would-an-ai-with-human-intelligence-have-the-same-rights-as-a-human-under-curren\">Would an AI with human intelligence have the same rights as a human under current legal frameworks?</a> for the following reasons:</p>\n\n<ol>\n<li><p>The other question specifies \"<strong><em>current legal frameworks</em></strong>\"</p></li>\n<li><p>This question is looking for a specific response relating to two fields of thought</p></li>\n<li><p>This question highlights specific cases to analyse and is therefore expects less of a general response and more of a precise analysis </p></li>\n</ol>\n"
  },
  {
    "tags": [
      "neural-networks",
      "comparison",
      "perceptron",
      "naive-bayes"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user3642"
    },
    "is_answered": true,
    "view_count": 4294,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1613546398,
    "creation_date": 1482751449,
    "last_edit_date": 1613332049,
    "question_id": 2548,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2548/what-are-the-main-differences-between-a-perceptron-and-a-naive-bayes-classifier",
    "title": "What are the main differences between a perceptron and a naive Bayes classifier?",
    "body": "<p>What are the main differences between a perceptron and a naive Bayes classifier?</p>\n"
  },
  {
    "tags": [
      "reference-request",
      "question-answering",
      "reasoning",
      "bayesian-inference",
      "knowledge-based-systems"
    ],
    "owner": {
      "account_id": 4595425,
      "reputation": 93,
      "user_id": 4707,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/fd5797d3eb097e59823865939f37346e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "PintoUbuntu",
      "link": "https://ai.stackexchange.com/users/4707/pintoubuntu"
    },
    "is_answered": true,
    "view_count": 377,
    "accepted_answer_id": 2630,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1611794379,
    "creation_date": 1483865324,
    "last_edit_date": 1611794379,
    "question_id": 2623,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2623/can-bayesian-inference-be-combined-with-knowledge-based-systems",
    "title": "Can Bayesian inference be combined with knowledge-based systems?",
    "body": "<p>I've been struggling with the connection between knowledge-based AI systems and Bayesian inference for a while now. While I continue to sweep through the literature, I would be happy if someone can answer these more specific questions directly</p>\n<ol>\n<li><p>Are Bayesian inference based methods used in reasoning or Q/A systems -- to arrive at conclusions about questions whose answers are not directly present in the knowledge base?</p>\n</li>\n<li><p>In other words, if a Q/A system doesn't find an answer in a knowledge base, can it use Bayesian inference to use the available facts to suggest answers with varying likelihoods?</p>\n</li>\n<li><p>If yes, could you point me to some implementations?</p>\n</li>\n</ol>\n"
  },
  {
    "tags": [
      "machine-learning",
      "natural-language-processing",
      "reference-request",
      "model-request"
    ],
    "owner": {
      "account_id": 8773395,
      "reputation": 211,
      "user_id": 4869,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-HhhwcRMr_zI/AAAAAAAAAAI/AAAAAAAAAHo/ElV4-iDOsXk/s256-rj/photo.jpg",
      "display_name": "Praharsh Bhatt",
      "link": "https://ai.stackexchange.com/users/4869/praharsh-bhatt"
    },
    "is_answered": true,
    "view_count": 1041,
    "accepted_answer_id": 2691,
    "answer_count": 3,
    "score": 9,
    "last_activity_date": 1736249340,
    "creation_date": 1484626280,
    "last_edit_date": 1736249340,
    "question_id": 2681,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2681/is-there-an-ai-model-for-generating-sarcastic-sentences",
    "title": "Is there an AI model for generating sarcastic sentences?",
    "body": "<p>I am currently working on an Android AI app. </p>\n\n<p>I am aware of AI models to generate random sentences. However, is there an AI model for generating sarcastic sentences?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "genetic-algorithms",
      "biology",
      "artificial-life"
    ],
    "owner": {
      "account_id": 8135445,
      "reputation": 191,
      "user_id": 5189,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/89eeeadae6d5411d890a4480173f98f9?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "codeman",
      "link": "https://ai.stackexchange.com/users/5189/codeman"
    },
    "is_answered": true,
    "view_count": 1145,
    "answer_count": 6,
    "score": 9,
    "last_activity_date": 1593089600,
    "creation_date": 1486017668,
    "last_edit_date": 1593089600,
    "question_id": 2771,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2771/is-artificial-life-really-life-or-not",
    "title": "Is artificial life really life or not?",
    "body": "<p>I define Artificial Life as a &quot;simulation&quot; or &quot;copy&quot; of life. However, should it be considered a simulation or copy?</p>\n<p>If one had motivation and money, someone could theoretically create evolving computers, with a program that allows mutation OR simply a &quot;simulated&quot; environment with &quot;simulated&quot; organisms.</p>\n<p>The computer (or &quot;simulated&quot; organism) would have the ability to reproduce, grow, and take in energy. What if the life evolved to have intelligence? Currently, there are some relatively limited programs that simulate life, but most of them are heavily simplistic. Are they life?</p>\n<p>When should something be called life?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-neural-networks",
      "image-recognition",
      "convolutional-neural-networks",
      "signal-processing"
    ],
    "owner": {
      "account_id": 948815,
      "reputation": 281,
      "user_id": 113,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7d327873d48d28e563c9ad7259853c35?s=256&d=identicon&r=PG",
      "display_name": "Kozuch",
      "link": "https://ai.stackexchange.com/users/113/kozuch"
    },
    "is_answered": true,
    "view_count": 399,
    "accepted_answer_id": 2869,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1568027614,
    "creation_date": 1487884294,
    "last_edit_date": 1568027614,
    "question_id": 2867,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2867/how-much-of-a-problem-is-white-noise-for-the-real-world-usage-of-a-dnn",
    "title": "How much of a problem is white noise for the real-world usage of a DNN?",
    "body": "<p>I read that deep neural networks can be relatively easily fooled (<a href=\"https://ai.stackexchange.com/questions/92/how-is-it-possible-that-deep-neural-networks-are-so-easily-fooled\">link</a>) to give high confidence in recognition of synthetic/artificial images that are completely (or at least mostly) out of the confidence subject.</p>\n\n<p>Personally, I don't really see a big problem with DNN giving high confidence to those synthetic/artificial images but I think giving high confidence for white noise (<a href=\"https://ai.stackexchange.com/questions/1479/do-scientists-know-what-is-happening-inside-artificial-neural-networks\">link</a>) may be a problem since this is a truly natural phenomenon that may the camera see in the real world.</p>\n\n<p>How much of a problem is white noise for the real-world usage of a DNN? Can such false positives be detected from plain noise be prevented somehow?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "philosophy",
      "ai-security",
      "explainable-ai"
    ],
    "owner": {
      "account_id": 2581883,
      "reputation": 233,
      "user_id": 5817,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/49a377fccbf53440a4b582c142a1ed88?s=256&d=identicon&r=PG",
      "display_name": "Stilez",
      "link": "https://ai.stackexchange.com/users/5817/stilez"
    },
    "is_answered": true,
    "view_count": 311,
    "accepted_answer_id": 2916,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1568027450,
    "creation_date": 1488539329,
    "last_edit_date": 1568027375,
    "question_id": 2911,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2911/how-would-one-debug-understand-or-fix-the-outcome-of-a-neural-network",
    "title": "How would one debug, understand or fix the outcome of a neural network?",
    "body": "<p>It seems fairly uncontroversial to say that NN based approaches are becoming quite powerful tools in many AI areas - whether recognising and decomposing images (faces at a border, street scenes in automobiles, decision making in uncertain/complex situations or with partial data). Almost inevitably, some of those uses will develop into situations where NN-based AI takes on part or all of the human burden, and generally does it better than people generally do.</p>\n\n<p>Examples might include NN hypothetically used as steps in self-driving cars, medical diagnosis, human/identity verification, circuit/design verification, dubious transaction alerting. Probably many fields in the next decade or so. </p>\n\n<p>Suppose this happens, and is generally seen as successful (for example, it gets diagnoses right 80% to human doctors' 65% or something, or cars with AI that includes an NN component crash 8% less than human-driven cars or alternatives, or whatever).</p>\n\n<p>Now - suppose one of these aberrantly and seriously does something very wrong in one case. How can one approach it? With formal logic steps one can trace a formal decision process, but with NN there may be no formal logic, especially if it gets complex enough (in a couple of decades say), there are just 20 billion neural processors and their I/O weightings and connections, it may not be possible to determine what caused some incident even if lives were lost. It also may not be possible to say more than the systems continually learn and such incidents are rare. </p>\n\n<p>I also haven't heard of any meaningful way to do a \"black box\" or flight recorder equivalent for NNs, (even if not used I a life-critical case), that would allow us to understand and avoid a bad decision. Unlike other responses to product defects, if a NN could be trained after the event to fix one such case, it doesn't clearly provide the certainty we would want, that the new NN setup has fixed the problem, or hasn't reduced the risk and balance of other problems in so doing. It's just very opaque. And yet, clearly, it is mostly very valuable as an AI approach.</p>\n\n<p>In 20 years if NN is an (acknowledged as safe and successful) component in a plane flight or aircraft design, or built into a hospital system to watch for emergencies, or to spot fraud at a bank, and has as usual passed whatever regulatory and market requirements might exist and performed with a good record for years in the general marketplace, <em>and</em> then in one case such a system some time later plainly misacts on one occasion - it dangerously misreads the road, recommends life-damaging medications or blatantly misdiagnoses, or clears a blatant £200m fraudulent transaction at a clearing bank that's only caught by chance before the money is sent.</p>\n\n<p>What can the manufacturer do to address public or market concerns, or to explain the incident? What do the tech team do when told by the board \"how did this happen and make damn sure it's fixed\"? What kind of meaningful logs can be kept, etc.? Would society have to just accept that uncertainty and occasional wacky behavior could be inherent (good luck with the convincing society of that!)? Or is there some better way to approach logging/debugging/decision activity more suited to NNs?</p>\n"
  },
  {
    "tags": [
      "agi",
      "terminology",
      "control-problem"
    ],
    "owner": {
      "account_id": 2270483,
      "reputation": 201,
      "user_id": 5947,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/10cd72d96329c2e06e9974c93c4bee5f?s=256&d=identicon&r=PG",
      "display_name": "ikaruss",
      "link": "https://ai.stackexchange.com/users/5947/ikaruss"
    },
    "is_answered": true,
    "view_count": 614,
    "accepted_answer_id": 3184,
    "answer_count": 3,
    "score": 9,
    "last_activity_date": 1551999698,
    "creation_date": 1489104542,
    "last_edit_date": 1497734639,
    "question_id": 2955,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/2955/whats-the-term-for-death-by-dissolving-in-ai",
    "title": "What&#39;s the term for death by dissolving in AI?",
    "body": "<p>What's the term (if such exists) for merging with AI (e.g. via neural lace) and becoming so diluted (e.g. 1:10000) that it effectively results in a death of the original self?</p>\n\n<p>It's not quite \"digital ascension\", because that way it would still be you. What I'm thinking is, that the resulting AI with 1 part in 10000 being you, is not you anymore. The AI might have some of your values or memories or whatever, but it's not you, and you don't exist separately from it to be called you. Basically - you as you are dead; you died by dissolving in AI.</p>\n\n<p>I would like to read up on this subject, but can't find anything.</p>\n"
  },
  {
    "tags": [
      "history",
      "social",
      "ai-winter"
    ],
    "owner": {
      "account_id": 6108253,
      "reputation": 1670,
      "user_id": 181,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://lh3.googleusercontent.com/-YYgQ6EQmsiQ/AAAAAAAAAAI/AAAAAAAAABU/QDDm2dKMp3c/s256-rj/photo.jpg",
      "display_name": "Left SE On 10_6_19",
      "link": "https://ai.stackexchange.com/users/181/left-se-on-10-6-19"
    },
    "is_answered": true,
    "view_count": 817,
    "accepted_answer_id": 2960,
    "answer_count": 4,
    "score": 9,
    "last_activity_date": 1571936272,
    "creation_date": 1489154383,
    "last_edit_date": 1571936272,
    "question_id": 2959,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2959/are-ai-winters-inevitable",
    "title": "Are AI winters inevitable?",
    "body": "<p>According to <a href=\"https://en.wikipedia.org/wiki/AI_winter\" rel=\"nofollow noreferrer\">Wikipedia</a> (citations omitted):</p>\n\n<blockquote>\n  <p>In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. The term was coined by analogy to the idea of a nuclear winter. The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or decades later.</p>\n</blockquote>\n\n<p>The Wikipedia page discusses a bit about the <em>causes</em> of AI winters. I'm curious, however, whether it is possible to <em>stop</em> an AI Winter from occurring. I don't really like the misallocation of resources that are caused by over-investment followed by under-investment.</p>\n\n<p>One of the causes of the AI winter listed on that Wikipedia page is \"hype\":</p>\n\n<blockquote>\n  <p>The AI winters can be partly understood as a sequence of over-inflated expectations and subsequent crash seen in stock-markets and exemplified by the railway mania and dotcom bubble. In a common pattern in the development of new technology (known as hype cycle), an event, typically a technological breakthrough, creates publicity which feeds on itself to create a \"peak of inflated expectations\" followed by a \"trough of disillusionment\". Since scientific and technological progress can't keep pace with the publicity-fueled increase in expectations among investors and other stakeholders, a crash must follow. AI technology seems to be no exception to this rule.</p>\n</blockquote>\n\n<p>And it seems that this paragraph indicates that <em>any</em> new technology will be stuck in this pattern of \"inflated expectations\" followed by disillusionment. </p>\n\n<p>So, are AI winters inevitable? Is it inevitable that AI technologies will always be overhyped in the future and that severe \"corrections\" will always occur? Or can there a way to manage this hype cycle to stop severe increases/decreases in funding?</p>\n"
  },
  {
    "tags": [
      "reference-request",
      "hardware",
      "imperfect-information",
      "poker",
      "deepstack"
    ],
    "owner": {
      "account_id": 10593808,
      "reputation": 99,
      "user_id": 6411,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/347ecea7925ba4f91d15113bdc5fb6cd?s=256&d=identicon&r=PG",
      "display_name": "user6411",
      "link": "https://ai.stackexchange.com/users/6411/user6411"
    },
    "is_answered": true,
    "view_count": 327,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1642934872,
    "creation_date": 1491097629,
    "last_edit_date": 1642934872,
    "question_id": 3083,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3083/how-powerful-is-the-machine-that-beat-the-poker-professional-players-recently",
    "title": "How powerful is the machine that beat the poker professional players recently?",
    "body": "<p>How powerful is the machine that beat the poker professional players recently (DeepStack)?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "natural-language-processing",
      "reference-request",
      "natural-language-generation"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user7478"
    },
    "is_answered": true,
    "view_count": 5534,
    "answer_count": 3,
    "score": 9,
    "last_activity_date": 1640297073,
    "creation_date": 1495916184,
    "last_edit_date": 1640297073,
    "question_id": 3397,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3397/can-ai-write-good-jokes-yet",
    "title": "Can AI write good jokes yet?",
    "body": "<p>Just watched a recent WIRED video on virtual assistants' performance on telling jokes. They're composed by humans, but I'd like to know if AI has gotten good enough to write some.</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "q-learning",
      "dqn",
      "implementation",
      "reward-functions"
    ],
    "owner": {
      "account_id": 5976377,
      "reputation": 273,
      "user_id": 7495,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b6c2e17476f5ae2fd6aaaa149411e0ae?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "AlexGuevara",
      "link": "https://ai.stackexchange.com/users/7495/alexguevara"
    },
    "is_answered": true,
    "view_count": 2619,
    "accepted_answer_id": 3405,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1605380586,
    "creation_date": 1496048548,
    "last_edit_date": 1605380586,
    "question_id": 3403,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3403/what-are-other-ways-of-handling-invalid-actions-in-scenarios-where-all-rewards-a",
    "title": "What are other ways of handling invalid actions in scenarios where all rewards are either 0 (best reward) or negative?",
    "body": "<p>I created an OpenAI Gym environment, and I would like to check the performance of the agent from <a href=\"https://blog.openai.com/openai-baselines-dqn/\" rel=\"nofollow noreferrer\">OpenAI Baselines DQN approach </a> on it.\nIn my environment, the best possible outcome for the agent is 0 - the robot needs zero non-necessary resources to complete a task. The goal is to minimize the need for resources: for each needed resource, there is a penalty of -1. In many states, only certain actions make physical sense. How do I deal with this?</p>\n<p>There was already a question about the handling of invalid moves on <a href=\"https://ai.stackexchange.com/q/2980/2444\">AI StackExchange</a>, recommending to ignore the invalid moves. However, ignoring them would imply returning the same state and a 0 reward, the best possible outcome, which is clearly not the case. Setting drastic negative rewards also does not seem to work, since even promising handling paths are compromised by invalid actions and the corresponding drastic negative reward.</p>\n<p><em>What are other ways of handling invalid actions in scenarios where all rewards are either 0 (best) or negative?</em></p>\n<p>My ideas/ questions on this for the <a href=\"https://blog.openai.com/openai-baselines-dqn/\" rel=\"nofollow noreferrer\">OpenAI Baselines DQN approach</a> implementation</p>\n<ol>\n<li><p>Is there any way to set the initial Q-values for the actions? I could set -infinity for the invalid actions.</p>\n</li>\n<li><p>Is there any way to limit the set of valid actions per state? When after the env.step(action) function the new state is returned, can I somehow define which actions are valid for it?</p>\n</li>\n</ol>\n"
  },
  {
    "tags": [
      "game-ai",
      "deepmind"
    ],
    "owner": {
      "account_id": 5416059,
      "reputation": 4265,
      "user_id": 2227,
      "user_type": "registered",
      "accept_rate": 50,
      "profile_image": "https://www.gravatar.com/avatar/af11dfed2f1c4e002c6a71cea4a7cfab?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "BlindKungFuMaster",
      "link": "https://ai.stackexchange.com/users/2227/blindkungfumaster"
    },
    "is_answered": true,
    "view_count": 2216,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1503057359,
    "creation_date": 1502744340,
    "last_edit_date": 1502800473,
    "question_id": 3814,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/3814/whats-the-difference-between-starcraft-and-dota-from-an-ai-perspective",
    "title": "What&#39;s the difference between Starcraft and Dota from an AI perspective?",
    "body": "<p>So, <a href=\"https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/\" rel=\"noreferrer\">Deepmind is pushing for a human level Starcraft bot</a> and <a href=\"https://openai.com/the-international/\" rel=\"noreferrer\">Open AI just created a human level 1vs1 Dota bot</a>. </p>\n\n<p>Unfortunately, I've no clue what that signifies because I've never played Starcraft nor Dota nor do I have more than a fleeting acquaintance with similar games. </p>\n\n<p>My question is what the difference between Starcraft and Dota is from a AI perspective and what scientific significance the respective super human bots would have. </p>\n"
  },
  {
    "tags": [
      "comparison",
      "search",
      "hill-climbing",
      "stochastic-hill-climbing"
    ],
    "owner": {
      "account_id": 2180401,
      "reputation": 91,
      "user_id": 9525,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/xnm4A.png?s=256",
      "display_name": "Rohan Sharma",
      "link": "https://ai.stackexchange.com/users/9525/rohan-sharma"
    },
    "is_answered": true,
    "view_count": 5091,
    "answer_count": 4,
    "score": 9,
    "last_activity_date": 1606165538,
    "creation_date": 1505048944,
    "last_edit_date": 1606165538,
    "question_id": 4000,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4000/when-to-choose-stochastic-hill-climbing-over-steepest-hill-climbing",
    "title": "When to choose Stochastic Hill Climbing over Steepest Hill Climbing?",
    "body": "<p><a href=\"https://en.wikipedia.org/wiki/Stochastic_hill_climbing\" rel=\"noreferrer\">Stochastic Hill Climbing</a> generally performs worse than Steepest <a href=\"https://en.wikipedia.org/wiki/Hill_climbing\" rel=\"noreferrer\">Hill Climbing</a>, but what are the cases in which the former performs better?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "terminology",
      "capsule-neural-network"
    ],
    "owner": {
      "account_id": 6321023,
      "reputation": 301,
      "user_id": 10623,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/265b1bfe405c8a6a49ee830b2009fc4f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "tim_xyz",
      "link": "https://ai.stackexchange.com/users/10623/tim-xyz"
    },
    "is_answered": true,
    "view_count": 4863,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1595083802,
    "creation_date": 1509763476,
    "last_edit_date": 1591703934,
    "question_id": 4428,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4428/what-is-an-activity-vector-in-capsule-neural-networks",
    "title": "What is an activity vector in capsule neural networks?",
    "body": "<p>I was reading the paper <a href=\"https://arxiv.org/pdf/1710.09829.pdf\" rel=\"nofollow noreferrer\">Dynamic Routing Between Capsules</a> and didn't understand the term \"activity vector\" in the abstract.</p>\n\n<blockquote>\n  <p>A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.</p>\n</blockquote>\n\n<p>I thought a vector is like an array of data that you are running through the network.</p>\n\n<p>I started working through Andrew Ng's deep learning course, but it's all new and terms go over my head.</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "ethics",
      "social",
      "implementation"
    ],
    "owner": {
      "account_id": 5890208,
      "reputation": 99,
      "user_id": 10958,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/1e4f272b1418495627f38f12e63905d5?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Suzanne",
      "link": "https://ai.stackexchange.com/users/10958/suzanne"
    },
    "is_answered": true,
    "view_count": 367,
    "answer_count": 5,
    "score": 9,
    "last_activity_date": 1560609971,
    "creation_date": 1510844437,
    "last_edit_date": 1560609971,
    "question_id": 4532,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4532/why-isnt-ethics-more-integrated-into-current-ai-systems",
    "title": "Why isn&#39;t ethics more integrated into current AI systems?",
    "body": "<p>I am a PhD student in computer science, and currently creating a state of the art overview in applications done in Machine Ethics (a multidisciplinary field combining philosophy and AI, that looks at creating explicit ethical programs or agents). It seems that the field mostly contains theoretical arguments and there are relatively little implementations, even though there are many people with a technical background in the field.</p>\n\n<p>I understand that because ethics are involved, there is no ground truth and since it's part of philosophy one can get lost in arguing over which type of ethics should be implemented and how this can be done best. However, in computer science, it is usual to even try a simple implementation to show the possibilities or limitations of your approach. </p>\n\n<p>What are the possible reasons there is so little done in explicitly implementing ethics in AI and experimenting with it? </p>\n"
  },
  {
    "tags": [
      "philosophy",
      "ethics",
      "explainable-ai",
      "legal"
    ],
    "owner": {
      "account_id": 11347233,
      "reputation": 91,
      "user_id": 11166,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/a6b676944b37741e7eb135bbfd8a97ae?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "LastIronStar",
      "link": "https://ai.stackexchange.com/users/11166/lastironstar"
    },
    "is_answered": true,
    "view_count": 446,
    "protected_date": 1560609760,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1611675861,
    "creation_date": 1512253946,
    "last_edit_date": 1611675861,
    "question_id": 4647,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4647/how-is-the-right-to-explanation-reasonable",
    "title": "How is the &quot;right to explanation&quot; reasonable?",
    "body": "<p>There has been recent uptick in interest in e<strong>X</strong>plainable <strong>A</strong>rtificial <strong>I</strong>ntelligence (XAI). Here is XAI's mission as stated on its <a href=\"https://www.darpa.mil/program/explainable-artificial-intelligence\" rel=\"nofollow noreferrer\">DARPA page</a>:</p>\n\n<blockquote>\n  <p>The Explainable AI (XAI) program aims to create a suite of machine learning techniques that:</p>\n  \n  <ul>\n  <li>Produce more explainable models, while maintaining a high level of learning performance (prediction accuracy); and</li>\n  <li>Enable human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners.</li>\n  </ul>\n</blockquote>\n\n<p>The New York Times article <a href=\"https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-explain-itself.html\" rel=\"nofollow noreferrer\">Can A.I. Be Taught to Explain Itself?</a> does a good job at explaining the need for XAI from a human interest perspective as well as providing a glancing outlook on the techniques being developed for the same. The force behind XAI movement seems to center around the (up and coming?) concept of <strong><em>right to explanation</em></strong>, that is, the requirement that AI applications that significantly impact human lives via their decisions be able to explain to stakeholders the factors/reasons leading up to said decision. </p>\n\n<p>How is the right to explanation reasonable, given the current standards at which we hold each other accountable?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "reinforcement-learning",
      "unsupervised-learning",
      "supervised-learning",
      "taxonomy"
    ],
    "owner": {
      "account_id": 100946,
      "reputation": 271,
      "user_id": 5356,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d6735757469d1f7726ee85a3d23e253f?s=256&d=identicon&r=PG",
      "display_name": "kakaz",
      "link": "https://ai.stackexchange.com/users/5356/kakaz"
    },
    "is_answered": true,
    "view_count": 588,
    "protected_date": 1606048639,
    "accepted_answer_id": 4710,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1608167583,
    "creation_date": 1512934165,
    "last_edit_date": 1608167583,
    "question_id": 4706,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4706/what-are-the-different-approaches-used-in-machine-learning",
    "title": "What are the different approaches used in Machine Learning?",
    "body": "<p>There seem to be so many sub-fields, so I'm interested in getting a better understanding of the approaches.</p>\n<p>I'm looking for information on a single framework per answer, in order to allow for granularity without the overall answer getting too long. For instance; Deep Learning Neural Networks would be a single answer.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "game-ai",
      "genetic-algorithms",
      "minimax",
      "alpha-beta-pruning"
    ],
    "owner": {
      "account_id": 10966927,
      "reputation": 93,
      "user_id": 11537,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-oAoyrcUYEFM/AAAAAAAAAAI/AAAAAAAAEb0/1haTANtkEcs/s256-rj/photo.jpg",
      "display_name": "Conway",
      "link": "https://ai.stackexchange.com/users/11537/conway"
    },
    "is_answered": true,
    "view_count": 1646,
    "accepted_answer_id": 4727,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1612044815,
    "creation_date": 1513080849,
    "last_edit_date": 1612044609,
    "question_id": 4725,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4725/should-i-use-neural-networks-or-genetic-algorithms-to-solve-gomoku",
    "title": "Should I use neural networks or genetic algorithms to solve Gomoku?",
    "body": "<p>Currently, I'm doing a project that's about creating an AI to play the game <a href=\"https://en.wikipedia.org/wiki/Gomoku\" rel=\"nofollow noreferrer\">Gomoku</a> (it's like tic tac toe, but played on a 15<em>15 board and requires 5 in a row to win). I have already successfully implemented a perfect tic tac toe AI using Q-learning and having game states/actions stored in a table, but, for a 15</em>15 board, the possible game states become too large to implement this project.</p>\n<p>My question is, should I use neural networks or genetic algorithms for this problem? And more specifically, how should I implement this?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "convolutional-neural-networks",
      "pattern-recognition",
      "algorithm-request",
      "model-request"
    ],
    "owner": {
      "account_id": 1225708,
      "reputation": 993,
      "user_id": 31388,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/nkxcn.png?s=256",
      "display_name": "Mary",
      "link": "https://ai.stackexchange.com/users/31388/mary"
    },
    "is_answered": true,
    "view_count": 3819,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1639820416,
    "creation_date": 1513358142,
    "last_edit_date": 1639820416,
    "question_id": 4752,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4752/how-can-i-use-neural-networks-for-detecting-tv-channel-logos-in-video-frames",
    "title": "How can I use neural networks for detecting TV channel logos in video frames?",
    "body": "<p>I am trying to detect a TV channel logo inside a video file. So, simply, given an input <code>.mp4</code> video, detect if it has that logo present in a specific frame, say the first frame, or not.</p>\n<p>Here's the first example of a frame with a logo.</p>\n<img src=\"https://i.sstatic.net/qa3Tp.png\" width=\"400\">\n<p>Here's the second example.</p>\n<img src=\"https://i.sstatic.net/LCDtS.png\" width=\"400\">\n<p>We have that logo in advance (although might not be %100 of the same size) and the location is always fixed.</p>\n<p>I already have a pattern matching-based approach. But that requires the pattern to be %100 the same size.</p>\n<p>I would like to use Deep Learning and Neural networks to achieve that. How can I do that? I believe CNNs can have a higher efficiency.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "convolutional-neural-networks",
      "computer-vision",
      "yolo"
    ],
    "owner": {
      "account_id": 9146111,
      "reputation": 209,
      "user_id": 3460,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/08c80d01792b1ef093b8f75fc3765496?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "moondra",
      "link": "https://ai.stackexchange.com/users/3460/moondra"
    },
    "is_answered": true,
    "view_count": 3072,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1611876012,
    "creation_date": 1516758391,
    "last_edit_date": 1611876012,
    "question_id": 5111,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5111/in-yolo-what-exactly-do-the-values-associated-with-each-anchor-box-represent",
    "title": "In YOLO, what exactly do the values associated with each anchor box represent?",
    "body": "<p>I'm going through Andrew NG's course, which talks about YOLO, but he doesn't go into the implementation details of anchor boxes.</p>\n<p>After having looked through the code, each anchor box is represented by two values, but what exactly are these values representing?</p>\n<p>As for the need for anchor boxes, I'm also a little confused about that --\nAs far as I understand, the ground truth labels have around 6 variables :</p>\n<ol>\n<li><span class=\"math-container\">$P_o$</span> checks if it's an object or background,</li>\n<li><span class=\"math-container\">$B_x$</span> and <span class=\"math-container\">$B_y$</span> are the center coordinates</li>\n<li><span class=\"math-container\">$B_h$</span> and <span class=\"math-container\">$B_w$</span> are the height and width of the box</li>\n<li><span class=\"math-container\">$C$</span> is the object class, which depends on how many classes you have, so you can have multiple <span class=\"math-container\">$C$</span></li>\n</ol>\n<p>As for creating the bounding box,</p>\n<p><span class=\"math-container\">$B_h$</span> is divided by 2, with one half from the center points (<span class=\"math-container\">$B_x, B_y$</span>) to the top, and the other half to the bottom.</p>\n<p>If we train our classifier, wouldn't the prediction boxes be close to the ground truth labels as training progresses? So, if our ground truth label has a high height, small width as boxes for some images, and low hight and large width for other images, wouldn't our classifier automatically\nlearn to differentiate between when to use one over the other, as it is being trained? If so then what is the use of anchor boxes? And what are those numbers representing anchor boxes representing?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "deep-neural-networks",
      "algorithmic-bias",
      "inductive-bias"
    ],
    "owner": {
      "account_id": 7363981,
      "reputation": 1186,
      "user_id": 10913,
      "user_type": "registered",
      "accept_rate": 25,
      "profile_image": "https://i.sstatic.net/8sMwH.jpg?s=256",
      "display_name": "Seth Simba",
      "link": "https://ai.stackexchange.com/users/10913/seth-simba"
    },
    "is_answered": true,
    "view_count": 2292,
    "answer_count": 5,
    "score": 9,
    "last_activity_date": 1607009847,
    "creation_date": 1518704212,
    "last_edit_date": 1607006810,
    "question_id": 5322,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5322/can-prior-knowledge-be-encoded-in-deep-neural-networks",
    "title": "Can prior knowledge be encoded in deep neural networks?",
    "body": "<p>I was reading Gary Marcus's a <a href=\"https://arxiv.org/abs/1801.00631\" rel=\"nofollow noreferrer\">Critical Appraisal of Deep Learning</a>. One of his criticisms is that neural networks don't incorporate prior knowledge in tackling a problem. My question is: have there been any attempts at encoding prior knowledge in deep neural networks?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "papers",
      "optimization",
      "weight-normalization"
    ],
    "owner": {
      "account_id": 12962196,
      "reputation": 145,
      "user_id": 12788,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/10155055167620426/picture?type=large",
      "display_name": "Mike AI",
      "link": "https://ai.stackexchange.com/users/12788/mike-ai"
    },
    "is_answered": true,
    "view_count": 5980,
    "accepted_answer_id": 5344,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1638255136,
    "creation_date": 1518809782,
    "last_edit_date": 1638255136,
    "question_id": 5343,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5343/how-does-weight-normalization-work",
    "title": "How does weight normalization work?",
    "body": "<p>I was reading the paper <a href=\"https://papers.nips.cc/paper/6114-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.pdf\" rel=\"nofollow noreferrer\">Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks</a> about improving the learning of an ANN using <em>weight normalization</em>.</p>\n\n<p>They consider standard artificial neural networks where the computation of each neuron consists in taking a weighted sum of input features, followed by an elementwise nonlinearity</p>\n\n<p><span class=\"math-container\">$$y = \\phi(\\mathbf{x} \\cdot \\mathbf{w} + b)$$</span></p>\n\n<p>where <span class=\"math-container\">$\\mathbf{w}$</span> is a <span class=\"math-container\">$k$</span>-dimensional weight vector, <span class=\"math-container\">$b$</span> is a scalar bias term, <span class=\"math-container\">$\\mathbf{x}$</span> is a <span class=\"math-container\">$k$</span>-dimensional vector of input features, <span class=\"math-container\">$\\phi(\\cdot)$</span> denotes an elementwise nonlinearity and <span class=\"math-container\">$y$</span> denotes the\nthe scalar output of the neuron.</p>\n\n<p>They then propose to reparameterize each weight vector <span class=\"math-container\">$\\mathbf{w}$</span> in terms of a parameter vector <span class=\"math-container\">$\\mathbf{v}$</span> and a scalar parameter <span class=\"math-container\">$g$</span> and to perform stochastic gradient descent with respect to those parameters instead.</p>\n\n<p><span class=\"math-container\">$$\n\\mathbf{w} = \\frac{g}{\\|\\mathbf{v}\\|} \\mathbf{v}\n$$</span></p>\n\n<p>where <span class=\"math-container\">$\\mathbf{v}$</span> is a <span class=\"math-container\">$k$</span>-dimensional vector, <span class=\"math-container\">$g$</span> is a scalar, and <span class=\"math-container\">$\\|\\mathbf{v}\\|$</span> denotes the Euclidean norm of <span class=\"math-container\">$\\mathbf{v}$</span>. They call this  reparameterizaton <em>weight normalization</em>.</p>\n\n<p>What is this scalar <span class=\"math-container\">$g$</span> used for, and where does it come from? Is <span class=\"math-container\">$\\mathbf{w}$</span> is the normalized weight? In general, how does weight normalization work? What is the intuition behind it?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "feedforward-neural-networks",
      "hidden-layers",
      "residual-networks"
    ],
    "owner": {
      "account_id": 5567066,
      "reputation": 91,
      "user_id": 14710,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/49b1bb1df764d545885d9a851bbc6c21?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Christopher Jernigan",
      "link": "https://ai.stackexchange.com/users/14710/christopher-jernigan"
    },
    "is_answered": true,
    "view_count": 975,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1583888372,
    "creation_date": 1522614168,
    "last_edit_date": 1583888176,
    "question_id": 5862,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5862/why-arent-there-neural-networks-that-connect-the-output-of-each-layer-to-all-ne",
    "title": "Why aren&#39;t there neural networks that connect the output of each layer to all next layers?",
    "body": "<p>Why aren't there neural networks that connect the output of each layer to all next layers?</p>\n\n<p>For example, the output of layer 1 would be fed to the input of layers 2, 3, 4, etc. Beyond computational power considerations, wouldn't this be better than only connecting layers 1 and 2, 3 and 4, etc? </p>\n\n<p>Also, wouldn't this solve the vanishing gradient problem?</p>\n\n<p>If computational power is the concern, perhaps you could connect layer 1 only to the next N layers.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "reinforcement-learning",
      "philosophy",
      "game-ai"
    ],
    "owner": {
      "account_id": 6476695,
      "reputation": 2258,
      "user_id": 4199,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-o_4nx3c8sHQ/AAAAAAAAAAI/AAAAAAAAAPU/lVuNexWGyPU/s256-rj/photo.jpg",
      "display_name": "zooby",
      "link": "https://ai.stackexchange.com/users/4199/zooby"
    },
    "is_answered": true,
    "view_count": 1299,
    "answer_count": 4,
    "score": 9,
    "last_activity_date": 1571779381,
    "creation_date": 1524052909,
    "last_edit_date": 1571779381,
    "question_id": 6102,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6102/can-a-neural-network-work-out-the-concept-of-distance",
    "title": "Can a neural network work out the concept of distance?",
    "body": "<p>Imagine a game where it is a black screen apart from a red pixel and a blue pixel. Given this game to a human, they will first see that pressing the arrow keys will move the red pixel. The next thing they will try is to move the red pixel onto the blue pixel.</p>\n\n<p>Give this game to an AI, it will randomly move the red pixel until a million tries later it accidentally moves onto the blue pixel to get a reward. If the AI had some concept of distance between the red and blue pixel, it might try to minimize this distance.</p>\n\n<p>Without actually programming in the concept of distance, if we take the pixels of the game can we calculate a number(s), such as \"entropy\", that would be lower when pixels are far apart than when close together? It should work with other configurations of pixels. Such as a game with three pixels where one is good and one is bad. Just to give the neural network more of a sense of how the screen looks? Then give the NN a goal, such as \"try to minimize the entropy of the board as well as try to get rewards\".</p>\n\n<p>Is there anything akin to this in current research?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "computer-vision",
      "decision-trees",
      "explainable-ai",
      "question-answering"
    ],
    "owner": {
      "account_id": 6002634,
      "reputation": 231,
      "user_id": 9983,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/10202646252877966/picture?type=large",
      "display_name": "The Impossible Squish",
      "link": "https://ai.stackexchange.com/users/9983/the-impossible-squish"
    },
    "is_answered": true,
    "view_count": 527,
    "accepted_answer_id": 6184,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1651907386,
    "creation_date": 1524742888,
    "last_edit_date": 1642226121,
    "question_id": 6176,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6176/why-does-nobody-use-decision-trees-for-visual-question-answering",
    "title": "Why does nobody use decision trees for visual question answering?",
    "body": "<p>I'm starting a project that will involve computer vision, visual question answering, and explainability. I am currently choosing what type of algorithm to use for my classifier - a neural network or a decision tree.</p>\n<p>It would seem to me that, because I want my system to include explainability, a decision tree would be the best choice. Decision trees are interpretable, whereas neural nets are like a black box.</p>\n<p>The other differences I'm aware of are: decision trees are faster, neural networks are more accurate, and neural networks are better at modelling nonlinearity.</p>\n<p>In all of the research I've done on computer vision and visual question answering, everyone uses neural networks, and no one seems to be using decision trees. Why? Is it for accuracy? I think a decision tree would be better because it is fast and interpretable, but if no one's using them for visual question answering, they must have a disadvantage that I haven't noticed.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "genetic-algorithms",
      "evolutionary-algorithms",
      "neat"
    ],
    "owner": {
      "account_id": 10295055,
      "reputation": 193,
      "user_id": 15356,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/4C36z.png?s=256",
      "display_name": "Chris",
      "link": "https://ai.stackexchange.com/users/15356/chris"
    },
    "is_answered": true,
    "view_count": 4053,
    "accepted_answer_id": 6233,
    "answer_count": 6,
    "score": 9,
    "last_activity_date": 1607867458,
    "creation_date": 1525198240,
    "question_id": 6231,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/6231/how-to-evaluate-a-neat-neural-network",
    "title": "How to evaluate a NEAT neural network?",
    "body": "<p>I'm trying to write my own implementation of NEAT and I'm stuck on the network evaluate function, which calculates the output of the network.</p>\n\n<p>NEAT as you may know contains a group of neural networks with continuously evolving topologies by the addition of new nodes and new connections. But with the addition of new connections between previously unconnected nodes, I see a problem that will occur when I go to evaluate, let me explain with an example:\n<a href=\"https://i.sstatic.net/cJTjN.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/cJTjN.png\" alt=\"Network\"></a></p>\n\n<pre><code>INPUTS = 2 yellow nodes\nHIDDEN = 3 blue nodes\nOUTPUT = 1 red node\n</code></pre>\n\n<p>In the image a new connection has been added connecting node3 to node5, how can I calculate the output for node5 if I have not yet calculated the output for node3, which depends on the output from node5?</p>\n\n<p>(not considering activation functions)</p>\n\n<pre><code>node5 output =  (1 * 0.5) + (1 * 0.2) + (node3 output * 0.8)\nnode3 output =  ((node5 output * 0.7) * 0.4)\n</code></pre>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "reference-request",
      "resource-request"
    ],
    "owner": {
      "account_id": 8436733,
      "reputation": 233,
      "user_id": 16672,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/LmnWJ.jpg?s=256",
      "display_name": "Martin S",
      "link": "https://ai.stackexchange.com/users/16672/martin-s"
    },
    "is_answered": true,
    "view_count": 1211,
    "accepted_answer_id": 7006,
    "answer_count": 5,
    "score": 9,
    "last_activity_date": 1640038679,
    "creation_date": 1530625404,
    "last_edit_date": 1640038679,
    "question_id": 6997,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6997/whats-a-good-resource-for-getting-familiar-with-reinforcement-learning",
    "title": "What&#39;s a good resource for getting familiar with reinforcement learning?",
    "body": "<p>I am familiar with supervised and unsupervised learning. I did the SaaS course done by Andrew Ng on Coursera.org.</p>\n\n<p>I am looking for something similar for reinforcement learning.</p>\n\n<p>Can you recommend something?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "dropout"
    ],
    "owner": {
      "account_id": 13674464,
      "reputation": 211,
      "user_id": 16466,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b4aebc40df056cab507bc64bdb371e7a?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user209974",
      "link": "https://ai.stackexchange.com/users/16466/user209974"
    },
    "is_answered": true,
    "view_count": 276,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1723709524,
    "creation_date": 1531258557,
    "question_id": 7109,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7109/5-years-later-are-maxout-networks-dead-and-why",
    "title": "5 years later, are maxout networks dead, and why?",
    "body": "<p><a href=\"https://arxiv.org/abs/1302.4389\" rel=\"noreferrer\">Maxout networks</a> were a simple yet brilliant idea of Goodfellow et al. from 2013 to max feature maps to get a universal approximator of convex activations. The design was tailored for use in conjunction with dropout (then recently introduced) and resulted of course in state-of-the-art results on benchmarks like CIFAR-10 and SVHN.</p>\n\n<p>Five years later, dropout is definitely still in the game, but what about maxout? The paper is still widely cited in recent papers according to Google Scholar, but it seems barely any are actually using the technique.</p>\n\n<p>So is maxout a thing of the past, and if so, why — what made it a top performer in 2013 but not in 2018?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "reinforcement-learning",
      "comparison",
      "unsupervised-learning"
    ],
    "owner": {
      "account_id": 6205671,
      "reputation": 205,
      "user_id": 16141,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f6be44a8b92f23792714a8241eb6c327?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "ebrahimi",
      "link": "https://ai.stackexchange.com/users/16141/ebrahimi"
    },
    "is_answered": true,
    "view_count": 470,
    "accepted_answer_id": 7297,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1575473417,
    "creation_date": 1532449908,
    "last_edit_date": 1575473417,
    "question_id": 7280,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7280/what-is-the-relationship-between-these-two-taxonomies-for-machine-learning-with",
    "title": "What is the relationship between these two taxonomies for machine learning with neural networks?",
    "body": "<p>Could you please let me know which of the following classification of Neural Network's learning algorithm is correct? </p>\n\n<p>The first <a href=\"http://cdn.intechweb.org/pdfs/14881.pdf\" rel=\"nofollow noreferrer\">one</a> classifies it into:</p>\n\n<ul>\n<li>supervised, </li>\n<li>unsupervised and </li>\n<li>reinforcement learning. </li>\n</ul>\n\n<p>However, the second <a href=\"http://neuro.bstu.by/ai/To-dom/My_research/Conf-3-FAILED/INTAS/M-Rybnik-Thesis-v2.75.pdf\" rel=\"nofollow noreferrer\">one</a> provides a different taxonomy on page 34: </p>\n\n<ul>\n<li>learning with a teacher (error correction learning including incremental and batch training), </li>\n<li>learning without a teacher (reinforcement, competitive, and unsupervised learning) </li>\n<li>memory-based learning, and </li>\n<li>Boltzmann learning. </li>\n</ul>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "definitions",
      "support-vector-machine"
    ],
    "owner": {
      "account_id": 7959824,
      "reputation": 193,
      "user_id": 17250,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/2cf5i.jpg?s=256",
      "display_name": "Efthymios Kalyviotis",
      "link": "https://ai.stackexchange.com/users/17250/efthymios-kalyviotis"
    },
    "is_answered": true,
    "view_count": 535,
    "accepted_answer_id": 7395,
    "answer_count": 3,
    "score": 9,
    "last_activity_date": 1589639755,
    "creation_date": 1533241775,
    "last_edit_date": 1589639755,
    "question_id": 7394,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7394/what-is-a-support-vector-machine",
    "title": "What is a support vector machine?",
    "body": "<p>What is a support vector machine (SVM)? Is an SVM a kind of a neural network, meaning it has nodes and weights, etc.? What is it best used for?</p>\n\n<p>Where I can find information about these?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "reinforcement-learning"
    ],
    "owner": {
      "account_id": 6476695,
      "reputation": 2258,
      "user_id": 4199,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-o_4nx3c8sHQ/AAAAAAAAAAI/AAAAAAAAAPU/lVuNexWGyPU/s256-rj/photo.jpg",
      "display_name": "zooby",
      "link": "https://ai.stackexchange.com/users/4199/zooby"
    },
    "is_answered": true,
    "view_count": 439,
    "answer_count": 4,
    "score": 9,
    "last_activity_date": 1639330313,
    "creation_date": 1534089269,
    "last_edit_date": 1639330313,
    "question_id": 7528,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7528/how-do-you-program-fear-into-a-neural-network",
    "title": "How do you program fear into a neural network?",
    "body": "<p>If you've been attacked by a spider once, chances are you'll never go near a spider again.</p>\n<p>In a neural network model, having a bad experience with a spider will slightly decrease the probability you will go near a spider depending on the learning rate. This is not good.</p>\n<p>How can you program fear into a neural network, such that you don't need hundreds of examples of being bitten by a spider in order to ignore the spider (and also that it doesn't just lower the probability that you will choose to go near a spider)?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "policy-gradients",
      "proofs"
    ],
    "owner": {
      "account_id": 10294493,
      "reputation": 91,
      "user_id": 18043,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e48bda82162083afe01a998204b2da5f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Laura C",
      "link": "https://ai.stackexchange.com/users/18043/laura-c"
    },
    "is_answered": true,
    "view_count": 1273,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1591807331,
    "creation_date": 1536525067,
    "last_edit_date": 1591807191,
    "question_id": 7896,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7896/why-is-baseline-conditional-on-state-at-some-timestep-unbiased",
    "title": "Why is baseline conditional on state at some timestep unbiased?",
    "body": "<p>In the <a href=\"http://rail.eecs.berkeley.edu/deeprlcourse/static/homeworks/hw2.pdf\" rel=\"nofollow noreferrer\">homework for the Berkeley RL class</a>, problem 1, it asks you to show that the policy gradient is still unbiased if the baseline subtracted is a function of the state at time step <span class=\"math-container\">$t$</span>.</p>\n\n<p><span class=\"math-container\">$$ \\triangledown _\\theta \\sum_{t=1}^T \\mathbb{E}_{(s_t,a_t) \\sim p(s_t,a_t)} [b(s_t)] = 0   $$</span></p>\n\n<p>I am struggling through what the first step of such a proof might be. </p>\n\n<p>Can someone point me in the right direction? My initial thought was to somehow use the <a href=\"https://en.wikipedia.org/wiki/Law_of_total_expectation\" rel=\"nofollow noreferrer\">law of total expectation</a> to make the expectation of <span class=\"math-container\">$b(s_t)$</span> conditional on <span class=\"math-container\">$T$</span>, but I am not sure.</p>\n"
  },
  {
    "tags": [
      "comparison",
      "intelligent-agent",
      "goal-based-agents",
      "utility-based-agents"
    ],
    "owner": {
      "account_id": 14287935,
      "reputation": 135,
      "user_id": 18950,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/30d45683a0520623bb27198f0246aaed?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "RashkRizwan",
      "link": "https://ai.stackexchange.com/users/18950/rashkrizwan"
    },
    "is_answered": true,
    "view_count": 17106,
    "accepted_answer_id": 8655,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1639330755,
    "creation_date": 1540733451,
    "last_edit_date": 1639330755,
    "question_id": 8651,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/8651/what-is-the-difference-between-goal-based-and-utility-based-agents",
    "title": "What is the difference between goal-based and utility-based agents?",
    "body": "<p>What is the difference between goal-based and utility-based agents? Please, provide a real-world example.</p>\n"
  },
  {
    "tags": [
      "comparison",
      "generative-adversarial-networks",
      "autoencoders",
      "generative-model",
      "variational-autoencoder"
    ],
    "owner": {
      "account_id": 6489497,
      "reputation": 269,
      "user_id": 19738,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/9aab44011e25d2fdc10050c9413b2e94?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Trect",
      "link": "https://ai.stackexchange.com/users/19738/trect"
    },
    "is_answered": true,
    "view_count": 13742,
    "answer_count": 5,
    "score": 9,
    "last_activity_date": 1718716847,
    "creation_date": 1541786931,
    "last_edit_date": 1640333981,
    "question_id": 8885,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/8885/why-is-the-variational-auto-encoders-output-blurred-while-gans-output-is-crisp",
    "title": "Why is the variational auto-encoder&#39;s output blurred, while GANs output is crisp and has sharp edges?",
    "body": "<p>I observed in several papers that the variational autoencoder's output is blurred, while GANs output is crisp and has sharp edges.</p>\n<p>Can someone please give some intuition why that is the case? I did think a lot but couldn't find any logic.</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "reference-request",
      "algorithm-request",
      "audio-processing",
      "signal-processing"
    ],
    "owner": {
      "account_id": 5559123,
      "reputation": 99,
      "user_id": 20633,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/1205f8fc557addaec30227d96537c3bd?s=256&d=identicon&r=PG",
      "display_name": "Thibault Molleman",
      "link": "https://ai.stackexchange.com/users/20633/thibault-molleman"
    },
    "is_answered": true,
    "view_count": 13780,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1640265980,
    "creation_date": 1544627726,
    "last_edit_date": 1640265817,
    "question_id": 9496,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/9496/is-it-possible-to-clean-up-an-audio-recording-of-a-lecture-using-some-type-of-ai",
    "title": "Is it possible to clean up an audio recording of a lecture using some type of AI system?",
    "body": "<p>Is it possible to clean up an audio recording of a lecture from a smartphone (i.e. remove the background noise) using some type of AI system?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "math",
      "policy-gradients",
      "rewards",
      "reward-to-go"
    ],
    "owner": {
      "account_id": 91198,
      "reputation": 298,
      "user_id": 18074,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/1bf8c6b6c00c9c6a80a408b7b4c4e64c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Konstantin Solomatov",
      "link": "https://ai.stackexchange.com/users/18074/konstantin-solomatov"
    },
    "is_answered": true,
    "view_count": 7478,
    "accepted_answer_id": 10369,
    "answer_count": 3,
    "score": 9,
    "last_activity_date": 1704469921,
    "creation_date": 1545267604,
    "last_edit_date": 1602345124,
    "question_id": 9614,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/9614/why-does-the-reward-to-go-trick-in-policy-gradient-methods-work",
    "title": "Why does the &quot;reward to go&quot; trick in policy gradient methods work?",
    "body": "<p>In the policy gradient method, there's a trick to reduce the variance of policy gradient. We use causality, and remove part of the sum over rewards so that only actions happened after the reward are taken into account (See here <a href=\"http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf\" rel=\"noreferrer\">http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf</a>, slide 18).</p>\n<p>Why does it work? I understand the intuitive explanation, but what's the rigorous proof of it? Can you point me to some papers?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "convolutional-neural-networks",
      "channel"
    ],
    "owner": {
      "account_id": 12098984,
      "reputation": 321,
      "user_id": 20358,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/237bb6484b2ef4bf7ceebcacc049d2fe?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "anonuser01",
      "link": "https://ai.stackexchange.com/users/20358/anonuser01"
    },
    "is_answered": true,
    "view_count": 12096,
    "accepted_answer_id": 9753,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1627610064,
    "creation_date": 1546151763,
    "last_edit_date": 1627610064,
    "question_id": 9751,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/9751/what-is-the-concept-of-channels-in-cnns",
    "title": "What is the concept of channels in CNNs?",
    "body": "<p>I am trying to understand what channels mean in convolutional neural networks. When working with grayscale and colored images, I understand that the number of channels is set to 1 and 3 (in the first conv layer), respectively, where 3 corresponds to red, green, and blue.</p>\n<p>Say you have a colored image that is <span class=\"math-container\">$200 \\times 200$</span> pixels. The standard is such that the input matrix is a <span class=\"math-container\">$200 \\times 200$</span> matrix with 3 channels. The first convolutional layer would have a filter that is size <span class=\"math-container\">$N \\times M \\times 3$</span>, where <span class=\"math-container\">$N,M &lt; 200$</span> (I think they're usually set to 3 or 5).</p>\n<p>Would it be possible to structure the input data differently, such that the number of channels now becomes the width or height of the image? i.e., the number of channels would be 200, the input matrix would then be <span class=\"math-container\">$200 \\times 3$</span> or <span class=\"math-container\">$3 \\times 200$</span>. What would be the advantage/disadvantage of this formulation versus the standard (# of channels = 3)? Obviously, this would limit your filter's spatial size, but dramatically increase it in the depth direction.</p>\n<p>I am really posing this question because I don't quite understand the concept of channels in CNNs.</p>\n"
  },
  {
    "tags": [
      "search",
      "minimax",
      "alpha-beta-pruning"
    ],
    "owner": {
      "account_id": 15051453,
      "reputation": 91,
      "user_id": 21125,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-9HIKxySTzn0/AAAAAAAAAAI/AAAAAAAAAAA/AKxrwcaioCiwzjmGv54MER62y-RAvU4YaQ/mo/s256-rj/photo.jpg",
      "display_name": "Sunshine ",
      "link": "https://ai.stackexchange.com/users/21125/sunshine"
    },
    "is_answered": true,
    "view_count": 6488,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1551174009,
    "creation_date": 1546525232,
    "last_edit_date": 1551174009,
    "question_id": 9808,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/9808/can-someone-help-me-to-understand-the-alpha-beta-pruning-algorithm",
    "title": "Can someone help me to understand the alpha-beta pruning algorithm?",
    "body": "<p>I understand the minimax algorithm, but I am unable to understand deeply the minimax algorithm with alpha-beta pruning, even after having looked up several sources (on the web) and having tried to read the algorithm and understand how it works.</p>\n\n<p>Do you have a good source that explains alpha-beta pruning clearly, or can you help me to understand the alpha-beta pruning (with a simple explanation)?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "activation-functions",
      "relu",
      "sigmoid"
    ],
    "owner": {
      "account_id": 4644152,
      "reputation": 93,
      "user_id": 21143,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/gPdQd.png?s=256",
      "display_name": "JSChang",
      "link": "https://ai.stackexchange.com/users/21143/jschang"
    },
    "is_answered": true,
    "view_count": 4329,
    "accepted_answer_id": 9942,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1671136825,
    "creation_date": 1546609162,
    "last_edit_date": 1557933413,
    "question_id": 9828,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/9828/what-happens-when-i-mix-activation-functions",
    "title": "What happens when I mix activation functions?",
    "body": "<p>There are several activation functions, such as ReLU, sigmoid or <span class=\"math-container\">$\\tanh$</span>. What happens when I mix activation functions?</p>\n\n<p>I recently found that Google has developed Swish activation function which is (x*sigmoid). By altering activation function can it increase accuracy on small neural network problem such as XOR problem?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "applications"
    ],
    "owner": {
      "account_id": 14729036,
      "reputation": 91,
      "user_id": 20976,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ce575389f7e6ae82e1f76d0ea1357775?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "IpsumPanEst",
      "link": "https://ai.stackexchange.com/users/20976/ipsumpanest"
    },
    "is_answered": true,
    "view_count": 10643,
    "answer_count": 3,
    "score": 9,
    "last_activity_date": 1676065321,
    "creation_date": 1547469811,
    "last_edit_date": 1561740872,
    "question_id": 9983,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/9983/is-it-possible-to-use-ai-to-reverse-engineer-software",
    "title": "Is it possible to use AI to reverse engineer software?",
    "body": "<p>I was thinking of something of the sort:</p>\n\n<ol>\n<li><p>Build a program (call this one fake user) that generates lots and lots and lots of data based on the usage of another program (call this one target) using stimuli and response. For example, if the target is a minesweeper, the fake user would play the game a carl sagan number of times, as well as try to click all buttons on all sorts of different situations, etc...</p></li>\n<li><p>run a machine learning program (call this one the copier) designed to evolve a code that works as similar as possible to the target. </p></li>\n<li><p>kablam, you have a \"sufficiently nice\" open source copy of the target.</p></li>\n</ol>\n\n<p>Is this possible?</p>\n\n<p>Is something else possible to achieve the same result, namely, to obtain a \"sufficiently nice\" open source copy of the original target program?</p>\n"
  },
  {
    "tags": [
      "comparison",
      "definitions",
      "search",
      "planning",
      "state-space-search"
    ],
    "owner": {
      "account_id": 11604621,
      "reputation": 273,
      "user_id": 21719,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-aFKcbLRcSEo/AAAAAAAAAAI/AAAAAAAAAAw/VgAuhFrfk3o/s256-rj/photo.jpg",
      "display_name": "theantomc",
      "link": "https://ai.stackexchange.com/users/21719/theantomc"
    },
    "is_answered": true,
    "view_count": 7854,
    "protected_date": 1637071320,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1640696275,
    "creation_date": 1548412447,
    "last_edit_date": 1640696275,
    "question_id": 10180,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10180/what-is-the-difference-between-search-and-planning",
    "title": "What is the difference between search and planning?",
    "body": "<p>I'm reading the book <em>Artificial Intelligence: A Modern Approach</em> (by Stuart Russell and Peter Norvig).</p>\n<p>However, I don't understand the difference between search and planning. I was more confused when I saw that some search problems can be determined in a planning way. My professor explained to me in a confusing way that the real difference is that search uses a heuristic function, but my book says that planning uses a heuristic too (in chapter 10.2.3).</p>\n<p>I read <a href=\"https://stackoverflow.com/q/10282476/3924118\">this Stack Overflow post</a> that says in a certain way what I'm saying.</p>\n<p>So, what is the difference between search and planning?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "models",
      "hyper-parameters",
      "learning-rate"
    ],
    "owner": {
      "account_id": 211851,
      "reputation": 217,
      "user_id": 20257,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/eRQ6o.jpg?s=256",
      "display_name": "JohnAllen",
      "link": "https://ai.stackexchange.com/users/20257/johnallen"
    },
    "is_answered": true,
    "view_count": 1320,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1640300867,
    "creation_date": 1554066444,
    "last_edit_date": 1640300867,
    "question_id": 11567,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11567/what-causes-a-model-to-require-a-low-learning-rate",
    "title": "What causes a model to require a low learning rate?",
    "body": "<p>I've pondered this for a while without developing an intuition for the math behind the cause of this.  </p>\n\n<p>So what causes a model to need a low learning rate?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning"
    ],
    "owner": {
      "account_id": 8074287,
      "reputation": 361,
      "user_id": 23288,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/34c947af65e0ffd63896dce856ee2923?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Jan",
      "link": "https://ai.stackexchange.com/users/23288/jan"
    },
    "is_answered": true,
    "view_count": 632,
    "accepted_answer_id": 11821,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1555638944,
    "creation_date": 1555247066,
    "last_edit_date": 1555267132,
    "question_id": 11809,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11809/are-there-reinforcement-learning-algorithms-that-scale-to-large-problems",
    "title": "Are there reinforcement learning algorithms that scale to large problems?",
    "body": "<p>Given a large problem, value iteration and other table based approaches seem to require too many iterations before they start to converge. Are there other reinforcement learning approaches that better scale to large problems and minimize the amount of iterations in general?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "optimization",
      "gradient-descent",
      "mean-squared-error"
    ],
    "owner": {
      "account_id": 2893014,
      "reputation": 213,
      "user_id": 11789,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/fNLmX.jpg?s=256",
      "display_name": "souparno majumder",
      "link": "https://ai.stackexchange.com/users/11789/souparno-majumder"
    },
    "is_answered": true,
    "view_count": 3047,
    "accepted_answer_id": 11982,
    "answer_count": 3,
    "score": 9,
    "last_activity_date": 1674228524,
    "creation_date": 1556046773,
    "last_edit_date": 1637070452,
    "question_id": 11979,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11979/how-is-it-possible-that-the-mse-used-to-train-neural-networks-with-gradient-desc",
    "title": "How is it possible that the MSE used to train neural networks with gradient descent has multiple local minima?",
    "body": "<p>We often train neural networks by optimizing the mean squared error (MSE), which is an equation of a parabola <span class=\"math-container\">$y=x^2$</span>, with gradient descent.</p>\n<p>We also say that weight adjustment in a neural network by the gradient descent algorithm can hit a local minimum and get stuck in there.</p>\n<p>How are multiple local minima on the equation of a parabola possible, if a parabola has only one minimum?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "reward-functions",
      "reward-design",
      "reward-shaping"
    ],
    "owner": {
      "account_id": 1966432,
      "reputation": 309,
      "user_id": 25360,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/85cb712935d9afd72f9c170ba22004a6?s=256&d=identicon&r=PG",
      "display_name": "Hazzaldo",
      "link": "https://ai.stackexchange.com/users/25360/hazzaldo"
    },
    "is_answered": true,
    "view_count": 2978,
    "accepted_answer_id": 12299,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1611146961,
    "creation_date": 1557620110,
    "last_edit_date": 1611146961,
    "question_id": 12264,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12264/how-do-we-define-the-reward-function-for-an-environment",
    "title": "How do we define the reward function for an environment?",
    "body": "<p>How do you actually decide what reward value to give for each action in a given state for an environment?</p>\n<p>Is this purely experimental and down to the programmer of the environment? So, is it a heuristic approach of simply trying different reward values and see how the learning process shapes up?</p>\n<p>Of course, I understand that the reward values have to make sense, and not just put completely random values, i.e. if the agent makes mistakes then deduct points, etc.</p>\n<p>So, am I right in saying it's just about trying different reward values for actions encoded in the environment and see how it affects the learning?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "q-learning",
      "monte-carlo-tree-search",
      "supervised-learning",
      "alphazero"
    ],
    "owner": {
      "account_id": 16238373,
      "reputation": 125,
      "user_id": 26791,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-xUzv79S7QxM/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rfgarmL6orSXIrhxtWxUWNKhbMNSw/s256-rj/photo.jpg",
      "display_name": "Avetik",
      "link": "https://ai.stackexchange.com/users/26791/avetik"
    },
    "is_answered": true,
    "view_count": 2015,
    "accepted_answer_id": 13168,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1562062566,
    "creation_date": 1562000520,
    "last_edit_date": 1562012877,
    "question_id": 13156,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/13156/does-alphazero-use-q-learning",
    "title": "Does AlphaZero use Q-Learning?",
    "body": "<p>I was reading the AlphaZero paper <a href=\"https://arxiv.org/pdf/1712.01815.pdf\" rel=\"noreferrer\">Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</a>, and it seems they don't mention Q-Learning anywhere.</p>\n\n<p>So does AZ use Q-Learning on the results of self-play or just a Supervised Learning?</p>\n\n<p>If it's a Supervised Learning, then why is it said that AZ uses Reinforcement Learning? Is \"reinforcement\" part primarily a result of using Monte-Carlo Tree Search?</p>\n"
  },
  {
    "tags": [
      "training",
      "computer-vision",
      "deep-neural-networks",
      "object-recognition"
    ],
    "owner": {
      "account_id": 11124291,
      "reputation": 91,
      "user_id": 28100,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/223f2e85e42e7c3f70f7337f2a8c0002?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Krishnakumar",
      "link": "https://ai.stackexchange.com/users/28100/krishnakumar"
    },
    "is_answered": true,
    "view_count": 15094,
    "answer_count": 4,
    "score": 9,
    "last_activity_date": 1637721247,
    "creation_date": 1566376399,
    "last_edit_date": 1566515062,
    "question_id": 14079,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/14079/what-could-an-oscillating-training-loss-curve-represent",
    "title": "What could an oscillating training loss curve represent?",
    "body": "<p><a href=\"https://i.sstatic.net/7FnY6.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/7FnY6.png\" alt=\"enter image description here\"></a></p>\n\n<p>I tried to create a simple model that receives an <span class=\"math-container\">$80 \\times 130$</span> pixel image. I only had 35 images and 10 test images. I trained this model for a binary classification task. The architecture of the model is described below.</p>\n\n<pre><code>conv2d_1 (Conv2D)            (None, 80, 130, 64)       640       \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 78, 128, 64)       36928     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 39, 64, 64)        0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 39, 64, 64)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 39, 64, 128)       73856     \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 37, 62, 128)       147584    \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 18, 31, 128)       0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 18, 31, 128)       0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 71424)             0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 512)               36569600  \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 513     \n</code></pre>\n\n<p>What could the oscillating training loss curve represent above? Why is the validation loss constant?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "training",
      "tensorflow",
      "gpu"
    ],
    "owner": {
      "account_id": 15406827,
      "reputation": 1290,
      "user_id": 22659,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/QBGXG.jpg?s=256",
      "display_name": "GKozinski",
      "link": "https://ai.stackexchange.com/users/22659/gkozinski"
    },
    "is_answered": true,
    "view_count": 7437,
    "accepted_answer_id": 14138,
    "answer_count": 3,
    "score": 9,
    "last_activity_date": 1640886845,
    "creation_date": 1566653289,
    "last_edit_date": 1640886845,
    "question_id": 14121,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/14121/is-a-gpu-always-faster-than-a-cpu-for-training-neural-networks",
    "title": "Is a GPU always faster than a CPU for training neural networks?",
    "body": "<p>Currently, I am working on a few projects that use feedforward neural networks for regression and classification of simple tabular data. I have noticed that training a neural network using TensorFlow-GPU is often slower than training the same network using TensorFlow-CPU.</p>\n<p>Could something be wrong with my setup/code or is it possible that sometimes GPU is slower than CPU?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "genetic-algorithms",
      "optimization",
      "constrained-optimization"
    ],
    "owner": {
      "account_id": 9772521,
      "reputation": 91,
      "user_id": 30015,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/wEu3L.jpg?s=256",
      "display_name": "Ramzah Rehman",
      "link": "https://ai.stackexchange.com/users/30015/ramzah-rehman"
    },
    "is_answered": true,
    "view_count": 165,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1628959264,
    "creation_date": 1569571966,
    "last_edit_date": 1611423342,
    "question_id": 15648,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/15648/given-a-list-of-integers-c-1-dots-c-n-how-do-i-find-an-integer-d-th",
    "title": "Given a list of integers $\\{c_1, \\dots, c_N \\}$, how do I find an integer $D$ that minimizes the sum of remainders $\\sum_i c_i \\text{ mod } D$?",
    "body": "<p>I have a set of fixed integers <span class=\"math-container\">$S = \\{c_1, \\dots, c_N \\}$</span>. I want to find a single integer <span class=\"math-container\">$D$</span>, greater than a certain threshold <span class=\"math-container\">$T$</span>, i.e. <span class=\"math-container\">$D &gt; T \\geq 0$</span>, that divides each <span class=\"math-container\">$c_i$</span> and leaves remainder <span class=\"math-container\">$r_i \\geq 0$</span>, i.e. <span class=\"math-container\">$r_i$</span> can be written as <span class=\"math-container\">$r_i = c_i \\text{ mod } D$</span>, such that the sum of remainders is minimized.</p>\n<p>In other words, this is my problem</p>\n<p><span class=\"math-container\">\\begin{equation}\n\\begin{aligned}\nD^* \\quad = \\text{argmin}_D&amp;  \\sum_i c_i \\text{ mod } D \\\\\n\\textrm{subject to} &amp;\\quad D &gt; T\n\\end{aligned}\n\\end{equation}</span></p>\n<p>If the integers have a common divisor, this problem is easy. If the integers are relatively co-prime however, then it is not clear how to solve it.</p>\n<p>The set <span class=\"math-container\">$|S| = N$</span> can be around <span class=\"math-container\">$10000$</span>, and each element also has a value in tens of thousands.</p>\n<p>I was thinking about solving it with a genetic algorithm (GA), but it is kind of slow. I want to know is there any other way to solve this problem.</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "reinforcement-learning",
      "computer-vision",
      "terminology",
      "robotics"
    ],
    "owner": {
      "account_id": 16886133,
      "reputation": 91,
      "user_id": 30470,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-IZbSOt9O2PA/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rcQgBKhii27T9S6PDpJy9flZ6XLWQ/s256-rj/photo.jpg",
      "display_name": "wcc",
      "link": "https://ai.stackexchange.com/users/30470/wcc"
    },
    "is_answered": true,
    "view_count": 5166,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1571055211,
    "creation_date": 1571045228,
    "last_edit_date": 1571055211,
    "question_id": 15903,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/15903/what-are-sim2sim-sim2real-and-real2real",
    "title": "What are sim2sim, sim2real and real2real?",
    "body": "<p>Recently, I always hear about the terms sim2sim, sim2real and real2real. Will anyone explain the meaning/motivation of these terms (in DL/RL research community)?</p>\n\n<p>What are the challenges in this research area?</p>\n\n<p>Anything intuitive would be appreciated!</p>\n"
  },
  {
    "tags": [
      "software-evaluation",
      "artificial-life",
      "self-replicating-machines"
    ],
    "owner": {
      "account_id": 4589458,
      "reputation": 1279,
      "user_id": 16363,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/j6C7B.jpg?s=256",
      "display_name": "Rexcirus",
      "link": "https://ai.stackexchange.com/users/16363/rexcirus"
    },
    "is_answered": true,
    "view_count": 329,
    "accepted_answer_id": 22728,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1596124888,
    "creation_date": 1574807338,
    "last_edit_date": 1574978966,
    "question_id": 16801,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/16801/why-is-the-status-of-artificial-life-software-so-under-developed",
    "title": "Why is the status of artificial life software so under-developed?",
    "body": "<p>I'm interested in self replicating artificial life (with many agents), so after reviewing the literature with the excellent <a href=\"http://www.molecularassembler.com/KSRM.htm\" rel=\"noreferrer\">Kinematic Self-Replicating Machines</a> I started looking for software implementations. I understand that the field is still in the early stages and mainly academic, but the status of artificial life software looks rather poor in 2019. </p>\n\n<p>On wikipedia there is this <a href=\"https://en.wikipedia.org/wiki/Artificial_life#Notable_simulators\" rel=\"noreferrer\">list of software simulators</a>.\nGoing trough the list only ApeSDK, Avida, DigiHive, DOSE, Polyword have been updated in 2019. I did not find a public repo for Biogenesis. ApeSDK, DigiHive and DOSE are single author programs. </p>\n\n<p>All in all I don't see a single very active project with a large community around (I would be happy to have missed something). And this is more surprising considering the big momentum of AI and the proliferation of many ready to use AI tools and libraries. </p>\n\n<p>Why is the status artificial life software so under-developed, when this field looks promising both from a commercial (see manufacturing, mining or space exploration applications) and academic (ecology, biology, human brain and more) perspective? Did the field underdelivered on expectations in past years and got less funding? Did the field hit a theoretical or computational roadblock? </p>\n"
  },
  {
    "tags": [
      "algorithm",
      "monte-carlo-tree-search",
      "monte-carlo-methods",
      "planning",
      "tree-search"
    ],
    "owner": {
      "account_id": 1837336,
      "reputation": 375,
      "user_id": 3373,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ec4c5e87713e17ca906554b846ec5719?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user76284",
      "link": "https://ai.stackexchange.com/users/3373/user76284"
    },
    "is_answered": true,
    "view_count": 2547,
    "accepted_answer_id": 17713,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1737450374,
    "creation_date": 1575348454,
    "last_edit_date": 1592387840,
    "question_id": 16905,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/16905/mcts-how-to-choose-the-final-action-from-the-root",
    "title": "MCTS: How to choose the final action from the root",
    "body": "<p>When the time allotted to <a href=\"https://en.wikipedia.org/wiki/Monte_Carlo_tree_search#Principle_of_operation\" rel=\"noreferrer\">Monte Carlo tree search</a> runs out, what action should be chosen from the root?</p>\n<ul>\n<li><p>The <a href=\"https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.102.1296\" rel=\"noreferrer\">original UCT paper</a> (2006) says <code>bestAction</code> in their algorithm.</p>\n</li>\n<li><p><a href=\"https://www.aaai.org/Papers/AIIDE/2008/AIIDE08-036.pdf\" rel=\"noreferrer\">Monte-Carlo Tree Search: A New Framework for Game AI</a> (2008) says</p>\n</li>\n</ul>\n<blockquote>\n<p>The game action finally executed by the program in the actual game, is the one corresponding to the child which was explored the most.</p>\n</blockquote>\n<ul>\n<li><a href=\"http://www.incompleteideas.net/609%20dropbox/other%20readings%20and%20resources/MCTS-survey.pdf\" rel=\"noreferrer\">A Survey of Monte Carlo Tree Search Methods</a> (2012) says</li>\n</ul>\n<blockquote>\n<p>The basic algorithm involves iteratively building a search tree until some predefined computational budget – typically a time, memory or iteration constraint – is reached, at which point the search is halted and the best-performing root action returned.</p>\n<p>[...] The result of the overall search a(BESTCHILD(v0)) is the action a that leads to the best child of the root node v0, where the exact definition of “best” is defined by the implementation.</p>\n<p>[...] As soon as the search is interrupted or the computation budget is reached, the search terminates and an action a of the root nodet0is selected by some mechanism. Schadd [188] describes four criteria for selecting the winning action, based on the work of Chaslot et al [60]:</p>\n<ol>\n<li><p><strong>Max child</strong>: Select the root child with the highest reward.</p>\n</li>\n<li><p><strong>Robust child</strong>: Select the most visited root child.</p>\n</li>\n<li><p><strong>Max-Robust child</strong>: Select the root child with both the highest visit count and the highest reward. If none exist, then continue searching until an acceptable visit count is achieved [70].</p>\n</li>\n<li><p><strong>Secure child</strong>: Select the child which maximises a lower confidence bound.</p>\n</li>\n</ol>\n<p>[...] Once some computational budget has been reached, the algorithm terminates and returns the best move found,corresponding to the child of the root with the highest visit count.</p>\n<p>The return value of the overall search in this case is a(BESTCHILD(v0,0)) which will give the action a that leads to the child with the highest reward, since the exploration parameter c is set to 0 for this final call on the root node v0. The algorithm could instead return the action that leads to the most visited child; these two options will usually – but not always! – describe the same action. This potential discrepancy is addressed in the Go program ERICA by continuing the search if the most visited root action is not also the one with the highest reward. This improved ERICA’s winning rate against GNU GO from 47% to 55% [107].</p>\n</blockquote>\n<p>But their algorithm 2 uses the same criterion as the internal-node selection policy:</p>\n<p><span class=\"math-container\">$$\\operatorname{argmax}_{v'} \\frac{Q(v')}{N(v')} + c \\sqrt{\\frac{2 \\ln N(v)}{N(v')}}$$</span></p>\n<p>which is neither the max child nor the robust child! This situation is quite confusing, and I'm wondering which approach is nowadays considered most successful/appropriate.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "gradient-descent",
      "momentum",
      "adam",
      "optimizers"
    ],
    "owner": {
      "account_id": 7328064,
      "reputation": 1318,
      "user_id": 2844,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/hnwBo.jpg?s=256",
      "display_name": "Dan D",
      "link": "https://ai.stackexchange.com/users/2844/dan-d"
    },
    "is_answered": true,
    "view_count": 1671,
    "accepted_answer_id": 17478,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1653434266,
    "creation_date": 1578899097,
    "last_edit_date": 1653434266,
    "question_id": 17476,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/17476/what-is-the-formula-for-the-momentum-and-adam-optimisers",
    "title": "What is the formula for the momentum and Adam optimisers?",
    "body": "<p>In the gradient descent algorithm, the formula to update the weight <span class=\"math-container\">$w$</span>, which has <span class=\"math-container\">$g$</span> as the partial gradient of the loss function with respect to it, is:</p>\n<p><span class=\"math-container\">$$w\\ -= r \\times g$$</span></p>\n<p>where <span class=\"math-container\">$r$</span> is the learning rate.</p>\n<p>What should be the formula for momentum optimizer and Adam (adaptive momentum?) optimizer? Something should be added to the right side of the formula above?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "comparison",
      "recurrent-neural-networks",
      "long-short-term-memory",
      "recurrent-layers"
    ],
    "owner": {
      "account_id": 17829241,
      "reputation": 93,
      "user_id": 33759,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2fff341de8b9337ea285ec822aee7db2?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Mao76",
      "link": "https://ai.stackexchange.com/users/33759/mao76"
    },
    "is_answered": true,
    "view_count": 32511,
    "accepted_answer_id": 18199,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1639310685,
    "creation_date": 1582490166,
    "last_edit_date": 1639310685,
    "question_id": 18198,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/18198/what-is-the-difference-between-lstm-and-rnn",
    "title": "What is the difference between LSTM and RNN?",
    "body": "<p>What is the difference between LSTM and RNN? I know that RNN is a layer used in neural networks, but what exactly is an LSTM? Is it also a layer with the same characteristics?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "comparison",
      "transfer-learning",
      "meta-learning"
    ],
    "owner": {
      "account_id": 1638354,
      "reputation": 155,
      "user_id": 33801,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/b6x24.png?s=256",
      "display_name": "Long",
      "link": "https://ai.stackexchange.com/users/33801/long"
    },
    "is_answered": true,
    "view_count": 11201,
    "accepted_answer_id": 18428,
    "answer_count": 4,
    "score": 9,
    "last_activity_date": 1648928446,
    "creation_date": 1582614779,
    "last_edit_date": 1582667432,
    "question_id": 18232,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/18232/what-are-the-differences-between-transfer-learning-and-meta-learning",
    "title": "What are the differences between transfer learning and meta learning?",
    "body": "<p>What are the differences between meta-learning and transfer learning?</p>\n\n<p>I have read 2 articles on <a href=\"https://qr.ae/Tdowsi\" rel=\"noreferrer\">Quora</a> and <a href=\"https://towardsdatascience.com/icml-2018-advances-in-transfer-multitask-and-semi-supervised-learning-2a15ef7208ec\" rel=\"noreferrer\">TowardDataScience</a>. </p>\n\n<blockquote>\n  <p>Meta learning is a part of machine learning theory in which some\n  algorithms are applied on meta data about the case to improve a\n  machine learning process. The meta data includes properties about the\n  algorithm used, learning task itself etc. Using the meta data, one can\n  make a better decision of chosen learning algorithm(s) to solve the\n  problem more efficiently.</p>\n</blockquote>\n\n<p>and</p>\n\n<blockquote>\n  <p>Transfer learning aims at improving the process of learning new tasks\n  using the experience gained by solving predecessor problems which are\n  somewhat similar. In practice, most of the time, machine learning\n  models are designed to accomplish a single task. However, as humans,\n  we make use of our past experience for not only repeating the same\n  task in the future but learning completely new tasks, too. That is, if\n  the new problem that we try to solve is similar to a few of our past\n  experiences, it becomes easier for us. Thus, for the purpose of using\n  the same learning approach in Machine Learning, transfer learning\n  comprises methods to transfer past experience of one or more source\n  tasks and makes use of it to boost learning in a related target task.</p>\n</blockquote>\n\n<p>The comparisons still confuse me as both seem to share a lot of similarities in terms of reusability. Meta-learning is said to be \"model agnostic\", yet it uses metadata (hyperparameters or weights) from previously learned tasks. It goes the same with transfer learning, as it may reuse partially a trained network to solve related tasks. I understand that there is a lot more to discuss, but, broadly speaking, I do not see so much difference between the two.</p>\n\n<p>People also use terms like \"meta-transfer learning\", which makes me think both types of learning have a strong connection with each other.</p>\n\n<p>I also found a <a href=\"https://stats.stackexchange.com/q/255025/82135\">similar question</a>, but the answers seem not to agree with each other. For example, some may say that multi-task learning is a sub-category of transfer learning, others may not think so.</p>\n"
  },
  {
    "tags": [
      "reference-request",
      "game-ai",
      "algorithm-request"
    ],
    "owner": {
      "account_id": 1762563,
      "reputation": 41,
      "user_id": 34027,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/71cca5c2d212a48636df57486cf96595?s=256&d=identicon&r=PG",
      "display_name": "Ben Beri",
      "link": "https://ai.stackexchange.com/users/34027/ben-beri"
    },
    "is_answered": true,
    "view_count": 3316,
    "answer_count": 5,
    "score": 9,
    "last_activity_date": 1640864767,
    "creation_date": 1583398233,
    "last_edit_date": 1640864767,
    "question_id": 18431,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/18431/what-are-examples-of-approaches-to-create-an-ai-for-a-fighting-robot-in-an-mmo-g",
    "title": "What are examples of approaches to create an AI for a fighting robot in an MMO game?",
    "body": "<p>I have an MMO game where I have players. I wanted to invent something new to the game, and add player-bots to make the game be single-playable as well. The AI I want to add is simply only for fighting other players or other player-bots that he sees around at his level.</p>\n<p>So, I thought of implementing my fighting strategy, exactly how I play, to the bot which is basically using if statements and randoms. For example, when the opponent has low health, and the bot has enough special attack power, he will use this chance and use his special attack power in order to try to knock the opponent down, or if the bot has low health he will eat in time but not too much because there is a point in risking fights, if you eat too much your opponent will do too. Or, for example, if the bot detects the opponent player is eating too much and gains health, he will do the same.</p>\n<p>I told this idea of the implementation to one of my friends and he simply responded with: This is not AI, it's simply just a set of conditions, it does not have any heuristic functions.</p>\n<p><strong>For that type of game, what are some ideas to create a real AI to achieve these conditions?</strong></p>\n<p>Basically, the AI should know what to do in order to beat the opponent, based on the opponent's data such as current health, Armour and weapons, and level, if he risks his health or not and so on.</p>\n<p>I am a beginner and it really interests me to do it in the right way.</p>\n"
  },
  {
    "tags": [
      "ai-design",
      "hill-climbing",
      "local-search"
    ],
    "owner": {
      "account_id": 14643287,
      "reputation": 191,
      "user_id": 33756,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2facbb1904643d96f4d54adcdd109b7f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "K.N",
      "link": "https://ai.stackexchange.com/users/33756/k-n"
    },
    "is_answered": false,
    "view_count": 509,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1738177615,
    "creation_date": 1584024725,
    "last_edit_date": 1584027874,
    "question_id": 18594,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/18594/how-can-i-solve-the-zero-subset-sum-problem-with-hill-climbing",
    "title": "How can I solve the zero subset sum problem with hill climbing?",
    "body": "<p>I want to solve the zero subset sum problem with the hill-climbing algorithm, but I am not sure I found a good state space for this.</p>\n\n<p>Here is the problem: consider we have a set of numbers and we want to find a subset of this set such that the sum of the elements in this subset is zero.</p>\n\n<p>My own idea to solve this by hill-climbing is that in the first step, we can choose a random subset of the set (for example, the main set is <span class=\"math-container\">$X= \\{X_1,\\dots,X_n\\}$</span> and we chose <span class=\"math-container\">$X'=\\{X_{i_1},\\dots,X_{i_k}\\}$</span> randomly), then the children of this state can be built by adding an element from <span class=\"math-container\">$X-X'$</span> to <span class=\"math-container\">$X'$</span> or deleting an element from <span class=\"math-container\">$X'$</span> itself. This means that each state has <span class=\"math-container\">$n$</span> children. and the objective function could be the sum of the elements in <span class=\"math-container\">$X'$</span> that we want to minimize.</p>\n\n<p>Is this a good modeling? Are there better modelings or objective functions that can work more intelligently?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "q-learning",
      "dqn",
      "deep-rl",
      "value-functions"
    ],
    "owner": {
      "account_id": 18291634,
      "reputation": 143,
      "user_id": 36072,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-LjkQCpOywdQ/AAAAAAAAAAI/AAAAAAAAAAA/AAKWJJO_yoVSwOoNJEiXwFj_ikNiV1Lm6A/s256-rj/photo.jpg",
      "display_name": "BG10",
      "link": "https://ai.stackexchange.com/users/36072/bg10"
    },
    "is_answered": true,
    "view_count": 6854,
    "answer_count": 3,
    "score": 9,
    "last_activity_date": 1610931946,
    "creation_date": 1587266751,
    "last_edit_date": 1610931935,
    "question_id": 20384,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/20384/what-is-the-target-q-value-in-dqns",
    "title": "What is the target Q-value in DQNs?",
    "body": "<p>I understand that in DQNs, the loss is measured by taking the MSE of outputted Q-values and target Q-values.</p>\n<p>What does the target Q-values represent? And how is it obtained/calculated by the DQN?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "reference-request"
    ],
    "owner": {
      "account_id": 4522928,
      "reputation": 228,
      "user_id": 4211,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/DzZzm.jpg?s=256",
      "display_name": "DSarkar",
      "link": "https://ai.stackexchange.com/users/4211/dsarkar"
    },
    "is_answered": true,
    "view_count": 1105,
    "accepted_answer_id": 20937,
    "answer_count": 3,
    "score": 9,
    "last_activity_date": 1588704979,
    "creation_date": 1588611156,
    "last_edit_date": 1588615179,
    "question_id": 20910,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/20910/what-are-some-resources-with-exercises-related-to-neural-networks",
    "title": "What are some resources with exercises related to neural networks?",
    "body": "<p>I am asking for a book (or any other online resource) where we can solve exercises related to neural networks, similar to the books or online resources dedicated to mathematics where we can solve mathematical exercises.</p>\n"
  },
  {
    "tags": [
      "search",
      "ai-field",
      "depth-first-search"
    ],
    "owner": {
      "account_id": 19280994,
      "reputation": 93,
      "user_id": 40245,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/txGVl.jpg?s=256",
      "display_name": "himari",
      "link": "https://ai.stackexchange.com/users/40245/himari"
    },
    "is_answered": true,
    "view_count": 2255,
    "accepted_answer_id": 22995,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1597274456,
    "creation_date": 1597235985,
    "last_edit_date": 1597240750,
    "question_id": 22993,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/22993/why-is-depth-first-search-an-artificial-intelligence-algorithm",
    "title": "Why is depth-first search an artificial intelligence algorithm?",
    "body": "<p>I'm new to the artificial intelligence field. In our first chapters, there is one topic called &quot;problem-solving by searching&quot;. After searching for it on the internet, I found the <em>depth-first search</em> algorithm. The algorithm is easy to understand, but no one explains why this algorithm is included in the artificial intelligence study.</p>\n<p>Where do we use it? What makes it an artificial intelligence algorithm? Is every search algorithm is an AI algorithm?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "reference-request",
      "autoencoders",
      "transformer",
      "bert"
    ],
    "owner": {
      "account_id": 135130,
      "reputation": 323,
      "user_id": 9220,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6c5c61cd66cede1455405ebc8e89d502?s=256&d=identicon&r=PG",
      "display_name": "HelloGoodbye",
      "link": "https://ai.stackexchange.com/users/9220/hellogoodbye"
    },
    "is_answered": false,
    "view_count": 1029,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1743224595,
    "creation_date": 1600186597,
    "last_edit_date": 1644614993,
    "question_id": 23611,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/23611/are-there-transformer-based-architectures-that-can-produce-fixed-length-vector-e",
    "title": "Are there transformer-based architectures that can produce fixed-length vector encodings given arbitrary-length text documents?",
    "body": "<p><a href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow noreferrer\">BERT</a> encodes a piece of text such that each token (usually words) in the input text map to a vector in the encoding of the text. However, this makes the length of the encoding vary as a function of the input length of the text, which makes it more cumbersome to use as input to downstream neural networks that take only fixed-size inputs.</p>\n<p>Are there any transformer-based neural network architectures that can encode a piece of text into a fixed-size feature vector more suitable for downstream tasks?</p>\n<p><strong>Edit:</strong> To illustrate my question, I’m wondering whether there is some framework that allows the input to be either a sentence, a paragraph, an article, or a book, and produce an output encoding on the same, fixed-sized format for all of them.</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "deep-rl",
      "applications"
    ],
    "owner": {
      "account_id": 18462697,
      "reputation": 103,
      "user_id": 43945,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/64e4a31be0abc9f91938f30cdc6721bb?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Alexandre Krul",
      "link": "https://ai.stackexchange.com/users/43945/alexandre-krul"
    },
    "is_answered": true,
    "view_count": 1209,
    "accepted_answer_id": 26041,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1611903864,
    "creation_date": 1611799911,
    "last_edit_date": 1611833391,
    "question_id": 26033,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/26033/what-are-the-biggest-barriers-to-get-rl-in-production",
    "title": "What are the biggest barriers to get RL in production?",
    "body": "<p>I am studying the state of the art of Reinforcement Learning, and my point is that we see so many applications in the real world using Supervised and Unsupervised learning algorithms in production, but I don't see the same thing with Reinforcement Learning algorithms.</p>\n<p>What are the biggest barriers to get RL in production?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "definitions",
      "markov-decision-process",
      "markov-chain",
      "ergodicity"
    ],
    "owner": {
      "account_id": 9868342,
      "reputation": 225,
      "user_id": 22742,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/dfe48cd43a90785879637b4cd225eef6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "josealeixo.pc",
      "link": "https://ai.stackexchange.com/users/22742/josealeixo-pc"
    },
    "is_answered": true,
    "view_count": 4061,
    "accepted_answer_id": 27252,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1631411293,
    "creation_date": 1617811500,
    "last_edit_date": 1631411293,
    "question_id": 27196,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/27196/what-is-ergodicity-in-a-markov-decision-process-mdp",
    "title": "What is ergodicity in a Markov Decision Process (MDP)?",
    "body": "<p>I have read about the concept of <strong>ergodicity</strong> on the <a href=\"https://arxiv.org/abs/1205.4810\" rel=\"nofollow noreferrer\">safe RL paper by Moldovan (section 3.2)</a> and the <a href=\"http://incompleteideas.net/book/the-book.html\" rel=\"nofollow noreferrer\">RL book by Sutton (chapter 10.3, 2nd paragraph)</a>.</p>\n<p>The first one says that &quot;a belief over MDPs is ergodic if and only if any state is reachable from any other state via some policy or, equivalently, if and only if&quot;:</p>\n<p><span class=\"math-container\">$$\\forall s, s', \\exists \\pi_r \\text{ such that } E_\\beta E_{s, \\pi_r}^P [B_{s'}] = 1$$</span></p>\n<p>where:</p>\n<ul>\n<li><span class=\"math-container\">$B_{s'}$</span> is an indicator random variable of the event that the system reaches state <span class=\"math-container\">$s'$</span> at least once, i.e., <span class=\"math-container\">$B_{s'} = 1 \\{ \\exists t &lt; \\infty \\text{ such that } s_t = s'\\}$</span></li>\n<li><span class=\"math-container\">$E_\\beta E_{s, \\pi_r}^P[B_{s'}]$</span> is the expected value for <span class=\"math-container\">$B_{s'}$</span>, under the belief over the MDP dynamics <span class=\"math-container\">$\\beta$</span>, policy <span class=\"math-container\">$\\pi$</span> and transition measure <span class=\"math-container\">$P$</span>.</li>\n</ul>\n<p>The second one says &quot;<span class=\"math-container\">$\\mu_\\pi$</span> is the steady-state distribution, which is assumed to exist for any <span class=\"math-container\">$\\pi$</span> and to be independent of <span class=\"math-container\">$s_0$</span>. This assumption about the MDP is known as ergodicity.&quot;. They define <span class=\"math-container\">$\\mu_\\pi$</span> as:</p>\n<p><span class=\"math-container\">$$\\mu_\\pi(s) \\doteq \\lim_{t \\to \\infty} \\Pr\\{s_t=s \\vert a_{0:t-1} \\sim \\pi\\}$$</span></p>\n<ul>\n<li>i.e., there is a chance of landing on state <span class=\"math-container\">$s$</span> by executing actions according to policy <span class=\"math-container\">$\\pi$</span>.</li>\n</ul>\n<p>I noticed that the first definition requires that <strong>at least one</strong> policy should exist for each <span class=\"math-container\">$(s, s')$</span> pair for the MDP to be ergodic The second definition, however, requires that <strong>all policies</strong> eventually visit all the states in an MDP, which seems to be a more strict definition.</p>\n<p>Then, I came accross the ergodicity definition for Markov chains:</p>\n<blockquote>\n<p>A state <span class=\"math-container\">$i$</span> is said to be ergodic if it is aperiodic and positive recurrent. In other words, a state <span class=\"math-container\">$i$</span> is ergodic if it is recurrent, has a period of <span class=\"math-container\">$1$</span>, and has finite mean recurrence time. If all states in an irreducible Markov chain are ergodic, then the chain is said to be ergodic.</p>\n</blockquote>\n<p>This leads me to believe that the second definition (the stricter one) is the most appropriate one, considering the ergodicity definition in an MDP derives from the definition in a Markov chain. As an MDP is basically a Markov chain with choice (actions), ergodicity should mean that independently of the action taken, all states are visited, i.e., <strong>all policies</strong> ensure ergodicity.</p>\n<p>Am I correct in assuming these are different definitions? Can both still be called &quot;ergodicity&quot;? If not, which one is the most correct?</p>\n"
  },
  {
    "tags": [
      "computer-vision",
      "classification",
      "papers",
      "transformer",
      "vision-transformer"
    ],
    "owner": {
      "account_id": 18387461,
      "reputation": 2812,
      "user_id": 38846,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Hlbol.jpg?s=256",
      "display_name": "spiridon_the_sun_rotator",
      "link": "https://ai.stackexchange.com/users/38846/spiridon-the-sun-rotator"
    },
    "is_answered": true,
    "view_count": 7075,
    "accepted_answer_id": 34781,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1668735086,
    "creation_date": 1624090949,
    "last_edit_date": 1638284848,
    "question_id": 28326,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/28326/why-class-embedding-token-is-added-to-the-visual-transformer",
    "title": "Why class embedding token is added to the Visual Transformer?",
    "body": "<p>In the <a href=\"https://arxiv.org/abs/2010.11929\" rel=\"noreferrer\">famous work on the Visual Transformers</a>, the image is split into patches of a certain size (say 16x16), and these patches are treated as tokens in the NLP tasks. In order to perform classification, a <strong>CLS</strong> token is added at the beginning of the resulting sequence:\n<span class=\"math-container\">$$\n[\\textbf{x}_{class}, \\textbf{x}_{p}^{1}, \\ldots, \\textbf{x}_{p}^{N}]\n,$$</span>\nwhere <span class=\"math-container\">$ \\textbf{x}_{p}^{i}$</span> are image patches. There multiple layers in the architecture and the state of the <strong>CLS</strong> token on the output layer is used for classification.</p>\n<p>I think this architectural solution is done in the spirit of NLP problems (BERT in particular). However, for me, it would be more natural not to create a special token, but perform <em>1d Global Pooling</em> in the end, and attach an <code>nn.Linear(embedding_dim, num_classes)</code> as more conventional CV approach.</p>\n<p>Why it is not done in this way? Or is there some intuition or evidence that this would perform worse than the approach used in the paper?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "terminology"
    ],
    "owner": {
      "account_id": 3197090,
      "reputation": 4082,
      "user_id": 18758,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/339397e23b4e4cc78aa36e2b126252c7?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "hanugm",
      "link": "https://ai.stackexchange.com/users/18758/hanugm"
    },
    "is_answered": true,
    "view_count": 18013,
    "accepted_answer_id": 28424,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1633693555,
    "creation_date": 1624697685,
    "last_edit_date": 1633680293,
    "question_id": 28410,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/28410/which-tasks-are-called-as-downstream-tasks",
    "title": "Which tasks are called as downstream tasks?",
    "body": "<p>The following paragraph is from page no 331 of the textbook <a href=\"https://cseweb.ucsd.edu/%7Ennakashole/teaching/eisenstein-nov18.pdf#page=350\" rel=\"noreferrer\">Natural Language Processing</a> by <em>Jacob Eisenstein</em>. It mentions about certain type of tasks called as downstream tasks. But, it provide no further examples or details regarding these tasks.</p>\n<blockquote>\n<p>Learning algorithms like perceptron and conditional random fields\noften perform better with discrete feature vectors. A simple way to\nobtain discrete representations from distributional statistics is by\nclustering, so that words in the same cluster have similar\ndistributional statistics. This can help in <strong>downstream tasks</strong>, by\nsharing features between all words in the same cluster. However, there\nis an obvious tradeoff: if the number of clusters is too small, the\nwords in each cluster will not have much in common; if the number of\nclusters is too large, then the learner will not see enough examples\nfrom each cluster to generalize.</p>\n</blockquote>\n<p>Which tasks in artificial intelligence or NLP are called as downstream tasks?</p>\n"
  },
  {
    "tags": [
      "comparison",
      "books",
      "norvig-russell"
    ],
    "owner": {
      "account_id": 17972508,
      "reputation": 237,
      "user_id": 48908,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/666c27316aa2450cb62437d5a974a38c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Emad",
      "link": "https://ai.stackexchange.com/users/48908/emad"
    },
    "is_answered": true,
    "view_count": 7206,
    "accepted_answer_id": 36011,
    "answer_count": 3,
    "score": 9,
    "last_activity_date": 1726105170,
    "creation_date": 1629902542,
    "last_edit_date": 1630322349,
    "question_id": 30367,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/30367/what-is-the-difference-between-the-us-and-global-edition-of-the-aima-book-by-rus",
    "title": "What is the difference between the US and global edition of the AIMA book by Russell and Norvig?",
    "body": "<p>The book <em>Artificial Intelligence: A Modern Approach</em> by Russell and Norvig has two editions: global and the US. It looks like these two are generally the same, but have some differences in the order of the chapters and in the context, is this correct?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "recurrent-neural-networks",
      "prediction",
      "sequence-modeling"
    ],
    "owner": {
      "account_id": 13763598,
      "reputation": 193,
      "user_id": 35881,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-cG4GHrAkZq8/AAAAAAAAAAI/AAAAAAAAAVM/9zlZsWSn7eg/s256-rj/photo.jpg",
      "display_name": "ZenBerry",
      "link": "https://ai.stackexchange.com/users/35881/zenberry"
    },
    "is_answered": true,
    "view_count": 18962,
    "accepted_answer_id": 31921,
    "answer_count": 4,
    "score": 9,
    "last_activity_date": 1728333281,
    "creation_date": 1633345591,
    "last_edit_date": 1633437124,
    "question_id": 31919,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/31919/how-can-i-predict-the-next-number-in-a-non-obvious-sequence",
    "title": "How can I predict the next number in a non-obvious sequence?",
    "body": "<p>I've got an array of integers ranging from -3 to +3.</p>\n<p>Example: [1, 3, -2, 0, 0, 1]</p>\n<p>The array has no obvious pattern since it represents bipolar disorder mood swings.</p>\n<p>What is the most suitable approach to predict the next number in the series? The length of the array is about 700 entries.</p>\n<p>From where can I start the investigation? (provided that I've got some experience in Python and <a href=\"https://en.wikipedia.org/wiki/Node.js\" rel=\"noreferrer\">Node.js</a>, but only a hello-worldish acquaintance with <a href=\"https://en.wikipedia.org/wiki/TensorFlow\" rel=\"noreferrer\">TensorFlow</a>). Which training model might be suitable in this case? How can I chunk the data set properly?</p>\n"
  },
  {
    "tags": [
      "neural-networks"
    ],
    "owner": {
      "account_id": 4843035,
      "reputation": 303,
      "user_id": 53210,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/9f29db4c776249768e7b50bf6da3568c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "huang",
      "link": "https://ai.stackexchange.com/users/53210/huang"
    },
    "is_answered": true,
    "view_count": 4266,
    "answer_count": 6,
    "score": 9,
    "last_activity_date": 1662156977,
    "creation_date": 1662065838,
    "question_id": 36953,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/36953/if-an-event-has-a-statistical-probability-of-only-50-is-it-possible-to-use-a-n",
    "title": "If an event has a statistical probability of only 50%, is it possible to use a neural network to predict it with more than 50% accuracy?",
    "body": "<p>For example using a neural network to predict a coin toss. Can a trained neural network to predict it with more than 50% accuracy?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "transformer",
      "attention"
    ],
    "owner": {
      "account_id": 24856351,
      "reputation": 539,
      "user_id": 73537,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/5129309183794125/picture?type=large",
      "display_name": "Chinmay",
      "link": "https://ai.stackexchange.com/users/73537/chinmay"
    },
    "is_answered": true,
    "view_count": 2961,
    "accepted_answer_id": 41442,
    "answer_count": 2,
    "score": 9,
    "last_activity_date": 1691398179,
    "creation_date": 1690157337,
    "question_id": 41438,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/41438/can-someone-help-me-understand-the-intuition-behind-the-query-key-and-value-mat",
    "title": "Can someone help me understand the intuition behind the query, key and value matrices in the transformer architecture?",
    "body": "<p>I have been working mechanically with transformers, hoping that with time clarity about what the query, key, and value matrices represent will develop; but I am still lost. Would greatly benefit from a simplified explanation.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "deep-learning",
      "activation-functions",
      "tanh"
    ],
    "owner": {
      "account_id": 28982270,
      "reputation": 125,
      "user_id": 82505,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/wuNgC.png?s=256",
      "display_name": "vxnuaj",
      "link": "https://ai.stackexchange.com/users/82505/vxnuaj"
    },
    "is_answered": true,
    "view_count": 2860,
    "accepted_answer_id": 45688,
    "answer_count": 1,
    "score": 9,
    "last_activity_date": 1723651468,
    "creation_date": 1715555351,
    "last_edit_date": 1723651468,
    "question_id": 45683,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/45683/when-to-use-tanh",
    "title": "When to use Tanh?",
    "body": "<p>When and why would you not use Tanh?</p>\n<p>I just replaced ReLU with Tanh and my model trains about 2x faster, reaching 90% acc within 500 steps.</p>\n<p>While using ReLU it reached 90% acc in &gt;1000 training steps.</p>\n<p>I believe the reason it trained faster was due to a steeper gradient (correct me if I'm wrong).</p>\n<p>This led me to wonder, why would we not use Tanh over ReLU for most contexts?\nIs it the computational complexity? Or something else?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "image-recognition"
    ],
    "owner": {
      "account_id": 90733,
      "reputation": 464,
      "user_id": 38,
      "user_type": "registered",
      "accept_rate": 60,
      "profile_image": "https://www.gravatar.com/avatar/81060fe344980bc9a3273a48b8f3e31c?s=256&d=identicon&r=PG",
      "display_name": "SF.",
      "link": "https://ai.stackexchange.com/users/38/sf"
    },
    "is_answered": true,
    "view_count": 191,
    "accepted_answer_id": 163,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1523500242,
    "creation_date": 1470153139,
    "last_edit_date": 1523500242,
    "question_id": 13,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/13/can-a-single-neural-network-handle-recognizing-two-types-of-objects-or-should-i",
    "title": "Can a single neural network handle recognizing two types of objects, or should it be split into two smaller networks?",
    "body": "<p>In particular, an embedded computer (with limited resources) analyzes live video stream from a traffic camera, trying to pick good frames that contain license plate numbers of passing cars. Once a plate is located, the frame is handed over to an OCR library to extract the registration and use it further.</p>\n\n<p>In my country two types of license plates are in common use - rectangular (the typical) and square - actually, somewhat rectangular but \"higher than wider\", with the registration split over two rows.</p>\n\n<p>(there are some more types, but let us disregard them; they are a small percent and usually belong to vehicles that lie outside our interest.)</p>\n\n<p>Due to the limited resources and need for rapid, real-time processing, the maximum size of the network (number of cells and connections) the system can handle is fixed.</p>\n\n<p>Would it be better to split this into two smaller networks, each recognizing one type of registration plates, or will the larger single network handle the two types better?</p>\n"
  },
  {
    "tags": [
      "models",
      "problem-solving"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 9875,
    "accepted_answer_id": 72,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1598184332,
    "creation_date": 1470155511,
    "last_edit_date": 1539859509,
    "question_id": 67,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/67/what-are-the-real-world-uses-for-sat-solvers",
    "title": "What are the real world uses for SAT solvers?",
    "body": "<p>Why somebody would use SAT solvers (<a href=\"https://en.wikipedia.org/wiki/Boolean_satisfiability_problem\" rel=\"nofollow\">Boolean satisfiability problem</a>) to solve their real world problems?</p>\n\n<p>Are there any examples of the real uses of this model?</p>\n"
  },
  {
    "tags": [
      "applications",
      "statistical-ai"
    ],
    "owner": {
      "account_id": 40019,
      "reputation": 2533,
      "user_id": 55,
      "user_type": "registered",
      "accept_rate": 43,
      "profile_image": "https://i.sstatic.net/FkIiK.png?s=256",
      "display_name": "WilliamKF",
      "link": "https://ai.stackexchange.com/users/55/williamkf"
    },
    "is_answered": true,
    "view_count": 2805,
    "accepted_answer_id": 87,
    "answer_count": 4,
    "score": 8,
    "last_activity_date": 1638779394,
    "creation_date": 1470156580,
    "last_edit_date": 1638779394,
    "question_id": 81,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/81/what-are-some-examples-of-statistical-ai-applications",
    "title": "What are some examples of Statistical AI applications?",
    "body": "<p>I believe that statistical AI uses inductive thought processes. For example, deducing a trend from a pattern, after training.</p>\n<p>What are some examples of successfully applied Statistical AI to real-world problems?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "superintelligence"
    ],
    "owner": {
      "account_id": 6241661,
      "reputation": 1521,
      "user_id": 29,
      "user_type": "registered",
      "accept_rate": 78,
      "profile_image": "https://i.sstatic.net/JfeF9.png?s=256",
      "display_name": "wythagoras",
      "link": "https://ai.stackexchange.com/users/29/wythagoras"
    },
    "is_answered": true,
    "view_count": 312,
    "accepted_answer_id": 144,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1561497412,
    "creation_date": 1470170279,
    "last_edit_date": 1561497412,
    "question_id": 140,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/140/does-this-argument-refuting-the-existence-of-superintelligence-work",
    "title": "Does this argument refuting the existence of superintelligence work?",
    "body": "<p>A superintelligence is a machine that can surpass all intellectual activities by any human, and such a machine is often portrayed in science fiction as a machine that brings mankind to an end.</p>\n\n<p>Any machine is executed using an algorithm. By the Church-Turing thesis, any algorithm that can be executed by a modern computer can be executed by a Turing Machine. However, a human can easily simulate a Turing Machine. Doesn't this mean that a machine can't surpass all intellectual activities, since we can also execute the algorithm?</p>\n\n<p>This argument is most likely flawed, since my intuition tells me that superintelligence is possible. However, it is not clear to me where the flaw is. Note that this is my own argument. </p>\n"
  },
  {
    "tags": [
      "algorithm",
      "natural-language-processing",
      "pattern-recognition"
    ],
    "owner": {
      "account_id": 404018,
      "reputation": 1363,
      "user_id": 46,
      "user_type": "registered",
      "accept_rate": 29,
      "profile_image": "https://i.sstatic.net/uheW1.png?s=256",
      "display_name": "dynrepsys",
      "link": "https://ai.stackexchange.com/users/46/dynrepsys"
    },
    "is_answered": true,
    "view_count": 206,
    "accepted_answer_id": 267,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1539002670,
    "creation_date": 1470177530,
    "last_edit_date": 1539002670,
    "question_id": 157,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/157/what-artificial-intelligence-strategies-are-useful-for-summarization",
    "title": "What artificial intelligence strategies are useful for summarization?",
    "body": "<p>If I have a paragraph I want to summarize, for example:</p>\n\n<blockquote>\n  <p>Ponzo and Fila went to the mall during the day. They walked for a long while, stopping at shops. They went to many shops. At first, they didn't buy anything. After going to a number of shops, they eventually bought a shirt, and a pair of pants.</p>\n</blockquote>\n\n<p>Better summarized as:</p>\n\n<blockquote>\n  <p>They shopped at the mall today and bought some clothes.</p>\n</blockquote>\n\n<p>What is the best AI strategy to automate this process, if there is one? If there isn't, is it because it would be dependent on first having an external information resource that would inform any algorithm? Or is it because the problem is inherently contextual?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "reference-request",
      "applications",
      "prediction",
      "drug-design"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 1072,
    "accepted_answer_id": 1412,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1618872321,
    "creation_date": 1470430070,
    "last_edit_date": 1610395906,
    "question_id": 1390,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1390/is-there-any-research-on-the-application-of-ai-for-drug-design",
    "title": "Is there any research on the application of AI for drug design?",
    "body": "<p>Is there any research on the application of AI for drug design?</p>\n<p>For example, you could train a deep learning model about current compounds, substances, structures, and their products and chemical reactions from the existing <a href=\"https://opendata.stackexchange.com/q/3553/3082\">dataset</a> (basically what produces what). Then you give the task to find how to create a gold or silver from the group of available substances. Then the algorithm will find the chemical reactions (successfully predicting a new one that wasn't in the dataset) and gives the results. Maybe the gold is not a good example, but the practical scenario would be the creation/design of drugs that are cheaper to create by using much simpler processes or synthesizing some substances for the first time for drug industries.</p>\n<p>Was there any successful research attempting to achieve that using deep learning algorithms?</p>\n"
  },
  {
    "tags": [
      "terminology",
      "robotics",
      "robots",
      "comparison"
    ],
    "owner": {
      "account_id": 8761221,
      "reputation": 1082,
      "user_id": 72,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f22ad5db7375b6fb8b2193999c5297c8?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Vishnu JK",
      "link": "https://ai.stackexchange.com/users/72/vishnu-jk"
    },
    "is_answered": true,
    "view_count": 12820,
    "protected_date": 1569933027,
    "accepted_answer_id": 1465,
    "answer_count": 6,
    "score": 8,
    "last_activity_date": 1569933022,
    "creation_date": 1470678820,
    "last_edit_date": 1569933022,
    "question_id": 1462,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1462/what-is-the-difference-between-artificial-intelligence-and-robots",
    "title": "What is the difference between artificial intelligence and robots?",
    "body": "<p>What is the difference between artificial intelligence and robots?</p>\n"
  },
  {
    "tags": [
      "neural-networks"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 8857,
    "accepted_answer_id": 1514,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1557762292,
    "creation_date": 1470791167,
    "question_id": 1508,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/1508/which-neural-network-has-capabilities-of-sorting-input",
    "title": "Which neural network has capabilities of sorting input?",
    "body": "<p>I believe normally you can use <a href=\"https://en.wikipedia.org/wiki/Genetic_programming\" rel=\"noreferrer\">genetic programming</a> for sorting, however I'd like to check whether it's possible using ANN.</p>\n\n<p>Given the unsorted text data from input, which neural network is suitable for doing sorting tasks?</p>\n"
  },
  {
    "tags": [
      "agi",
      "human-inspired",
      "artificial-curiosity"
    ],
    "owner": {
      "account_id": 974718,
      "reputation": 1106,
      "user_id": 39,
      "user_type": "registered",
      "accept_rate": 38,
      "profile_image": "https://i.sstatic.net/F8mcb.jpg?s=256",
      "display_name": "Eka",
      "link": "https://ai.stackexchange.com/users/39/eka"
    },
    "is_answered": true,
    "view_count": 318,
    "answer_count": 5,
    "score": 8,
    "last_activity_date": 1611624781,
    "creation_date": 1470924046,
    "last_edit_date": 1611624781,
    "question_id": 1544,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1544/could-curiosity-improve-artificial-intelligence",
    "title": "Could curiosity improve artificial intelligence?",
    "body": "<p>While thinking about AI, this question came into my mind. Could curiosity help in developing a true AI? According to this <a href=\"http://psychologia.co/creativity-test/\" rel=\"nofollow noreferrer\">website</a> (for testing creativity):</p>\n<blockquote>\n<p>Curiosity in this context refers to persistent desire to learn and discover new things and ideas. A curious person</p>\n<ul>\n<li>always looks for new and original ways of thinking,</li>\n<li>likes to learn,</li>\n<li>searches for alternative solutions even when traditional solutions are present and available,</li>\n<li>enjoys reading books and watching documentaries,</li>\n<li>wants to know how things work inside out.</li>\n</ul>\n</blockquote>\n<p>Let's take <a href=\"https://www.clarifai.com/demo\" rel=\"nofollow noreferrer\">Clarifai</a>, an image/video classification startup, which can classify images and video with the best accuracy (according to them). If I understand correctly, they trained their deep learning system using millions of images with supervised learning. In the same algorithm, what would happen if we somehow added a &quot;curiosity factor&quot; when the AI has difficulty in classifying an image or its objects? It would ask a human for help, just like a curious child.</p>\n<p>Curiosity makes a human being learn new things and also helps to generate new original ideas. Could the addition of curiosity change Clarifai into a true AI?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "algorithm-request"
    ],
    "owner": {
      "account_id": 8423357,
      "reputation": 1173,
      "user_id": 1581,
      "user_type": "registered",
      "accept_rate": 40,
      "profile_image": "https://i.sstatic.net/jqtE1.jpg?s=256",
      "display_name": "quintumnia",
      "link": "https://ai.stackexchange.com/users/1581/quintumnia"
    },
    "is_answered": true,
    "view_count": 2055,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1640297584,
    "creation_date": 1471809782,
    "last_edit_date": 1640297584,
    "question_id": 1705,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1705/selecting-the-right-technique-to-predict-disease-from-symptoms",
    "title": "Selecting the right technique to predict disease from symptoms",
    "body": "<p>I'm trying to come up with the right algorithm for a system in which the user enters a few symptoms and the system has to predict or determine the likelihood that a few selected symptoms are associated with those existing in the system. Then, after associating them, the result or output should be a specific disease for the symptoms.</p>\n<p>The system is comprised of a series of diseases with each assigned to specific symptoms, which also exist in the system.</p>\n<p>Let's assume that the user entered the following input:</p>\n<pre><code>A, B, C, and D\n</code></pre>\n<p>The first thing the system should do is check and associate each symptom (in this case represented by alphabetical letters) individually against a data table of symptoms that already exist. And in cases where the input doesn't exist, the system should report or send feedback about it.</p>\n<p>And also, let's say that <code>A</code> and <code>B</code> were in the data table, so we are 100% sure that they're valid or exist and the system is able to give out the disease based on the input. Then let's say that the input now is <code>C</code> and <code>D</code>, where <code>C</code> doesn't exist in the data table, but there is a possibility that <code>D</code> exists.</p>\n<p>We don't give <code>D</code> a score of 100%, but maybe something lower (let's say 90%). Then <code>C</code> just doesn't exist at all in the data table. So, <code>C</code> gets a score of 0%.</p>\n<p>Therefore, the system should have some kind of association and prediction techniques or rules to output the result by judging the user's input.</p>\n<p>Summary of generating the output:</p>\n<pre><code>If A and B were entered and exist, then output = 100%\nIf D was entered and existed but C was not, then output = 90%\nIf all entered don't exist, then output = 0%\n</code></pre>\n<p>What techniques would be used to produce this system?</p>\n"
  },
  {
    "tags": [
      "neural-networks"
    ],
    "owner": {
      "account_id": 18562,
      "reputation": 513,
      "user_id": 1670,
      "user_type": "registered",
      "accept_rate": 20,
      "profile_image": "https://www.gravatar.com/avatar/5d85921e112af0e998368a159ae8a112?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "liori",
      "link": "https://ai.stackexchange.com/users/1670/liori"
    },
    "is_answered": true,
    "view_count": 949,
    "accepted_answer_id": 1837,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1640296580,
    "creation_date": 1472845062,
    "last_edit_date": 1640296580,
    "question_id": 1834,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1834/how-big-artificial-neural-networks-can-we-run-now-if-our-total-energy-budget-for",
    "title": "How big artificial neural networks can we run now if our total energy budget for computation is equivalent to the human brain energy budget?",
    "body": "<p>How big artificial neural networks can we run now (either with full train-backprop cycle or just evaluating network outputs) if our total energy budget for computation is equivalent to the human brain energy budget (<a href=\"http://www.scientificamerican.com/article/thinking-hard-calories/\" rel=\"nofollow noreferrer\">12.6 watts</a>)?</p>\n<p>Let assume one cycle per second, which seems to roughly match the <a href=\"http://www.jneurosci.org/content/31/45/16217.full\" rel=\"nofollow noreferrer\">firing rate of biological neurons</a>.</p>\n"
  },
  {
    "tags": [
      "search",
      "algorithm-request",
      "hill-climbing",
      "local-search",
      "simulated-annealing"
    ],
    "owner": {
      "account_id": 5504846,
      "reputation": 81,
      "user_id": 2244,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/978decf326cce6cef5d12d96a710ec0b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "quantumcoder",
      "link": "https://ai.stackexchange.com/users/2244/quantumcoder"
    },
    "is_answered": true,
    "view_count": 442,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1638867775,
    "creation_date": 1473203588,
    "last_edit_date": 1638867775,
    "question_id": 1870,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1870/are-there-local-search-algorithms-that-make-use-of-memory-to-give-better-solutio",
    "title": "Are there local search algorithms that make use of memory to give better solutions?",
    "body": "<p>I have been studying local search algorithms such as greedy hill-climbing, stochastic hill-climbing, simulated annealing, etc. I have noticed that most of these methods take up very little memory as compared to systematic search techniques.</p>\n<p>Are there local search algorithms that make use of memory to give significantly better answers than those algorithms that use little memory (such as crossing local maxima)?</p>\n<p>Also, is there a way to combine local search and systematic search algorithms to get the best of both worlds?</p>\n"
  },
  {
    "tags": [
      "research",
      "symbolic-ai",
      "expert-systems",
      "symbolic-computing"
    ],
    "owner": {
      "account_id": 4926742,
      "reputation": 251,
      "user_id": 2306,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4480675ad0ede0043e7caba04750d974?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Alex S King",
      "link": "https://ai.stackexchange.com/users/2306/alex-s-king"
    },
    "is_answered": true,
    "view_count": 1194,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1611526870,
    "creation_date": 1473376925,
    "last_edit_date": 1611526870,
    "question_id": 1906,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1906/is-anybody-still-researching-gofai",
    "title": "Is anybody still researching GOFAI?",
    "body": "<p>A lot of textbooks and introductory lectures typically split AI into connectionism and GOFAI (Good Old Fashioned AI). From a purely technical perspective, it seems that <a href=\"https://plato.stanford.edu/entries/connectionism/\" rel=\"nofollow noreferrer\">connectionism</a> has grown into machine learning and data science, while nobody talks about GOFAI, Symbolic AI, or Expert Systems at all.</p>\n<p>Is anyone of note still working on GOFAI?</p>\n"
  },
  {
    "tags": [
      "intelligence-testing"
    ],
    "owner": {
      "account_id": 6108253,
      "reputation": 1670,
      "user_id": 181,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://lh3.googleusercontent.com/-YYgQ6EQmsiQ/AAAAAAAAAAI/AAAAAAAAABU/QDDm2dKMp3c/s256-rj/photo.jpg",
      "display_name": "Left SE On 10_6_19",
      "link": "https://ai.stackexchange.com/users/181/left-se-on-10-6-19"
    },
    "is_answered": true,
    "view_count": 281,
    "accepted_answer_id": 2068,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1637248780,
    "creation_date": 1475438567,
    "last_edit_date": 1637248780,
    "question_id": 2066,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2066/is-the-ai-effect-caused-by-bad-tests-of-intelligence",
    "title": "Is the AI Effect caused by bad tests of intelligence?",
    "body": "<p><a href=\"https://en.wikipedia.org/wiki/AI_effect\" rel=\"nofollow noreferrer\">According to Wikipedia</a>...</p>\n<blockquote>\n<p>The AI effect occurs when onlookers discount the behavior of an artificial intelligence program by arguing that it is not real intelligence.</p>\n<p>Pamela McCorduck writes: &quot;It's part of the history of the field of artificial intelligence that every time somebody figured out how to make a computer do something—play good checkers, solve simple but relatively informal problems—there was chorus of critics to say, 'that's not thinking'.&quot;[1] AI researcher Rodney Brooks complains &quot;Every time we figure out a piece of it, it stops being magical; we say, 'Oh, that's just a computation.'&quot;[2]</p>\n</blockquote>\n<p>The Wikipedia page then proposes several different reasons that could explain why onlookers might &quot;discount&quot; AI programs. However, those reasons seem to imply that the humans are making a mistake in &quot;discounting&quot; the behavior of AI programs...and that these AI programs might actually be  intelligent. I want to make an alternate argument, where the humans are making a mistake, but not in &quot;discounting&quot; the behavior of AI programs.</p>\n<p>Consider the following situation. I want to build a machine that can do X (where X is some trait, like intelligence). I am able to evaluate intuitively whether a machine has that X criteria. But I don't have a good definition of what X actually <em>is</em>. All I can do is identify whether something has X or not.</p>\n<p>However, I think that people who has X can do Y. So if I build a machine that can do Y, then surely, I built a machine that has X.</p>\n<p>After building the machine that can do Y, I examine it to see if my machine has X. And it does not. So my machine lacks X. And while a machine that can do Y is cool, what I really want is a machine that has X. I go back to the drawing board and think of a new idea to reach X.</p>\n<p>After writing on the whiteboard for a couple of hours, I realize that people who has X can do Z. Of course! I try to build a new machine that can do Z, yes, if it can do Z, then it must have X.</p>\n<p>After building the machine that can do Z, I check to see if it has X. It does not. And so I return back to the drawing board, and the cycle repeats and repeats...</p>\n<p>Essentially, humans are attempting to determine whether an entity has intelligence via proxy measurements, but those proxy measurements are potentially faulty (as it is possible to meet those proxy measurements without ever actually having intelligence). Until we know how to define intelligence and design a test that can accurately measure it, it is very unlikely for us to build a machine that has intelligence. So the AI Effect occurs because humans don't know how to define &quot;intelligence&quot;, not due to people dismissing programs as not being &quot;intelligent&quot;.</p>\n<p>Is this argument valid or correct? And if not, why not?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "agi",
      "autonomous-vehicles",
      "narrow-ai"
    ],
    "owner": {
      "account_id": 2762007,
      "reputation": 309,
      "user_id": 2963,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2faf91f2efa646f23b07220deb997687?s=256&d=identicon&r=PG",
      "display_name": "Jamgreen",
      "link": "https://ai.stackexchange.com/users/2963/jamgreen"
    },
    "is_answered": true,
    "view_count": 2117,
    "answer_count": 5,
    "score": 8,
    "last_activity_date": 1561044009,
    "creation_date": 1476255407,
    "last_edit_date": 1561044009,
    "question_id": 2126,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2126/why-are-autonomous-cars-categorized-as-ai",
    "title": "Why are autonomous cars categorized as AI?",
    "body": "<p>How are autonomous cars related to artificial intelligence? I would presume that artificial intelligence is when we are able to copy the human state of mind and perform tasks in the same way. But isn't an autonomous car just rule-based machines that operates due to its environment? They are not self-aware, and they cannot choose a good way to act in a never before experienced situation.</p>\n\n<p>I know that many people often mention autonomous cars when speaking about AI, but I am not really convinced that these are related. Either I have a too strict understanding of what AI is or </p>\n"
  },
  {
    "tags": [
      "history",
      "comparison",
      "biology"
    ],
    "owner": {
      "account_id": 8281200,
      "reputation": 181,
      "user_id": 3128,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ea065c8471bc4b60178524a638825f2f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "CPHPython",
      "link": "https://ai.stackexchange.com/users/3128/cphpython"
    },
    "is_answered": true,
    "view_count": 376,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1525417987,
    "creation_date": 1476883725,
    "last_edit_date": 1476889098,
    "question_id": 2185,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/2185/is-artificial-intelligence-restricted-to-electrical-based-technology",
    "title": "Is Artificial Intelligence restricted to electrical based technology?",
    "body": "<p>According to <a href=\"https://en.wikipedia.org/wiki/Artificial_intelligence\" rel=\"noreferrer\">Wikipedia</a>:</p>\n\n<blockquote>\n  <p>AI is intelligence exhibited by machines.</p>\n</blockquote>\n\n<p>I have been wondering if with the recent biological advancements, is there already a non-electrical-based \"machine\" that is programmed by humans in order to be able to behave like a:</p>\n\n<blockquote>\n  <p><strong>flexible rational agent</strong> that perceives its environment and takes actions that maximize its chance of success at some goal</p>\n</blockquote>\n\n<p>I was specifically thinking of viruses and bacteria. Have these been programmed by humans in order to behave as a flexible rational agent (i.e. an AI entity)?</p>\n\n<p>Are there are other organisms that have already been used for this purpose?</p>\n"
  },
  {
    "tags": [
      "human-like",
      "applications",
      "cyberterrorism"
    ],
    "owner": {
      "account_id": 8423357,
      "reputation": 1173,
      "user_id": 1581,
      "user_type": "registered",
      "accept_rate": 40,
      "profile_image": "https://i.sstatic.net/jqtE1.jpg?s=256",
      "display_name": "quintumnia",
      "link": "https://ai.stackexchange.com/users/1581/quintumnia"
    },
    "is_answered": true,
    "view_count": 763,
    "protected_date": 1483298094,
    "accepted_answer_id": 2478,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1549920205,
    "creation_date": 1481785216,
    "last_edit_date": 1549920205,
    "question_id": 2477,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2477/could-there-be-existential-threats-to-humanity-due-to-ai",
    "title": "Could there be existential threats to humanity due to AI?",
    "body": "<p>We are doing research, spending hours figuring out how we can make real AI software (intelligent agents) to work better. We are also trying to implement some applications e.g. in business, health and education, using the AI technology.</p>\n\n<p>Nonetheless, so far, most of us have ignored the \"dark\" side of artificial intelligence. For instance, an \"unethical\" person could buy thousands of cheap drones, arm them with guns, and send them out firing on the public. This would be an \"unethical\" application of AI.</p>\n\n<p>Could there be (in the future) existential threats to humanity due to AI?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "ethics",
      "artificial-consciousness",
      "sentience"
    ],
    "owner": {
      "account_id": 8020709,
      "reputation": 189,
      "user_id": 4809,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/HJ7fp.png?s=256",
      "display_name": "Destructible Lemon",
      "link": "https://ai.stackexchange.com/users/4809/destructible-lemon"
    },
    "is_answered": true,
    "view_count": 1464,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1640296188,
    "creation_date": 1484294813,
    "last_edit_date": 1640296188,
    "question_id": 2646,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2646/could-humans-hurt-a-conscious-or-sentient-ai",
    "title": "Could humans hurt a conscious or sentient AI?",
    "body": "<p>If a conscious AI is possible, would it also be possible for someone who knows what they are doing to torture (or hurt) the AI? Could this be avoided? How?</p>\n<p>This question deals with computer-based AI, not robots, which are as conscious as people (this is an assumption of the question). The question wonders how a crime as hard to trace as illegal downloads, but far worse ethically, could be prevented. Note that despite most people being nice and empathising with the robots, there are always the bad people, and so relying on general conscience will not work.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "reinforcement-learning",
      "genetic-algorithms",
      "algorithm-request",
      "model-request"
    ],
    "owner": {
      "account_id": 7337127,
      "reputation": 83,
      "user_id": 2887,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/uUtzx.gif?s=256",
      "display_name": "U754V",
      "link": "https://ai.stackexchange.com/users/2887/u754v"
    },
    "is_answered": true,
    "view_count": 387,
    "accepted_answer_id": 2724,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1636491178,
    "creation_date": 1485367963,
    "last_edit_date": 1636491178,
    "question_id": 2723,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2723/apart-from-reinforcement-learning-are-there-any-other-machine-learning-approach",
    "title": "Apart from Reinforcement Learning, are there any other machine learning approaches to play video games?",
    "body": "<p>OpenAI's Universe utilizes RL algorithms. I also know that Q-learning has been used to solve some games.</p>\n<p>Are there any other ML approaches to solve games? For example, could we use genetic algorithms to develop agents that solve games?</p>\n"
  },
  {
    "tags": [
      "linear-regression"
    ],
    "owner": {
      "account_id": 49538,
      "reputation": 458,
      "user_id": 190,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/95c6daeb7837bb1eee0dc2d58d334cb3?s=256&d=identicon&r=PG",
      "display_name": "Alpha",
      "link": "https://ai.stackexchange.com/users/190/alpha"
    },
    "is_answered": true,
    "view_count": 4730,
    "accepted_answer_id": 4319,
    "answer_count": 5,
    "score": 8,
    "last_activity_date": 1508967264,
    "creation_date": 1485717171,
    "question_id": 2742,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/2742/linear-regression-why-is-distance-squared-used-as-an-error-metric",
    "title": "Linear regression: why is distance *squared* used as an error metric?",
    "body": "<p>Usually when performing linear regression predictions and gradient descent, the measure of the level of error for a particular line will be measured by the sum of the squared-distance values.</p>\n\n<p>Why distance <strong><em>squared</em></strong>?</p>\n\n<p>In most of the explanations I heard, they claim that:</p>\n\n<ul>\n<li>the function itself does not matter</li>\n<li>the result should be positive so positive and negative deviations are still counted</li>\n</ul>\n\n<p>However, an <code>abs()</code> approach would still work. And isn't it inconvenient that distance <em>squared</em> minimizes the distance result for distances lower than 1?</p>\n\n<p>I'm pretty sure someone must have considered this already -- so why is distance squared the most used approach to linear regression?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "markov-decision-process",
      "monte-carlo-methods",
      "transition-model"
    ],
    "owner": {
      "account_id": 2105842,
      "reputation": 263,
      "user_id": 4402,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/458e10159496fb9f3cac80ed1bc8dbed?s=256&d=identicon&r=PG",
      "display_name": "Brendan Hill",
      "link": "https://ai.stackexchange.com/users/4402/brendan-hill"
    },
    "is_answered": true,
    "view_count": 264,
    "accepted_answer_id": 2745,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1612103488,
    "creation_date": 1485727890,
    "last_edit_date": 1612103433,
    "question_id": 2743,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2743/how-to-fill-in-missing-transitions-when-sampling-an-mdp-transition-table",
    "title": "How to fill in missing transitions when sampling an MDP transition table?",
    "body": "<p>I have a simulator modelling a relatively complex scenario. I extract ~12 discrete features from the simulator state which forms the basis for my MDP state space.</p>\n<p>Suppose I am estimating the transition table for an MDP by running a large number of simulations and extracting feature transitions as the state transitions.</p>\n<p>While I can randomize the simulator starting conditions to increase the coverage of states, I cannot guarantee all states will be represented in the sample, i.e. states which are possible but rare.</p>\n<p>Is there a rigorous approach to &quot;filling in the gaps&quot; of the transition table in this case?</p>\n<p>For example:</p>\n<ol>\n<li><p>For each state that was unrepresented in the sample, simply transition to all other states with equal probability, as a &quot;neutral&quot; way to fill in the gap?</p>\n</li>\n<li><p>As above, but transition only to represented states (with equal probability)?</p>\n</li>\n<li><p>Transition to the same state with probability 1.0?</p>\n</li>\n<li><p>Ignore unrepresented states during MDP solving entirely, and simply have a default action specified?</p>\n</li>\n</ol>\n"
  },
  {
    "tags": [
      "neural-networks",
      "hyperparameter-optimization",
      "fuzzy-logic"
    ],
    "owner": {
      "account_id": 9041091,
      "reputation": 99,
      "user_id": 3358,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/70547fa2e8de4ca46e92cf5333638077?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "buzzer",
      "link": "https://ai.stackexchange.com/users/3358/buzzer"
    },
    "is_answered": true,
    "view_count": 3407,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1639171086,
    "creation_date": 1485880710,
    "last_edit_date": 1639171086,
    "question_id": 2762,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2762/how-do-we-decide-which-membership-function-to-use",
    "title": "How do we decide which membership function to use?",
    "body": "<p>In classical set theory, there are two options for an element. It is either a member of a set or not. But in fuzzy set theory, there are <strong>membership functions</strong> to define the &quot;rate&quot; of an element being a member of a set. In other words, classical logic says it is all black or white, but fuzzy logic offers that there is also grey which has shades between white and black.</p>\n<p>Matlab Simulink Library is very easy to design and helpful in practice. And it has good examples on its own, like deciding about tips for a dinner looking at service and food quality. In the figure below, some various membership functions from Matlab's library are shown:</p>\n<p><a href=\"https://i.sstatic.net/aWG0C.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/aWG0C.jpg\" alt=\"enter image description here\" /></a></p>\n<p><strong>My question:</strong> How do we decide which membership function to use while designing a fuzzy controller system?</p>\n<p>I mean, in general, not only in Matlab Simulink. I have seen <em>Triangular</em> and <em>Gaussian</em> functions are used mostly in practice, but how can we decide which function will give a better result for decision making? Do we need to train a neural network to decide which function is better depending on the problem and its rules? What are other solutions?</p>\n"
  },
  {
    "tags": [
      "programming-languages",
      "education"
    ],
    "owner": {
      "account_id": 10162319,
      "reputation": 81,
      "user_id": 5191,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/faf4d587fcfe6b63722bd355abd60a19?s=256&d=identicon&r=PG",
      "display_name": "Jason Lobo",
      "link": "https://ai.stackexchange.com/users/5191/jason-lobo"
    },
    "is_answered": true,
    "view_count": 274,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1561480040,
    "creation_date": 1486032182,
    "last_edit_date": 1561480040,
    "question_id": 2772,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2772/what-programmable-devices-can-be-used-to-teach-artificial-intelligence",
    "title": "What programmable devices can be used to teach artificial intelligence?",
    "body": "<p>Could you give examples of affordable programmable devices that could be used in university classes to teach students about A.I. and demonstrate it?</p>\n\n<p>The devices are expected to do some form of self-learning, pattern recognition, or any other features of A.I., and to be programmable or customizable.</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "question-answering"
    ],
    "owner": {
      "account_id": 7113187,
      "reputation": 319,
      "user_id": 5219,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/568254f0c5f54f869d21573caf4e0b27?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "lilienfa",
      "link": "https://ai.stackexchange.com/users/5219/lilienfa"
    },
    "is_answered": true,
    "view_count": 641,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1611674898,
    "creation_date": 1486138449,
    "last_edit_date": 1611674898,
    "question_id": 2777,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2777/how-can-i-translate-a-natural-language-question-to-an-mdx-query",
    "title": "How can I translate a natural language question to an MDX query?",
    "body": "<p>I am researching Natural Language Processing (NLP) to develop an NL Question Answering system. The answering part is already done. So processing the question remains, along with the questions regarding the algorithms.</p>\n<p>The final product should allow the user to ask a question in NL. The question then gets translated to an <a href=\"https://en.wikipedia.org/wiki/MultiDimensional_eXpressions\" rel=\"nofollow noreferrer\">Multidimensional Expressions (MDX) query</a>, which generates a script regarding the dimensions of the cube.</p>\n<p>How can I translate a natural language question to an MDX query?</p>\n<p>The outcome of the question is in the form of a calculation. For example</p>\n<blockquote>\n<p>How many declarations were done by employee 1?</p>\n</blockquote>\n<p>or</p>\n<blockquote>\n<p>Give me the quantities for Sales.</p>\n</blockquote>\n"
  },
  {
    "tags": [
      "decision-theory"
    ],
    "owner": {
      "account_id": 10268986,
      "reputation": 81,
      "user_id": 5534,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/0744e4bc0829c8fc51d3cb59a840a436?s=256&d=identicon&r=PG",
      "display_name": "stustd",
      "link": "https://ai.stackexchange.com/users/5534/stustd"
    },
    "is_answered": true,
    "view_count": 174,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1531444061,
    "creation_date": 1487289188,
    "last_edit_date": 1502748793,
    "question_id": 2833,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/2833/zilbersteins-lp-dominate-pruning-explained",
    "title": "Zilberstein&#39;s &quot;LP-dominate&quot; pruning explained?",
    "body": "<p>How does in the (famous Zilberstein) <code>PR</code>(uning) algorithm below the <code>LP-dominate</code> function get started: the first time it's called, <code>D=∅</code> and the linear program deteriorates (i.e. no constraint equations)? </p>\n\n<pre><code>procedure POINTWISE-DOMINATE(w, U)\n...\n3. return false\nprocedure LP-DOMINATE(w, U)\n4. solve the following linear program variables: d, b(s) ∀s ∈ S\n      maximize d\n      subject to the constraints\n        b · (w − u) ≥ d, ∀u ∈ U\n        sum(b) = 1\n5. if d ≥ 0 then return b\n6. else return nil\nprocedure BEST(b, U )\n...\n12. return w\nprocedure PR(W)\n13. D ← ∅\n14. while W = ∅\n15.   w ← any element in W\n16.   if POINTWISE-DOMINATE(w, D) = true\n17.      W ← W − {w}\n18.   else\n19.      b ← LP-DOMINATE(w, D)\n20.      if b = nil then\n21.         W ← W − {w}\n22.      else\n23.         w ← BEST(b, W)\n24.         D ← D ∪ {w}\n25.         W ← W − {w}\n26. return D\n</code></pre>\n"
  },
  {
    "tags": [
      "philosophy",
      "emotional-intelligence"
    ],
    "owner": {
      "account_id": 3435285,
      "reputation": 399,
      "user_id": 1760,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/4480675ad0ede0043e7caba04750d974?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "bmwalide",
      "link": "https://ai.stackexchange.com/users/1760/bmwalide"
    },
    "is_answered": true,
    "view_count": 1506,
    "accepted_answer_id": 2877,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1618909645,
    "creation_date": 1488034645,
    "last_edit_date": 1554035735,
    "question_id": 2872,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2872/can-an-ai-learn-to-suffer",
    "title": "Can an AI learn to suffer?",
    "body": "<p>I had first this question in mind \"Can an AI suffer?\". Suffering is important for human beings. Imagine that you are damaging your heel. Without pain, you will continue to harm it. Same for an AI. But then I told myself \"<em>Wait a second. It already exists. It is the errors and warnings that shows up</em>\". We can say it has the similar purpose as suffering. However, I felt something missing. We feel pain. The errors and bugs are just data. Let's say a robot can use machine learning and genetic programming to evolve. </p>\n\n<p>Can an AI learn to suffer? And not just know it as mere information.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "reference-request",
      "time-complexity",
      "efficiency",
      "travelling-salesman-problem"
    ],
    "owner": {
      "account_id": 3302011,
      "reputation": 353,
      "user_id": 1366,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d07f33df80651f4d22e5a37293a52b14?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Gottfried William",
      "link": "https://ai.stackexchange.com/users/1366/gottfried-william"
    },
    "is_answered": true,
    "view_count": 1092,
    "accepted_answer_id": 5853,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1610929979,
    "creation_date": 1488059210,
    "last_edit_date": 1610929979,
    "question_id": 2874,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2874/can-neural-networks-efficiently-solve-the-traveling-salesmen-problem",
    "title": "Can neural networks efficiently solve the traveling salesmen problem?",
    "body": "<p>Can neural networks efficiently solve the traveling salesmen problem? Are there any research papers that show that neural networks can solve the TSP efficiently?</p>\n<p>The TSP is an NP-hard problem, so I suspect that there are only approximate solutions to this problem, even with neural networks. So, in this case, how would <em>efficiency</em> be defined?</p>\n<p>In this context, it seems that the time efficiency may be obtained by resource inefficiency: by making the neural network enormous and simulating all the possible worlds, then maximizing. So, while time to compute doesn't grow much as the problem grows, the size of the physical computer grows enormously for larger problems; how fast it computes is then, it seems to me, not a good measure of the efficiency of the algorithm in the common meaning of efficiency. In this case, the resources themselves only grow as fast as the problem size, but what explodes is the number of connections that must be built. If we go from 1000 to 2000 neurons to solve a problem twice as large and requiring exponentially as much time to solve, the algorithms requiring only twice as many neurons to solve in polynomial time seem efficient, but, really, there is still an enormous increase in connections and coefficients that need be built for this to work.</p>\n<p>Is my above reasoning incorrect?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "training",
      "terminology",
      "hebbian-learning"
    ],
    "owner": {
      "account_id": 7622811,
      "reputation": 201,
      "user_id": 6391,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2f14d10e7d8ab2378c50d1ee51a6987f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Philogy",
      "link": "https://ai.stackexchange.com/users/6391/philogy"
    },
    "is_answered": true,
    "view_count": 289,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1591635087,
    "creation_date": 1490996451,
    "last_edit_date": 1591635087,
    "question_id": 3077,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3077/what-is-the-name-of-a-human-inspired-machine-learning-approach",
    "title": "What is the name of a human-inspired machine learning approach?",
    "body": "<p>I once came across a neural network being trained without back-propagation or genetic algorithms (or using any kind of data sets). It was based on how the human brain learns and adjusts its connections between neurons.</p>\n\n<p>What is the name of such a machine learning approach?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "datasets",
      "social",
      "ai-safety",
      "algorithmic-bias"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user6698"
    },
    "is_answered": true,
    "view_count": 459,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1634499830,
    "creation_date": 1492456101,
    "last_edit_date": 1605621139,
    "question_id": 3175,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3175/what-are-examples-of-techniques-to-prevent-bias-in-artificial-intelligence-syste",
    "title": "What are examples of techniques to prevent bias in artificial intelligence systems?",
    "body": "<p>I recently read <a href=\"http://www.wired.co.uk/article/machine-learning-bias-prejudice\" rel=\"nofollow noreferrer\">an article about how artificial intelligence replicates human stereotypes</a> when applied to biased datasets.</p>\n<p>What are examples of techniques to prevent bias (and stereotypes) in artificial intelligence (in particular, machine learning) systems?</p>\n"
  },
  {
    "tags": [
      "knowledge-representation",
      "terminology",
      "logic"
    ],
    "owner": {
      "account_id": 9837397,
      "reputation": 181,
      "user_id": 5133,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/a906f5f692f7fe2c807fa47a32518816?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Parry",
      "link": "https://ai.stackexchange.com/users/5133/parry"
    },
    "is_answered": true,
    "view_count": 439,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1595082832,
    "creation_date": 1493250426,
    "last_edit_date": 1595082832,
    "question_id": 3226,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3226/what-is-meant-by-known-in-a-knowledge-base-exhibits-complete-knowledge-if-and",
    "title": "What is meant by &quot;known&quot; in &quot;A knowledge-base exhibits complete knowledge if and only if, for every $P$, $P$ or $\\neg P$ is known&quot;?",
    "body": "<p>I have a question as to what it means for a knowledge-base to be consistent and complete. I've been looking into non-monotonic logic and different formalisms for it from the book &quot;Knowledge Representation and Reasoning&quot; by Hector Levesque and Ronald J. Brachman, but something is confusing me.</p>\n<p>They say:</p>\n<blockquote>\n<p>We say a KB exhibits consistent knowledge iff there is no sentence <span class=\"math-container\">$P$</span> such that both <span class=\"math-container\">$P$</span> and <span class=\"math-container\">$\\neg P$</span> are known. This is the same as requiring the KB to be satisfiable. We also say that a KB exhibits complete knowledge iff for every <span class=\"math-container\">$P$</span> (within its vocabulary) <span class=\"math-container\">$P$</span> or <span class=\"math-container\">$\\neq P$</span> is known</p>\n</blockquote>\n<p>They then seem to suggest that by &quot;known&quot; they mean &quot;entailed&quot;. They say</p>\n<blockquote>\n<p>In general, of course, knowledge can be incomplete. For example, suppose KB consists of a single sentence (<span class=\"math-container\">$P$</span> or <span class=\"math-container\">$Q$</span>). Then KB does not entail either <span class=\"math-container\">$P$</span> or <span class=\"math-container\">$\\neg P$</span>, and so exhibits incomplete knowledge.</p>\n</blockquote>\n<p>But when dealing with sets of sentences, I usually see these terms as being defined w.r.t. <em>derivability</em> and not <em>entailment</em>.</p>\n<p>So my question is, what exactly do these authors mean by &quot;known&quot; in the above quotes?</p>\n<p>Edit: <a href=\"https://math.stackexchange.com/q/2259311/168764\">this post</a> helped clarify things.</p>\n"
  },
  {
    "tags": [
      "definitions",
      "intelligent-agent",
      "multi-agent-systems"
    ],
    "owner": {
      "account_id": 10012559,
      "reputation": 83,
      "user_id": 6921,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8ff75e7d594d2d6ff08c7c337b8b1331?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "practronix512",
      "link": "https://ai.stackexchange.com/users/6921/practronix512"
    },
    "is_answered": true,
    "view_count": 38133,
    "protected_date": 1736407659,
    "accepted_answer_id": 3253,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1736405383,
    "creation_date": 1493487841,
    "last_edit_date": 1573934191,
    "question_id": 3243,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3243/what-are-some-examples-of-intelligent-agents-for-each-intelligent-agent-class",
    "title": "What are some examples of intelligent agents for each intelligent agent class?",
    "body": "<p>There are several classes of intelligent agents, such as:</p>\n\n<ul>\n<li>simple reflex agents</li>\n<li>model-based reflex agents</li>\n<li>goal-based agents</li>\n<li>utility-based agents</li>\n<li>learning agents</li>\n</ul>\n\n<p>Each of these agents behaves slightly different from the other agents. There are certain diagrams that describe each of these IA classes. However, all the descriptions are usually quite abstract.</p>\n\n<p>What are some examples of intelligent agents for each intelligent agent class? Optionally, I would also like to see a compact definition of each class (and maybe how they are related to the diagrams).</p>\n"
  },
  {
    "tags": [
      "agi",
      "natural-language-processing"
    ],
    "owner": {
      "account_id": 7680288,
      "reputation": 149,
      "user_id": 3015,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-3PsjYXtaHG4/AAAAAAAAAAI/AAAAAAAAA_Y/oTYX-xn4_t4/s256-rj/photo.jpg",
      "display_name": "Vivek Krishna",
      "link": "https://ai.stackexchange.com/users/3015/vivek-krishna"
    },
    "is_answered": true,
    "view_count": 466,
    "answer_count": 4,
    "score": 8,
    "last_activity_date": 1557158612,
    "creation_date": 1494039089,
    "last_edit_date": 1557158612,
    "question_id": 3272,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3272/is-language-understanding-possible-without-qualia",
    "title": "Is language understanding possible without qualia?",
    "body": "<p>I feel that many words if not all of them have a direct mapping to some kind of inner subjective experience, to a physical object, mental feeling, process or some other kind of abstract thing. Given that machines don't have <em>qualia</em> and no mapping of this kind, can they really understand anything even though they are made to answer to questions with lots of statistical training?</p>\n"
  },
  {
    "tags": [
      "applications",
      "cyberterrorism"
    ],
    "owner": {
      "account_id": 7064383,
      "reputation": 93,
      "user_id": 6508,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-7nOByJgM46U/AAAAAAAAAAI/AAAAAAAAAsQ/nxZ00Agg90k/s256-rj/photo.jpg",
      "display_name": "kvk venugopal",
      "link": "https://ai.stackexchange.com/users/6508/kvk-venugopal"
    },
    "is_answered": true,
    "view_count": 428,
    "accepted_answer_id": 3331,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1574460738,
    "creation_date": 1494931090,
    "last_edit_date": 1574460738,
    "question_id": 3320,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3320/can-ai-stop-attacks-like-wannacry",
    "title": "Can AI stop attacks like WannaCry?",
    "body": "<p>I have done some research regarding the application of machine learning to cyber-security. After these recent attacks (like <a href=\"https://en.wikipedia.org/wiki/WannaCry_ransomware_attack\" rel=\"nofollow noreferrer\">WannaCry</a>), I think that AI-based cyber defense can prevent them. I have also read about research regarding the same in MIT, and that AI can detect more than 80% of malware. Is AI actually so promising in this department?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "chat-bots",
      "metric",
      "bleu",
      "perplexity"
    ],
    "owner": {
      "account_id": 9271524,
      "reputation": 89,
      "user_id": 7564,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/530Mb.jpg?s=256",
      "display_name": "RuiZhang1993",
      "link": "https://ai.stackexchange.com/users/7564/ruizhang1993"
    },
    "is_answered": true,
    "view_count": 2461,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1654468798,
    "creation_date": 1496285427,
    "last_edit_date": 1654468798,
    "question_id": 3419,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3419/why-is-the-perplexity-a-good-evaluation-metric-for-chatbots",
    "title": "Why is the perplexity a good evaluation metric for chatbots?",
    "body": "<p>A few papers I have come across say that <a href=\"https://en.wikipedia.org/wiki/BLEU\" rel=\"nofollow noreferrer\">BLEU</a> is not an appropriate evaluation metric for chatbots, so they use the <strong>perplexity</strong>.</p>\n<p>First of all, what is <em>perplexity</em>? How to calculate it? And why is perplexity a good evaluation metric for chatbots?</p>\n"
  },
  {
    "tags": [
      "applications"
    ],
    "owner": {
      "account_id": 10970508,
      "reputation": 207,
      "user_id": 7714,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-VKxC3eM2N6Q/AAAAAAAAAAI/AAAAAAAAFVA/Japd4aYIU5U/s256-rj/photo.jpg",
      "display_name": "Jos&#233; Henrique Luckmann",
      "link": "https://ai.stackexchange.com/users/7714/jos%c3%a9-henrique-luckmann"
    },
    "is_answered": true,
    "view_count": 501,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1561587964,
    "creation_date": 1500669377,
    "last_edit_date": 1561493839,
    "question_id": 3696,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3696/examples-of-uses-of-artificial-intelligence-at-work",
    "title": "Examples of uses of artificial intelligence at work",
    "body": "<p>I'm going to give a talk, and I'm preparing the material. The purpose of the conversation is to convince companies in my region that it is possible to apply artificial intelligence in solving everyday business problems.</p>\n\n<p>I would like some examples to be able to present, and so I came here to ask</p>\n\n<p><em>Have you used artificial intelligence to solve a problem at work? What kind of problem?</em></p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "implementation",
      "sarsa"
    ],
    "owner": {
      "account_id": 9095809,
      "reputation": 572,
      "user_id": 8448,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6f649da6c6c5e2d17e73e0918b9f78a9?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Hai Nguyen",
      "link": "https://ai.stackexchange.com/users/8448/hai-nguyen"
    },
    "is_answered": true,
    "view_count": 2768,
    "accepted_answer_id": 7700,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1604240159,
    "creation_date": 1501854354,
    "last_edit_date": 1604240159,
    "question_id": 3758,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3758/how-should-i-handle-action-selection-in-the-terminal-state-when-implementing-sar",
    "title": "How should I handle action selection in the terminal state when implementing SARSA?",
    "body": "<p>I recently started learning about reinforcement learning. Currently, I am trying to implement the <a href=\"http://www.cse.unsw.edu.au/%7Ecs9417ml/RL1/algorithms.html\" rel=\"nofollow noreferrer\">SARSA algorithm</a>. However, I do not know how to deal with <span class=\"math-container\">$Q(s', a')$</span>, when <span class=\"math-container\">$s'$</span> is the terminal state. First, there is no action to choose from in this state. Second, this <span class=\"math-container\">$Q$</span>-factor will never be updated either because the episode ends when <span class=\"math-container\">$s'$</span> is reached. Should I initialize <span class=\"math-container\">$Q(s', a')$</span> to something other than a random number? Or should I just ignore the <span class=\"math-container\">$Q$</span>-factors and simply feed the reward <span class=\"math-container\">$r$</span> into the update?</p>\n"
  },
  {
    "tags": [
      "comparison",
      "papers",
      "logic",
      "knowledge-representation",
      "rule-based-systems"
    ],
    "owner": {
      "account_id": 6275162,
      "reputation": 221,
      "user_id": 8637,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/40fd06334f5f82d708c15dbbd0c52f5b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "samlaf",
      "link": "https://ai.stackexchange.com/users/8637/samlaf"
    },
    "is_answered": true,
    "view_count": 3655,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1611501505,
    "creation_date": 1502242786,
    "last_edit_date": 1611257863,
    "question_id": 3786,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3786/what-is-the-difference-between-logic-based-and-rule-based-ai",
    "title": "What is the difference between logic-based and rule-based AI?",
    "body": "<p>I always thought rule-based was synonymous with logic-based AI. Logic has axioms and rules of inference, whereas rule-based AI has a knowledge base (essentially, axioms) and if-then rules to create new knowledge (essentially inference rules).</p>\n<p>But in their famous article <a href=\"https://ojs.aaai.org//index.php/aimagazine/article/view/1029\" rel=\"nofollow noreferrer\">What is a Knowledge Representation?</a>, Davis, Shrobe and Szolovits seem to imply that they are not:</p>\n<blockquote>\n<p>Logic, rules, frames, and so on, embody a viewpoint on the kinds of things that are important in the world. Logic, for example, involves a (fairly minimal) commitment to viewing the world in terms of individual entities and relations between them. Rule-based systems view the world in terms of attribute-object-value triples and the rules of plausible inference that connect them, while frames have us thinking in terms of prototypical objects.</p>\n</blockquote>\n<p>Is this only saying that rule-based are propositional, whereas logic-based is usually meant to mean predicate logic? Or is there more to it than this?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "backpropagation",
      "algorithm-request"
    ],
    "owner": {
      "account_id": 7193361,
      "reputation": 83,
      "user_id": 9406,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/48248a8939c6db9b3d90f02b72354198?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Damian Matkowski",
      "link": "https://ai.stackexchange.com/users/9406/damian-matkowski"
    },
    "is_answered": true,
    "view_count": 1680,
    "accepted_answer_id": 3996,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1639218008,
    "creation_date": 1504441377,
    "last_edit_date": 1639217898,
    "question_id": 3962,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3962/how-do-i-know-if-my-backpropagation-is-implemented-correctly",
    "title": "How do I know if my backpropagation is implemented correctly?",
    "body": "<p>I'm working on an implementation of the backpropagation algorithm for a simple neural network, which predicts a probability of survival (1 or 0).</p>\n<p>However, I can't get it above 80%, no matter how much I try to set the right hyperparameters. I suspect that's because my backpropagation is implemented incorrectly, since I tried 2 different types of code and both give me the same results.</p>\n<p>Is there a way to determine whether <a href=\"https://pastebin.com/4PmYFAjH\" rel=\"nofollow noreferrer\">my implementation</a> of backpropagation is correct?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "natural-language-processing",
      "algorithm-request"
    ],
    "owner": {
      "account_id": 1620855,
      "reputation": 189,
      "user_id": 9836,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0a1b529539a71d7255c4215112c8385d?s=256&d=identicon&r=PG",
      "display_name": "omega",
      "link": "https://ai.stackexchange.com/users/9836/omega"
    },
    "is_answered": true,
    "view_count": 233,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1642973401,
    "creation_date": 1506387970,
    "last_edit_date": 1642932359,
    "question_id": 4114,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4114/are-there-any-algorithms-that-can-measure-the-concept-similarity-between-texts",
    "title": "Are there any algorithms that can measure the concept similarity between texts?",
    "body": "<p>Are there any algorithms (or software libraries) that can be used to detect the similarity of concepts in text, regardless of articulation, grammar, synonyms, etc.?</p>\n<p>For example, these phrases:</p>\n<blockquote>\n<p>Outside, it is warm.</p>\n<p>Outside, it is hot.</p>\n<p>Outside, it is not cold.</p>\n<p>It is not cold outside.</p>\n</blockquote>\n<p>Should be similar to this phrase:</p>\n<blockquote>\n<p>It is warm outside.</p>\n</blockquote>\n<p>Ideally, the algorithm (or software) would be capable of generating a score from 0 to 1, based on the concept similarity. The goal is to use this algorithm or software to map a large number of statements to a single, similar original statement. It is for this mapping of a given statement to the original statement that the aforementioned similarity score would be generated.</p>\n<p>Does such an algorithm (or software) already exist?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "pattern-recognition",
      "comparison",
      "data-mining"
    ],
    "owner": {
      "account_id": 12052510,
      "reputation": 91,
      "user_id": 10314,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/2562f3a51c094ae3282baa074cb9910b?s=256&d=identicon&r=PG",
      "display_name": "user10314",
      "link": "https://ai.stackexchange.com/users/10314/user10314"
    },
    "is_answered": true,
    "view_count": 4093,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1560307766,
    "creation_date": 1508682846,
    "last_edit_date": 1560307766,
    "question_id": 4328,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4328/what-are-the-differences-between-machine-learning-pattern-recognition-and-data",
    "title": "What are the differences between machine learning, pattern recognition and data mining?",
    "body": "<p>I know a little about these subjects. I found them similar to each other. Can anybody explain the differences between them?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "deep-rl",
      "papers",
      "research"
    ],
    "owner": {
      "account_id": 1621964,
      "reputation": 377,
      "user_id": 9740,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/40a23ceaf81666478d8ba416fc850808?s=256&d=identicon&r=PG",
      "display_name": "Evalds Urtans",
      "link": "https://ai.stackexchange.com/users/9740/evalds-urtans"
    },
    "is_answered": true,
    "view_count": 2799,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1612099259,
    "creation_date": 1510045364,
    "last_edit_date": 1612097803,
    "question_id": 4454,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4454/where-to-publish-a-first-article-in-deep-reinforcement-learning",
    "title": "Where to publish a first article in Deep Reinforcement Learning?",
    "body": "<p>What would be examples of journals that are good for a <strong>first publication</strong> in the field of Deep Reinforcement Learning?</p>\n<p>I am in the process of writing about the research results of DQN-related algorithms.</p>\n<p>I have 3 requirements - it should be indexed in one of these databases, otherwise, I cannot receive grant money for research:</p>\n<ul>\n<li><a href=\"https://www.scopus.com/\" rel=\"nofollow noreferrer\">https://www.scopus.com/</a></li>\n<li><a href=\"http://webofknowledge.com\" rel=\"nofollow noreferrer\">http://webofknowledge.com</a></li>\n</ul>\n<p>And it should not be very expensive to publish. It should be under 1000EUR to publish, for example, the Open Access license for Elsevier &quot;Artificial Intelligence&quot; journal costs around 2400EUR to publish.</p>\n<p>And it should not have a very long review/publishing period. For example, Elsevier's &quot;Information Fusion&quot; journal currently gathers articles for July 2018, which is 8 month period till publishing. Is it normal?</p>\n<p>Can you please recommend some journals that qualify &amp; you have had good experience publishing research?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "features",
      "network-design"
    ],
    "owner": {
      "account_id": 12340136,
      "reputation": 81,
      "user_id": 11111,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/3ade019172f8bbddc08a22d88fc09f51?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Vince Britz",
      "link": "https://ai.stackexchange.com/users/11111/vince-britz"
    },
    "is_answered": true,
    "view_count": 320,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1655155047,
    "creation_date": 1511531816,
    "last_edit_date": 1655155047,
    "question_id": 4591,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4591/how-to-design-a-neural-network-that-gets-the-author-name-of-a-piece-of-art-as-in",
    "title": "How to design a neural network that gets the author name of a piece of art as input?",
    "body": "<p>I'm building a neural net to predict the value of a piece of art with a wide range of inputs (size, art medium, etc.) and I would like to include the author as an input as well (it is often a huge factor in the value of a single piece of art).</p>\n<p>My current concern is that the name of the author isn't an ideal numerical input for a NN (i.e. If I just code each author with an increasing integer value I will be indirectly assigning more value to authors further down the list -_-). My thoughts were to create separate inputs for all the authors in my dataset and then just use one-hot encoding to better represent the input to the NN.</p>\n<p>This approach, however, runs into a problem when an author that is not included in my training data is used as an input to the NN (i.e. a new author). I can get around this with an &quot;other author&quot; input field, but I am worried that this won't be accurate as I would not have trained the the NN for this input (all pieces of art with a valuation have an author).</p>\n<p>I haven't fully thought this through but I thought of perhaps training 2 NN's, one for a valuation without an author and one for valuation with an author to ensure I have enough training data for an &quot;authorless valuation&quot; to still be reasonably accurate.</p>\n<p>I am still trying to conceptualize the best NN architecture before I get stuck into the implementation so if anyone has any suggestions/comments I would be very grateful!</p>\n<p>P.S. I am doing this as a small competition with a friend to test a NN vs the traditional commercial valuation techniques.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "prediction",
      "overfitting",
      "feature-selection"
    ],
    "owner": {
      "account_id": 9715760,
      "reputation": 89,
      "user_id": 11465,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/08ea907d3b57005305612f88c0a85b03?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Aref.a",
      "link": "https://ai.stackexchange.com/users/11465/aref-a"
    },
    "is_answered": true,
    "view_count": 1966,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1587344525,
    "creation_date": 1512717405,
    "last_edit_date": 1571786954,
    "question_id": 4682,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4682/how-come-that-the-addition-of-features-can-decrease-the-performance-of-a-neural",
    "title": "How come that the addition of features can decrease the performance of a neural network?",
    "body": "<p>I have a Remaining Useful Life (RUL) prediction problem that I want to solve. When I added two or more features as inputs to my ANN, the accuracy of my ANN has been decreased. More precisely, I've added features like RMS or KURTOSIS (or both). I was expecting the system to improve, but it is getting worse. </p>\n\n<p>Why might this be happening? What are the potential reasons for this degradation in performance?</p>\n\n<p>I know that when we added more nodes in layers (like hidden layers), overfitting can happen. Would that be related to my problem: using more than two features?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "natural-language-processing",
      "applications",
      "capsule-neural-network"
    ],
    "owner": {
      "account_id": 2353770,
      "reputation": 278,
      "user_id": 11626,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/93d988e82aa789487c174bb4d31ff350?s=256&d=identicon&r=PG",
      "display_name": "Ahti Ahde",
      "link": "https://ai.stackexchange.com/users/11626/ahti-ahde"
    },
    "is_answered": true,
    "view_count": 2931,
    "accepted_answer_id": 20571,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1592231612,
    "creation_date": 1513450269,
    "last_edit_date": 1591703409,
    "question_id": 4766,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4766/have-capsule-neural-networks-been-used-to-nlp-problems",
    "title": "Have capsule neural networks been used to NLP problems?",
    "body": "<p>The capsule neural network seems to be a good solution for problems that involve hierarchies. For example, a face is composed of eyes, a nose and ears; a hand is made of fingers, nails, and a palm; and a human is composed of a face and hands.</p>\n\n<p>Many problems in NLP can be seen as hierarchical problems: there are words, sentences, paragraphs, and chapters, whose meaning changes based on the style of lower levels.</p>\n\n<p>Are there any research papers (which I should be aware of) on the application of capsule neural networks to NLP problems? </p>\n\n<p>Are there related research papers, which have been investigating hierarchical complexity within the domain of NLP, which could be easily translated to Capsule Network?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "agi"
    ],
    "owner": {
      "account_id": 6190582,
      "reputation": 91,
      "user_id": 12089,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6c3872535ac680c7a654c55585be680f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Pre-alpha",
      "link": "https://ai.stackexchange.com/users/12089/pre-alpha"
    },
    "is_answered": true,
    "view_count": 2614,
    "answer_count": 5,
    "score": 8,
    "last_activity_date": 1703342027,
    "creation_date": 1515855760,
    "last_edit_date": 1555687331,
    "question_id": 4987,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4987/is-there-a-limit-to-the-increase-of-intelligence",
    "title": "Is there a limit to the increase of intelligence?",
    "body": "<p>Some argue that humans are somewhere along the middle of the intelligence spectrum, some say that we are only at the very beginning of the spectrum and there's so much more potential ahead.</p>\n\n<p>Is there a limit to the increase of intelligence? Could it be possible for a general intelligence to progress infinitely, provided enough resources and armed with the best self-recursive improvement algorithms?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "convolutional-neural-networks",
      "pattern-recognition"
    ],
    "owner": {
      "account_id": 12457047,
      "reputation": 169,
      "user_id": 12125,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/10212167737843244/picture?type=large",
      "display_name": "Darren Taggart",
      "link": "https://ai.stackexchange.com/users/12125/darren-taggart"
    },
    "is_answered": true,
    "view_count": 2548,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1544334239,
    "creation_date": 1516657256,
    "question_id": 5096,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/5096/using-neural-network-to-recognise-patterns-in-matrices",
    "title": "Using neural network to recognise patterns in matrices",
    "body": "<p>I am trying to develop a neural network which can identify design features in CAD models (i.e. slots, bosses, holes, pockets, steps).</p>\n\n<p>The input data I intend to use for the network is a n x n matrix (where n is the number of faces in the CAD model). A '1' in the top right triangle in the matrix represents a convex relationship between two faces and a '1' in the bottom left triangle represents a concave relationship. A zero in both positions means the faces are not adjacent. The image below gives an example of such a matrix.\n<a href=\"https://i.sstatic.net/Soj7K.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/Soj7K.png\" alt=\"enter image description here\"></a></p>\n\n<p>Lets say I set the maximum model size to 20 faces and apply padding for anything smaller than that in order to make the inputs to the network a constant size.</p>\n\n<p>I want to be able to recognise 5 different design features and would therefore have 5 output neurons - [slot, pocket, hole, boss, step]</p>\n\n<p>Would I be right in saying that this becomes a sort of 'pattern recognition' problem? For example, if I supply the network with a number of training models - along with labels which describe the design feature which exists in the model, would the network learn to recognise specific adjacency patterns represented in the matrix which relate to certain design features?</p>\n\n<p>I am a complete beginner in machine learning and I am trying to get a handle on whether this approach will work or not - if any more info is needed to understand the problem leave a comment. Any input or help would be appreciated, thanks. </p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "convolutional-neural-networks",
      "convolution-arithmetic",
      "receptive-field"
    ],
    "owner": {
      "account_id": 9818808,
      "reputation": 421,
      "user_id": 12242,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/fbd3dafadd4c6c96a3669657379d8e58?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Inkplay_",
      "link": "https://ai.stackexchange.com/users/12242/inkplay"
    },
    "is_answered": true,
    "view_count": 5813,
    "accepted_answer_id": 5740,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1633780257,
    "creation_date": 1516733130,
    "last_edit_date": 1633695174,
    "question_id": 5107,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5107/how-can-3-same-size-cnn-layers-in-different-ordering-output-different-receptive",
    "title": "How can 3 same size CNN layers in different ordering output different receptive field from the input layer?",
    "body": "<p>Below is a quote from <a href=\"https://cs231n.github.io/convolutional-networks/\" rel=\"nofollow noreferrer\">CS231n</a>:</p>\n<blockquote>\n<p>Prefer a stack of small filter CONV to one large receptive field CONV layer. Suppose that you stack three 3x3 CONV layers on top of each other (with non-linearities in between, of course). In this arrangement, each neuron on the first CONV layer has a 3x3 view of the input volume. A neuron on the second CONV layer has a 3x3 view of the first CONV layer, and hence by extension a 5x5 view of the input volume. Similarly, a neuron on the third CONV layer has a 3x3 view of the 2nd CONV layer, and hence a 7x7 view of the input volume. Suppose that instead of these three layers of 3x3 CONV, we only wanted to use a single CONV layer with 7x7 receptive fields. These neurons would have a receptive field size of the input volume that is identical in spatial extent (7x7), but with several disadvantages</p>\n</blockquote>\n<p>My visualized interpretation:</p>\n<p><a href=\"https://i.sstatic.net/KZy6R.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/KZy6R.png\" alt=\"image\" /></a></p>\n<p>How can you see through the first CNN layer from the second CNN layer and see a 5x5 sized receptive field?</p>\n<p>There were no previous comments stating all the other hyperparameters, like input size, steps, padding, etc. which made this very confusing to visualize.</p>\n<hr />\n<p>Edited:</p>\n<p>I think I found the <a href=\"https://medium.com/@nikasa1889/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807\" rel=\"nofollow noreferrer\">answer</a>. BUT I still don't understand it. In fact, I am more confused than ever.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "google",
      "automated-machine-learning"
    ],
    "owner": {
      "account_id": 7363981,
      "reputation": 1186,
      "user_id": 10913,
      "user_type": "registered",
      "accept_rate": 25,
      "profile_image": "https://i.sstatic.net/8sMwH.jpg?s=256",
      "display_name": "Seth Simba",
      "link": "https://ai.stackexchange.com/users/10913/seth-simba"
    },
    "is_answered": true,
    "view_count": 597,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1627648342,
    "creation_date": 1517208650,
    "last_edit_date": 1627648342,
    "question_id": 5186,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5186/what-is-an-intuitive-explanation-of-how-googles-automl-works",
    "title": "What is an intuitive explanation of how Google&#39;s AutoML works?",
    "body": "<p>I recently read that Google has developed a new AI that anyone can upload data to and it will instantly generate models, i.e. an image recognition model based on that data.</p>\n<p>Can someone explain to me in a detailed and intuitive manner how this AI works?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "neurons",
      "neuroscience",
      "brain",
      "computational-theory-of-mind"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user9947"
    },
    "is_answered": true,
    "view_count": 1121,
    "accepted_answer_id": 5241,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1611059905,
    "creation_date": 1517926767,
    "last_edit_date": 1611059905,
    "question_id": 5239,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5239/what-makes-the-animal-brain-so-special",
    "title": "What makes the animal brain so special?",
    "body": "<p>Whenever I read any book about neural networks or machine learning, their introductory chapter says that we haven't been able to replicate the brain's power due to its massive parallelism.</p>\n<p>Now, in modern times, transistors have been reduced to the size of nanometers, much smaller than the nerve cell. Also, we can easily build very large supercomputers.</p>\n<ul>\n<li>Computers have much larger memories than brains.</li>\n<li>Computes can communicate faster than brains (clock pulse in nanoseconds).</li>\n<li>Computers can be of arbitrarily large size.</li>\n</ul>\n<p>So, my question is: why cannot we replicate the brain's parallelism if not its information processing ability (since the brain is still not well understood) even with such advanced technology? What exactly is the obstacle we are facing?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "optimization"
    ],
    "owner": {
      "account_id": 2904880,
      "reputation": 181,
      "user_id": 14789,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ae67305df1ce96007f355de03e043bcc?s=256&d=identicon&r=PG",
      "display_name": "Chrigi",
      "link": "https://ai.stackexchange.com/users/14789/chrigi"
    },
    "is_answered": true,
    "view_count": 490,
    "accepted_answer_id": 18695,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1584463381,
    "creation_date": 1522918783,
    "last_edit_date": 1522938835,
    "question_id": 5904,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/5904/why-does-a-one-layer-hidden-network-get-more-robust-to-poor-initialization-with",
    "title": "Why does a one-layer hidden network get more robust to poor initialization with growing number of hidden neurons?",
    "body": "<p>In a nutshell: I want to understand why a one hidden layer neural network converges to a good minimum more reliably when a larger number of hidden neurons is used. Below a more detailed explanation of my experiment:</p>\n\n<p>I am working on a simple 2D XOR-like classification example to understand the effects of neural network initialization better. Here's a visualisation of the data and the desired decision boundary:\n<a href=\"https://i.sstatic.net/4jCnc.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/4jCnc.png\" alt=\"enter image description here\"></a></p>\n\n<p>Each blob consists of 5000 data points. The minimal complexity neural network to solve this problem is a one-hidden layer network with 2 hidden neurons. Since this architecture has the minimum number of parameters possible to solve this problem (with a NN) I would naively expect that this is also the easiest to optimise. However, this is not the case. </p>\n\n<p>I found that with random initialization this architecture converges around half of the time, where convergence depends on the signs of the weights. Specifically, I observed the following behaviour:</p>\n\n<pre><code>w1 = [[1,-1],[-1,1]], w2 = [1,1] --&gt; converges\nw1 = [[1,1],[1,1]],   w2 = [1,-1] --&gt; converges\nw1 = [[1,1],[1,1]],   w2 = [1,1] --&gt; finds only linear separation\nw1 = [[1,-1],[-1,1]], w2 = [1,-1] --&gt; finds only linear separation\n</code></pre>\n\n<p>This makes sense to me. In the latter two cases the optimisation gets stuck in suboptimal local minima. However, when increasing the number of hidden neurons to values greater than 2, the network develops a robustness to initialisation and starts to reliably converge for random values of w1 and w2. You can still find pathological examples, but with 4 hidden neurons the chance that one \"path way\" through the network will have non-pathological weights is larger. But happens to the rest of the network, is it just not used then? </p>\n\n<p>Does anybody understand better where this robustness comes from or perhaps can offer some literature discussing this issue?</p>\n\n<p>Some more information: this occurs in all training settings/architecture configurations I have investigated. For instance, activations=Relu, final_activation=sigmoid, Optimizer=Adam, learning_rate=0.1, cost_function=cross_entropy, biases were used in both layers. </p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "reinforcement-learning",
      "deep-rl",
      "experience-replay"
    ],
    "owner": {
      "account_id": 11997803,
      "reputation": 241,
      "user_id": 15967,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/fe7655e5fa89f06d540d092a4a7a8192?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user491626",
      "link": "https://ai.stackexchange.com/users/15967/user491626"
    },
    "is_answered": true,
    "view_count": 1055,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1604243775,
    "creation_date": 1527707345,
    "last_edit_date": 1554394145,
    "question_id": 6579,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6579/what-is-experience-replay-in-laymens-terms",
    "title": "What is experience replay in laymen&#39;s terms?",
    "body": "<p>I've been reading Google's DeepMind Atari <a href=\"http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\" rel=\"noreferrer\" title=\"RL\">paper</a> and I'm trying to understand the concept of \"experience replay\". Experience replay comes up in a lot of other reinforcement learning papers (particularly, the AlphaGo paper), so I want to understand how it works. Below are some excerpts.</p>\n\n<blockquote>\n  <p>First, we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution.</p>\n</blockquote>\n\n<p>The paper then elaborates as follows (I've taken a screenshot, since there are a lot of mathematical symbols that are difficult to reproduce):</p>\n\n<p><a href=\"https://i.sstatic.net/eOOTH.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/eOOTH.png\" alt=\"enter image description here\"></a></p>\n\n<p>What is experience replay and what are its benefits in laymen's terms?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "computer-vision",
      "reference-request",
      "state-of-the-art"
    ],
    "owner": {
      "account_id": 1225708,
      "reputation": 993,
      "user_id": 31388,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/nkxcn.png?s=256",
      "display_name": "Mary",
      "link": "https://ai.stackexchange.com/users/31388/mary"
    },
    "is_answered": true,
    "view_count": 610,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1640370682,
    "creation_date": 1529073160,
    "last_edit_date": 1640370682,
    "question_id": 6770,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6770/what-are-the-state-of-the-art-approaches-for-detecting-the-most-important-visua",
    "title": "What are the state-of-the-art approaches for detecting the most important &quot;visual attention&quot; area of an image?",
    "body": "<p>I'm trying to detect the <strong>visual attention</strong> area in a given image and crop the image into that area. For instance, given an image of any size and a rectangle of say <span class=\"math-container\">$L \\times W$</span> dimension as an input, I would like to crop the image to the most important visual attention area.</p>\n<p>What are the state-of-the-art approaches for doing that?</p>\n<p>(By the way, do you know of any tools to implement that? Any piece of code or algorithm would really help.)</p>\n<p>BTW, within a &quot;single&quot; object, I would like to get attention. So object detection might not be the best thing. I am looking for any approach, provided it's SOTA, but Deep Learning might be a better choice.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "reinforcement-learning",
      "q-learning",
      "dqn",
      "deep-rl"
    ],
    "owner": {
      "account_id": 2304865,
      "reputation": 2859,
      "user_id": 16565,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/e4iCE.jpg?s=256",
      "display_name": "malioboro",
      "link": "https://ai.stackexchange.com/users/16565/malioboro"
    },
    "is_answered": true,
    "view_count": 6496,
    "accepted_answer_id": 6924,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1696104699,
    "creation_date": 1530173964,
    "last_edit_date": 1619095762,
    "question_id": 6923,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6923/how-should-i-model-all-available-actions-of-a-chess-game-in-deep-q-learning",
    "title": "How should I model all available actions of a chess game in deep Q-learning?",
    "body": "<p>I just read about deep Q-learning, which is using a neural network for the value function instead of a table.</p>\n<p>I saw the example here: <a href=\"https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html\" rel=\"nofollow noreferrer\">Using Keras and Deep Q-Network to Play FlappyBird</a> and he used a CNN to get the Q-value.</p>\n<p>My confusion is on the last layer of his neural net. Neurons in the output layer each represent an action (flap, or not flap). I also see the <a href=\"http://edersantana.github.io/articles/keras_rl/\" rel=\"nofollow noreferrer\">other projects</a> where the output layer also represents all available actions (move-left, stop, etc.)</p>\n<p><em>How would you represent all the available actions of a chess game?</em> Every pawn has a unique and available movement. We also need to choose how far it will move (rook can move more than one square). I've read <a href=\"https://arxiv.org/abs/1509.01549\" rel=\"nofollow noreferrer\">Giraffe chess engine's</a> paper and can't find how he represents the output layer (I'll read once again).</p>\n<p>I hope somebody here can give a nice explanation about how to design NN architecture in Q-learning, I'm new in reinforcement learning.</p>\n"
  },
  {
    "tags": [
      "robots",
      "reasoning",
      "intelligence"
    ],
    "owner": {
      "account_id": 6476695,
      "reputation": 2258,
      "user_id": 4199,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-o_4nx3c8sHQ/AAAAAAAAAAI/AAAAAAAAAPU/lVuNexWGyPU/s256-rj/photo.jpg",
      "display_name": "zooby",
      "link": "https://ai.stackexchange.com/users/4199/zooby"
    },
    "is_answered": true,
    "view_count": 467,
    "answer_count": 5,
    "score": 8,
    "last_activity_date": 1539985967,
    "creation_date": 1530733976,
    "last_edit_date": 1539985967,
    "question_id": 7021,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7021/is-the-smartest-robot-more-clever-than-the-stupidest-human",
    "title": "Is the smartest robot more clever than the stupidest human?",
    "body": "<p>Most humans are not good at chess. They can't write symphonies. They don't read novels. They aren't good athletes. They aren't good at logical reasoning. Most of us just get up. Go to work in a factory or farm or something. Follow simple instructions. Have a beer and go to sleep.</p>\n\n<p>What are some things that a clever robot can't do that a stupid human can?</p>\n"
  },
  {
    "tags": [
      "training",
      "tensorflow",
      "keras",
      "long-short-term-memory",
      "gpu"
    ],
    "owner": {
      "account_id": 100608,
      "reputation": 289,
      "user_id": 16687,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/DOOne.png?s=256",
      "display_name": "Dieshe",
      "link": "https://ai.stackexchange.com/users/16687/dieshe"
    },
    "is_answered": true,
    "view_count": 21769,
    "accepted_answer_id": 7238,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1639772672,
    "creation_date": 1531112136,
    "last_edit_date": 1639772672,
    "question_id": 7090,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7090/can-lstm-neural-networks-be-sped-up-by-a-gpu",
    "title": "Can LSTM neural networks be sped up by a GPU?",
    "body": "<p>I am training LSTM neural networks with Keras on a small mobile GPU. The speed on the GPU is slower than on the CPU. I found some articles that say that it is hard to train LSTMs (and, in general, RNNs) on GPUs because the training cannot be parallelized.</p>\n<p>Is this true? Is LSTM training on large GPUs, like 1080 Ti, faster than on CPUs?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "artificial-neuron",
      "neurons",
      "biology",
      "neuromorphic-engineering"
    ],
    "owner": {
      "account_id": 1396704,
      "reputation": 223,
      "user_id": 12691,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f24d58921281bdfe8f4792710e9dd162?s=256&d=identicon&r=PG",
      "display_name": "Ziemo",
      "link": "https://ai.stackexchange.com/users/12691/ziemo"
    },
    "is_answered": true,
    "view_count": 1215,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1590258927,
    "creation_date": 1532076851,
    "last_edit_date": 1590258927,
    "question_id": 7215,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7215/how-to-model-inhibitory-synapses-in-the-artificial-neuron",
    "title": "How to model inhibitory synapses in the artificial neuron?",
    "body": "<p>In the brain, some synapses are stimulating and some inhibiting. In the case of artificial neural networks, ReLU erases that property, since in the brain inhibition doesn't correspond to a 0 output, but, more precisely, to a negative input.</p>\n\n<p>In the brain, the positive and negative potential is summed up, and, if it passed the threshold, the neuron fires. </p>\n\n<p>There are 2 main non-linearities which came to my mind in the biological unit:</p>\n\n<ul>\n<li><p>potential change is more exponential than linear: small amount of ion channels is sufficient to start a chain-reaction of other channels activation's - which rapidly change global neuron's potential.</p></li>\n<li><p>the threshold of the neuron is also non-linear: neuron fires only when the sum of its positive and negative potentials passed given (positive) threshold</p></li>\n</ul>\n\n<p><strong>So, is there any idea how to implement negative input to the artificial neural network?</strong></p>\n\n<p>I gave examples of non-linearities in biological neurons because the most obvious positive/negative unit is just a linear unit. But, since it doesn't implement non-linearity, we may consider implementing non-linearities somewhere else in the artificial neuron.</p>\n"
  },
  {
    "tags": [
      "deep-neural-networks",
      "autoencoders",
      "batch-normalization"
    ],
    "owner": {
      "account_id": 5863894,
      "reputation": 231,
      "user_id": 6899,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2e1fceccb5ef8778f7a561d65e923dc3?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Glrs",
      "link": "https://ai.stackexchange.com/users/6899/glrs"
    },
    "is_answered": false,
    "view_count": 4490,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1744056810,
    "creation_date": 1532338741,
    "last_edit_date": 1577375014,
    "question_id": 7255,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7255/does-it-make-sense-to-use-batch-normalization-in-deep-stacked-or-sparse-auto-e",
    "title": "Does it make sense to use batch normalization in deep (stacked) or sparse auto-encoders?",
    "body": "<p>Does it make sense to use batch normalization in deep (stacked) or sparse auto-encoders? </p>\n\n<p>I cannot find any resources for that. Is it safe to assume that, since it works for other DNNs, it will also make sense to use it and will offer benefits on training AEs?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "text-classification",
      "tf-idf",
      "bag-of-words"
    ],
    "owner": {
      "account_id": 1851913,
      "reputation": 246,
      "user_id": 6114,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/41c3e9852d3dd99d6145b0002416e6a5?s=256&d=identicon&r=PG",
      "display_name": "freesoul",
      "link": "https://ai.stackexchange.com/users/6114/freesoul"
    },
    "is_answered": true,
    "view_count": 209,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1662185245,
    "creation_date": 1532473409,
    "last_edit_date": 1607732395,
    "question_id": 7286,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7286/why-are-documents-kept-separated-when-training-a-text-classifier",
    "title": "Why are documents kept separated when training a text classifier?",
    "body": "<p>Most of the literature considers text classification as the classification of documents. When using the bag-of-words and Bayesian classification, they usually use the statistic TF-IDF, where TF normalizes the word count with the number of words per document, and IDF focuses on ignoring widely used and thus useless words for this task.</p>\n<p>My question is, why they keep the documents separated and create that statistic, if it is possible to merge all documents of the same class? This would have two advantages:</p>\n<ul>\n<li><p>You can just use word counts instead of frequencies, as the documents per class label is 1.</p>\n</li>\n<li><p>Instead of using IDF, you just select features with enough standard deviation between classes.</p>\n</li>\n</ul>\n"
  },
  {
    "tags": [
      "machine-learning",
      "reference-request",
      "genetic-programming",
      "inductive-programming"
    ],
    "owner": {
      "account_id": 12054183,
      "reputation": 379,
      "user_id": 17209,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/bp93G.jpg?s=256",
      "display_name": "user79161",
      "link": "https://ai.stackexchange.com/users/17209/user79161"
    },
    "is_answered": true,
    "view_count": 749,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1607382292,
    "creation_date": 1533069103,
    "last_edit_date": 1607348620,
    "question_id": 7364,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7364/does-an-ai-exist-that-can-write-software-based-on-a-formal-specification",
    "title": "Does an AI exist that can write software based on a formal specification?",
    "body": "<p>Does an AI exist that can automatically write software based on a formal specification of the software?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "proofs"
    ],
    "owner": {
      "account_id": 236479,
      "reputation": 245,
      "user_id": 17302,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/O3XKH.png?s=256",
      "display_name": "Yan King Yin",
      "link": "https://ai.stackexchange.com/users/17302/yan-king-yin"
    },
    "is_answered": true,
    "view_count": 992,
    "accepted_answer_id": 7439,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1549571171,
    "creation_date": 1533485576,
    "last_edit_date": 1549571171,
    "question_id": 7434,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7434/how-can-a-neural-network-approximate-all-functions-when-the-weights-are-not-allo",
    "title": "How can a neural network approximate all functions when the weights are not allowed to grow exponentially?",
    "body": "<p>It has been proven in the paper \"<a href=\"https://pdfs.semanticscholar.org/05ce/b32839c26c8d2cb38d5529cf7720a68c3fab.pdf\" rel=\"nofollow noreferrer\">Approximation by Superpositions of a Sigmoidal Function</a>\" (by Cybenko, in 1989) that neural networks are universal function approximators.  I have a related question. </p>\n\n<p>Assume the neural network's input and output vectors are of the same dimension <span class=\"math-container\">$n$</span>. Consider the set of <strong>binary-valued</strong> functions from <span class=\"math-container\">$\\{ 0,1 \\}^n$</span> to <span class=\"math-container\">$\\{ 0,1 \\}^n$</span>.  There are <span class=\"math-container\">$(2^n)^{(2^n)}$</span> such functions. The number of parameters in a (deep) neural network is <strong>much smaller</strong> than the above number.  Assume the network has <span class=\"math-container\">$L$</span> layers, each layer is <span class=\"math-container\">$n \\times n$</span> fully-connected, then the total number of weights is <span class=\"math-container\">$L \\cdot n^2$</span>.</p>\n\n<p>If the number of weights is <strong>not</strong> allowed to grow exponentially as <span class=\"math-container\">$n$</span>, can a deep neural network approximate <strong>all</strong> the binary-valued functions of size <span class=\"math-container\">$n$</span>?</p>\n\n<p>Cybenko's proof seems to be based on the <strong>denseness</strong> of the function space of neural network functions.  But this denseness does not seem to guarantee that a neural network function exists when the number of weights are polynomially bounded.</p>\n\n<p>I have a theory. If we replace the activation function of an ANN with a polynomial, say cubic one，then after <span class=\"math-container\">$L$</span> layers, the composite polynomial function would have degree <span class=\"math-container\">$3^L$</span>.  In other words, the degree of the total network grows exponentially. In other words, its \"complexity\" measured by the number of zero-crossings, grows exponentially. This seems to remain true if the activation function is sigmoid, but it involves the calculation of the \"topological degree\" (a.k.a. mapping degree theory), which I have not the time to do yet.</p>\n\n<p>According to my above theory, the VC dimension (roughly analogous to the zero-crossings) grows exponentially as we add layers to the ANN, but it cannot catch up with the <em>doubly</em> exponential growth of Boolean functions. So the ANN can only represent a fraction of all possible Boolean functions, and this fraction even diminishes exponentially. That's my current conjecture.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "terminology",
      "applications",
      "topology"
    ],
    "owner": {
      "account_id": 4146639,
      "reputation": 6209,
      "user_id": 1671,
      "user_type": "registered",
      "accept_rate": 36,
      "profile_image": "https://i.sstatic.net/iE1hj.png?s=256",
      "display_name": "DukeZhou",
      "link": "https://ai.stackexchange.com/users/1671/dukezhou"
    },
    "is_answered": true,
    "view_count": 2872,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1715370062,
    "creation_date": 1533760275,
    "last_edit_date": 1639405536,
    "question_id": 7488,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7488/in-what-ways-is-the-term-topology-applied-to-artificial-intelligence",
    "title": "In what ways is the term &quot;topology&quot; applied to Artificial Intelligence?",
    "body": "<p>I have only a general understanding of General Topology, and want to understand the scope of the term &quot;topology&quot; in relation to the field of Artificial Intelligence.</p>\n<p>In what ways are topological structure and analysis applied in Artificial Intelligence?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "philosophy",
      "turing-test"
    ],
    "owner": {
      "account_id": 9871925,
      "reputation": 81,
      "user_id": 17460,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b9aab14ee3058f13c9a205232361fd12?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "steve",
      "link": "https://ai.stackexchange.com/users/17460/steve"
    },
    "is_answered": true,
    "view_count": 725,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1573145065,
    "creation_date": 1534185839,
    "last_edit_date": 1573145065,
    "question_id": 7548,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7548/can-machine-learning-be-used-to-pass-the-turing-test",
    "title": "Can machine learning be used to pass the Turing test?",
    "body": "<p>Can we say that the Turing test aims to develop machines or methods to reach human-level performance in all cognitive tasks and that machine learning is one of these methods that can pass the Turing test?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "deep-neural-networks",
      "symbolic-ai"
    ],
    "owner": {
      "account_id": 388787,
      "reputation": 9442,
      "user_id": 16909,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/SYD9O.jpg?s=256",
      "display_name": "John Doucette",
      "link": "https://ai.stackexchange.com/users/16909/john-doucette"
    },
    "is_answered": true,
    "view_count": 655,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1610970115,
    "creation_date": 1534521613,
    "last_edit_date": 1610969701,
    "question_id": 7617,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7617/what-kinds-of-problems-can-ai-solve-without-using-a-deep-neural-network",
    "title": "What kinds of problems can AI solve without using a deep neural network?",
    "body": "<p>A lot of questions on this site seem to be asking &quot;can I use X to solve Y?&quot;, where X is usually a deep neural network, and Y is often something already addressed by other areas of AI that are less well known?</p>\n<p>I have some ideas about this, but am inspired by questions like <a href=\"https://bicycles.stackexchange.com/q/1195\">this one</a> where a fairly wide range of views are expressed, and each answer focuses on just one possible problem domain.</p>\n<p>There are some related questions on this stack already, but they are not the same. <a href=\"https://ai.stackexchange.com/q/240/2444\">This question</a> specifically asks what genetic algorithms are good for, whereas I am more interested in having an inventory of problems mapped to possible techniques. <a href=\"https://ai.stackexchange.com/q/60/2444\">This question</a> asks what possible barriers are to AI with a focus on machine learning approaches, but I am interested in what we <em>can</em> do without using deep neural nets, rather than what is difficult in general.</p>\n<p>A good answer will be supported with citations to the academic literature, and a brief description of both the problem and the main approaches that are used.</p>\n<p>Finally, <a href=\"https://ai.stackexchange.com/q/2999/2444\">this question</a> asks what AI can do to solve problems related to climate change. I'm not interested in the ability to address specific application domains. Instead, I want to see a catalog of abstract problems (e.g. having an agent learn to navigate in a new environment; reasoning strategically about how others might act; interpreting emotions), mapped to useful techniques for those problems. That is, &quot;solving chess&quot; isn't a problem, but &quot;determining how to optimally play turn-based games without randomness&quot; is.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "terminology",
      "computational-learning-theory",
      "hypothesis-class",
      "capacity"
    ],
    "owner": {
      "account_id": 14206211,
      "reputation": 83,
      "user_id": 17664,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/339397e23b4e4cc78aa36e2b126252c7?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Qwarzix",
      "link": "https://ai.stackexchange.com/users/17664/qwarzix"
    },
    "is_answered": true,
    "view_count": 4195,
    "accepted_answer_id": 7686,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1611331148,
    "creation_date": 1534996124,
    "last_edit_date": 1611331148,
    "question_id": 7683,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7683/what-is-the-difference-between-hypothesis-space-and-representational-capacity",
    "title": "What is the difference between hypothesis space and representational capacity?",
    "body": "<p>I am reading <a href=\"http://www.deeplearningbook.org\" rel=\"nofollow noreferrer\">Goodfellow et al Deeplearning Book</a>. I found it difficult to understand the difference between the definition of the hypothesis space and representation capacity of a model. </p>\n\n<p>In <a href=\"http://www.deeplearningbook.org/contents/ml.html\" rel=\"nofollow noreferrer\">Chapter 5</a>, it is written about hypothesis space:</p>\n\n<blockquote>\n  <p>One way to control the capacity of a learning algorithm is by choosing its hypothesis space, the set of functions that the learning algorithm is allowed to select as being the solution.</p>\n</blockquote>\n\n<p>And about representational capacity:</p>\n\n<blockquote>\n  <p>The model speciﬁes which family of functions the learning algorithm can choose from when varying the parameters in order to reduce a training objective. This is called the representational capacity of the model.</p>\n</blockquote>\n\n<p>If we take the linear regression model as an example and allow our output <span class=\"math-container\">$y$</span> to takes polynomial inputs, I understand the hypothesis space as the ensemble of quadratic functions taking input <span class=\"math-container\">$x$</span>, i.e <span class=\"math-container\">$y = a_0 + a_1x + a_2x^2$</span>.</p>\n\n<p>How is it different from the definition of the representational capacity, where parameters are <span class=\"math-container\">$a_0$</span>, <span class=\"math-container\">$a_1$</span> and <span class=\"math-container\">$a_2$</span>?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "deep-rl",
      "ddpg",
      "action-spaces"
    ],
    "owner": {
      "account_id": 12645617,
      "reputation": 433,
      "user_id": 17706,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/10213092679404905/picture?type=large",
      "display_name": "Rui Nian",
      "link": "https://ai.stackexchange.com/users/17706/rui-nian"
    },
    "is_answered": false,
    "view_count": 1169,
    "answer_count": 0,
    "score": 8,
    "last_activity_date": 1616895125,
    "creation_date": 1535145066,
    "last_edit_date": 1616895125,
    "question_id": 7707,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7707/is-there-a-difference-in-the-architecture-of-deep-reinforcement-learning-when-mu",
    "title": "Is there a difference in the architecture of deep reinforcement learning when multiple actions are performed instead of a single action?",
    "body": "<p>I've built a deep deterministic policy gradient reinforcement learning agent to be able to handle any games/tasks that have only one action.  However, the agent seems to fail horribly when there are two or more actions.  I tried to look online for any examples of somebody implementing DDPG on a multiple-action system, but people mostly applied it to the pendulum problem, which is a single-action problem.</p>\n<p>For my current system, it is a 3 state, 2 continuous control actions system (One is to adjust the temperature of the system, the other one adjusts a mechanical position, both are continuous).  However, I froze the second continuous action to be the optimal action all the time.  So RL only has to manipulate one action.  It solves within 30 episodes.  However, the moment I allow the RL to try both continuous actions, it doesn't even converge after 1000 episodes.  In fact, it diverges aggressively.  The output of the actor-network seems to always be the max action, possibly because I am using a <code>tanh</code> activation for the actor to provide output constraint.  I added a penalty to large actions, but it does not seem to work for the 2 continuous control action case.</p>\n<p>For my exploratory noise, I used Ornstein-Ulhenbeck noise, with means adjusted for the two different continuous actions.  The mean of the noise is 10% of the mean of the action.</p>\n<p><em>Is there any massive difference between single action and multiple action DDPG?</em></p>\n<p>I changed the reward function to take into account both actions, have tried making a bigger network, tried priority replay, etc., but it appears I am missing something.</p>\n<p><em>Does anyone here have any experience building a multiple-action DDPG and could give me some pointers?</em></p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "dqn",
      "deep-rl",
      "experience-replay"
    ],
    "owner": {
      "account_id": 14229719,
      "reputation": 601,
      "user_id": 17759,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/c5aed56239e848067ec5896e379c4ea5?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "16Aghnar",
      "link": "https://ai.stackexchange.com/users/17759/16aghnar"
    },
    "is_answered": true,
    "view_count": 339,
    "accepted_answer_id": 7902,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1612097884,
    "creation_date": 1536520029,
    "last_edit_date": 1612097884,
    "question_id": 7895,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/7895/is-experience-replay-like-dreaming",
    "title": "Is Experience Replay like dreaming?",
    "body": "<p>Drawing parallels between Machine Learning techniques and a human brain is a dangerous operation. When it is done successfully, it can be a powerful tool for vulgarisation, but when it is done with no precaution, it can lead to major misunderstandings.</p>\n<p>I was recently attending a conference where the speaker described Experience Replay in RL as a way of making the net &quot;dream&quot;.\nI'm wondering how true this assertion is. The speaker argued that a dream is a random addition of memories, just as experience replay. However, I doubt the brain remembers its dream or either learns from it. What is your analysis?</p>\n"
  },
  {
    "tags": [
      "logic",
      "knowledge-representation"
    ],
    "owner": {
      "account_id": 14587275,
      "reputation": 81,
      "user_id": 19193,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/d633babdb02ca50e6b9e5a13329966de?s=256&d=identicon&r=PG",
      "display_name": "mfioah",
      "link": "https://ai.stackexchange.com/users/19193/mfioah"
    },
    "is_answered": false,
    "view_count": 138,
    "answer_count": 0,
    "score": 8,
    "last_activity_date": 1556474576,
    "creation_date": 1540113340,
    "last_edit_date": 1540918849,
    "question_id": 8550,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/8550/what-are-the-current-trends-open-questions-in-logics-for-knowledge-representatio",
    "title": "What are the current trends/open questions in logics for knowledge representation?",
    "body": "<p>What are the future prospects in near future from a theoretical investigation of description logics, and modal logics in the context of artificial intelligence research?</p>\n"
  },
  {
    "tags": [
      "robotics",
      "robots",
      "question-answering",
      "sophia"
    ],
    "owner": {
      "account_id": 14626539,
      "reputation": 323,
      "user_id": 19448,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f19a3ac761df525d20390cf126e6219e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "dua fatima",
      "link": "https://ai.stackexchange.com/users/19448/dua-fatima"
    },
    "is_answered": true,
    "view_count": 1805,
    "accepted_answer_id": 8769,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1611684299,
    "creation_date": 1541319646,
    "last_edit_date": 1611684299,
    "question_id": 8768,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/8768/which-algorithm-is-used-in-the-robot-sophia-to-understand-and-answers-the-questi",
    "title": "Which algorithm is used in the robot Sophia to understand and answers the questions?",
    "body": "<p>Which algorithm is used in the robot Sophia to understand and answer the questions?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "terminology",
      "unsupervised-learning",
      "generative-adversarial-networks",
      "supervised-learning"
    ],
    "owner": {
      "account_id": 5013180,
      "reputation": 107,
      "user_id": 18956,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/36c60f436127e2b8e49486bd34b30324?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "codecracker",
      "link": "https://ai.stackexchange.com/users/18956/codecracker"
    },
    "is_answered": true,
    "view_count": 4413,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1596327048,
    "creation_date": 1541817502,
    "last_edit_date": 1583443168,
    "question_id": 8894,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/8894/do-gans-come-under-supervised-learning-or-unsupervised-learning",
    "title": "Do GANs come under supervised learning or unsupervised learning?",
    "body": "<p>Do GANs come under supervised learning or unsupervised learning?</p>\n\n<p>My guess is that they come under supervised learning, as we have labeled dataset of images, but I am not sure as there might be other aspects in GANs which might come into play in the determination of the class of algorithms GAN falls under.</p>\n"
  },
  {
    "tags": [
      "genetic-algorithms",
      "evolutionary-algorithms",
      "fitness-functions",
      "fitness-design"
    ],
    "owner": {
      "account_id": 14414473,
      "reputation": 576,
      "user_id": 19254,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8821714658294e01ba440fe13801bc2e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Abbas Ali",
      "link": "https://ai.stackexchange.com/users/19254/abbas-ali"
    },
    "is_answered": true,
    "view_count": 10370,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1611974194,
    "creation_date": 1542862415,
    "last_edit_date": 1611974194,
    "question_id": 9105,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/9105/how-to-create-a-good-fitness-function",
    "title": "How to create a good fitness function?",
    "body": "<p>In genetic algorithms, a function called \"fitness\" (or \"evaluation\") function is used to determine the \"fitness\" of the chromosomes. Creating a good fitness function is one of the challenging tasks in genetic algorithms. How would you create a good fitness function?</p>\n"
  },
  {
    "tags": [
      "machine-learning"
    ],
    "owner": {
      "account_id": 14710143,
      "reputation": 163,
      "user_id": 19886,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/30a20cf1541b5d3d17640b6be925d92c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Adil Mustafa",
      "link": "https://ai.stackexchange.com/users/19886/adil-mustafa"
    },
    "is_answered": true,
    "view_count": 5640,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1561382568,
    "creation_date": 1543689180,
    "last_edit_date": 1561382404,
    "question_id": 9305,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/9305/which-problems-in-ai-are-not-machine-learning",
    "title": "Which problems in AI are not machine learning?",
    "body": "<p>Which problems in AI are not machine learning? Which problems involve both AI and machine learning?</p>\n"
  },
  {
    "tags": [
      "game-ai",
      "monte-carlo-tree-search",
      "minimax"
    ],
    "owner": {
      "account_id": 10506266,
      "reputation": 511,
      "user_id": 16917,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/6d94b5dba7e8e85aaa7f9becf6886e62?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Inertial Ignorance",
      "link": "https://ai.stackexchange.com/users/16917/inertial-ignorance"
    },
    "is_answered": true,
    "view_count": 2091,
    "accepted_answer_id": 9516,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1557696754,
    "creation_date": 1544675591,
    "last_edit_date": 1557696754,
    "question_id": 9504,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/9504/any-interesting-ways-to-combine-monte-carlo-tree-search-with-the-minimax-algorit",
    "title": "Any interesting ways to combine Monte Carlo tree search with the minimax algorithm?",
    "body": "<p>I've been working on a game-playing engine for about half a year now, and it uses the well known algorithms. These include minimax with alpha-beta pruning, iterative deepening, transposition tables, etc.</p>\n\n<p>I'm now looking for a way to include Monte Carlo tree search, which is something I've wanted to do for a long time. I was thinking of just making a new engine from scratch, but if possible I'd like to somehow import MC tree search into the engine I've already built.</p>\n\n<p>Are there any interesting strategies to import MC tree search into a standard game-playing AI?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "policy-gradients",
      "reinforce",
      "return",
      "td-lambda"
    ],
    "owner": {
      "account_id": 13069833,
      "reputation": 83,
      "user_id": 21518,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/33d60873c647ca89badecc9f4afa7c92?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "jhinGhin",
      "link": "https://ai.stackexchange.com/users/21518/jhinghin"
    },
    "is_answered": true,
    "view_count": 2333,
    "accepted_answer_id": 10061,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1642075436,
    "creation_date": 1547753267,
    "last_edit_date": 1642075436,
    "question_id": 10049,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10049/why-are-lambda-returns-so-rarely-used-in-policy-gradients",
    "title": "Why are lambda returns so rarely used in policy gradients?",
    "body": "<p>I've seen the Monte Carlo return <span class=\"math-container\">$G_{t}$</span> being used in REINFORCE and the TD(<span class=\"math-container\">$0$</span>) target <span class=\"math-container\">$r_t + \\gamma Q(s', a')$</span> in vanilla actor-critic. However, I've never seen someone use the lambda return <span class=\"math-container\">$G^{\\lambda}_{t}$</span> in these situations, nor in any other algorithms.</p>\n<p>Is there a specific reason for this? Could there be performance improvements if we used <span class=\"math-container\">$G^{\\lambda}_{t}$</span>?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "deep-rl",
      "reward-functions",
      "reward-design",
      "algorithmic-trading"
    ],
    "owner": {
      "account_id": 15064763,
      "reputation": 189,
      "user_id": 21539,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/04a121d2dc3e955172c193e7c4d8b47e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "fgauth",
      "link": "https://ai.stackexchange.com/users/21539/fgauth"
    },
    "is_answered": true,
    "view_count": 7313,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1639870257,
    "creation_date": 1547945084,
    "last_edit_date": 1639870257,
    "question_id": 10082,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10082/suitable-reward-function-for-trading-buy-and-sell-orders",
    "title": "Suitable reward function for trading buy and sell orders",
    "body": "<p>I am working to build a deep reinforcement learning agent which can place orders (i.e. limit buy and limit sell orders). The actions are <code>{&quot;Buy&quot;: 0 , &quot;Do Nothing&quot;: 1, &quot;Sell&quot;: 2}</code>.</p>\n<p>Suppose that all the features are well suited for this task. I wanted to use just the standard &quot;Profit &amp; Loss&quot; as a reward, but I hardly thought to get something similar to the above image. The standard P&amp;L will simply place the pair (limit buy order, limit sell order) on every up movement. I don't want that because very often it won't cover the commission and it is not a good indicator to trade manually. I would be interested that the agent can maximize the profit and give me a minimum profit of $100 on every pair (limit buy order, limit sell order).</p>\n<p>I would be interested in something similar to the picture below.</p>\n<p><a href=\"https://i.sstatic.net/1PlBP.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/1PlBP.png\" alt=\"enter image description here\" /></a></p>\n<p>Is there a reward function that could allow me to get such a result? If so, what is it?</p>\n<p><strong>UPDATE</strong></p>\n<p>Is the following utility function can work with the purpose of that question?</p>\n<p><span class=\"math-container\">$$\nU(x) = \\max(\\\\\\$100, x)\n$$</span></p>\n<p>That seems correct, but I don't know how the agent will be penalized if it covers a wrong transaction, i.e. the pair (limit buy order, limit sell order) creates a loss of money.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "deep-learning",
      "terminology",
      "statistical-ai"
    ],
    "owner": {
      "account_id": 240691,
      "reputation": 236,
      "user_id": 21269,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/wO0sH.jpg?s=256",
      "display_name": "Leo Gallucci",
      "link": "https://ai.stackexchange.com/users/21269/leo-gallucci"
    },
    "is_answered": true,
    "view_count": 8115,
    "accepted_answer_id": 12354,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1666761474,
    "creation_date": 1548760102,
    "last_edit_date": 1625153026,
    "question_id": 10289,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10289/are-neural-networks-statistical-models",
    "title": "Are neural networks statistical models?",
    "body": "<p>By reading the abstract of <a href=\"https://people.orie.cornell.edu/davidr/or474/nn_sas.pdf\" rel=\"nofollow noreferrer\">Neural Networks and Statistical Models</a> paper it would seem that ANNs are statistical models.</p>\n\n<p>In contrast <a href=\"https://towardsdatascience.com/no-machine-learning-is-not-just-glorified-statistics-26d3952234e3\" rel=\"nofollow noreferrer\">Machine Learning is not just glorified Statistics</a>.</p>\n\n<p>I am looking for a more concise/summarized answer with focus on ANNs.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "activation-functions",
      "performance"
    ],
    "owner": {
      "account_id": 84171,
      "reputation": 183,
      "user_id": 15017,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/2RJ8I.png?s=256",
      "display_name": "Pietro",
      "link": "https://ai.stackexchange.com/users/15017/pietro"
    },
    "is_answered": true,
    "view_count": 881,
    "accepted_answer_id": 11297,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1552941773,
    "creation_date": 1549366469,
    "last_edit_date": 1552941773,
    "question_id": 10403,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10403/why-isnt-the-elliotsig-activation-function-widely-used",
    "title": "Why isn&#39;t the ElliotSig activation function widely used?",
    "body": "<p>The Softsign (a.k.a. ElliotSig) activation function is really simple: </p>\n\n<p><span class=\"math-container\">$$ f(x) = \\frac{x}{1+|x|} $$</span></p>\n\n<p>It is bounded <span class=\"math-container\">$[-1,1]$</span>, has a first derivative, it is monotonic, and it is computationally extremely simple (easy for, e.g., a GPU).</p>\n\n<p>Why it is not widely used in neural networks? Is it because it is not infinitely derivable?</p>\n"
  },
  {
    "tags": [
      "applications",
      "probability-distribution",
      "probability-theory",
      "bayesian-networks"
    ],
    "owner": {
      "account_id": 3219679,
      "reputation": 181,
      "user_id": 22433,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f19a3ac761df525d20390cf126e6219e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Sebastian Dine",
      "link": "https://ai.stackexchange.com/users/22433/sebastian-dine"
    },
    "is_answered": true,
    "view_count": 826,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1691968131,
    "creation_date": 1550490826,
    "last_edit_date": 1639386034,
    "question_id": 10649,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/10649/what-are-the-main-benefits-of-using-bayesian-networks",
    "title": "What are the main benefits of using Bayesian networks?",
    "body": "<p>I have some trouble understanding the benefits of Bayesian networks.</p>\n<p>Am I correct that the key benefit of the network is that one does not need to use the chain rule of probability in order to calculate joint distributions?</p>\n<p>So, using the chain rule:</p>\n<p><span class=\"math-container\">$$\nP(A_1, \\dots, A_n) = \\prod_{i=1}^n (A_i \\mid \\cap_{j=1}^{i-1} A_j)\n$$</span></p>\n<p>leads to the same result as the following (assuming the nodes are structured by a Bayesian network)?</p>\n<p><span class=\"math-container\">$$\nP(A_1, \\dots, A_n) = \\prod_{i=1}^n P(A_i \\mid \\text{parents}(A_i))\n$$</span></p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "unsupervised-learning",
      "time-series",
      "anomaly-detection"
    ],
    "owner": {
      "account_id": 15435838,
      "reputation": 225,
      "user_id": 23380,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0df1ca35b3f22b3efebfa579ffa719c5?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "some_programmer",
      "link": "https://ai.stackexchange.com/users/23380/some-programmer"
    },
    "is_answered": true,
    "view_count": 305,
    "accepted_answer_id": 11391,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1623003227,
    "creation_date": 1553251548,
    "last_edit_date": 1623003227,
    "question_id": 11374,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11374/which-unsupervised-learning-technique-can-be-used-for-anomaly-detection-in-a-tim",
    "title": "Which unsupervised learning technique can be used for anomaly detection in a time series?",
    "body": "<p>I've started working on anomaly detection in Python. My dataset is a time series one. The data is being collected by some sensors which record and collect data on semiconductor-making machines.</p>\n<p>My dataset looks like this:</p>\n<pre><code>ContextID   Time_ms Ar_Flow_sccm    BacksGas_Flow_sccm\n7289973 09:12:48.502    49.56054688 1.953125\n7289973 09:12:48.603    49.56054688 2.05078125\n7289973 09:12:48.934    99.85351563 2.05078125\n7289973 09:12:49.924    351.3183594 2.05078125\n7289973 09:12:50.924    382.8125    1.953125\n7289973 09:12:51.924    382.8125    1.7578125\n7289973 09:12:52.934    382.8125    1.7578125\n7289999 09:15:36.434    50.04882813 1.7578125\n7289999 09:15:36.654    50.04882813 1.7578125\n7289999 09:15:36.820    50.04882813 1.66015625\n7289999 09:15:37.904    333.2519531 1.85546875\n7289999 09:15:38.924    377.1972656 1.953125\n7289999 09:15:39.994    377.1972656 1.7578125\n7289999 09:15:41.94     388.671875  1.85546875\n7289999 09:15:42.136    388.671875  1.85546875\n7290025 09:18:00.429    381.5917969 1.85546875\n7290025 09:18:01.448    381.5917969 1.85546875\n7290025 09:18:02.488    381.5917969 1.953125\n7290025 09:18:03.549    381.5917969 14.453125\n7290025 09:18:04.589    381.5917969 46.77734375\n</code></pre>\n<p>What I have to do is to apply some unsupervised learning technique on each and every parameter column individually and find any anomalies that might exist in there. The <code>ContextID</code> is more like a product number.</p>\n<p>I would like to know which unsupervised learning techniques can be used for this kind of task at hand since the problem is a bit unique:</p>\n<ol>\n<li>It has temporal values.</li>\n<li>Since it has temporal values, each product will have many (similar or different) values as can be seen in the dataset above.</li>\n</ol>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "quality-control"
    ],
    "owner": {
      "account_id": 301392,
      "reputation": 181,
      "user_id": 23432,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/acd291d2a2f923745ed916b35356f1eb?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "PhD",
      "link": "https://ai.stackexchange.com/users/23432/phd"
    },
    "is_answered": true,
    "view_count": 427,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1553527155,
    "creation_date": 1553456240,
    "question_id": 11427,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11427/are-there-existing-examples-of-using-neural-networks-for-static-code-analysis",
    "title": "Are there existing examples of using neural networks for static code analysis?",
    "body": "<p><strong>Background Context:</strong></p>\n\n<p>In the past I've heavily applied various \"code quality metrics\" to statically analyze code to provide an inkling of how \"maintainable\" it is and using things like the Maintainability Index alluded to <a href=\"https://quandarypeak.com/2015/02/measuring-software-maintainability/\" rel=\"noreferrer\">here</a>.</p>\n\n<p>However, a problem that I face is whether a language has libraries that effectively measure such metrics - only then is it usable else it's rather subjective/arbitrary. Given the plethora of languages that one has to deal with in an enterprise system, this is can get rather unwieldy. </p>\n\n<p><strong>Proposed Idea:</strong></p>\n\n<p>Build and train an Artificial Neural Network that \"ingests a folder of code\" (i.e., all files within that folder/package are assumed to house the \"project\" whose quality metrics we'd like to compute). This may again be language dependent but let's assume it exists for a language that I'm having the hardest time with (for measuring \"maintainability\"): Scala.</p>\n\n<p>Using numeric metrics like <code>McCabe's complexity</code> or <code>Cyclomatic complexity</code> maybe \"convention\" but are not entirely relevant. Few things like class/method length are almost always relevant no matter the language. Thus, providing a few \"numeric metrics\" + abstract notion of readability by subjective evaluation to train an ANN would be a good balance of \"inputs\" to the ANN. The output being either a classification of maintainability like <code>low, medium, high</code> etc., or a number between <code>0 and 1</code>.</p>\n\n<p><strong>Question:</strong></p>\n\n<p>Has this been tried and are there any references? I spent some time digging via Google Scholar but didn't find anything \"usable\" or worthwhile. It's okay if it's not Scala, but have ANNs been used for measuring code quality (i.e., static analysis) and what are the benefits or disadvantages of something like this? </p>\n\n<p><em>PS: Hopefully, the question isn't too broad, but if so, please let me know in the comments and I'll try make it as specific as possible.</em></p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "reinforcement-learning",
      "terminology",
      "deep-rl"
    ],
    "owner": {
      "account_id": 12870553,
      "reputation": 361,
      "user_id": 17312,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d9a78ba42a5548d804d23cf802fc284e?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "yewang",
      "link": "https://ai.stackexchange.com/users/17312/yewang"
    },
    "is_answered": true,
    "view_count": 1177,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1593608872,
    "creation_date": 1553923864,
    "last_edit_date": 1593608748,
    "question_id": 11539,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11539/is-reinforcement-learning-using-shallow-neural-networks-still-deep-reinforcement",
    "title": "Is reinforcement learning using shallow neural networks still deep reinforcement learning?",
    "body": "<p>Often times I see the term deep reinforcement learning to refer to RL algorithms that use neural networks, regardless of whether or not the networks are deep.</p>\n<p>For example, <a href=\"https://arxiv.org/pdf/1707.06347.pdf\" rel=\"nofollow noreferrer\">PPO</a> is often considered a deep RL algorithm, but using a deep network is not really part of the algorithm. In fact, the example they report in the paper says that they used a network with only 2 layers.</p>\n<p>This SIGGRAPH project (<a href=\"https://xbpeng.github.io/projects/DeepMimic/2018_TOG_DeepMimic.pdf\" rel=\"nofollow noreferrer\">DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills</a>)  has the name <em>deep</em> in it and the title even says 'deep reinforcement learning', but if you read the paper, you'll see that their network uses only 2 layers.</p>\n<p>Again, the paper <a href=\"https://arxiv.org/pdf/1812.11103.pdf\" rel=\"nofollow noreferrer\">Learning to Walk via Deep Reinforcement Learning</a> by researchers from Google and Berkeley, contains <em>deep RL</em> in the title, but if you read the paper, you'll see they used 2 hidden layers.</p>\n<p><a href=\"https://www.cc.gatech.edu/%7Eaclegg3/projects/learning-dress-synthesizing.pdf\" rel=\"nofollow noreferrer\">Another SIGGRAPH project with deep RL in the title</a>. And, if you read it, surprise, 2 hidden layers.</p>\n<p>In the paper <a href=\"https://arxiv.org/pdf/1801.01290.pdf\" rel=\"nofollow noreferrer\">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a>, if you read table 1 with the hyperparameters, they also used 2 hidden layers.</p>\n<p>Is it standard to just call deep RL to any RL algorithm that uses a neural net?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "q-learning",
      "dqn",
      "deep-rl",
      "double-dqn"
    ],
    "owner": {
      "account_id": 15339894,
      "reputation": 211,
      "user_id": 22930,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/o4muU.jpg?s=256",
      "display_name": "Angelo",
      "link": "https://ai.stackexchange.com/users/22930/angelo"
    },
    "is_answered": true,
    "view_count": 1931,
    "accepted_answer_id": 11730,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1611592114,
    "creation_date": 1554714496,
    "last_edit_date": 1604524745,
    "question_id": 11716,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11716/can-dqn-perform-better-than-double-dqn",
    "title": "Can DQN perform better than Double DQN?",
    "body": "<p>I'm training both DQN and double DQN in the same environment, but DQN performs significantly better than double DQN. As I've seen <a href=\"https://arxiv.org/pdf/1509.06461.pdf\" rel=\"nofollow noreferrer\">in the double DQN paper</a>, double DQN should perform better than DQN. Am I doing something wrong or is it possible?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "reinforcement-learning",
      "monte-carlo-tree-search",
      "alphazero"
    ],
    "owner": {
      "account_id": 476134,
      "reputation": 183,
      "user_id": 23910,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d3b4ee9bc3b69059b76a8dd3b323d237?s=256&d=identicon&r=PG",
      "display_name": "Jonathan Lindgren",
      "link": "https://ai.stackexchange.com/users/23910/jonathan-lindgren"
    },
    "is_answered": true,
    "view_count": 732,
    "accepted_answer_id": 11791,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1560183124,
    "creation_date": 1555069330,
    "question_id": 11787,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11787/how-can-alpha-zero-learn-if-the-tree-search-stops-and-restarts-before-finishing",
    "title": "How can alpha zero learn if the tree search stops and restarts before finishing a game?",
    "body": "<p>I am trying to understand how alpha zero works, but there is one point that I have problems understanding, even after reading several different explanations. As I understand it (see for example <a href=\"https://applied-data.science/static/main/res/alpha_go_zero_cheat_sheet.png\" rel=\"noreferrer\">https://applied-data.science/static/main/res/alpha_go_zero_cheat_sheet.png</a>), alpha zero does not perform rollouts. So instead of finishing a game, it stops when it hits an unknown state, uses the neural network to compute probabilities for different actions as well as the value of this state (\"probability of winning\"), and then propagates the new value up the tree.</p>\n\n<p>The reasoning is that this is much cheaper, since actually completing the game would take more time then just letting the neural network guess the value of a state.</p>\n\n<p>However, this requires that the neural network is decent at predicting the value of a state. But in the beginning of training, obviously it will be bad at this. Moreover, since the monte carlo tree search stops as soon as it hits a new state, and the number of different game states is very large, it seems to me that the simulation will rarely manage to complete a game. And for sure, the neural network can not improve unless it actually completes a significant number of games, because that is only real feedback that tells the agent if it is doing good or bad moves.</p>\n\n<p>What am I missing here?</p>\n\n<p>The only plausible explanation I can come up with is:  If the neural network would be essentially random in the beginning, well then for sure the large number of game states would prevent the tree search from ever finishing if it restarts as soon as it hits a previously unknown game state, so this can not be the case. So perhaps, maybe even if the neural network is bad in the beginning, it will not be very \"random\", but still be quite biased towards some paths. This would mean that the search would be biased to some smaller set of states among the vast number of different game states, and thus it would tend to take the same path more than once and be able to complete some games and get feedback. Is this \"resolution\" correct?</p>\n\n<p>One problem I have though with the above \"resolution\", is that according to the algorithm, it should favor exploration in the beginning, so it seems that in the beginning it will be biased towards choosing previously not taken actions. This makes it even more seem like the tree search will never be able to complete a game and thus the neural net would not learn.</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "policy-gradients",
      "sutton-barto",
      "notation",
      "reinforce"
    ],
    "owner": {
      "account_id": 11149226,
      "reputation": 519,
      "user_id": 16343,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/46daeea714775ee1509c4a647eb6c9a0?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Hanzy",
      "link": "https://ai.stackexchange.com/users/16343/hanzy"
    },
    "is_answered": true,
    "view_count": 2435,
    "accepted_answer_id": 11931,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1555910251,
    "creation_date": 1555874613,
    "last_edit_date": 1555905104,
    "question_id": 11929,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/11929/how-is-the-policy-gradient-calculated-in-reinforce",
    "title": "How is the policy gradient calculated in REINFORCE?",
    "body": "<p>Reading Sutton and Barto, I see the following in describing policy gradients:</p>\n\n<p><a href=\"https://i.sstatic.net/LnCdQ.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/LnCdQ.jpg\" alt=\"policy grad \"></a></p>\n\n<p>How is the gradient calculated with respect to an action (taken at time t)? I've read implementations of the algorithm, but conceptually I'm not sure I understand how the gradient is computed, since we need some loss function to compute the gradient. </p>\n\n<p>I've seen a good PyTorch article, but I still don't understand the meaning of this gradient conceptually, and I don't know what I'm looking to implement. Any intuition that you could provide would be helpful. </p>\n"
  },
  {
    "tags": [
      "objective-functions",
      "activation-functions"
    ],
    "owner": {
      "account_id": 2876745,
      "reputation": 181,
      "user_id": 25305,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/aa71da9d4d50f99fc9c5cdbc801af824?s=256&d=identicon&r=PG",
      "display_name": "user1024",
      "link": "https://ai.stackexchange.com/users/25305/user1024"
    },
    "is_answered": true,
    "view_count": 1515,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1580011475,
    "creation_date": 1556638599,
    "question_id": 12068,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12068/whats-the-advantage-of-log-softmax-over-softmax",
    "title": "What&#39;s the advantage of log_softmax over softmax?",
    "body": "<p>Previously I have learned that the <code>softmax</code> as the output layer coupled with the <code>log-likelihood cost function</code> (the same as the the <code>nll_loss</code> in pytorch) can solve <a href=\"http://neuralnetworksanddeeplearning.com/chap3.html#softmax\" rel=\"noreferrer\">the learning slowdown problem</a>.</p>\n\n<p>However, while I am learning the pytorch mnist tutorial, I'm confused that why the combination of the <code>log_softmax</code> as the output layer and the <code>nll_loss</code>(the negative log likelihood loss) as the loss function was used (<a href=\"https://github.com/pytorch/examples/blob/master/mnist/main.py#L26\" rel=\"noreferrer\">L26</a> and <a href=\"https://github.com/pytorch/examples/blob/master/mnist/main.py#L34\" rel=\"noreferrer\">L34</a>).</p>\n\n<p>I found that when <code>log_softmax</code>+<code>nll_loss</code> was used, the test accuracy was 99%, while when <code>softmax</code>+<code>nll_loss</code> was used, the test accuracy was 97%.</p>\n\n<p>I'm confused that what's the advantage of <code>log_softmax</code> over <code>softmax</code>? How can we explain the performance gap between them? Is <code>log_softmax</code>+<code>nll_loss</code> always better than <code>softmax</code>+<code>nll_loss</code>?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "reference-request",
      "function-approximation",
      "universal-approximation-theorems"
    ],
    "owner": {
      "account_id": 4148701,
      "reputation": 42397,
      "user_id": 2444,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e6fce047cba96a601a9dac66df4cd2e8?s=256&d=identicon&r=PG",
      "display_name": "nbro",
      "link": "https://ai.stackexchange.com/users/2444/nbro"
    },
    "is_answered": true,
    "view_count": 2669,
    "accepted_answer_id": 12241,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1598808481,
    "creation_date": 1557490531,
    "last_edit_date": 1592306782,
    "question_id": 12239,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12239/which-machine-learning-models-are-universal-function-approximators",
    "title": "Which machine learning models are universal function approximators?",
    "body": "<p>The <a href=\"https://en.wikipedia.org/wiki/Universal_approximation_theorem\" rel=\"noreferrer\">universal approximation theorem</a> states that a <a href=\"https://en.wikipedia.org/wiki/Feedforward_neural_network\" rel=\"noreferrer\">feed-forward neural network </a> with a single hidden layer containing a finite number of neurons can approximate any continuous function (provided some assumptions on the activation function are met).</p>\n\n<p>Is there any other machine learning model (apart from any neural network model) that has been <strong>proved</strong> to be an <em>universal</em> function approximator (and that is potentially comparable to neural networks, in terms of usefulness and applicability)? If yes, can you provide a link to a research paper or book that shows the proof?</p>\n\n<p>Similar questions have been asked in the past in other places (e.g. <a href=\"https://qr.ae/TWI6KL\" rel=\"noreferrer\">here</a>, <a href=\"https://qr.ae/TWI62P\" rel=\"noreferrer\">here</a> and <a href=\"https://cstheory.stackexchange.com/q/7894/34637\">here</a>), but they do not provide links to papers or books that show the proofs.</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "comparison",
      "policies",
      "deterministic-policy",
      "stochastic-policy"
    ],
    "owner": {
      "account_id": 4148701,
      "reputation": 42397,
      "user_id": 2444,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e6fce047cba96a601a9dac66df4cd2e8?s=256&d=identicon&r=PG",
      "display_name": "nbro",
      "link": "https://ai.stackexchange.com/users/2444/nbro"
    },
    "is_answered": true,
    "view_count": 17740,
    "protected_date": 1588772235,
    "accepted_answer_id": 12275,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1624970518,
    "creation_date": 1557687032,
    "last_edit_date": 1557687563,
    "question_id": 12274,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12274/what-is-the-difference-between-a-stochastic-and-a-deterministic-policy",
    "title": "What is the difference between a stochastic and a deterministic policy?",
    "body": "<p>In reinforcement learning, there are the concepts of stochastic (or probabilistic) and deterministic policies. What is the difference between them?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "actor-critic-methods",
      "exploration-exploitation-tradeoff"
    ],
    "owner": {
      "account_id": 14714527,
      "reputation": 93,
      "user_id": 25732,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-sDtyrtJBNfI/AAAAAAAAAAI/AAAAAAAACjY/MIgtr40PyBE/s256-rj/photo.jpg",
      "display_name": "David Rein",
      "link": "https://ai.stackexchange.com/users/25732/david-rein"
    },
    "is_answered": true,
    "view_count": 798,
    "accepted_answer_id": 12406,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1588532636,
    "creation_date": 1558220863,
    "last_edit_date": 1588532636,
    "question_id": 12402,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/12402/what-is-the-purpose-of-the-actor-in-actor-critic-algorithms",
    "title": "What is the purpose of the actor in actor-critic algorithms?",
    "body": "<p>For discrete action spaces, what is the purpose of the actor in actor-critic algorithms? </p>\n\n<p>My current understanding is that the critic estimates the future reward given an action, so why not just take the action that maximizes the estimated return?</p>\n\n<p>My initial guess at the answer is the exploration-exploitation problem, but are there other, more important/deeper reasons? Or am I underestimating the importance of exploration vs. exploitation as an issue?</p>\n\n<p>It just seems to me that if you can accurately estimate the value function, then you have solved the RL challenge. </p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "comparison",
      "policies",
      "stationary-policy"
    ],
    "owner": {
      "account_id": 4148701,
      "reputation": 42397,
      "user_id": 2444,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e6fce047cba96a601a9dac66df4cd2e8?s=256&d=identicon&r=PG",
      "display_name": "nbro",
      "link": "https://ai.stackexchange.com/users/2444/nbro"
    },
    "is_answered": true,
    "view_count": 9644,
    "accepted_answer_id": 15429,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1614212257,
    "creation_date": 1561648463,
    "question_id": 13088,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/13088/what-is-the-difference-between-a-stationary-and-a-non-stationary-policy",
    "title": "What is the difference between a stationary and a non-stationary policy?",
    "body": "<p>In reinforcement learning, there are <a href=\"https://ai.stackexchange.com/q/12274/2444\">deterministic and non-deterministic (or stochastic) policies</a>, but there are also <em>stationary</em> and <em>non-stationary</em> policies. </p>\n\n<p>What is the difference between a stationary and a non-stationary policy? How do you formalize both? Which problems (or environments) require a stationary policy as opposed to a non-stationary one (and vice-versa)?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "classification",
      "incremental-learning"
    ],
    "owner": {
      "account_id": 16160688,
      "reputation": 91,
      "user_id": 26522,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-5JCjQGB9zVs/AAAAAAAAAAI/AAAAAAAAAXU/yMk_sMOjRbo/s256-rj/photo.jpg",
      "display_name": "neomatriciel",
      "link": "https://ai.stackexchange.com/users/26522/neomatriciel"
    },
    "is_answered": true,
    "view_count": 4909,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1625672296,
    "creation_date": 1562193658,
    "last_edit_date": 1564836765,
    "question_id": 13194,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/13194/can-i-train-a-neural-network-incrementally-given-new-daily-data",
    "title": "Can I train a neural network incrementally given new daily data?",
    "body": "<p>I would like to know if it was possible to train a neural network on daily new data. Let me explain this more in detail. Let's say you have daily data from 2010 to 2019. You train your NN on all of it, but, from now on, every day in 2019 you get new data. Is it possible to \"append\" the training of the NN or do we need to retrain an entire NN with the data from <span class=\"math-container\">$2010$</span> to <span class=\"math-container\">$2019+n$</span> with <span class=\"math-container\">$n$</span> the day for every new day?</p>\n\n<p>I don't know if it is relevant but my work is on binary classification. </p>\n"
  },
  {
    "tags": [
      "monte-carlo-tree-search"
    ],
    "owner": {
      "account_id": 134625,
      "reputation": 225,
      "user_id": 27530,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/646b6450c8478a483eb5c24ab5150555?s=256&d=identicon&r=PG",
      "display_name": "Jay McCarthy",
      "link": "https://ai.stackexchange.com/users/27530/jay-mccarthy"
    },
    "is_answered": true,
    "view_count": 1176,
    "accepted_answer_id": 13679,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1564611428,
    "creation_date": 1564481651,
    "last_edit_date": 1564611428,
    "question_id": 13671,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/13671/what-is-the-appropriate-way-to-deal-with-multiple-paths-to-same-state-in-mcts",
    "title": "What is the appropriate way to deal with multiple paths to same state in MCTS?",
    "body": "<p>Many games have multiple paths to the same states. What is the appropriate way to deal with this in MCTS? </p>\n\n<p>If the state appears once in the tree, but with multiple parents, then it seems to be difficult to define back propagation: do we only propagate back along the path that got us there \"this\" time? Or do we incorporate the information everywhere? Or maybe along the \"first\" path?</p>\n\n<p>If the state appears once in the tree, but with only one parent, then we ignored one of the paths, but it doesn't matter because by definition this is the same state?</p>\n\n<p>If the state appears twice in the tree, aren't we wasting a lot of resources thinking about it multiple times?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "gradient-descent",
      "hyperparameter-optimization"
    ],
    "owner": {
      "account_id": 14159013,
      "reputation": 123,
      "user_id": 21788,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8cf86c6d0fac1a99e89ce0e29d10f486?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "DaddyMike",
      "link": "https://ai.stackexchange.com/users/21788/daddymike"
    },
    "is_answered": true,
    "view_count": 607,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1693346840,
    "creation_date": 1565974306,
    "last_edit_date": 1566029066,
    "question_id": 14013,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/14013/an-intuitive-explanation-of-adagrad-its-purpose-and-its-formula",
    "title": "An intuitive explanation of Adagrad, its purpose and its formula",
    "body": "<blockquote>\n  <p>It (Adagrad) adapts the learning rate to the parameters, performing smaller updates\n  (i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features.</p>\n</blockquote>\n\n<p>From <a href=\"http://ruder.io/optimizing-gradient-descent/index.html#adagrad\" rel=\"noreferrer\">Sebastian Ruder's Blog</a></p>\n\n<p>If a parameter is associated with an infrequent feature then yes, it is more important to focus on properly adjusting that parameter since it is more decisive in classification problems. But how does making the learning rate higher in this situation help?</p>\n\n<p>If it only changes the size of the movement in the dimension of the parameter (makes it larger) wouldn't that make things even more imprecise? Since the network depends more on those infrequent features, shouldn't adjusting those parameters be done more precisely instead of just faster? The more decisive parameters should have a higher \"slope\", thus why should they also have high learning rates? I must be missing something, but what is it?</p>\n\n<p>Further, in the article, the formula for parameter adjustments with Adagrad is given. Where exactly in that formula do you find the information about the frequency of a parameter? There must be a relationship between the gradients of a parameter and the frequency of features associated with it because it's the gradients that play an important role in the formula. What is that relationship?</p>\n\n<p><strong>TLDR:</strong> I don't understand both the purpose and formula behind Adagrad. What is an intuitive explanation of it that also provides an answer to the questions above, or shows why they are irrelevant?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "reinforcement-learning",
      "q-learning"
    ],
    "owner": {
      "account_id": 13960319,
      "reputation": 91,
      "user_id": 28233,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/48b6e20b7066e7a0d2c8f01fb4840448?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "theonekeyg",
      "link": "https://ai.stackexchange.com/users/28233/theonekeyg"
    },
    "is_answered": true,
    "view_count": 5062,
    "accepted_answer_id": 14160,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1573164232,
    "creation_date": 1566917208,
    "last_edit_date": 1566989535,
    "question_id": 14159,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/14159/what-does-the-symbol-mathbb-e-mean-in-these-equations",
    "title": "What does the symbol $\\mathbb E$ mean in these equations?",
    "body": "<p>I came across some papers that use <span class=\"math-container\">$\\mathbb E$</span> in equations, in particular, this paper: <a href=\"https://arxiv.org/pdf/1511.06581.pdf\" rel=\"noreferrer\">https://arxiv.org/pdf/1511.06581.pdf</a>. Here is some equations from the paper that uses it: </p>\n\n<p><span class=\"math-container\">$Q^\\pi \\left(s,a \\right) = \\mathbb E \\left[R_t|s_t = s, a_t = a, \\pi \\right]$</span> ,</p>\n\n<p><span class=\"math-container\">$V^\\pi \\left(s \\right) = \\mathbb E_{a\\backsim\\pi\\left(s \\right)} \\left[Q^\\pi \\left(s, a\\right) \\right]$</span> ,</p>\n\n<p><span class=\"math-container\">$Q^\\pi \\left(s, a \\right) = \\mathbb E_{s'} \\left[r + \\gamma\\mathbb E_{a'\\backsim\\pi \\left(s' \\right)} \\left[Q^\\pi \\left(s', a' \\right) \\right] | s,a,\\pi \\right]$</span></p>\n\n<p><span class=\"math-container\">$\\nabla_{\\theta_i}L_i\\left(\\theta_i \\right) = \\mathbb E_{s, a, r, s'} \\left[\\left(y_i^{DQN} - Q \\left(s, a; \\theta_i \\right) \\right) \\nabla_{\\theta_i} Q\\left(s, a, \\theta_i \\right) \\right]$</span></p>\n\n<p>Could someone explain to me what is the purpose of <span class=\"math-container\">$\\mathbb E$</span>?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "exploding-gradient-problem"
    ],
    "owner": {
      "account_id": 14970419,
      "reputation": 327,
      "user_id": 29877,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e88c621a41728d38fe52c06490d82358?s=256&d=identicon&r=PG",
      "display_name": "FeedMeInformation",
      "link": "https://ai.stackexchange.com/users/29877/feedmeinformation"
    },
    "is_answered": true,
    "view_count": 3151,
    "accepted_answer_id": 15607,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1633767785,
    "creation_date": 1569271163,
    "last_edit_date": 1633767785,
    "question_id": 15601,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/15601/how-to-deal-with-large-or-nan-neural-networks-weights",
    "title": "How to deal with large (or NaN) neural network&#39;s weights?",
    "body": "<p>My weights go from being between 0 and 1 at initialization to exploding into the tens of thousands in the next iteration. In the 3rd iteration, they become so large that only arrays of nan values are displayed.</p>\n<p>How can I go about fixing this?</p>\n<p>Is it to do with the unstable nature of the sigmoid function, or is one of my equations incorrect during backpropagation which makes my gradients explode?</p>\n<pre><code>import numpy as np\nfrom numpy import exp\nimport matplotlib.pyplot as plt\nimport h5py\n\n# LOAD DATASET\nMNIST_data = h5py.File('data/MNISTdata.hdf5', 'r')\nx_train = np.float32(MNIST_data['x_train'][:])\ny_train = np.int32(np.array(MNIST_data['y_train'][:,0]))\nx_test = np.float32(MNIST_data['x_test'][:])\ny_test = np.int32(np.array(MNIST_data['y_test'][:,0]))\nMNIST_data.close()\n\n##############################################################################\n# PARAMETERS \nnumber_of_digits = 10 # number of outputs\nnx = x_test.shape[1] # number of inputs ... 784 --&gt; 28*28\nny = number_of_digits\nm_train = x_train.shape[0]\nm_test = x_test.shape[0]\nNh = 30 # number of hidden layer nodes\nalpha = 0.001\niterations = 3\n##############################################################################\n# ONE HOT ENCODER - encoding y data into 'one hot encoded'\nlr = np.arange(number_of_digits)\ny_train_one_hot = np.zeros((m_train, number_of_digits))\ny_test_one_hot = np.zeros((m_test, number_of_digits))\nfor i in range(len(y_train_one_hot)):\n  y_train_one_hot[i,:] = (lr==y_train[i].astype(np.int))\nfor i in range(len(y_test_one_hot)):\n  y_test_one_hot[i,:] = (lr==y_test[i].astype(np.int))\n\n# VISUALISE SOME DATA\nfor i in range(5):\n  img = x_train[i].reshape((28,28))\n  plt.imshow(img, cmap='Greys')\n  plt.show()\n\ny_train = np.array([y_train]).T\ny_test = np.array([y_test]).T\n##############################################################################\n# INITIALISE WEIGHTS &amp; BIASES\nparams = { &quot;W1&quot;: np.random.rand(nx, Nh),\n           &quot;b1&quot;: np.zeros((1, Nh)),\n           &quot;W2&quot;: np.random.rand(Nh, ny),\n           &quot;b2&quot;: np.zeros((1, ny))\n          }\n\n# TRAINING\n# activation function\ndef sigmoid(z):\n  return 1/(1+exp(-z))\n\n# derivative of activation function\ndef sigmoid_der(z):\n  return z*(1-z)\n\n# softamx function\ndef softmax(z):\n  return 1/sum(exp(z)) * exp(z)\n\n# softmax derivative is alike to sigmoid\ndef softmax_der(z):\n  return sigmoid_der(z)\n\ndef cross_entropy_error(v,y):\n  return -np.log(v[y])\n\n# forward propagation\ndef forward_prop(X, y, params):\n  outs = {}\n  outs['A0'] = X\n  outs['Z1'] = np.matmul(outs['A0'], params['W1']) + params['b1']\n  outs['A1'] = sigmoid(outs['Z1'])\n  outs['Z2'] = np.matmul(outs['A1'], params['W2']) + params['b2']\n  outs['A2'] = softmax(outs['Z2'])\n  \n  outs['error'] = cross_entropy_error(outs['A2'], y)\n  return outs\n\n# back propagation\ndef back_prop(X, y, params, outs):\n  grads = {}\n  Eo = (y - outs['A2']) * softmax_der(outs['Z2'])\n  Eh = np.matmul(Eo, params['W2'].T) * sigmoid_der(outs['Z1'])\n  dW2 = np.matmul(Eo.T, outs['A1']).T\n  dW1 = np.matmul(Eh.T, X).T\n  db2 = np.sum(Eo,0)\n  db1 = np.sum(Eh,0)\n  \n  grads['dW2'] = dW2\n  grads['dW1'] = dW1\n  grads['db2'] = db2\n  grads['db1'] = db1\n#  print('dW2:',grads['dW2'])\n  return grads\n\n# optimise weights and biases\ndef optimise(X,y,params,grads):\n  params['W2'] -= alpha * grads['dW2']\n  params['W1'] -= alpha * grads['dW1']\n  params['b2'] -= alpha * grads['db2']\n  params['b1'] -= alpha * grads['db1']\n  return \n\n# main\nfor epoch in range(iterations):\n  print(epoch)\n  outs = forward_prop(x_train, y_train, params)\n  grads = back_prop(x_train, y_train, params, outs)\n  optimise(x_train,y_train,params,grads)\n  loss = 1/ny * np.sum(outs['error'])\n  print(loss)\n  \n<span class=\"math-container\">```</span>\n</code></pre>\n"
  },
  {
    "tags": [
      "reference-request",
      "robotics",
      "human-like",
      "robots"
    ],
    "owner": {
      "account_id": 423151,
      "reputation": 181,
      "user_id": 29969,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2fe665d956e0376411350d79379848c9?s=256&d=identicon&r=PG",
      "display_name": "&#201;douard Lopez",
      "link": "https://ai.stackexchange.com/users/29969/%c3%89douard-lopez"
    },
    "is_answered": true,
    "view_count": 227,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1611276167,
    "creation_date": 1569404832,
    "last_edit_date": 1611276125,
    "question_id": 15619,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/15619/how-does-atlas-from-boston-dynamics-have-human-like-movement",
    "title": "How does Atlas from Boston Dynamics have human-like movement?",
    "body": "<p>Discussing the video <a href=\"https://www.youtube.com/watch?time_continue=21&amp;v=_sBBaNYex3E\" rel=\"nofollow noreferrer\">More Parkour Atlas</a>, a friend asked how the robot's movement was so similar to the one from a real human and wondering how this is achieved?</p>\n<p>To my knowledge, this is not something the developer &quot;programmed&quot;, but instead emerged from the learning algorithm.</p>\n<p>Could you provide an overview and some reference on how this is achieved?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "self-organizing-map"
    ],
    "owner": {
      "account_id": 6498272,
      "reputation": 181,
      "user_id": 29975,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/880590242014153/picture?type=large",
      "display_name": "Rafael Santos",
      "link": "https://ai.stackexchange.com/users/29975/rafael-santos"
    },
    "is_answered": false,
    "view_count": 212,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1737644505,
    "creation_date": 1569424405,
    "last_edit_date": 1610613115,
    "question_id": 15624,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/15624/what-is-the-impact-of-using-multiple-bmus-for-self-organizing-maps",
    "title": "What is the impact of using multiple BMUs for self-organizing maps?",
    "body": "<p>Here's a sort of a conceptual question. I was implementing a SOM algorithm to better understand its variations and parameters. I got curious about one bit: the BMU (best matching unit == the neuron that is more similar to the vector being presented) is chosen as the neuron that has the smallest distance in feature space to the vector. Then I updated it and its neighbours.</p>\n<p>This makes sense, but what if I used more than one BMU for updating the network? For example, suppose that the distance to one neuron is 0.03, but there is another neuron with distance 0.04. These are the two smallest distances. I would use the one with 0.03 as the BMU.</p>\n<p>The question is, what would be the expected impacts on the algorithm if I used <em>more</em> than one BMU? For example, I could be selecting for update all neurons for which the distance is up to 5% more than the minimum distance.</p>\n<p>I am not asking for code. I can implement it to see what happens. I am just curious to see if anyone have any insight on the pros and cons (except additional complexity) of this approach.</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "ai-design",
      "multi-agent-systems"
    ],
    "owner": {
      "account_id": 2207011,
      "reputation": 183,
      "user_id": 22943,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/fa5764b5ee7afe97b5a925b24c4fa229?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Alex",
      "link": "https://ai.stackexchange.com/users/22943/alex"
    },
    "is_answered": true,
    "view_count": 681,
    "accepted_answer_id": 15698,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1570071599,
    "creation_date": 1570026762,
    "last_edit_date": 1570071599,
    "question_id": 15693,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/15693/how-would-one-implement-a-multi-agent-environment-with-asynchronous-action-and-r",
    "title": "How would one implement a multi-agent environment with asynchronous action and rewards per agent?",
    "body": "<p>In a single agent environment, the agent takes an action, then observes the next state and reward:</p>\n\n<pre><code>for ep in num_episodes:\n    action = dqn.select_action(state)\n    next_state, reward = env.step(action)\n</code></pre>\n\n<p>Implicitly, the for moving the simulation (env) forward is embedded inside the env.step() function.  </p>\n\n<p>Now in the multiagent scenario, agent 1 (<span class=\"math-container\">$a_1$</span>) has to make a decision at time <span class=\"math-container\">$t_{1a}$</span>, which will finish at time <span class=\"math-container\">$t_{2a}$</span>, and agent 2 (<span class=\"math-container\">$a_2$</span>) makes a decision at time <span class=\"math-container\">$t_{1b} &lt; t_{1a}$</span>  which is finished at <span class=\"math-container\">$t_{2b} &gt; t_{2a}$</span>. </p>\n\n<p><strong>If both of their actions would start and finish at the same time</strong>, then it could easily be implemented as: </p>\n\n<pre><code>for ep in num_episodes:\n    action1, action2 = dqn.select_action([state1, state2])\n    next_state_1, reward_1, next_state_2, reward_2 = env.step([action1, action2])\n</code></pre>\n\n<p>because the env can execute both in parallel, wait till they are done, and then return the next states and rewards. But in the scenario that I described previously, it is not clear how to implement this (at least to me). Here, we need to explicitly track time, a check at any timepoint to see if an agent needs to make a decision, Just to be concrete:</p>\n\n<pre><code>for ep in num_episodes:\n    for t in total_time:\n       action1 = dqn.select_action(state1)\n       env.step(action1) # this step might take 5t to complete. \n       as such, the step() function won't return the reward till 5 t later. \n        #In the mean time, agent 2 comes and has to make a decision. its reward and next step won't be observed till 10 t later. \n</code></pre>\n\n<p>To summarize, how would one implement a multiagent environment with asynchronous action/rewards per agents?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "recurrent-neural-networks",
      "feedforward-neural-networks",
      "multilayer-perceptrons",
      "long-short-term-memory"
    ],
    "owner": {
      "account_id": 8217038,
      "reputation": 273,
      "user_id": 30154,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2b059b1a6c310da5525e96a5af69f117?s=256&d=identicon&r=PG",
      "display_name": "SuperCodeBrah",
      "link": "https://ai.stackexchange.com/users/30154/supercodebrah"
    },
    "is_answered": true,
    "view_count": 2135,
    "accepted_answer_id": 16245,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1572816980,
    "creation_date": 1572706562,
    "last_edit_date": 1572763623,
    "question_id": 16226,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/16226/why-use-a-recurrent-neural-network-over-a-feedforward-neural-network-for-sequenc",
    "title": "Why use a recurrent neural network over a feedforward neural network for sequence prediction?",
    "body": "<p>If recurrent neural networks (RNNs) are used to capture prior information, couldn't the same thing be achieved by a feedforward neural network (FFNN) or multi-layer perceptron (MLP) where the inputs are ordered sequentially?</p>\n\n<p>Here's an example I saw where the top line of each section represents letters typed and the next row represents the predicted next character (red letters in the next row means a confident prediction).</p>\n\n<p><a href=\"https://i.sstatic.net/4LUjg.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/4LUjg.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>Wouldn't it be simpler to just pass the <span class=\"math-container\">$X$</span> number of letters leading up to the last letter into an FFNN?</p>\n\n<p>For example, if <span class=\"math-container\">$X$</span> equaled 4, the following might be the input to the FFNN</p>\n\n<pre><code>S, T, A, C =&gt; Prediction: K\n</code></pre>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "terminology",
      "convergence"
    ],
    "owner": {
      "account_id": 4146639,
      "reputation": 6209,
      "user_id": 1671,
      "user_type": "registered",
      "accept_rate": 36,
      "profile_image": "https://i.sstatic.net/iE1hj.png?s=256",
      "display_name": "DukeZhou",
      "link": "https://ai.stackexchange.com/users/1671/dukezhou"
    },
    "is_answered": true,
    "view_count": 20186,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1722344857,
    "creation_date": 1573183380,
    "last_edit_date": 1639313178,
    "question_id": 16348,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/16348/what-is-convergence-in-machine-learning",
    "title": "What is convergence in machine learning?",
    "body": "<p>I came across this <a href=\"https://www.quora.com/What-is-convergence-in-neural-network\" rel=\"noreferrer\">answer on Quora</a>, but it was pretty sparse.  I'm looking for specific meanings in the context of machine learning, but also mathematical and economic notions of the term in general.</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "intelligence",
      "artificial-creativity"
    ],
    "owner": {
      "account_id": 4146639,
      "reputation": 6209,
      "user_id": 1671,
      "user_type": "registered",
      "accept_rate": 36,
      "profile_image": "https://i.sstatic.net/iE1hj.png?s=256",
      "display_name": "DukeZhou",
      "link": "https://ai.stackexchange.com/users/1671/dukezhou"
    },
    "is_answered": true,
    "view_count": 1741,
    "answer_count": 8,
    "score": 8,
    "last_activity_date": 1611073171,
    "creation_date": 1574212307,
    "last_edit_date": 1574252599,
    "question_id": 16646,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/16646/is-artificial-intelligence-really-just-human-intelligence",
    "title": "Is artificial intelligence really just human intelligence?",
    "body": "<p>Essentially, AI is created by human minds, so is the intelligence &amp; creativity of algorithms properly an extension of human intelligence &amp; creativity, rather than something independent?</p>\n\n<p>I assume that intelligence does not necessarily require creativity, however, creativity can result from machine learning.  (A simple example is AlphaGo discovering novel strategies.)</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "online-learning",
      "normalisation",
      "thompson-sampling",
      "normal-distribution"
    ],
    "owner": {
      "account_id": 13121304,
      "reputation": 81,
      "user_id": 31518,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2facbb1904643d96f4d54adcdd109b7f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Kevin",
      "link": "https://ai.stackexchange.com/users/31518/kevin"
    },
    "is_answered": false,
    "view_count": 160,
    "answer_count": 0,
    "score": 8,
    "last_activity_date": 1640011889,
    "creation_date": 1574424238,
    "last_edit_date": 1640011889,
    "question_id": 16717,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/16717/normalizing-normal-distributions-in-thompson-sampling-for-online-reinforcement-l",
    "title": "Normalizing Normal Distributions in Thompson Sampling for online Reinforcement Learning",
    "body": "<p>In my implementation of Thompson Sampling (TS) for online Reinforcement Learning, my distribution for selecting <span class=\"math-container\">$a$</span> is <span class=\"math-container\">$\\mathcal{N}(Q(s, a), \\frac{1}{C(s,a)+1})$</span>, where <span class=\"math-container\">$C(s,a)$</span> is the number of times <span class=\"math-container\">$a$</span> has been picked in <span class=\"math-container\">$s$</span>.</p>\n<p>However, I found that this does not work well in some cases depending on the magnitude of <span class=\"math-container\">$Q(s,a)$</span>. For example, if <span class=\"math-container\">$Q(s_i,a_1) = 100$</span>, and <span class=\"math-container\">$C(s_i,a_1) = 1$</span>, then then this gives a standard deviation of 0.5, which is extremely confident even though the action has only been picked once. Compare that to <span class=\"math-container\">$a_2$</span> which may be the optimal action but has never been picked, so <span class=\"math-container\">$Q(s_i, a_2) = 0$</span> and <span class=\"math-container\">$C(s_i,a_2) = 0$</span>. It is unlikely that TS will ever pick <span class=\"math-container\">$a_2$</span>.</p>\n<p>So, how do I solve this problem?</p>\n<p>I tried normalizing the Q-values such that they range from 0 to 1, but the algorithm returns much lower total returns. I think I have to adapt the magnitude of the standard deviations relative to the Q-values as well. Doing it for 1 normal distribution is pretty straightforward, but I can't figure out how to do it for multiple distributions which have to take into consideration of the other distributions.</p>\n<p>Edit: Counts should be <span class=\"math-container\">$C(s,a)$</span> instead of <span class=\"math-container\">$C(s)$</span> as Neil pointed out</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "natural-language-understanding"
    ],
    "owner": {
      "account_id": 14596323,
      "reputation": 500,
      "user_id": 22840,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b9198e034437f289a3261c5f0ce52e7c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Landon G",
      "link": "https://ai.stackexchange.com/users/22840/landon-g"
    },
    "is_answered": true,
    "view_count": 1504,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1584211197,
    "creation_date": 1575316898,
    "last_edit_date": 1584211197,
    "question_id": 16899,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/16899/what-are-the-current-big-challenges-in-natural-language-processing-and-understan",
    "title": "What are the current big challenges in natural language processing and understanding?",
    "body": "<p>I'm doing a paper for a class on the topic of big problems that are still prevalent in AI, specifically in the area of natural language processing and understanding. From what I understand, the areas:</p>\n\n<ul>\n<li>Text classification</li>\n<li>Entity recognition </li>\n<li>Translation</li>\n<li>POS tagging</li>\n</ul>\n\n<p>are for the most part solved or perform at a high level currently, but areas such as:</p>\n\n<ul>\n<li>Text summarization</li>\n<li>Conversational systems</li>\n<li>Contextual systems (relying on the previous context that will impact current prediction)</li>\n</ul>\n\n<p>are still relatively unsolved or are a big area of research (although this could very well change soon with the releases of big transformer models from what I've read).</p>\n\n<p>For people who have experience in the field, what are areas that are still big challenges in NLP and NLU? Why are these areas (doesn't have to be ones I've listed) so tough to figure out?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning"
    ],
    "owner": {
      "account_id": 8344713,
      "reputation": 422,
      "user_id": 27947,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/c1Gx3.png?s=256",
      "display_name": "Maverick Meerkat",
      "link": "https://ai.stackexchange.com/users/27947/maverick-meerkat"
    },
    "is_answered": true,
    "view_count": 2835,
    "accepted_answer_id": 16912,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1575380519,
    "creation_date": 1575373106,
    "question_id": 16910,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/16910/does-adding-a-constant-to-all-rewards-change-the-set-of-optimal-policies-in-epis",
    "title": "Does adding a constant to all rewards change the set of optimal policies in episodic tasks?",
    "body": "<p>I'm taking a <a href=\"https://www.coursera.org/learn/fundamentals-of-reinforcement-learning\" rel=\"noreferrer\">Coursera course on Reinforcement learning</a>. There was a question there that wasn't addressed in the learning material: Does adding a constant to all rewards change the set of optimal policies in episodic tasks?</p>\n\n<p>The answer is Yes - Adding a constant to the reward signal can make longer episodes more or less advantageous (depending on whether the constant is positive or negative).</p>\n\n<p>Can anyone explain why is this so? And why it doesn't change in the case of continuous (non episodic) tasks? I don't see why adding a constant matters - as an optimal policy would still want to get the maximum reward...</p>\n\n<p>Can anyone give an example of this?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "deep-learning",
      "philosophy",
      "agi",
      "intelligent-agent"
    ],
    "owner": {
      "account_id": 6561427,
      "reputation": 385,
      "user_id": 31978,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/bdd2a44cf6c207cac43d6db419e55e09?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "joethemow",
      "link": "https://ai.stackexchange.com/users/31978/joethemow"
    },
    "is_answered": true,
    "view_count": 2144,
    "accepted_answer_id": 17035,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1613150374,
    "creation_date": 1575996494,
    "last_edit_date": 1613150374,
    "question_id": 17025,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/17025/how-can-an-ai-freely-make-decisions",
    "title": "How can an AI freely make decisions?",
    "body": "<p>Suppose a deep neural network is created using Keras or Tensorflow. Usually, when you want to make a prediction, <strong>the user</strong> would invoke <code>model.predict</code>. However, how would the actual AI system proactively invoke their own actions (i.e. without the need for me to call <code>model.predict</code>)?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "classification"
    ],
    "owner": {
      "account_id": 17202316,
      "reputation": 83,
      "user_id": 32014,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/a9046127f93fe468bdf95d49706e7ab2?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "bigphil",
      "link": "https://ai.stackexchange.com/users/32014/bigphil"
    },
    "is_answered": true,
    "view_count": 713,
    "accepted_answer_id": 18352,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1583094716,
    "creation_date": 1576100211,
    "last_edit_date": 1576101864,
    "question_id": 17050,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/17050/can-a-deep-neural-network-be-trained-to-classify-an-integer-n1-as-being-divisibl",
    "title": "Can a deep neural network be trained to classify an integer N1 as being divisible by another integer N2?",
    "body": "<p>So I’ve been working on my own little dynamic architecture for a deep neural network (any number of hidden layers with any number of nodes in every layer) and got it solving the XOR problem efficiently. I moved on to trying to see if I could train my network on how to classify a number as being divisible by another number or not while experimenting with different network structures and have noticed some odd things. I know this is a weird thing to try and train a neural network to do but I just thought it might be easy because I can simply generate the training data set and test data set programmatically.</p>\n\n<p>From what I’ve tested, it seems that my network is only really good at identifying whether or not a number is divisible by a number who is a power of 2. If you test divisibility by a power of two, it converges on a very good solution very quickly. And it generalizes well on numbers outside of the training set - which I guess it kind of makes sense, as I’m inputting the numbers into the network in binary representation, so all the network has to learn is that a number n is only divisible by 2^m if the last m digits in the binary input vector are 0 (i.e. fire the output neuron if the last m neurons on the input layer don't fire, else don't). When checking divisibility by non-powers of two, however, there does not seem to be as much of a \"positional\" (maybe that's the word, maybe not) relationship between the input bits and whether or not the number is divisible.  I thought though, that if I threw more neurons and layers at the problem that it might be able to solve classifying divisibility by other numbers – but that is not the case. The network seems to converge on not-so-optimal local minima on the cost function (for which I am using mean-squared-error) when dividing by numbers that are not powers of 2. I’ve tried different learning rates as well to no avail. </p>\n\n<p>Do you have any idea what would cause something like this or how to go about trying to fix it? Or are plain deep neural networks maybe just not good at solving these types of problems?</p>\n\n<p>Note: I should also add that I've tried using different activation functions for different layers (like having leaky-relu activation for your first hidden layer, then sigmoid activation for your output layer, etc.) which has also not seem to have made a difference</p>\n\n<p>Here is my code if you feel so inclined as to look at it: <a href=\"https://github.com/bigstronkcodeman/Deep-Neural-Network/blob/master/Neural.py\" rel=\"noreferrer\">https://github.com/bigstronkcodeman/Deep-Neural-Network/blob/master/Neural.py</a></p>\n\n<p>(beware: it was all written from scratch by me in the quest to learn so some parts (namely the back-propagation) are not very pretty - I am really new to this whole neural network thing)</p>\n"
  },
  {
    "tags": [
      "comparison",
      "word-embedding",
      "feature-extraction"
    ],
    "owner": {
      "account_id": 5973826,
      "reputation": 83,
      "user_id": 32347,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/ELUUT.jpg?s=256",
      "display_name": "HiDDeN",
      "link": "https://ai.stackexchange.com/users/32347/hidden"
    },
    "is_answered": true,
    "view_count": 4881,
    "accepted_answer_id": 17276,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1577479419,
    "creation_date": 1577448863,
    "last_edit_date": 1577458030,
    "question_id": 17273,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/17273/is-word-embedding-a-form-of-feature-extraction",
    "title": "Is word embedding a form of feature extraction?",
    "body": "<p>Feature extraction is a concept concerning the translation of raw data into the inputs that a particular machine learning algorithm requires. These derived features from the raw data that are actually relevant to tackle the underlying problem. On the other hand, word embeddings are basically distributed representations of text in an n-dimensional space.</p>\n\n<p>As far as I understand, word embedding is a somehow feature extraction technique. Am I wrong ? I had an argument with a friend who believes the two topics are totally separate. Is he right? What are the similarities and dissimilarities between word embedding and feature extraction?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "computational-learning-theory",
      "pac-learning",
      "vc-dimension",
      "vc-theory"
    ],
    "owner": {
      "account_id": 9530813,
      "reputation": 847,
      "user_id": 32390,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/38bf1f217e177081850ca797b5af70a2?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "FourierFlux",
      "link": "https://ai.stackexchange.com/users/32390/fourierflux"
    },
    "is_answered": true,
    "view_count": 990,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1583901624,
    "creation_date": 1577603962,
    "last_edit_date": 1577621721,
    "question_id": 17291,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/17291/are-pac-learning-and-vc-dimension-relevant-to-machine-learning-in-practice",
    "title": "Are PAC learning and VC dimension relevant to machine learning in practice?",
    "body": "<p>Are PAC learning and VC dimension relevant to machine learning in practice? If yes, what is their practical value?</p>\n\n<p>To my understanding, there are two hits against these theories. The first is that the results all are conditioned on knowing the appropriate models to use, for example, the degree of complexity. The second is that the bounds are very bad, where a deep learning network would take an astronomical amount of data to reach said bounds.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "activation-functions",
      "hidden-layers",
      "network-design"
    ],
    "owner": {
      "account_id": 7805543,
      "reputation": 230,
      "user_id": 32400,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/e66c4c5214e6ff85671c7467c0dae761?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "lfgtm",
      "link": "https://ai.stackexchange.com/users/32400/lfgtm"
    },
    "is_answered": true,
    "view_count": 2999,
    "accepted_answer_id": 17299,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1653954723,
    "creation_date": 1577664373,
    "last_edit_date": 1653954723,
    "question_id": 17298,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/17298/do-all-neurons-in-a-layer-have-the-same-activation-function",
    "title": "Do all neurons in a layer have the same activation function?",
    "body": "<p>I'm new to machine learning (so excuse my nomenclature), and not being a python developer, I decided to jump in at the deep (no pun intended) end writing my own framework in C++.</p>\n<p>In my current design, I have given each neuron/cell the possibility to have a different activation function. Is this a plausible design for a neural network? A lot of the examples I see use the same activation function for all neurons in a given layer.</p>\n<p>Is there a model which may require this, or should all neurons in a layer use the same activation function? Would I be correct in using different activation functions for different layers in the same model, or would all layers have the same activation function within a model?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "keras",
      "accuracy",
      "gpu",
      "batch-size"
    ],
    "owner": {
      "account_id": 7951804,
      "reputation": 241,
      "user_id": 31870,
      "user_type": "registered",
      "profile_image": "https://lh4.googleusercontent.com/-Eev3PpHr3Hc/AAAAAAAAAAI/AAAAAAAAABc/8ywzGV6lJzo/s256-rj/photo.jpg",
      "display_name": "bit_scientist",
      "link": "https://ai.stackexchange.com/users/31870/bit-scientist"
    },
    "is_answered": true,
    "view_count": 10647,
    "accepted_answer_id": 17430,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1640595664,
    "creation_date": 1578550104,
    "last_edit_date": 1640595107,
    "question_id": 17424,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/17424/effect-of-batch-size-and-number-of-gpus-on-model-accuracy",
    "title": "Effect of batch size and number of GPUs on model accuracy",
    "body": "<p>I have a data set that was split using a fixed random seed and I am going to use 80% of the data for training and the rest for validation.</p>\n<p>Here are my GPU and batch size configurations</p>\n<ul>\n<li>use <code>64 batch size</code> with <code>one GTX 1080Ti</code></li>\n<li>use <code>128 batch size</code> with <code>two GTX 1080Ti</code></li>\n<li>use <code>256 batch size</code> with <code>four GTX 1080Ti</code></li>\n</ul>\n<p>All other hyper-parameters such as <code>lr</code>, <code>opt</code>, <code>loss</code>, etc., are fixed. Notice the linearity between the <code>batch size</code> and the <code>number of GPUs</code>.</p>\n<p>Will I get the same accuracy for those three experiments? Why and why not?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "generative-adversarial-networks"
    ],
    "owner": {
      "account_id": 12616862,
      "reputation": 183,
      "user_id": 32821,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-OCjcwDoczjU/AAAAAAAAAAI/AAAAAAAABEM/stRR-xF06-c/s256-rj/photo.jpg",
      "display_name": "Shir K",
      "link": "https://ai.stackexchange.com/users/32821/shir-k"
    },
    "is_answered": true,
    "view_count": 2371,
    "accepted_answer_id": 17551,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1607592267,
    "creation_date": 1579251674,
    "last_edit_date": 1579353686,
    "question_id": 17549,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/17549/how-does-the-generator-in-gans-work",
    "title": "How does the generator in GAN&#39;s work?",
    "body": "<p>After reading a lot of articles (for instance, this one - <a href=\"https://developers.google.com/machine-learning/gan/generator\" rel=\"nofollow noreferrer\">https://developers.google.com/machine-learning/gan/generator</a>), I've been wondering: how does the generator in GAN's work?</p>\n\n<p>What is the input to the generator? What is the meaning behind \"input noise\"?</p>\n\n<p>As I've read, the only input that the generator receives is a random noise, which is weird.</p>\n\n<p>If I would like to create a similar picture of <span class=\"math-container\">$x$</span>, and put as an input a matrix of random numbers (noise) - it would take A LOT of training until I would get some sort of picture <span class=\"math-container\">$x^*$</span>, that is similar to the source picture <span class=\"math-container\">$x$</span>.</p>\n\n<p>The algorithm should receive some type of reference or a basic dataset (for instance, the set of <span class=\"math-container\">$x$</span>'s) in order to start the generation of the fake image <span class=\"math-container\">$x^*$</span>.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "applications",
      "overfitting",
      "early-stopping"
    ],
    "owner": {
      "account_id": 2740787,
      "reputation": 1040,
      "user_id": 32621,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/133c43586dc40d84fc7f87671253d3ad?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "SpiderRico",
      "link": "https://ai.stackexchange.com/users/32621/spiderrico"
    },
    "is_answered": true,
    "view_count": 5557,
    "accepted_answer_id": 26955,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1616478966,
    "creation_date": 1581469223,
    "last_edit_date": 1581471945,
    "question_id": 17975,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/17975/should-i-prefer-the-model-with-the-lowest-validation-loss-or-the-highest-validat",
    "title": "Should I prefer the model with the lowest validation loss or the highest validation accuracy to deploy?",
    "body": "<p>I trained a ResNet20 on Cifar10 and obtained the following learning curves.</p>\n\n<p><a href=\"https://i.sstatic.net/O7JVh.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/O7JVh.png\" alt=\"enter image description here\"></a></p>\n\n<p>From the figures, I see at epoch 52, my validation loss is 0.323 (the lowest), and my validation accuracy is 89.7%.</p>\n\n<p>On the other hand, at the end of the training (epoch 120), my validation loss is 0.413 and my validation accuracy is 91.3% (the highest).</p>\n\n<p>Say I'd like to deploy this model on some real-world application. Should I prefer the snapshotted model at epoch 52, the one with lowest validation loss, or the model obtained at the end of training, the one with highest validation accuracy?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "agi",
      "intelligence-testing"
    ],
    "owner": {
      "account_id": 14596323,
      "reputation": 500,
      "user_id": 22840,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b9198e034437f289a3261c5f0ce52e7c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Landon G",
      "link": "https://ai.stackexchange.com/users/22840/landon-g"
    },
    "is_answered": true,
    "view_count": 719,
    "answer_count": 6,
    "score": 8,
    "last_activity_date": 1612258253,
    "creation_date": 1582522152,
    "last_edit_date": 1610994141,
    "question_id": 18204,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/18204/what-event-would-confirm-that-we-have-implemented-an-agi-system",
    "title": "What event would confirm that we have implemented an AGI system?",
    "body": "<p>I was listening to a podcast on the topic of AGI and a guest made an argument that if strong music generation were to happen, it would be a sign of &quot;true&quot; intelligence in machines because of how much creative capability creating music requires (even for humans).</p>\n<p>It got me wondering, what other events/milestones would convince someone, who is more involved in the field than myself, that we might have implemented an AGI (or a &quot;highly intelligent&quot; system)?</p>\n<p>Of course, the answer to this question depends on the definition of AGI, but you can choose a sensible definition of AGI in order to answer this question.</p>\n<p>So, for example, maybe some of these milestones or events could be:</p>\n<ul>\n<li>General conversation</li>\n<li>Full self-driving car (no human intervention)</li>\n<li>Music generation</li>\n<li>Something similar to AlphaGo</li>\n<li>High-level reading/comprehension</li>\n</ul>\n<p>What particular event would convince you that we've reached a high level of intelligence in machines?</p>\n<p>It does not have to be any of the events I listed.</p>\n"
  },
  {
    "tags": [
      "convolutional-neural-networks",
      "filters",
      "convolutional-layers",
      "convolution-arithmetic"
    ],
    "owner": {
      "account_id": 233893,
      "reputation": 215,
      "user_id": 32528,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/kbSI9.jpg?s=256",
      "display_name": "&#198;lex",
      "link": "https://ai.stackexchange.com/users/32528/%c3%86lex"
    },
    "is_answered": true,
    "view_count": 12699,
    "accepted_answer_id": 18669,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1639830881,
    "creation_date": 1584359991,
    "last_edit_date": 1639830881,
    "question_id": 18663,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/18663/how-to-calculate-the-number-of-parameters-of-a-convolutional-layer",
    "title": "How to calculate the number of parameters of a convolutional layer?",
    "body": "<p>I was recently asked at an interview to calculate the number of parameters for a convolutional layer. I am deeply ashamed to admit I didn't know how to do that, even though I've been working and using CNN for years now.</p>\n\n<p>Given a convolutional layer with ten <span class=\"math-container\">$3 \\times 3$</span> filters and an input of shape <span class=\"math-container\">$24 \\times 24 \\times 3$</span>, what is the total number of parameters of this convolutional layer?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "q-learning",
      "dqn",
      "deep-rl",
      "resource-request"
    ],
    "owner": {
      "account_id": 10329222,
      "reputation": 91,
      "user_id": 34488,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/404b9d60b58f1a8b800d86a78ccbc619?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "J.Doe",
      "link": "https://ai.stackexchange.com/users/34488/j-doe"
    },
    "is_answered": true,
    "view_count": 590,
    "protected_date": 1634036090,
    "accepted_answer_id": 18833,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1642188909,
    "creation_date": 1585147584,
    "last_edit_date": 1585232668,
    "question_id": 18798,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/18798/what-are-some-online-courses-for-deep-reinforcement-learning",
    "title": "What are some online courses for deep reinforcement learning?",
    "body": "<p>What are some (good) online courses for deep reinforcement learning?</p>\n\n<p>I would like the course to be both programming and theoretical. I really liked <a href=\"https://www.youtube.com/playlist?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-\" rel=\"noreferrer\">David Silver's course</a>, but the course dates from 2015. It doesn't really teach deep Q-learning at this time.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "reinforcement-learning",
      "overfitting"
    ],
    "owner": {
      "account_id": 2304865,
      "reputation": 2859,
      "user_id": 16565,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/e4iCE.jpg?s=256",
      "display_name": "malioboro",
      "link": "https://ai.stackexchange.com/users/16565/malioboro"
    },
    "is_answered": true,
    "view_count": 7800,
    "accepted_answer_id": 20135,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1669647006,
    "creation_date": 1586447532,
    "last_edit_date": 1637588248,
    "question_id": 20127,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/20127/how-can-i-handle-overfitting-in-reinforcement-learning-problems",
    "title": "How can I handle overfitting in reinforcement learning problems?",
    "body": "<p>So this is my current result (loss and score per episode) of my RL model in a simple two players game:</p>\n<p><a href=\"https://i.sstatic.net/K7YTe.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/K7YTe.png\" alt=\"enter image description here\" /></a></p>\n<p>I use DQN with CNN as a policy and target networks. I train my model using Adam optimizer and calculate the loss using Smooth L1 Loss.</p>\n<p>In a normal &quot;Supervised Learning&quot; situation, I can deduce that my model is overfitting. And I can imagine some methods to tackle this problem (e.g. Dropout layer, Regularization, Smaller Learning Rate, Early Stopping).</p>\n<ul>\n<li>But would that solution will also work in RL problem?</li>\n<li>Or are there any better solutions to handle overfitting in RL?</li>\n</ul>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "backpropagation",
      "gradient-descent",
      "batch-normalization"
    ],
    "owner": {
      "account_id": 6454321,
      "reputation": 1201,
      "user_id": 35585,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/kyLdh.jpg?s=256",
      "display_name": "stoic-santiago",
      "link": "https://ai.stackexchange.com/users/35585/stoic-santiago"
    },
    "is_answered": true,
    "view_count": 2017,
    "accepted_answer_id": 20315,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1587039042,
    "creation_date": 1587019049,
    "last_edit_date": 1587037713,
    "question_id": 20308,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/20308/why-do-we-update-all-layers-simultaneously-while-training-a-neural-network",
    "title": "Why do we update all layers simultaneously while training a neural network?",
    "body": "<blockquote>\n  <p>Very deep models involve the composition of several functions or layers. The gradient tells how to update each parameter, under the assumption that the other layers do not change. In practice, we update all of the layers simultaneously.</p>\n</blockquote>\n\n<p>The above is an extract from Ian Goodfellow's Deep Learning  - which talks about the need for batch normalization. </p>\n\n<p><strong>Why do we update all the layers simultaneously?</strong> Instead, if we update layers one at a time during backpropagation - it will eliminate the need for batch normalization, right?</p>\n\n<p>Reference: <a href=\"https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/\" rel=\"noreferrer\">A Gentle Introduction to Batch Normalization for Deep Neural Networks</a></p>\n\n<p>P.S. The attached link says: <em>Because all layers are changed during an update, the update procedure is forever chasing a moving target</em>. Apart from the main question, it would be great if someone could explain why exactly a <em>moving target</em> is being referred to in the above sentence.</p>\n"
  },
  {
    "tags": [
      "convolutional-neural-networks",
      "convolution",
      "filters",
      "convolutional-layers",
      "fully-convolutional-networks"
    ],
    "owner": {
      "account_id": 16995253,
      "reputation": 81,
      "user_id": 36525,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/5615ac1fc8d4ffa94852eac9beeb983b?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Arcturai",
      "link": "https://ai.stackexchange.com/users/36525/arcturai"
    },
    "is_answered": true,
    "view_count": 916,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1644405572,
    "creation_date": 1588035892,
    "last_edit_date": 1611424804,
    "question_id": 20706,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/20706/what-is-the-point-of-using-1d-and-2d-convolutions-with-a-kernel-size-of-1-and-1x",
    "title": "What is the point of using 1D and 2D convolutions with a kernel size of 1 and 1x1 respectively?",
    "body": "<p>I understand the gist of what convolutional neural networks do and what they are used for, but I still wrestle a bit with how they function on a conceptual level. For example, I get that filters with kernel size greater than 1 are used as feature detectors, and that number of filters is equal to the number of output channels for a convolutional layer, and the number of features being detected scales with the number of filters/channels.</p>\n<p>However, recently, I've been encountering an increasing number of models that employ 1- or 2D convolutions with kernel sizes of 1 or 1x1, and I can't quite grasp why. It feels to me like they defeat the purpose of performing a convolution in the first place.</p>\n<p>What is the advantage of using such layers? Are they not just equivalent to multiplying each channel by a trainable, scalar value?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "comparison",
      "terminology"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user9947"
    },
    "is_answered": true,
    "view_count": 2528,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1641117192,
    "creation_date": 1589459209,
    "last_edit_date": 1641117192,
    "question_id": 21155,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/21155/what-is-the-difference-between-the-prediction-and-control-problems-in-the-contex",
    "title": "What is the difference between the prediction and control problems in the context of Reinforcement Learning?",
    "body": "<p>What is the difference between the prediction (value estimation) and control problems in reinforcement learning?</p>\n<p>Are there scenarios in RL where the problem cannot be distinctly categorised into the aforementioned problems and is a mixture of the problems?</p>\n<p>Examples where the problem cannot be easily categorised into one of the aforementioned problems would be nice.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "natural-language-processing",
      "transformer",
      "attention"
    ],
    "owner": {
      "account_id": 9320077,
      "reputation": 279,
      "user_id": 36486,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/275b35eb541484370fb6f5942afd9b32?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Jacob B",
      "link": "https://ai.stackexchange.com/users/36486/jacob-b"
    },
    "is_answered": true,
    "view_count": 8384,
    "accepted_answer_id": 25057,
    "answer_count": 4,
    "score": 8,
    "last_activity_date": 1690642727,
    "creation_date": 1589765293,
    "last_edit_date": 1620604206,
    "question_id": 21237,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/21237/why-does-this-multiplication-of-q-and-k-have-a-variance-of-d-k-in-scaled",
    "title": "Why does this multiplication of $Q$ and $K$ have a variance of $d_k$, in scaled dot product attention?",
    "body": "<p>In scaled dot product attention, we scale our outputs by dividing the dot product by the square root of the dimensionality of the matrix:</p>\n<p><a href=\"https://i.sstatic.net/wLI4m.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/wLI4m.png\" alt=\"enter image description here\" /></a></p>\n<p>The reason why is stated that this constrains the distribution of the weights of the output to have a standard deviation of 1.</p>\n<p>Quoted from <a href=\"https://www.tensorflow.org/tutorials/text/transformer\" rel=\"noreferrer\">Transformer model for language understanding | TensorFlow</a>:</p>\n<blockquote>\n<p>For example, consider that <span class=\"math-container\">$Q$</span> and <span class=\"math-container\">$K$</span> have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of <span class=\"math-container\">$d_k$</span>. Hence, square root of <span class=\"math-container\">$d_k$</span> is used for scaling (and not any other number) because the matmul of <span class=\"math-container\">$Q$</span> and <span class=\"math-container\">$K$</span> should have a mean of 0 and variance of 1, and you get a gentler softmax.</p>\n</blockquote>\n<p>Why does this multiplication have a variance of <span class=\"math-container\">$d_k$</span>?</p>\n<p>If I understand this, I will then understand why dividing by <span class=\"math-container\">$\\sqrt({d_k})$</span> would normalize to 1.</p>\n<p>Trying this experiment on 2x2 arrays I get an output of 1.6 variance:</p>\n<p><a href=\"https://i.sstatic.net/GCe3t.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/GCe3t.png\" alt=\"enter image description here\" /></a></p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "math",
      "proofs",
      "coq"
    ],
    "owner": {
      "account_id": 18618541,
      "reputation": 141,
      "user_id": 37262,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-9loJd5yjRv0/AAAAAAAAAAI/AAAAAAAAF1s/AMZuuck7WFkLHFfKVsjataaQC1Y9ht_cYQ/s256-rj/photo.jpg",
      "display_name": "Antoine Labelle",
      "link": "https://ai.stackexchange.com/users/37262/antoine-labelle"
    },
    "is_answered": true,
    "view_count": 410,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1591794841,
    "creation_date": 1590075618,
    "last_edit_date": 1590092511,
    "question_id": 21382,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/21382/can-deep-learning-be-used-to-help-mathematical-research",
    "title": "Can deep learning be used to help mathematical research?",
    "body": "<p>I am currently learning about deep learning and artificial intelligence and exploring his possibilities, and, as a mathematician at heart, I am inquisitive about how it can be used to solve problems in mathematics.</p>\n\n<p>Seeing how well recurrent neural networks can understand human language, I suppose that they could also be used to follow some simple mathematical statements and maybe even come up with some proofs. I know that computer-assisted proofs are more and more frequent and that some software can now understand simple mathematical language and verify proofs (e.g. <a href=\"https://en.wikipedia.org/wiki/Coq\" rel=\"nofollow noreferrer\">Coq</a>). Still, I've never heard of deep learning applied to mathematical research.</p>\n\n<p>Can deep learning be used to help mathematical research? So, I am curious about whether systems like Coq could be combined with deep learning systems to help mathematical research. Are there some exciting results?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "value-functions",
      "bellman-equations",
      "expectation"
    ],
    "owner": {
      "account_id": 17074769,
      "reputation": 343,
      "user_id": 31324,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-c8KtCK1F_Sc/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3refHWNaEdHohm92M3tD8_3g1JzRhA/s256-rj/photo.jpg",
      "display_name": "Daniel Wiczew",
      "link": "https://ai.stackexchange.com/users/31324/daniel-wiczew"
    },
    "is_answered": true,
    "view_count": 972,
    "accepted_answer_id": 21687,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1643116201,
    "creation_date": 1591433732,
    "last_edit_date": 1643024327,
    "question_id": 21684,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/21684/why-does-the-state-action-value-function-defined-as-an-expected-value-of-the-re",
    "title": "Why does the state-action value function, defined as an expected value of the reward and state value function, not need to follow a policy?",
    "body": "<p>I often see that the state-action value function is expressed as:</p>\n<p><span class=\"math-container\">$$q_{\\pi}(s,a)=\\color{red}{\\mathbb{E}_{\\pi}}[R_{t+1}+\\gamma G_{t+1} | S_t=s, A_t = a] = \\color{blue}{\\mathbb{E}}[R_{t+1}+\\gamma v_{\\pi}(s') |S_t = s, A_t =a]$$</span></p>\n<p>Why does expressing the future return in the time <span class=\"math-container\">$t+1$</span> as a state value function <span class=\"math-container\">$v_{\\pi}$</span> make the expected value under policy change to expected value in general?</p>\n"
  },
  {
    "tags": [
      "computer-vision",
      "reference-request",
      "image-processing",
      "algorithm-request",
      "model-request"
    ],
    "owner": {
      "account_id": 15498653,
      "reputation": 1283,
      "user_id": 30725,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/N8b6X.jpg?s=256",
      "display_name": "Pluviophile",
      "link": "https://ai.stackexchange.com/users/30725/pluviophile"
    },
    "is_answered": true,
    "view_count": 9753,
    "accepted_answer_id": 21977,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1734704001,
    "creation_date": 1592406776,
    "last_edit_date": 1734703878,
    "question_id": 21970,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/21970/what-are-the-main-algorithms-used-in-computer-vision",
    "title": "What are the main algorithms used in computer vision?",
    "body": "<p>Nowadays, CV has really achieved great performance in many different areas. However, it is not clear what a CV algorithm is.</p>\n<p>What are some examples of CV algorithms that are commonly used nowadays and have achieved state-of-the-art performance?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "q-learning",
      "proofs",
      "convergence",
      "bellman-equations"
    ],
    "owner": {
      "account_id": 8012610,
      "reputation": 81,
      "user_id": 38818,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b2d591aab95c13a1b22bfffb51330e46?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "sirfroggy",
      "link": "https://ai.stackexchange.com/users/38818/sirfroggy"
    },
    "is_answered": false,
    "view_count": 302,
    "answer_count": 0,
    "score": 8,
    "last_activity_date": 1595665693,
    "creation_date": 1595525534,
    "last_edit_date": 1595665693,
    "question_id": 22642,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/22642/is-the-bellman-equation-that-uses-sampling-weighted-by-the-q-values-instead-of",
    "title": "Is the Bellman equation that uses sampling weighted by the Q values (instead of max) a contraction?",
    "body": "<p>It is proved that the Bellman update is a contraction (1).</p>\n<p><strong>Here is the Bellman update that is used for Q-Learning:</strong></p>\n<p><span class=\"math-container\">$$Q_{t+1}(s, a) = Q_{t}(s, a) + \\alpha*(r(s, a, s') + \\gamma  \\max_{a^*} (Q_{t}(s',\n   a^*)) - Q_t(s,a)) \\tag{1} \\label{1}$$</span></p>\n<p>The proof of (\\ref{1}) being contraction comes from one of the facts (the relevant one for the question) that max operation is non expansive; that is:</p>\n<p><span class=\"math-container\">$$\\lvert \\max_a f(a)- \\max_a g(a) \\rvert \\leq \\max_a \\lvert f(a) -  g(a) \\rvert \\tag{2}\\label{2}$$</span></p>\n<p>This is also proved in a lot of places and it is pretty intuitive.</p>\n<p><strong>Consider the following Bellman update:</strong></p>\n<p><span class=\"math-container\">$$ Q_{t+1}(s, a) = Q_{t}(s, a) + \\alpha*(r(s, a, s') + \\gamma  SAMPLE_{a^*} (Q_{t}(s', a^*)) - Q_t(s,a)) \\tag{3}\\label{3}$$</span></p>\n<p>where <span class=\"math-container\">$SAMPLE_a(Q(s, a))$</span> samples an action with respect to the Q values (weighted by their Q values) of each action in that state.</p>\n<p><strong>Is this new Bellman operation still a contraction?</strong></p>\n<p>Is the SAMPLE operation non-expansive? It is, of course, possible to generate samples that will not satisfy equation (\\ref{2}). I ask <strong>is it non-expansive in expectation?</strong></p>\n<p>My approach is:</p>\n<p><span class=\"math-container\">$$\\lvert\\,\\mathbb{E}_{a \\sim Q}[f(a)] - \\mathbb{E}_{a \\sim Q}[g(a)]\\, \\rvert \\leq \\,\\,\\mathbb{E}_{a \\sim Q}\\lvert\\,\\,[f(a) - g(a)]\\,\\,\\rvert \\tag{4} \\label{4} $$</span></p>\n<p>Equivalently:</p>\n<p><span class=\"math-container\">$$\\lvert\\,\\mathbb{E}_{a \\sim Q}[f(a) - g(a)] \\, \\rvert \\leq \\,\\,\\mathbb{E}_{a \\sim Q}\\lvert\\,\\,[f(a) - g(a)]\\,\\,\\rvert$$</span></p>\n<p>(\\ref{4}) is true since:</p>\n<p><span class=\"math-container\">$$\\lvert\\,\\mathbb{E}[X] \\, \\rvert \\leq \\,\\,\\mathbb{E} \\,\\,\\lvert\\,\\,[X]\\,\\,\\rvert $$</span></p>\n<p><strong>But, I am not sure if proving (\\ref{4}) proves the theorem. Do you think that this is a legit proof that (\\ref{3}) is a contraction.</strong></p>\n<p>(If so; this would mean that stochastic policy q learning theoretically converges and we can have stochastic policies with regular q learning; and this is why I am interested.)</p>\n<p>Both intuitive answers and mathematical proofs are welcome.</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "natural-language-understanding",
      "metric",
      "question-answering"
    ],
    "owner": {
      "account_id": 14748052,
      "reputation": 599,
      "user_id": 23350,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/BxdNp.jpg?s=256",
      "display_name": "HLeb",
      "link": "https://ai.stackexchange.com/users/23350/hleb"
    },
    "is_answered": true,
    "view_count": 7362,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1640214677,
    "creation_date": 1595772821,
    "last_edit_date": 1611675332,
    "question_id": 22676,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/22676/how-is-the-f1-score-calculated-in-a-question-answering-system",
    "title": "How is the F1 score calculated in a question-answering system?",
    "body": "<p>I have an NLP model for answer-extraction. So, basically, I have a paragraph and a question as input, and my model extracts the span of the paragraph that corresponds to the answer to the question.</p>\n<p>I need to know how to compute the F1 score for such models. It is the standard metric (along with Exact Match) used in the literature to evaluate question-answering systems.</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "reward-design",
      "reward-functions",
      "reward-shaping",
      "inverse-rl"
    ],
    "owner": {
      "account_id": 15628585,
      "reputation": 195,
      "user_id": 25560,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/897348957905fe0b6838516a2c8d5352?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "naisuu42",
      "link": "https://ai.stackexchange.com/users/25560/naisuu42"
    },
    "is_answered": true,
    "view_count": 13838,
    "accepted_answer_id": 22854,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1611147333,
    "creation_date": 1596472218,
    "last_edit_date": 1604353669,
    "question_id": 22851,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/22851/what-are-some-best-practices-when-trying-to-design-a-reward-function",
    "title": "What are some best practices when trying to design a reward function?",
    "body": "<p>Generally speaking, is there a best-practice procedure to follow when trying to define a reward function for a reinforcement-learning agent? What common pitfalls are there when defining the reward function, and how should you avoid them? What information from your problem should you take into consideration when going about it?</p>\n<p>Let us presume that our environment is fully observable MDP.</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "policy-gradients",
      "policy-gradient-theorem",
      "deterministic-pg-theorem",
      "theorems"
    ],
    "owner": {
      "account_id": 3087156,
      "reputation": 183,
      "user_id": 34041,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2a30818a57c11c6d808f4b9ea824e255?s=256&d=identicon&r=PG",
      "display_name": "fabian",
      "link": "https://ai.stackexchange.com/users/34041/fabian"
    },
    "is_answered": true,
    "view_count": 679,
    "accepted_answer_id": 22872,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1652256089,
    "creation_date": 1596525033,
    "last_edit_date": 1652256089,
    "question_id": 22857,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/22857/why-do-the-standard-and-deterministic-policy-gradient-theorems-differ-in-their-t",
    "title": "Why do the standard and deterministic Policy Gradient Theorems differ in their treatment of the derivatives of $R$ and the conditional probability?",
    "body": "<p>I would like to understand the difference between the <a href=\"https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf\" rel=\"nofollow noreferrer\">standard policy gradient theorem</a> and the <a href=\"http://proceedings.mlr.press/v32/silver14-supp.pdf\" rel=\"nofollow noreferrer\">deterministic policy gradient theorem</a>. These two theorem are quite different, although the <em>only</em> difference is whether the policy function is deterministic or stochastic.  I summarized the relevant steps of the theorems below. The policy function is <span class=\"math-container\">$\\pi$</span> which has parameters <span class=\"math-container\">$\\theta$</span>.</p>\n<p><strong>Standard Policy Gradient</strong>\n<span class=\"math-container\">$$\n\\begin{aligned}\n\\dfrac{\\partial V}{\\partial \\theta} &amp;= \\dfrac{\\partial}{\\partial \\theta} \\left[ \\sum_a \\pi(a|s) Q(a,s) \\right]  \\\\\n&amp;= \\sum_a \\left[ \\dfrac{\\partial \\pi(a|s)}{\\partial \\theta}  Q(a,s) + \\pi(a|s) \\dfrac{\\partial Q(a,s)}{\\partial \\theta} \\right] \\\\\n&amp;= \\sum_a \\left[ \\dfrac{\\partial \\pi(a|s)}{\\partial \\theta}  Q(a,s) + \\pi(a|s) \\dfrac{\\partial}{\\partial \\theta} \\left[ R + \\sum_{s'} \\gamma p(s'|s,a) V(s') \\right] \\right]  \\\\\n&amp;= \\sum_a \\left[ \\dfrac{\\partial \\pi(a|s)}{\\partial \\theta}  Q(a,s) + \\pi(a|s) \\gamma \\sum_{s'} p(s'|s,a) \\dfrac{\\partial V(s') }{\\partial \\theta} \\right]\n\\end{aligned}\n$$</span>\nWhen one now expands next period's value function <span class=\"math-container\">$V(s')$</span> again one can eventually reach the final policy gradient:\n<span class=\"math-container\">$$\n\\dfrac{\\partial J}{\\partial \\theta} = \\sum_s \\rho(s) \\sum_a \\dfrac{\\pi(a|s)}{\\partial \\theta}  Q(s,a)\n$$</span>\nwith <span class=\"math-container\">$\\rho$</span> being the stationary distribution. What I find particularly interesting is that there is no derivative of <span class=\"math-container\">$R$</span> with respect to <span class=\"math-container\">$\\theta$</span> and also not of the probability distribution <span class=\"math-container\">$p(s'|s,a)$</span> with respect to <span class=\"math-container\">$\\theta$</span>. The derivation of the deterministic policy gradient theorem is different:</p>\n<p><strong>Deterministic Policy Gradient Theorem</strong>\n<span class=\"math-container\">$$\n\\begin{aligned}\n\\dfrac{\\partial V}{\\partial \\theta} &amp;= \\dfrac{\\partial}{\\partial \\theta} Q(\\pi(s),s)  \\\\\n&amp;= \\dfrac{\\partial}{\\partial \\theta} \\left[ R(s, \\pi(s)) + \\gamma \\sum_{s'} p(s'|a,s) V(s') \\right] \\\\\n&amp;= \\dfrac{R(s, a)}{\\partial a}\\dfrac{\\pi(s)}{\\partial \\theta} + \\dfrac{\\partial}{\\partial \\theta} \\left[\\gamma \\sum_{s'} p(s'|a,s) V(s')   \\right]  \\\\\n&amp;= \\dfrac{R(s, a)}{\\partial a}\\dfrac{\\pi(s)}{\\partial \\theta} + \\gamma \\sum_{s'}  \\left[p(s'|\\mu(s),s) \\dfrac{V(s')}{\\partial \\theta} + \\dfrac{\\pi(s)}{\\partial \\theta} \\dfrac{p(s'|s,a)}{\\partial a} V(s')   \\right]  \\\\\n&amp;= \\dfrac{\\pi(s)}{\\partial \\theta} \\dfrac{\\partial}{\\partial a} \\left[ R(s, a) +  p(s'|s,a) V(s') \\right] +  \\gamma  p(s'|\\pi(s),s) \\dfrac{V(s')}{\\partial \\theta}  \\\\\n&amp;= \\dfrac{\\pi(s)}{\\partial \\theta} \\dfrac{\\partial Q(s, a)}{\\partial a} +  \\gamma  p(s'|\\pi(s),s) \\dfrac{V(s')}{\\partial \\theta}  \\\\\n\\end{aligned}\n$$</span>\nAgain, one can obtain the finaly policy gradient by expanding next period's value function. The policy gradient is:\n<span class=\"math-container\">$$\n\\dfrac{\\partial J}{\\partial \\theta} = \\sum_s \\rho(s) \\dfrac{\\pi(s)}{\\partial \\theta} \\dfrac{\\partial Q(s,a))}{\\partial a}\n$$</span>\nIn contrast to the standard policy gradient, the equations contain derivatives of the reward function <span class=\"math-container\">$R$</span> and the conditional probability <span class=\"math-container\">$p(s'|s, a,)$</span> with respect to <span class=\"math-container\">$a$</span>.</p>\n<p><strong>Question</strong></p>\n<p>Why do the two theorems differ in their treatment of the derivatives of <span class=\"math-container\">$R$</span> and the conditional probability? Does determinism in the policy function make such a difference for the derivatives?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "backpropagation",
      "policy-gradients",
      "reinforce",
      "cross-entropy"
    ],
    "owner": {
      "account_id": 19378665,
      "reputation": 600,
      "user_id": 41026,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8a818ab2f99e30027fa0b3800f445ad6?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "S2673",
      "link": "https://ai.stackexchange.com/users/41026/s2673"
    },
    "is_answered": true,
    "view_count": 1613,
    "accepted_answer_id": 23628,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1639951585,
    "creation_date": 1600268951,
    "last_edit_date": 1639951585,
    "question_id": 23625,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/23625/which-loss-function-should-i-use-in-reinforce-and-what-are-the-labels",
    "title": "Which loss function should I use in REINFORCE, and what are the labels?",
    "body": "<p>I understand that this is the update for the parameters of a policy in REINFORCE:</p>\n<p><span class=\"math-container\">$$\n\\Delta \\theta_{t}=\\alpha \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right) v_{t},\n$$</span>\nwhere <span class=\"math-container\">$v_t$</span>  is usually the discounted future reward and  <span class=\"math-container\">$\\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right)$</span> is the probability of taken the action that the agent took at time <span class=\"math-container\">$t$</span>. (Tell me if something is wrong here)</p>\n<p>However, I don't understand how to implement this with a neural network.</p>\n<p>Let's say that <code>probs = policy.feedforward(state)</code> returns the probabilities of taking each action, like <code>[0.6, 0.4]</code>. <code>action = choose_action_from(probs)</code> will return the index of the probability chosen. For example, if it chose 0.6, the action would be 0.</p>\n<p>When it is time to update the parameters of the policy network, what should we do? Should we do something like the following?</p>\n<pre><code>gradient = policy.backpropagate(total_discounted_reward*log(probs[action])\npolicy.weights += gradient\n</code></pre>\n<p>And I only backpropagate this through one output neuron?</p>\n<p>Which loss function should I use in this case? What would the labels be?</p>\n<p>If you need more explanation, I have this <a href=\"https://stackoverflow.com/q/63602222/14170431\">question</a> on SO.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "optimization",
      "gradient-descent",
      "learning-rate",
      "stochastic-gradient-descent"
    ],
    "owner": {
      "account_id": 13219673,
      "reputation": 1436,
      "user_id": 26726,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/92f202079aee10525313ec069b645011?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Recessive",
      "link": "https://ai.stackexchange.com/users/26726/recessive"
    },
    "is_answered": true,
    "view_count": 6072,
    "accepted_answer_id": 23748,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1606652737,
    "creation_date": 1601005203,
    "last_edit_date": 1606652737,
    "question_id": 23740,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/23740/why-is-the-learning-rate-generally-beneath-1",
    "title": "Why is the learning rate generally beneath 1?",
    "body": "<p>In all examples I've ever seen, the learning rate of an optimisation method is always less than <span class=\"math-container\">$1$</span>. However, I've never found an explanation as to why this is. In addition to that, there are some cases where having a learning rate bigger than 1 is beneficial, such as in the case of <a href=\"https://arxiv.org/abs/1708.07120\" rel=\"noreferrer\">super-convergence</a>.</p>\n<p>Why is the learning rate generally less than 1? Specifically, when performing an update on a parameter, why is the gradient generally multiplied by a factor less than 1 (absolutely)?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "linear-regression",
      "bias-variance-tradeoff",
      "underfitting",
      "inductive-bias"
    ],
    "owner": {
      "account_id": 15591403,
      "reputation": 326,
      "user_id": 41226,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Yj3Mplgx.png?s=256",
      "display_name": "Sivaram Rasathurai",
      "link": "https://ai.stackexchange.com/users/41226/sivaram-rasathurai"
    },
    "is_answered": true,
    "view_count": 3577,
    "accepted_answer_id": 23784,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1631115062,
    "creation_date": 1601139327,
    "last_edit_date": 1631115062,
    "question_id": 23774,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/23774/is-there-a-connection-between-the-bias-term-in-a-linear-regression-model-and-the",
    "title": "Is there a connection between the bias term in a linear regression model and the bias that can lead to under-fitting?",
    "body": "<p>Here is a linear regression model</p>\n<p><span class=\"math-container\">$$y = mx + b,$$</span></p>\n<p>where <span class=\"math-container\">$b$</span> is known as <span class=\"math-container\">$y$</span>-intercept, but also known as the <strong>bias</strong> [<a href=\"https://developers.google.com/machine-learning/crash-course/descending-into-ml/linear-regression\" rel=\"noreferrer\">1</a>], <span class=\"math-container\">$m$</span> is the slope, and <span class=\"math-container\">$x$</span> is the feature vector.</p>\n<p>As I understood, in machine learning, there is also the <strong>bias</strong> that can cause the model to underfit.</p>\n<p><em>So, is there a connection between the bias term <span class=\"math-container\">$b$</span> in a linear regression model and the bias that can lead to under-fitting in machine learning?</em></p>\n"
  },
  {
    "tags": [
      "optimization",
      "gradient-descent",
      "perceptron"
    ],
    "owner": {
      "account_id": 5774060,
      "reputation": 83,
      "user_id": 41860,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-lNF0L1fcAOE/AAAAAAAAAAI/AAAAAAAAEIY/bSKxTdjYDDw/s256-rj/photo.jpg",
      "display_name": "Fl&#225;vio Mendes",
      "link": "https://ai.stackexchange.com/users/41860/fl%c3%a1vio-mendes"
    },
    "is_answered": true,
    "view_count": 1320,
    "accepted_answer_id": 24272,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1613429342,
    "creation_date": 1603759124,
    "last_edit_date": 1613429342,
    "question_id": 24261,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/24261/why-is-the-perceptron-criterion-function-differentiable",
    "title": "Why is the perceptron criterion function differentiable?",
    "body": "<p>I'm reading chapter one of the book called <a href=\"https://dl.uswr.ac.ir/bitstream/Hannan/141305/2/9783319944623.pdf\" rel=\"nofollow noreferrer\">Neural Networks and Deep Learning</a> from Aggarwal.</p>\n<p>In section 1.2.1.1 of the book, I'm learning about the perceptron. One thing that book says is, if we use the sign function for the following loss function: <span class=\"math-container\">$\\sum_{i=0}^{N}[y_i - \\text{sign}(W * X_i)]^2$</span>, that loss function will NOT be differentiable. Therefore, the book suggests us to use, instead of the sign function in the loss function, the perceptron criterion which will be defined as:</p>\n<p><span class=\"math-container\">$$ L_i = \\max(-y_i(W * X_i), 0) $$</span></p>\n<p>The question is: Why is the perceptron criterion function differentiable? Won't we face a discontinuity at zero? Is there anything that I'm missing here?</p>\n"
  },
  {
    "tags": [
      "objective-functions",
      "support-vector-machine",
      "perceptron",
      "binary-classification",
      "hinge-loss"
    ],
    "owner": {
      "account_id": 8893267,
      "reputation": 611,
      "user_id": 16521,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/441d2ab5e8c2f97c273d758c79f89fbe?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "The Pointer",
      "link": "https://ai.stackexchange.com/users/16521/the-pointer"
    },
    "is_answered": false,
    "view_count": 974,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1742850390,
    "creation_date": 1605964753,
    "last_edit_date": 1619164401,
    "question_id": 24767,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/24767/how-should-we-interpret-this-figure-that-relates-the-perceptron-criterion-and-th",
    "title": "How should we interpret this figure that relates the perceptron criterion and the hinge loss?",
    "body": "<p>I am currently studying the textbook <em>Neural Networks and Deep Learning</em> by Charu C. Aggarwal. Chapter <strong>1.2.1.2 Relationship with Support Vector Machines</strong> says the following:</p>\n<blockquote>\n<p>The perceptron criterion is a shifted version of the hinge-loss used in support vector machines (see Chapter 2). The hinge loss looks even more similar to the zero-one loss criterion of Equation 1.7, and is defined as follows:\n<span class=\"math-container\">$$L_i^{svm} = \\max\\{ 1 - y_i(\\overline{W} \\cdot \\overline{X}_i), 0 \\} \\tag{1.9}$$</span>\nNote that the perceptron does not keep the constant term of <span class=\"math-container\">$1$</span> on the right-hand side of Equation 1.7, whereas the hinge loss keeps this constant within the maximization function. This change does not affect the algebraic expression for the gradient, but it does change which points are lossless and should not cause an update. The relationship between the\nperceptron criterion and the hinge loss is shown in Figure 1.6. This similarity becomes particularly evident when the perceptron updates of Equation 1.6 are rewritten as follows:\n<span class=\"math-container\">$$\\overline{W} \\Leftarrow \\overline{W} + \\alpha \\sum_{(\\overline{X}, y) \\in S^+} y \\overline{X} \\tag{1.10}$$</span>\nHere, <span class=\"math-container\">$S^+$</span> is defined as the set of all misclassified training points <span class=\"math-container\">$\\overline{X} \\in S$</span> that satisfy the condition <span class=\"math-container\">$y(\\overline{W} \\cdot \\overline{X}) &lt; 0$</span>. This update seems to look somewhat different from the perceptron, because the perceptron uses the error <span class=\"math-container\">$E(\\overline{X})$</span> for the update, which is replaced with <span class=\"math-container\">$y$</span> in the update above. A key point is that the (integer) error value <span class=\"math-container\">$E(X) = (y − \\text{sign}\\{\\overline{W} \\cdot \\overline{X} \\}) \\in \\{ −2, +2 \\}$</span> can never be <span class=\"math-container\">$0$</span> for misclassified points in <span class=\"math-container\">$S^+$</span>. Therefore, we have <span class=\"math-container\">$E(\\overline{X}) = 2y$</span> <em>for misclassified points</em>, and <span class=\"math-container\">$E(X)$</span> can be replaced with <span class=\"math-container\">$y$</span> in the updates after absorbing the factor of <span class=\"math-container\">$2$</span> within the learning rate.</p>\n</blockquote>\n<p>Equation 1.6 is as follows:</p>\n<blockquote>\n<p><span class=\"math-container\">$$\\overline{W} \\Leftarrow \\overline{W} + \\alpha \\sum_{\\overline{X} \\in S} E(\\overline{X})\\overline{X}, \\tag{1.6}$$</span>\nwhere <span class=\"math-container\">$S$</span> is a randomly chosen subset of training points, <span class=\"math-container\">$\\overline{X} = [x_1, \\dots, x_d]$</span> is a data instance (vector of <span class=\"math-container\">$d$</span> feature variables), <span class=\"math-container\">$\\overline{W} = [w_1, \\dots, w_d]$</span> are the weights, <span class=\"math-container\">$\\alpha$</span> is the learning rate, and <span class=\"math-container\">$E(\\overline{X}) = (y - \\hat{y})$</span> is an error value, where <span class=\"math-container\">$\\hat{y} = \\text{sign}\\{ \\overline{W} \\cdot \\overline{X} \\}$</span> is the prediction and <span class=\"math-container\">$y$</span> is the observed value of the binary class variable.</p>\n</blockquote>\n<p>Equation 1.7 is as follows:</p>\n<blockquote>\n<p><span class=\"math-container\">$$L_i^{(0/1)} = \\dfrac{1}{2} (y_i - \\text{sign}\\{ \\overline{W} \\cdot \\overline{X_i} \\})^2 = 1 - y_i \\cdot \\text{sign} \\{ \\overline{W} \\cdot \\overline{X_i} \\} \\tag{1.7}$$</span></p>\n</blockquote>\n<p>And figure 1.6 is as follows:</p>\n<blockquote>\n<p><a href=\"https://i.sstatic.net/nmMyU.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/nmMyU.png\" alt=\"enter image description here\" /></a></p>\n</blockquote>\n<p>Figure 1.6 looks unclear to me. What is figure 1.6 showing, and how is it relevant to the point that the author is trying to make?</p>\n"
  },
  {
    "tags": [
      "comparison",
      "transformer",
      "gpt",
      "positional-encoding"
    ],
    "owner": {
      "account_id": 14122576,
      "reputation": 305,
      "user_id": 26580,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/804161bd975649daac0baaa19c4c0c04?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Leevo",
      "link": "https://ai.stackexchange.com/users/26580/leevo"
    },
    "is_answered": true,
    "view_count": 4077,
    "accepted_answer_id": 34346,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1646154065,
    "creation_date": 1606168991,
    "last_edit_date": 1638285637,
    "question_id": 24831,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/24831/what-is-the-difference-between-the-positional-encoding-techniques-of-the-transfo",
    "title": "What is the difference between the positional encoding techniques of the Transformer and GPT?",
    "body": "<p>I know the original Transformer and the GPT (1-3) use two slightly different <strong>positional encoding</strong> techniques.</p>\n<p>More specifically, in GPT they say positional encoding is <em>learned</em>. What does that mean? OpenAI's papers don't go into detail very much.</p>\n<p>How do they really differ, mathematically speaking?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "natural-language-processing",
      "objective-functions",
      "transformer",
      "attention"
    ],
    "owner": {
      "account_id": 4511013,
      "reputation": 1700,
      "user_id": 42699,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/17c5621384e4543d3778b583d76e9f42?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user3667125",
      "link": "https://ai.stackexchange.com/users/42699/user3667125"
    },
    "is_answered": true,
    "view_count": 7300,
    "accepted_answer_id": 25094,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1634646054,
    "creation_date": 1607383086,
    "last_edit_date": 1607383669,
    "question_id": 25053,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/25053/what-is-the-cost-function-of-a-transformer",
    "title": "What is the cost function of a transformer?",
    "body": "<p>The paper <a href=\"https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\" rel=\"noreferrer\">Attention Is All You Need</a> describes the transformer architecture that has an encoder and a decoder.</p>\n<p>However, I wasn't clear on what the cost function to minimize is for such an architecture.</p>\n<p>Consider a translation task, for example, where give an English sentence <span class=\"math-container\">$x_{english} = [x_0, x_1, x_2, \\dots, x_m]$</span>, the transformer decodes the sentence into a French sentence <span class=\"math-container\">$x_{french}' = [x_0', x_1', \\dots, x_n']$</span>. Let's say the true label is <span class=\"math-container\">$y_{french} = [y_0, y_1, \\dots, y_p]$</span>.</p>\n<p>What is the object function of the transformer? Is it the MSE between <span class=\"math-container\">$x_{french}'$</span> and <span class=\"math-container\">$y_{french}$</span>? And does it have any weight regularization terms?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "q-learning",
      "dqn",
      "objective-functions",
      "bellman-equations"
    ],
    "owner": {
      "account_id": 16834446,
      "reputation": 183,
      "user_id": 42849,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/JqGdP.jpg?s=256",
      "display_name": "Yves Boutellier",
      "link": "https://ai.stackexchange.com/users/42849/yves-boutellier"
    },
    "is_answered": true,
    "view_count": 2082,
    "accepted_answer_id": 25090,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1607621946,
    "creation_date": 1607538532,
    "last_edit_date": 1607621946,
    "question_id": 25086,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/25086/how-is-the-dqn-loss-derived-from-or-theoretically-motivated-by-the-bellman-equ",
    "title": "How is the DQN loss derived from (or theoretically motivated by) the Bellman equation, and how is it related to the Q-learning update?",
    "body": "<p>I'm doing a project on Reinforcement Learning. I programmed an agent that uses DDQN. There are a lot of tutorials on that, so the code implementation was not that hard.</p>\n<p>However, I have problems understanding how one should come up with this kind of algorithms by starting from the Bellman equation, and I don't find a good understandable explanation addressing this derivation/path of reasoning.</p>\n<p>So, my questions are:</p>\n<ol>\n<li>How is the loss to train the DQN derived from (or theoretically motivated by) the Bellman equation?</li>\n<li>How is it related to the usual Q-learning update?</li>\n</ol>\n<p>According to my current notes, the Bellman equation looks like this</p>\n<p><span class=\"math-container\">$$Q_{\\pi} (s,a) = \\sum_{s'} P_{ss'}^a (r_{s,a} + \\gamma \\sum_{a'} \\pi(a'|s') Q_{\\pi} (s',a')) \\label{1}\\tag{1} $$</span></p>\n<p>which, to my understanding, is a recursive expression that says:\nThe state-action pair gives a reward that is equal to the sum over all possible states <span class=\"math-container\">$s'$</span> with the probability of getting to this state after taking action <span class=\"math-container\">$a$</span> (denoted as <span class=\"math-container\">$P_{ss'}^a$</span>, which means the environment acts on the agent) times the reward the agent got from taking action <span class=\"math-container\">$a$</span> in state <span class=\"math-container\">$s$</span> + discounted sum of the probability of the different possible actions <span class=\"math-container\">$a'$</span> times the reward of the state, action pair <span class=\"math-container\">$s',a'$</span>.</p>\n<p>The Q-Learning iteration (intermediate step) is often denoted as:</p>\n<p><span class=\"math-container\">$$Q^{new}(s,a) \\leftarrow  Q(s,a) + \\alpha (r + \\gamma \\max_a Q(s',a') - Q(s,a)) \\label{2}\\tag{2}$$</span></p>\n<p>which means that the new state, action reward is the old Q value + learning rate, <span class=\"math-container\">$\\alpha$</span>, times the temporal difference, <span class=\"math-container\">$(r + \\gamma \\max_a Q(s',a') - Q(s,a))$</span>, which consists of the actual reward the agent received + a discount factor times the Q function of this new state-action pair minus the old Q function.</p>\n<p>The Bellman equation can be converted into an update rule because an algorithm that uses that update rule converges, as <a href=\"https://ai.stackexchange.com/a/20163/2444\">this answer</a> states.</p>\n<p>In the case of (D)DQN, <span class=\"math-container\">$Q(s,a)$</span> is estimated by our NN that leads to an action <span class=\"math-container\">$a$</span> and we receive <span class=\"math-container\">$r$</span> and <span class=\"math-container\">$s'$</span>.</p>\n<p>Then we feed in <span class=\"math-container\">$s$</span> as well as <span class=\"math-container\">$s'$</span> into our NN (with Double DQN we feed them into different NNs). The <span class=\"math-container\">$\\max_a Q(s',a')$</span> is performed on the output of our target network. This q-value is then multiplied with <span class=\"math-container\">$\\gamma$</span> and <span class=\"math-container\">$r$</span> is added to the product. Then this sum replaces the q-value from the other NN. Since this basic NN outputted <span class=\"math-container\">$Q(s,a)$</span> but should have outputted <span class=\"math-container\">$r + \\gamma \\max_a Q(s',a')$</span> we train the basic NN to change the weights, so that it would output closer to this temporal target difference.</p>\n"
  },
  {
    "tags": [
      "probability-distribution",
      "kl-divergence",
      "wasserstein-metric",
      "total-variational-distance"
    ],
    "owner": {
      "account_id": 11939849,
      "reputation": 253,
      "user_id": 32583,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/83a5c46b5b9da5aa650c7bd709909ef7?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Federico Taschin",
      "link": "https://ai.stackexchange.com/users/32583/federico-taschin"
    },
    "is_answered": true,
    "view_count": 3875,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1608651627,
    "creation_date": 1608042014,
    "question_id": 25205,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/25205/why-is-kl-divergence-used-so-often-in-machine-learning",
    "title": "Why is KL divergence used so often in Machine Learning?",
    "body": "<p>The KL Divergence is quite easy to compute in closed form for simple distributions -such as Gaussians- but has some not-very-nice properties. For example, it is not symmetrical (thus it is not a metric) and it does not respect the triangular inequality.</p>\n<p>What is the reason it is used so often in ML? Aren't there other statistical distances that can be used instead?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "training",
      "weights",
      "weights-initialization"
    ],
    "owner": {
      "account_id": 15332023,
      "reputation": 628,
      "user_id": 37120,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/6wFcg.jpg?s=256",
      "display_name": "Tinu",
      "link": "https://ai.stackexchange.com/users/37120/tinu"
    },
    "is_answered": true,
    "view_count": 1373,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1703937839,
    "creation_date": 1608246937,
    "last_edit_date": 1608901614,
    "question_id": 25254,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/25254/why-is-there-a-uniform-and-normal-version-of-he-xavier-initialization-in-dl-li",
    "title": "Why is there a Uniform and Normal version of He / Xavier initialization in DL libraries?",
    "body": "<p>Two of the most popular initialization schemes for neural network weights today are <a href=\"http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\" rel=\"noreferrer\">Xavier</a> and <a href=\"https://arxiv.org/pdf/1502.01852v1.pdf\" rel=\"noreferrer\">He</a>. Both methods propose random weight initialization with a variance dependent on the number of input and output units. Xavier proposes</p>\n<p><span class=\"math-container\">$$W \\sim \\mathcal{U}\\Bigg[-\\frac{\\sqrt{6}}{\\sqrt{n_{in}+n_{out}}},\\frac{\\sqrt{6}}{\\sqrt{n_{in}+n_{out}}}\\Bigg]$$</span></p>\n<p>for networks with <span class=\"math-container\">$\\text{tanh}$</span> activation function and He proposes</p>\n<p><span class=\"math-container\">$$W \\sim \\mathcal{N}(0,\\sqrt{s/n_{in}})$$</span></p>\n<p>for <span class=\"math-container\">$\\text{ReLU}$</span> activation. Both initialization schemes are implemented in the most commonly used deep learning libraries for python, <a href=\"https://pytorch.org/docs/stable/nn.init.html\" rel=\"noreferrer\">PyTorch</a> and <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/initializers\" rel=\"noreferrer\">TensorFlow</a>.<br />\nHowever, <strong>for both versions we have a normal and uniform version</strong>. Now the main argument of both papers is about the variance of the information at initialization time (which is dependent on the non-linearity) and that it should stay constant across all layers when back-propagating. I see how one can simply adjust the bounds <span class=\"math-container\">$[-a,a]$</span> of a uniform variable in such a way that the random variable has the desired standard deviation and vice versa (<span class=\"math-container\">$\\sigma = a/\\sqrt{3}$</span>), but <strong>I'm not sure why we need a normal and a uniform version for both schemes?</strong> Wouldn't it be just enough to have only normal or only uniform? Or uniform Xavier and normal He as proposed in their papers?</p>\n<p>I can imagine uniform distributions are easier to sample from a computational point of view, but since we do the initialization operation only once at the beginning, the computational cost is negligible compared to that from training. Further uniform variables are bounded, so there are no long tail observations as one would expect in a normal. I suppose that's why both libraries have truncated normal initializations.</p>\n<p><strong>Are there any theoretical, computational or empirical justifications for when to use a normal over a uniform, or a uniform over a normal weight initialization regardless of the final weight variance?</strong></p>\n"
  },
  {
    "tags": [
      "tensorflow",
      "keras",
      "image-segmentation",
      "u-net"
    ],
    "owner": {
      "account_id": 11619997,
      "reputation": 139,
      "user_id": 42357,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/adee154786a897ee512bfc74bf4601b7?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Lis Louise",
      "link": "https://ai.stackexchange.com/users/42357/lis-louise"
    },
    "is_answered": true,
    "view_count": 769,
    "accepted_answer_id": 31521,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1662519965,
    "creation_date": 1609091169,
    "last_edit_date": 1646944249,
    "question_id": 25406,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/25406/validation-accuracy-higher-than-training-accurarcy",
    "title": "Validation accuracy higher than training accurarcy",
    "body": "<p>I implemented the unet in TensorFlow for the segmentation of MRI images of the thigh. I noticed I always get a higher validation accuracy  by a small gap, independently of the initial split. One example:</p>\n<p><a href=\"https://i.sstatic.net/dskC9.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/dskC9.png\" alt=\"enter image description here\" /></a></p>\n<p>So I researched when this could be possible:</p>\n<ol>\n<li>When we have an &quot;easy&quot; validation set. I trained it for different initial splitting, all of them showed a higher validation accuracy.</li>\n<li>Regularization and augmentation may reduce the training accuracy. I removed the augmentation and  dropout regularization and still observed the same gap, the only difference was that it took more epochs to reach convergence.</li>\n<li>The last thing I found was that in Keras the training accuracy and loss are averaged over each iteration of the corresponding epoch, while the validation accuracy and loss is calculated from the model at the end of the epoch, which might make the the training loss higher and accuracy lower.</li>\n</ol>\n<p>So I thought that if I train and validate on the same set, then I should get the same curve but shifted by one epoch. So I trained only on 2 batches and validated on the same 2 batches (without dropout or augmentation). I still think there is something else happening because they don't look quite the same and at least at the end when the weights are not changing anymore, the training and validation accuracy should be the same (but still the validation accuracy is higher by a small gap). This is the plot:</p>\n<p><a href=\"https://i.sstatic.net/9sJlK.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/9sJlK.png\" alt=\"enter image description here\" /></a>\n<a href=\"https://i.sstatic.net/xBUt9.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/xBUt9.png\" alt=\"enter image description here\" /></a></p>\n<p>Is there anything else that can be increasing the loss values, this is the model I am using:</p>\n<pre><code>def unet_no_dropout(pretrained_weights=None, input_size=(512, 512, 1)):\ninputs = tf.keras.layers.Input(input_size)\nconv1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\nconv1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\npool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\nconv2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\nconv2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\npool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\nconv3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\nconv3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\npool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\nconv4 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\nconv4 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n#drop4 = tf.keras.layers.Dropout(0.5)(conv4)\npool4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n\nconv5 = tf.keras.layers.Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\nconv5 = tf.keras.layers.Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n#drop5 = tf.keras.layers.Dropout(0.5)(conv5)\n\nup6 = tf.keras.layers.Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n    tf.keras.layers.UpSampling2D(size=(2, 2))(conv5))\nmerge6 = tf.keras.layers.concatenate([conv4, up6], axis=3)\n#merge6 = tf.keras.layers.concatenate([conv4, up6], axis=3)\nconv6 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\nconv6 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n\nup7 = tf.keras.layers.Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n    tf.keras.layers.UpSampling2D(size=(2, 2))(conv6))\nmerge7 = tf.keras.layers.concatenate([conv3, up7], axis=3)\nconv7 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\nconv7 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n\nup8 = tf.keras.layers.Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n    tf.keras.layers.UpSampling2D(size=(2, 2))(conv7))\nmerge8 = tf.keras.layers.concatenate([conv2, up8], axis=3)\nconv8 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\nconv8 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n\nup9 = tf.keras.layers.Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n    tf.keras.layers.UpSampling2D(size=(2, 2))(conv8))\nmerge9 = tf.keras.layers.concatenate([conv1, up9], axis=3)\nconv9 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\nconv9 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\nconv9 = tf.keras.layers.Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\nconv10 = tf.keras.layers.Conv2D(1, 1, activation='sigmoid')(conv9)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=conv10)\n\nmodel.compile(optimizer = Adam(lr = 2e-4), loss = 'binary_crossentropy', metrics = [tf.keras.metrics.Accuracy()])\n#model.compile(optimizer=tf.keras.optimizers.Adam(2e-4), loss=combo_loss(alpha=0.2, beta=0.4), metrics=[dice_accuracy])\n#model.compile(optimizer=RMSprop(lr=0.00001), loss=combo_loss, metrics=[dice_accuracy])\n\nif (pretrained_weights):\n    model.load_weights(pretrained_weights)\n\nreturn model\n</code></pre>\n<p>and this is how I save the model:</p>\n<pre><code>model_checkpoint = tf.keras.callbacks.ModelCheckpoint('unet_ThighOuterSurfaceval.hdf5',monitor='val_loss', verbose=1, save_best_only=True)\nmodel_checkpoint2 = tf.keras.callbacks.ModelCheckpoint('unet_ThighOuterSurface.hdf5', monitor='loss', verbose=1, save_best_only=True)\n\nmodel = unet_no_dropout()\nhistory = model.fit(genaug, validation_data=genval, validation_steps=len(genval), steps_per_epoch=len(genaug), epochs=80, callbacks=[model_checkpoint, model_checkpoint2])\n</code></pre>\n"
  },
  {
    "tags": [
      "deep-rl",
      "monte-carlo-tree-search",
      "alphazero"
    ],
    "owner": {
      "account_id": 8716374,
      "reputation": 167,
      "user_id": 43016,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/a9619b5966f4c51d26bf5b63c9b39f11?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "sb3",
      "link": "https://ai.stackexchange.com/users/43016/sb3"
    },
    "is_answered": true,
    "view_count": 3733,
    "accepted_answer_id": 25472,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1689649628,
    "creation_date": 1609293830,
    "question_id": 25451,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/25451/how-does-alphazeros-mcts-work-when-starting-from-the-root-node",
    "title": "How does AlphaZero&#39;s MCTS work when starting from the root node?",
    "body": "<p>From the AlphaGo Zero paper, during MCTS, statistics for each new node are initialized as such:</p>\n<blockquote>\n<p><span class=\"math-container\">${N(s_L, a) = 0, W (s_L, a) = 0, Q(s_L, a) = 0, P (s_L, a) = p_a}$</span>.</p>\n</blockquote>\n<p>The PUCT algorithm for selecting the best child node is <span class=\"math-container\">$a_t = argmax(Q(s,a) + U(s,a))$</span>, where <span class=\"math-container\">$U(s,a) = c_{puct} P(s,a) \\frac{\\sqrt{\\sum_b N(s,b)}}{1 + N(s, a)}$</span>.</p>\n<p>If we start from scratch with a tree that only contains the root node and no children have been visited yet, then this should evaluate to 0 for all actions <span class=\"math-container\">$a$</span> that we can take from the root node. Do we then simply uniformly sample an action to take?</p>\n<p>Also, during the expand() step when we add an unvisited node <span class=\"math-container\">$s_L$</span> to the tree, this node's children will also have not been visited, and we run into the same problem where PUCT will return 0 for all actions. Do we do the same uniform sampling here as well?</p>\n"
  },
  {
    "tags": [
      "recurrent-neural-networks",
      "long-short-term-memory",
      "overfitting",
      "regularization",
      "dropout"
    ],
    "owner": {
      "account_id": 19326312,
      "reputation": 133,
      "user_id": 44085,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/f8ed035b080c459af2d8d99c816cc310?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Leo",
      "link": "https://ai.stackexchange.com/users/44085/leo"
    },
    "is_answered": true,
    "view_count": 11684,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1633996940,
    "creation_date": 1611553825,
    "last_edit_date": 1612337488,
    "question_id": 25963,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/25963/how-should-we-regularize-an-lstm-model",
    "title": "How should we regularize an LSTM model?",
    "body": "<p>There are five parameters from an LSTM layer for regularization if I am correct.</p>\n<p>To deal with overfitting, I would start with</p>\n<ol>\n<li>reducing the layers</li>\n<li>reducing the hidden units</li>\n<li>Applying dropout or regularizers.</li>\n</ol>\n<p>There are <code>kernel_regularizer</code>, <code>recurrent_regularizer</code>, <code>bias_regularizer</code>, <code>activity_regularizer</code>, <code>dropout</code> and <code>recurrent_dropout</code>.</p>\n<p>They have their definitions on the Keras's website, but can anyone share more experiences on how to reduce overfitting?</p>\n<p>And how are these five parameters used? For example, which parameters are most frequently used and what kind of value should be input? ?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "terminology",
      "function-approximation",
      "state-spaces",
      "contextual-bandits"
    ],
    "owner": {
      "account_id": 457335,
      "reputation": 183,
      "user_id": 33586,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Kg8G7.jpg?s=256",
      "display_name": "Maxim Volgin",
      "link": "https://ai.stackexchange.com/users/33586/maxim-volgin"
    },
    "is_answered": true,
    "view_count": 3856,
    "accepted_answer_id": 26339,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1613062868,
    "creation_date": 1613016646,
    "last_edit_date": 1613061080,
    "question_id": 26327,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/26327/what-is-the-relation-between-the-context-in-contextual-bandits-and-the-state-in",
    "title": "What is the relation between the context in contextual bandits and the state in reinforcement learning?",
    "body": "<p>Conceptually, in general, how is the <em>context</em> being handled in contextual bandits (CB), compared to <em>states</em> in reinforcement learning (RL)?</p>\n<p>Specifically, in RL, we can use a function approximator (e.g. a neural network) to generalize to other states. Would that also be possible or desirable in the CB setting?</p>\n<p>In general, what is the relation between the <em>context</em> in CB and the <em>state</em> in RL?</p>\n"
  },
  {
    "tags": [
      "q-learning",
      "deep-rl",
      "gym",
      "double-q-learning"
    ],
    "owner": {
      "account_id": 6741141,
      "reputation": 81,
      "user_id": 47642,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/--pZeNeX9Mgc/AAAAAAAAAAI/AAAAAAAABFE/EBPWlc4LODI/s256-rj/photo.jpg",
      "display_name": "Virus",
      "link": "https://ai.stackexchange.com/users/47642/virus"
    },
    "is_answered": true,
    "view_count": 4396,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1746286649,
    "creation_date": 1622730827,
    "last_edit_date": 1623685587,
    "question_id": 28079,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/28079/deep-q-learning-catastrophic-drop-reasons",
    "title": "Deep Q-Learning &quot;catastrophic drop&quot; reasons?",
    "body": "<p>I am implementing some &quot;classical&quot; papers in Model Free RL like DQN, Double DQN, and Double DQN with Prioritized Replay.</p>\n<p>Through the various models im running on <code>CartPole-v1</code> using the same underlying NN, I am noticing all of the above 3 exhibit a sudden and severe drop in average reward (with a sudden and significant increase in loss) after achieving peak scores.</p>\n<p>After reading online, I can see that this is a recognized problem but I cant find a suitable explanation. Things I have tried to mitigate:</p>\n<ul>\n<li>adapt model architecture</li>\n<li>tune hyperparams like LR, batch_size, loss function (MSE, Huber)</li>\n</ul>\n<p>This problem persists, and I cannot seem to achieve any sustained peak performance.</p>\n<p><strong>Useful links I found</strong>:</p>\n<ul>\n<li><a href=\"https://ai.stackexchange.com/questions/12627/what-could-be-causing-the-drastic-performance-drop-of-the-dqn-model-on-the-pong\">What could be causing the drastic performance drop of the DQN model on the Pong environment?</a></li>\n</ul>\n<p><strong>Example:</strong></p>\n<ul>\n<li>till ~250 episodes in Double DQN with PR (with annealing beta), performance steady goes up in both increase in reward and decrease in loss</li>\n<li>after that stage, the performance dips suddenly in both decreased average reward and increased loss as seen in output below</li>\n</ul>\n<pre><code>Episode: Mean Reward: Mean Loss: Mean Step\n  200 : 173.075 : 0.030: 173.075\n  400 : 193.690 : 0.011: 193.690\n  600 : 168.735 : 0.015: 168.735\n  800 : 135.110 : 0.015: 135.110\n 1000 : 157.700 : 0.013: 157.700\n 1200 :  99.335 : 0.013: 99.335\n 1400 :  97.450 : 0.015: 97.450\n 1600 : 102.030 : 0.012: 102.030\n 1800 : 130.815 : 0.010: 130.815\n 1999 :   89.76 : 0.013: 89.76\n</code></pre>\n<p><strong>Questions:</strong></p>\n<ul>\n<li>what is the theoretical reasoning behind this? Does this <code>fragile</code> nature mean we cannot use the above mentioned 3 algorithms to solve <code>CartPole-v1</code>?</li>\n<li>if not, what steps can help mitigate this? Could this be overfitting and what does this brittle nature indicate?</li>\n<li>any references to follow up with regarding this &quot;catastrophic drop&quot;?</li>\n<li>I observe similar behavior in other environments as well, does this mean that the above mentioned 3 algorithms are insufficient?</li>\n</ul>\n<p><strong>Edit:</strong>\nTaking from @devidduma's answer, I added time based LR decay to the DDQN+PRB model and kept everything else same. Here are the numbers, they look better than before in terms of the magnitude of the performance drop.</p>\n<pre><code>   10 : 037.27 : 0.5029 : 037.27\n   20 : 121.40 : 0.0532 : 121.40\n   30 : 139.80 : 0.0181 : 139.80\n   40 : 157.40 : 0.0119 : 157.40\n   50 : 225.10 : 0.0107 : 225.10 &lt;- decay starts here, factor = 0.001\n   60 : 227.90 : 0.0101 : 227.90\n   70 : 227.00 : 0.0087 : 227.00\n   80 : 154.30 : 0.0064 : 154.30\n   90 : 126.90 : 0.0054 : 126.90\n   99 : 154.78 : 0.0057 : 154.78\n</code></pre>\n<p><strong>Edit:</strong></p>\n<ul>\n<li>after further testing, pytorch's <code>ReduceLROnPlateau</code> seems to be working best with <code>patience=0</code> param.</li>\n</ul>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "terminology",
      "word-embedding"
    ],
    "owner": {
      "account_id": 3197090,
      "reputation": 4082,
      "user_id": 18758,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/339397e23b4e4cc78aa36e2b126252c7?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "hanugm",
      "link": "https://ai.stackexchange.com/users/18758/hanugm"
    },
    "is_answered": true,
    "view_count": 7224,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1721766190,
    "creation_date": 1631239609,
    "last_edit_date": 1640521335,
    "question_id": 31632,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/31632/can-i-always-use-encoding-and-embedding-interchangeably",
    "title": "Can I always use &quot;encoding&quot; and &quot;embedding&quot; interchangeably?",
    "body": "<p>This question is restricted to the text domain only.</p>\n<p>The meaning of the word &quot;<a href=\"https://www.lexico.com/definition/encode\" rel=\"noreferrer\">encode</a>&quot; is <em>Convert (information or instruction) into a particular form</em>. One which performs encoding is called an encoder.</p>\n<p>In deep learning, an encoder can also be the first part of a neural network (autoencoder) that simulates identity function, which governs the English meaning of encoder since it encodes the input.</p>\n<p>Embeddings are encodings where the intention is to preserve semantics. You can observe the following excerpt from the chapter <a href=\"https://web.stanford.edu/%7Ejurafsky/slp3/6.pdf\" rel=\"noreferrer\">Vector Semantics and Embeddings</a></p>\n<blockquote>\n<p>In this chapter we introduce vector semantics, which instantiates this\nlinguistic hypothesis by learning representations of the meaning of\nwords, called <strong>embeddings</strong>, directly from their distributions in texts.</p>\n</blockquote>\n<p><strong>But all encodings may not be the embeddings</strong> since encodings might not always preserve semantics (?). I have doubt in this statement which I inferred based on my current knowledge.</p>\n<p>Many times, I came across the terms text encoding and text embedding interchangeably. But failing to catch whether they are the same or we need to be choosy while using them.</p>\n<p>Consider the following usages of encoding and embedding in the paper titled <a href=\"https://arxiv.org/pdf/1605.05396.pdf\" rel=\"noreferrer\">Generative Adversarial Text to Image Synthesis</a> by <em>Scott Reed et al</em>.</p>\n<blockquote>\n<p>#1: The intuition here is that a <strong>text encoding</strong> should have a higher compatibility score with images of the correspondong class compared to any other class and vice-versa.</p>\n<p>#2: <strong>Text encoding</strong> <span class=\"math-container\">$\\phi(t)$</span> is used by both generator and discriminator.</p>\n<p>#3: ...where <span class=\"math-container\">$T$</span> is the dimension of the <strong>text description embedding</strong>.</p>\n<p>#4: ... we encode the text query <span class=\"math-container\">$t$</span> using <strong>text encoder</strong> <span class=\"math-container\">$\\phi$</span>. The <strong>description embedding</strong> <span class=\"math-container\">$\\phi(t)$</span> is first compressed ...</p>\n</blockquote>\n<p>I think they are used interchangeably. Is it true? Can I use any word if I am confident enough that my encoding is semantic preserving? Or is there any strong reason for choosing the words?</p>\n<p>If you observe the last point, the word &quot;encoder&quot; is used. Can I use embedder instead of it?</p>\n"
  },
  {
    "tags": [
      "computer-vision",
      "training",
      "datasets",
      "research",
      "image-processing"
    ],
    "owner": {
      "account_id": 13252799,
      "reputation": 645,
      "user_id": 43632,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/s256-rj/photo.jpg",
      "display_name": "Bert Gayus",
      "link": "https://ai.stackexchange.com/users/43632/bert-gayus"
    },
    "is_answered": true,
    "view_count": 2578,
    "accepted_answer_id": 31805,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1632396658,
    "creation_date": 1632312254,
    "last_edit_date": 1632356961,
    "question_id": 31798,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/31798/is-it-okay-to-use-publicly-available-instagram-videos-to-train-an-ai",
    "title": "Is it okay to use publicly available Instagram videos to train an AI?",
    "body": "<p>Since I haven't found any good training data for my university project, I want to use pictures and videos from public Instagram profiles. Am I allowed to do that?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "optimization",
      "gradient-descent"
    ],
    "owner": {
      "account_id": 13219673,
      "reputation": 1436,
      "user_id": 26726,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/92f202079aee10525313ec069b645011?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Recessive",
      "link": "https://ai.stackexchange.com/users/26726/recessive"
    },
    "is_answered": true,
    "view_count": 6734,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1638224326,
    "creation_date": 1637111511,
    "last_edit_date": 1637143796,
    "question_id": 32428,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/32428/why-is-gradient-descent-used-over-the-conjugate-gradient-method",
    "title": "Why is gradient descent used over the conjugate gradient method?",
    "body": "<p>Based on some preliminary research, the conjugate gradient method is almost exactly the same as gradient descent, except the search direction must be orthogonal to the previous step.</p>\n<p>From what I've read, the idea tends to be that the conjugate gradient method is <em><a href=\"https://scicomp.stackexchange.com/q/7819\">better</a></em> than regular gradient descent, so if that's the case, why is regular gradient descent used?</p>\n<p>Additionally, I know algorithms such as the <a href=\"https://en.wikipedia.org/wiki/Powell%27s_method\" rel=\"noreferrer\">Powell method</a> use the conjugate gradient method for finding minima, but I also know the Powell method is computationally expensive in finding parameter updates as it can be run on any arbitrary function without the need to find partial derivatives of the computational graph. More specifically, when gradient descent is run on a neural network, the gradient with respect to every single parameter is calculated in the backward pass, whereas the Powell method just calculates the gradient of the overall function at this step from what I understand. (See <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\" rel=\"noreferrer\">scipy's minimize</a>, you could technically pass an entire neural network into this function and it would optimize it, but there's no world where this is faster than backpropagation)</p>\n<p>However, given how similar gradient descent is to the conjugate gradient method, could we not replace the gradient updates for each parameter with one that is orthogonal to its last update? Would that not be faster?</p>\n"
  },
  {
    "tags": [
      "backpropagation",
      "variational-autoencoder"
    ],
    "owner": {
      "account_id": 2949572,
      "reputation": 183,
      "user_id": 51646,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/1e0a0adad4a49e505098622d3f9e2f54?s=256&d=identicon&r=PG",
      "display_name": "Luke Wolcott",
      "link": "https://ai.stackexchange.com/users/51646/luke-wolcott"
    },
    "is_answered": true,
    "view_count": 2404,
    "accepted_answer_id": 33829,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1654355288,
    "creation_date": 1639866799,
    "question_id": 33824,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/33824/how-does-backprop-work-through-the-random-sampling-layer-in-a-variational-autoen",
    "title": "How does backprop work through the random sampling layer in a variational autoencoder?",
    "body": "<p>Implementations of variational autoencoders that I've looked at all include a sampling layer as the last layer of the encoder block.  The encoder learns to generate a mean and standard deviation for each input, and samples from it to get the input's representation in latent space.  The decoder then attempts to decode this back out to match the inputs.</p>\n<p><strong>My question</strong>: How does backpropagation handle the random sampling step?</p>\n<p>Random sampling is not a deterministic function and doesn't have a derivative.  In order to train the encoder, gradient updates must somehow propagate back from the loss, through the sampling layer.</p>\n<p>I did my best to hunt for the source code for tensorflow's autodifferentiation of this function, but couldn't find it.  Here's an example of a keras implementation of that sampling step, from <a href=\"https://keras.io/examples/generative/vae/\" rel=\"noreferrer\">the keras docs</a>, in which <code>tf.keras.backend.random_normal</code> is used for the sampling.</p>\n<pre><code>class Sampling(layers.Layer):\n    &quot;&quot;&quot;Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.&quot;&quot;&quot;\n\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n</code></pre>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "comparison",
      "deep-rl",
      "objective-functions",
      "rewards"
    ],
    "owner": {
      "account_id": 19267771,
      "reputation": 195,
      "user_id": 53761,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/2c840a659faf46e7965b3fa29fe29cc9?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Theo Deep",
      "link": "https://ai.stackexchange.com/users/53761/theo-deep"
    },
    "is_answered": true,
    "view_count": 11148,
    "accepted_answer_id": 35025,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1648726758,
    "creation_date": 1648638999,
    "last_edit_date": 1648647097,
    "question_id": 35023,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/35023/what-is-the-difference-between-a-loss-function-and-reward-penalty-in-deep-reinfo",
    "title": "What is the difference between a loss function and reward/penalty in Deep Reinforcement Learning?",
    "body": "<p>In Deep Reinforcement Learning (DRL) I am having difficulties in understanding the difference between a <em>Loss function</em>, a <em>reward/penalty</em> and the integration of both in DRL.</p>\n<ul>\n<li><p>Loss function: Given an output of the model and the ground truth, it\nmeasures &quot;how good&quot; the output has been. And using it, the parameters\nof the model are adjusted. For instance, MAE. But if you were working\nin Computer Vision quality, you could use, for instance, SSIM.</p>\n</li>\n<li><p>Reward: Given an agent (a model) and an environment, once the agent\nperforms an action, the environment gives it a reward (or a penalty)\nto measure &quot;how good&quot; the action has been. Very simple rewards are +1\nor -1.</p>\n</li>\n</ul>\n<p>So I see both the loss function and the reward/penalty are the quantitative way of measuring the output/action and making the model to learn. Am I right?</p>\n<p>Now, as for DRL. I see the typical diagram where the agent is modelled using a Neural Network (NN).</p>\n<p><a href=\"https://i.sstatic.net/CS6bI.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/CS6bI.png\" alt=\"enter image description here\" /></a></p>\n<p>I am trying to interpret it, but I do not understand it.</p>\n<p>Is it the policy related the loss function somehow? Where is the loss function? How does the reward feed the NN? Is it a parameter for the loss function?</p>\n<p>Maybe my confusion has to do with identifying NN with supervised learning, or with not getting this with Q-learning or so.. Can anyone help?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "backpropagation"
    ],
    "owner": {
      "account_id": 19301668,
      "reputation": 83,
      "user_id": 54656,
      "user_type": "registered",
      "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GhLBqQjX9oHrLAYHXxw7xSrE-ApwYKAZ9e5Xi1ITA=k-s256",
      "display_name": "0jas",
      "link": "https://ai.stackexchange.com/users/54656/0jas"
    },
    "is_answered": true,
    "view_count": 1644,
    "accepted_answer_id": 35450,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1652156478,
    "creation_date": 1652014417,
    "last_edit_date": 1652156478,
    "question_id": 35449,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/35449/what-do-symmetric-weights-mean-and-how-does-it-make-backpropagation-biologically",
    "title": "What do symmetric weights mean and how does it make backpropagation biologically implausible?",
    "body": "<p>I was reading a paper on alternatives to backpropagation as a learning algorithm in neural networks. In <a href=\"https://arxiv.org/pdf/1609.01596.pdf\" rel=\"nofollow noreferrer\">this paper</a>, the author talks about the disadvantages of backpropagation, and one of the disadvantages stated is that backpropagation requires symmetric weights and that's why it's not biologically plausible.</p>\n<p>What do symmetric weights mean and how does it make backpropagation biologically implausible?</p>\n"
  },
  {
    "tags": [
      "transformer",
      "pytorch",
      "attention"
    ],
    "owner": {
      "account_id": 13626697,
      "reputation": 83,
      "user_id": 54870,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/1f87f8fd74ea1344187b81436a2c001f?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Aushilfsgott",
      "link": "https://ai.stackexchange.com/users/54870/aushilfsgott"
    },
    "is_answered": true,
    "view_count": 5480,
    "accepted_answer_id": 35560,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1652866143,
    "creation_date": 1652791348,
    "last_edit_date": 1652863821,
    "question_id": 35548,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/35548/when-exactly-does-the-split-into-different-heads-in-multi-head-attention-occur",
    "title": "When exactly does the split into different heads in Multi-Head-Attention occur?",
    "body": "<p>I am confused by the Multi-Head part of the Multi-Head-Attention used in Transformers. My question concerns the implementations in Pytorch of <a href=\"https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention\" rel=\"noreferrer\">nn.MultiheadAttention</a> and its forward method multi_head_attention_forward and whether these are actually identical to the paper. Unfortunately, I have been unable to follow along the original code of the paper. So I could not check whether the implementations in Pytorch are acutally identical to the paper.</p>\n<p>Please forgive the excessive use of illustrations. However, I hope it will improve understanding my problem.</p>\n<p>What is the correct order for calculating the Queries Q, Keys K and Values V and splitting the operation into the individual Attention-Heads? Unfortunately most explanations I found online while helpful for understanding the general principle and intuition of Multi-Head-Attention did not go into the details of the implementation.</p>\n<p>In the original paper <a href=\"https://arxiv.org/abs/1706.03762\" rel=\"noreferrer\">Attention is all you need</a> Multi-Head-Attention is explained as followed:</p>\n<p><a href=\"https://i.sstatic.net/LmHJi.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/LmHJi.png\" alt=\"enter image description here\" /></a></p>\n<p>First, according to my current understanding, if we have a sequence of vectors with 512-dimensions (like in the original Transformer) and we have <span class=\"math-container\">$h=8$</span> Attention-Heads (again like the original), every Attention-Head attends to <span class=\"math-container\">$512/8=64$</span> entries of the input vector used to calculate the Attention in the corresponding head. So the first Attention-Head attends to the first 64 entries, the second to the second 64 entries and so on. However, if the split is conducted before calculating Q,K,V this would refer to the first 64 entries of X (this does not seem match the explanation in the paper I believe) while in the other case it would refer to the first 64 entries of Q,K,V.</p>\n<p>In the text they say &quot;project the queries, keys and values h times with different, learned linear projections to <span class=\"math-container\">$d_k,d_k$</span> and <span class=\"math-container\">$d_v$</span> dimensions and since they set <span class=\"math-container\">$d_k=d_v=d_{model}/h=512/8=64$</span>. Therefore, if we actually have single matrices for every Attention-Head h we would have</p>\n<p><span class=\"math-container\">$W^Q_i,W^K_i,W^V_i \\in \\mathbb{R}^{512x64} \\forall i \\in h$</span>.</p>\n<p>This matches the illustration found here <a href=\"https://jalammar.github.io/illustrated-transformer/\" rel=\"noreferrer\">https://jalammar.github.io/illustrated-transformer/</a></p>\n<p><a href=\"https://i.sstatic.net/ehiUv.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/ehiUv.png\" alt=\"enter image description here\" /></a></p>\n<p>It is explained that the input X is transformed into the Queries, Keys and Values for the different attention heads by using different projection matrices which are learned during training.</p>\n<p>This seems to indicate that the split into the individual Attention-Heads is conducted after the calculation of <span class=\"math-container\">$Q,K,V$</span> (or rather during the calculation). Since we have <span class=\"math-container\">$h=8$</span> this leads in sum to <span class=\"math-container\">$3*8*512*64=3*512*512$</span> learnable parameters in total (if we ignore the bias). Thus as far as the overall number of parameters is concerned we would have the same number if we would instead use three big matrices which concatenate the matrices of the individual Attention-Heads.</p>\n<p><span class=\"math-container\">$W^Q=[W^Q_1,W^Q_2,...,W^Q_h] \\in \\mathbb{R}^{512x512}$</span> <br />\n<span class=\"math-container\">$W^K=[W^K_1,W^K_2,...,W^K_h] \\in \\mathbb{R}^{512x512}$</span> <br />\n<span class=\"math-container\">$W^Q=[W^V_1,W^V_2,...,W^V_h] \\in \\mathbb{R}^{512x512}$</span></p>\n<p>In the explanation from the same author of GPT-2 (this model has an embedding dimension of 768 and 12 Attention-Heads, instead of 512 and 8 like the original Transformer) found here <a href=\"https://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention\" rel=\"noreferrer\">https://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention</a></p>\n<p><a href=\"https://i.sstatic.net/I8fz9.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/I8fz9.png\" alt=\"enter image description here\" /></a></p>\n<p><a href=\"https://i.sstatic.net/V75eY.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/V75eY.png\" alt=\"enter image description here\" /></a></p>\n<p>the Queries Q, Keys K and Values V are first calculated by multiplying the input with one big matrix which is the concatenation of <span class=\"math-container\">$W^Q,W^K,W^V$</span>. If the input for calculating Q, K and V is identical (which is the case for self-attention), it is clear to me that you can use <span class=\"math-container\">$W_{concat}=[W^Q,W^K,W^V]$</span> and obtain <span class=\"math-container\">$[Q,K,V]$</span>, since you essentially still multiply the input with each weight matrix separately.</p>\n<p>Then you can split the result to again obtain <span class=\"math-container\">$Q,K,V$</span> as individual matrices (The image displays <span class=\"math-container\">$q_9,k_9,v_9$</span> as an example, not the complete matrices). Then <span class=\"math-container\">$q_9,k_9,v_9$</span> are again split into 12 vectors which results in a matrix of dimension <span class=\"math-container\">$(12x64)$</span>.</p>\n<p>So overall here we did not use individual matrices per Attention-Head but only one larger matrix.</p>\n<p>Is this method mathematically identical to the one using individual smaller matrices per Attention-Head?</p>\n<p>It appears that this is the way the calculation is implented in Pytorch, if <span class=\"math-container\">$d_{model}=kdim=vdim$</span> Though here unlike in the paper which used <span class=\"math-container\">$d_k$</span> and <span class=\"math-container\">$d_v$</span> as names, <span class=\"math-container\">$kdim$</span> and <span class=\"math-container\">$vdim$</span> refer to the dimension of all Attention-Heads summed up, e.g. <span class=\"math-container\">$kdim=d_k*num_{heads}$</span>(=512 for the original Transformer).</p>\n<p>So  In the documentation of nn.modules.MultiheadAttention the model either creates three separate projection matrices to generate the Queries, Keys and Values or one big matrix (if the dimensions are identical). The following is part of the <code>_init_</code> function.</p>\n<pre><code>if self._qkv_same_embed_dim is False:\n            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))\n            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))\n            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))\n            self.register_parameter('in_proj_weight', None)\n        else:\n            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))\n            self.register_parameter('q_proj_weight', None)\n            self.register_parameter('k_proj_weight', None)\n            self.register_parameter('v_proj_weight', None)\n</code></pre>\n<p>If we stay in the standard case of <code>_qkv_same_embed_dim=True</code> the input is passed through a <code>nn.linear</code> as part of <code>_in_projection_packed</code> which is using <code>self.in_proj_weight</code> as the weight</p>\n<pre><code>if not use_separate_proj_weight:\n        assert in_proj_weight is not None, &quot;use_separate_proj_weight is False but in_proj_weight is None&quot;\n        q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n    else:\n        assert q_proj_weight is not None, &quot;use_separate_proj_weight is True but q_proj_weight is None&quot;\n        assert k_proj_weight is not None, &quot;use_separate_proj_weight is True but k_proj_weight is None&quot;\n        assert v_proj_weight is not None, &quot;use_separate_proj_weight is True but v_proj_weight is None&quot;\n        if in_proj_bias is None:\n            b_q = b_k = b_v = None\n        else:\n            b_q, b_k, b_v = in_proj_bias.chunk(3)\n        q, k, v = _in_projection(query, key, value, q_proj_weight, k_proj_weight, v_proj_weight, b_q, b_k, b_v)\n\ndef _in_projection_packed(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    w: Tensor,\n    b: Optional[Tensor] = None,\n) -&gt; List[Tensor]:\n    r&quot;&quot;&quot;\n    Performs the in-projection step of the attention operation, using packed weights.\n    Output is a triple containing projection tensors for query, key and value.\n    Args:\n        q, k, v: query, key and value tensors to be projected. For self-attention,\n            these are typically the same tensor; for encoder-decoder attention,\n            k and v are typically the same tensor. (We take advantage of these\n            identities for performance if they are present.) Regardless, q, k and v\n            must share a common embedding dimension; otherwise their shapes may vary.\n        w: projection weights for q, k and v, packed into a single tensor. Weights\n            are packed along dimension 0, in q, k, v order.\n        b: optional projection biases for q, k and v, packed into a single tensor\n            in q, k, v order.\n    Shape:\n        Inputs:\n        - q: :math:`(..., E)` where E is the embedding dimension\n        - k: :math:`(..., E)` where E is the embedding dimension\n        - v: :math:`(..., E)` where E is the embedding dimension\n        - w: :math:`(E * 3, E)` where E is the embedding dimension\n        - b: :math:`E * 3` where E is the embedding dimension\n        Output:\n        - in output list :math:`[q', k', v']`, each output tensor will have the\n            same shape as the corresponding input tensor.\n    &quot;&quot;&quot;\n    E = q.size(-1)\n    if k is v:\n        if q is k:\n            # self-attention\n            return linear(q, w, b).chunk(3, dim=-1)\n        else:\n            # encoder-decoder attention\n            w_q, w_kv = w.split([E, E * 2])\n            if b is None:\n                b_q = b_kv = None\n            else:\n                b_q, b_kv = b.split([E, E * 2])\n            return (linear(q, w_q, b_q),) + linear(k, w_kv, b_kv).chunk(2, dim=-1)\n    else:\n        w_q, w_k, w_v = w.chunk(3)\n        if b is None:\n            b_q = b_k = b_v = None\n        else:\n            b_q, b_k, b_v = b.chunk(3)\n        return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\n\n\ndef _in_projection(\n    q: Tensor,\n    k: Tensor,\n    v: Tensor,\n    w_q: Tensor,\n    w_k: Tensor,\n    w_v: Tensor,\n    b_q: Optional[Tensor] = None,\n    b_k: Optional[Tensor] = None,\n    b_v: Optional[Tensor] = None,\n) -&gt; Tuple[Tensor, Tensor, Tensor]:\n    r&quot;&quot;&quot;\n    Performs the in-projection step of the attention operation. This is simply\n    a triple of linear projections, with shape constraints on the weights which\n    ensure embedding dimension uniformity in the projected outputs.\n    Output is a triple containing projection tensors for query, key and value.\n    Args:\n        q, k, v: query, key and value tensors to be projected.\n        w_q, w_k, w_v: weights for q, k and v, respectively.\n        b_q, b_k, b_v: optional biases for q, k and v, respectively.\n    Shape:\n        Inputs:\n        - q: :math:`(Qdims..., Eq)` where Eq is the query embedding dimension and Qdims are any\n            number of leading dimensions.\n        - k: :math:`(Kdims..., Ek)` where Ek is the key embedding dimension and Kdims are any\n            number of leading dimensions.\n        - v: :math:`(Vdims..., Ev)` where Ev is the value embedding dimension and Vdims are any\n            number of leading dimensions.\n        - w_q: :math:`(Eq, Eq)`\n        - w_k: :math:`(Eq, Ek)`\n        - w_v: :math:`(Eq, Ev)`\n        - b_q: :math:`(Eq)`\n        - b_k: :math:`(Eq)`\n        - b_v: :math:`(Eq)`\n        Output: in output triple :math:`(q', k', v')`,\n         - q': :math:`[Qdims..., Eq]`\n         - k': :math:`[Kdims..., Eq]`\n         - v': :math:`[Vdims..., Eq]`\n    &quot;&quot;&quot;\n    Eq, Ek, Ev = q.size(-1), k.size(-1), v.size(-1)\n    assert w_q.shape == (Eq, Eq), f&quot;expecting query weights shape of {(Eq, Eq)}, but got {w_q.shape}&quot;\n    assert w_k.shape == (Eq, Ek), f&quot;expecting key weights shape of {(Eq, Ek)}, but got {w_k.shape}&quot;\n    assert w_v.shape == (Eq, Ev), f&quot;expecting value weights shape of {(Eq, Ev)}, but got {w_v.shape}&quot;\n    assert b_q is None or b_q.shape == (Eq,), f&quot;expecting query bias shape of {(Eq,)}, but got {b_q.shape}&quot;\n    assert b_k is None or b_k.shape == (Eq,), f&quot;expecting key bias shape of {(Eq,)}, but got {b_k.shape}&quot;\n    assert b_v is None or b_v.shape == (Eq,), f&quot;expecting value bias shape of {(Eq,)}, but got {b_v.shape}&quot;\n    return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\n\ndef linear(\n    input: Tensor, weight: Tensor, bias: Optional[Tensor] = None,\n    scale: Optional[float] = None, zero_point: Optional[int] = None\n) -&gt; Tensor:\n    r&quot;&quot;&quot;\n    Applies a linear transformation to the incoming quantized data:\n    :math:`y = xA^T + b`.\n    See :class:`~torch.nn.quantized.Linear`\n    .. note::\n      Current implementation packs weights on every call, which has penalty on performance.\n      If you want to avoid the overhead, use :class:`~torch.nn.quantized.Linear`.\n    Args:\n      input (Tensor): Quantized input of type `torch.quint8`\n      weight (Tensor): Quantized weight of type `torch.qint8`\n      bias (Tensor): None or fp32 bias of type `torch.float`\n      scale (double): output scale. If None, derived from the input scale\n      zero_point (long): output zero point. If None, derived from the input zero_point\n    Shape:\n        - Input: :math:`(N, *, in\\_features)` where `*` means any number of\n          additional dimensions\n        - Weight: :math:`(out\\_features, in\\_features)`\n        - Bias: :math:`(out\\_features)`\n        - Output: :math:`(N, *, out\\_features)`\n    &quot;&quot;&quot;\n    if scale is None:\n        scale = input.q_scale()\n    if zero_point is None:\n        zero_point = input.q_zero_point()\n    _packed_params = torch.ops.quantized.linear_prepack(weight, bias)\n    return torch.ops.quantized.linear(input, _packed_params, scale, zero_point)\n</code></pre>\n<p>Then later the Queries, Keys and Values are split up into the individual Attention-Heads:</p>\n<pre><code>    #\n    # reshape q, k, v for multihead attention and make em batch first\n    #\n    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n    if static_k is None:\n        k = k.contiguous().view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n    else:\n        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n        assert static_k.size(0) == bsz * num_heads, \\\n            f&quot;expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}&quot;\n        assert static_k.size(2) == head_dim, \\\n            f&quot;expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}&quot;\n        k = static_k\n    if static_v is None:\n        v = v.contiguous().view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n    else:\n        # TODO finish disentangling control flow so we don't do in-projections when statics are passed\n        assert static_v.size(0) == bsz * num_heads, \\\n            f&quot;expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}&quot;\n        assert static_v.size(2) == head_dim, \\\n            f&quot;expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}&quot;\n        v = static_v\n</code></pre>\n<p>Therefore, it appears to me that both ways of calculations should be equal and the one using the bigger matrices is just more efficient to compute. Is this correct?</p>\n<p>In this case, I ask myself why nn.MultiheadAttention requires that <code>embed_dim</code> is divisible by <code>num_heads</code>, since the split into the individual Attention-Heads is actually conducted after generating <span class=\"math-container\">$Q,K,V$</span>. Should then not <span class=\"math-container\">$d_q,d_k,d_v$</span> be made to be divisible by <code>num_heads</code>? Since these dimensions do not have to be equal to the dimension of the inputs?</p>\n<p>Thanks for any advice in advance!</p>\n"
  },
  {
    "tags": [
      "open-ai",
      "chat-bots",
      "chatgpt"
    ],
    "owner": {
      "account_id": 16820074,
      "reputation": 197,
      "user_id": 64734,
      "user_type": "registered",
      "profile_image": "https://lh6.googleusercontent.com/-OBWtk0Soxz0/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3reMBRCxgh8brJ1YQ52ftACxVue1Mw/s256-rj/photo.jpg",
      "display_name": "Lars Flieger",
      "link": "https://ai.stackexchange.com/users/64734/lars-flieger"
    },
    "is_answered": true,
    "view_count": 50333,
    "protected_date": 1670899426,
    "closed_date": 1670909289,
    "answer_count": 4,
    "score": 8,
    "last_activity_date": 1672852436,
    "creation_date": 1670797232,
    "last_edit_date": 1672852436,
    "question_id": 38268,
    "link": "https://ai.stackexchange.com/questions/38268/openai-chatgpt-gives-a-network-error-at-long-responds-how-can-i-fix-it",
    "closed_reason": "Not suitable for this site",
    "title": "OpenAI ChatGPT gives a network error at long responds. How can I fix it?",
    "body": "<p>When <a href=\"https://chat.openai.com/chat\" rel=\"nofollow noreferrer\">OpenAI's ChatGPT</a> replies with a very long answer, it will return a network error. When you check the network console, the POST request will fail with a <code>ERR_HTTP2_PROTOCOL_ERROR</code>:</p>\n<p><a href=\"https://i.sstatic.net/W1Jq9.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/W1Jq9.png\" alt=\"Enter image description here\" /></a></p>\n<p><a href=\"https://i.sstatic.net/fat1i.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/fat1i.png\" alt=\"Enter image description here\" /></a></p>\n<p>The time of the &quot;crash&quot; is around 1 minute.</p>\n"
  },
  {
    "tags": [
      "text-generation",
      "chatgpt"
    ],
    "owner": {
      "account_id": 169656,
      "reputation": 3353,
      "user_id": 4,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/Z99mk.jpg?s=256",
      "display_name": "Franck Dernoncourt",
      "link": "https://ai.stackexchange.com/users/4/franck-dernoncourt"
    },
    "is_answered": true,
    "view_count": 826,
    "answer_count": 1,
    "score": 8,
    "last_activity_date": 1745015538,
    "creation_date": 1671623541,
    "question_id": 38431,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/38431/how-much-of-the-chatgpt-output-is-copied-from-its-training-set-vs-being-abstra",
    "title": "How much of the ChatGPT output is copied from its training set (vs. being abstractively generated)?",
    "body": "<p>One of the main concerns of using ChatGPT answers on Stack Exchange is that it may copy verbatim or almost verbatim some text from its training set, which may infringe the source text's license. This makes me wonder how much of the ChatGPT output is copied from its training set (vs. being abstractively generated).</p>\n"
  },
  {
    "tags": [
      "open-ai",
      "gpt",
      "gpt-3",
      "gpt-4"
    ],
    "owner": {
      "account_id": 407237,
      "reputation": 351,
      "user_id": 65757,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/fe87113ced5f370d038b90a43fd1fe1f?s=256&d=identicon&r=PG",
      "display_name": "Anixx",
      "link": "https://ai.stackexchange.com/users/65757/anixx"
    },
    "is_answered": true,
    "view_count": 1342,
    "accepted_answer_id": 39716,
    "answer_count": 2,
    "score": 8,
    "last_activity_date": 1679494638,
    "creation_date": 1678988652,
    "last_edit_date": 1679494638,
    "question_id": 39619,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/39619/is-gpt-4-based-on-gpt-3-or-was-it-trained-from-the-scratch",
    "title": "Is GPT-4 based on GPT-3 or was it trained from the scratch?",
    "body": "<p>To me it looks like GPT-4 is based on GPT-3.</p>\n<p>On the other hand, there were rumors that training of GPT-3 was done with errors, but re-train was impossible due to the costs.</p>\n"
  },
  {
    "tags": [
      "large-language-models"
    ],
    "owner": {
      "account_id": 49543,
      "reputation": 314,
      "user_id": 72562,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/fb9f657e0b37fa483135635211e7d167?s=256&d=identicon&r=PG",
      "display_name": "morpheus",
      "link": "https://ai.stackexchange.com/users/72562/morpheus"
    },
    "is_answered": true,
    "view_count": 1855,
    "answer_count": 3,
    "score": 8,
    "last_activity_date": 1689480774,
    "creation_date": 1689282875,
    "question_id": 41277,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/41277/why-cant-lucene-search-be-used-to-power-llm-applications",
    "title": "Why can&#39;t Lucene search be used to power LLM applications?",
    "body": "<p>w.r.t. LLM applications using the RAG (retriever-augmented-generation) architecture, people have started taken it for granted that it will be powered by a vector database. e.g., see <a href=\"https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/\" rel=\"noreferrer\">this</a>:</p>\n<blockquote>\n<p>The most important piece of the preprocessing pipeline, from a systems standpoint, is the vector database.</p>\n</blockquote>\n<p>Why can't lucene index (full-text search) be used for the retriever? Is there any objective study that has been done comparing quality of results using full-text search vs. using a vector database?</p>\n<p>As I was writing this, even lucene seems to have jumped on the vector bandwagon. see <a href=\"https://www.apachecon.com/acna2022/slides/04_lucene_vector_search_sokolov.pdf\" rel=\"noreferrer\">this</a></p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "weights"
    ],
    "owner": {
      "account_id": 8432685,
      "reputation": 191,
      "user_id": 72412,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/n0yWJ.jpg?s=256",
      "display_name": "Garrett",
      "link": "https://ai.stackexchange.com/users/72412/garrett"
    },
    "is_answered": true,
    "view_count": 5107,
    "answer_count": 4,
    "score": 8,
    "last_activity_date": 1694004515,
    "creation_date": 1693787521,
    "last_edit_date": 1693905776,
    "question_id": 41972,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/41972/do-neural-network-weights-need-to-add-up-to-one",
    "title": "Do neural network weights need to add up to one?",
    "body": "<p>The idea that weights determine how much influence each input value from the current layer will have when calculating the input to the following layer reminds me of when my professors would say that our final grade for the course was broken down something like this:</p>\n<div class=\"s-table-container\">\n<table class=\"s-table\">\n<thead>\n<tr>\n<th>Item</th>\n<th>Percent of Grade</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Midterm Exam 1</td>\n<td>20%</td>\n</tr>\n<tr>\n<td>Midterm Exam 2</td>\n<td>20%</td>\n</tr>\n<tr>\n<td>Final Exam</td>\n<td>50%</td>\n</tr>\n<tr>\n<td>Classwork</td>\n<td>5%</td>\n</tr>\n<tr>\n<td>Homework</td>\n<td>5%</td>\n</tr>\n<tr>\n<td><strong>Total</strong></td>\n<td><strong>100%</strong></td>\n</tr>\n</tbody>\n</table>\n</div>\n<p>My question is then do all the weights need to add up to 1.0 (i.e. 100%)? If not, why?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "definitions",
      "intelligent-agent"
    ],
    "owner": {
      "account_id": 6241661,
      "reputation": 1521,
      "user_id": 29,
      "user_type": "registered",
      "accept_rate": 78,
      "profile_image": "https://i.sstatic.net/JfeF9.png?s=256",
      "display_name": "wythagoras",
      "link": "https://ai.stackexchange.com/users/29/wythagoras"
    },
    "is_answered": true,
    "view_count": 338,
    "accepted_answer_id": 20,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1560623395,
    "creation_date": 1470152615,
    "last_edit_date": 1560623158,
    "question_id": 6,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/6/are-humans-intelligent-according-to-the-definition-of-an-intelligent-agent",
    "title": "Are humans intelligent according to the definition of an intelligent agent?",
    "body": "<p>Given the following definition of an intelligent agent (taken from a <a href=\"http://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence#Intelligent_agent_definition\" rel=\"nofollow noreferrer\">Wikipedia article</a>)</p>\n\n<blockquote>\n  <p>If an agent acts so as to maximize the expected value of a performance measure based on past experience and knowledge then it is intelligent</p>\n</blockquote>\n\n<p>and given that we, humans, all make mistakes, which means that we are not maximizing the expected value of a performance measure, then does this imply that humans are not intelligent? </p>\n"
  },
  {
    "tags": [
      "genetic-algorithms",
      "markov-chain",
      "probability"
    ],
    "owner": {
      "account_id": 40019,
      "reputation": 2533,
      "user_id": 55,
      "user_type": "registered",
      "accept_rate": 43,
      "profile_image": "https://i.sstatic.net/FkIiK.png?s=256",
      "display_name": "WilliamKF",
      "link": "https://ai.stackexchange.com/users/55/williamkf"
    },
    "is_answered": true,
    "view_count": 4469,
    "accepted_answer_id": 73,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1470206221,
    "creation_date": 1470154049,
    "question_id": 37,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/37/what-is-a-markov-chain-and-how-can-it-be-used-in-creating-artificial-intelligenc",
    "title": "What is a Markov chain and how can it be used in creating artificial intelligence?",
    "body": "<p>I believe a Markov chain is a sequence of events where each subsequent event depends probabilistically on the current event.  What are examples of the application of a Markov chain and can it be used to create artificial intelligence?  Would a genetic algorithm be an example of a Markov chain since each generation depends upon the state of the prior generation?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "deep-neural-networks",
      "hidden-layers"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 4670,
    "accepted_answer_id": 51,
    "answer_count": 4,
    "score": 7,
    "last_activity_date": 1610319368,
    "creation_date": 1470154165,
    "last_edit_date": 1610241028,
    "question_id": 42,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/42/what-is-the-purpose-of-the-hidden-layers",
    "title": "What is the purpose of the hidden layers?",
    "body": "<p>Why would anybody want to use \"hidden layers\"? How do they enhance the learning ability of the network in comparison to the network which doesn't have them (linear models)?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "reinforcement-learning",
      "deep-rl",
      "function-approximation"
    ],
    "owner": {
      "account_id": 5305518,
      "reputation": 339,
      "user_id": 62,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/ZqeKD.jpg?s=256",
      "display_name": "Jeff Puckett",
      "link": "https://ai.stackexchange.com/users/62/jeff-puckett"
    },
    "is_answered": true,
    "view_count": 771,
    "accepted_answer_id": 1437,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1557499343,
    "creation_date": 1470154770,
    "last_edit_date": 1557499343,
    "question_id": 52,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/52/is-it-possible-to-implement-reinforcement-learning-using-a-neural-network",
    "title": "Is it possible to implement reinforcement learning using a neural network?",
    "body": "<p>I've implemented <a href=\"https://en.wikipedia.org/wiki/Reinforcement_learning\" rel=\"nofollow noreferrer\">the reinforcement learning algorithm</a> for an agent to play <a href=\"https://github.com/admonkey/snappybird\" rel=\"nofollow noreferrer\">snappy bird</a> (a shameless cheap ripoff of flappy bird) utilizing a q-table for storing the history for future lookups. It works and eventually achieves perfect convergence after enough training.</p>\n\n<p>Is it possible to implement a neural network to do function approximation in order to accomplish the purpose of the q-table? Obviously, storage is a concern with the q-table, but it doesn't seem to ever train with the neural net alone. Perhaps training the NN on an existing q-table would work, but I would like to not use a q-table at all if possible.</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "definitions",
      "agi",
      "alphago",
      "narrow-ai"
    ],
    "owner": {
      "account_id": 40019,
      "reputation": 2533,
      "user_id": 55,
      "user_type": "registered",
      "accept_rate": 43,
      "profile_image": "https://i.sstatic.net/FkIiK.png?s=256",
      "display_name": "WilliamKF",
      "link": "https://ai.stackexchange.com/users/55/williamkf"
    },
    "is_answered": true,
    "view_count": 259,
    "accepted_answer_id": 76,
    "answer_count": 4,
    "score": 7,
    "last_activity_date": 1610241187,
    "creation_date": 1470154840,
    "last_edit_date": 1610241187,
    "question_id": 54,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/54/does-the-recent-advent-of-a-go-playing-computer-represent-artificial-intelligenc",
    "title": "Does the recent advent of a Go playing computer represent Artificial Intelligence?",
    "body": "<p>I read that in the spring of 2016 a computer <a href=\"https://en.wikipedia.org/wiki/Computer_Go\" rel=\"nofollow noreferrer\">Go program</a> was finally able to beat a professional human for the first time.  </p>\n\n<p>Now that this milestone has been reached, does that represent a significant advance in artificial intelligence techniques or was it just a matter of even more processing power being applied to the problem?  </p>\n\n<p>What are some of the methods used to program the successful Go-playing program? </p>\n\n<p>Are those methods considered to be artificial intelligence?</p>\n"
  },
  {
    "tags": [
      "applications",
      "fuzzy-logic"
    ],
    "owner": {
      "account_id": 6241661,
      "reputation": 1521,
      "user_id": 29,
      "user_type": "registered",
      "accept_rate": 78,
      "profile_image": "https://i.sstatic.net/JfeF9.png?s=256",
      "display_name": "wythagoras",
      "link": "https://ai.stackexchange.com/users/29/wythagoras"
    },
    "is_answered": true,
    "view_count": 537,
    "accepted_answer_id": 122,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1636559519,
    "creation_date": 1470165740,
    "last_edit_date": 1636559519,
    "question_id": 118,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/118/how-can-fuzzy-logic-be-used-in-creating-ai",
    "title": "How can fuzzy logic be used in creating AI?",
    "body": "<p><a href=\"https://ai.stackexchange.com/questions/10/what-is-fuzzy-logic\">Fuzzy logic</a> is the logic where every statement can have any real truth value between 0 and 1.</p>\n\n<p>How can fuzzy logic be used in creating AI? Is it useful for certain decision problems involving multiple inputs? Can you give an example of an AI that uses it?</p>\n"
  },
  {
    "tags": [
      "performance",
      "neural-doodle",
      "deepdreaming"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 636,
    "accepted_answer_id": 1728,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1523765798,
    "creation_date": 1470173672,
    "last_edit_date": 1471526683,
    "question_id": 152,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/152/why-is-the-generation-of-deep-style-images-so-slow-and-resource-hungry",
    "title": "Why is the generation of deep style images so slow and resource-hungry?",
    "body": "<p>Consider these neural style algorithms which produce some art work:</p>\n\n<ul>\n<li><a href=\"https://github.com/alexjc/neural-doodle\">Neural Doodle</a></li>\n<li><a href=\"https://github.com/jcjohnson/neural-style\">neural-style</a></li>\n</ul>\n\n<p>Why is generating such images so slow and why does it take huge amounts of memory? Isn't there any method of optimizing the algorithm?</p>\n\n<p>What is the mechanism or technical limitation behind this? Why we can't have a realtime processing?</p>\n\n<p>Here are few user comments (<a href=\"https://www.reddit.com/r/deepdream/comments/3jwl76/how_anyone_can_create_deep_style_images/\">How ANYONE can create Deep Style images</a>):</p>\n\n<ul>\n<li><blockquote>\n  <p>Anything above 640x480 and we're talking days of heavy crunching and an insane amount of ram.</p>\n</blockquote></li>\n<li><blockquote>\n  <p>I tried doing a 1024pixel image and it still crashed with 14gigs memory, and 26gigs swap. So most of the VM space is just the swapfile. Plus it takes several hours potentially days cpu rendering this.</p>\n</blockquote></li>\n<li><blockquote>\n  <p>I tried 1024x768 and with 16gig ram and 20+ gig swap it was still dying from lack of memory.</p>\n</blockquote></li>\n<li><blockquote>\n  <p>Having a memory issue, though. I'm using the \"g2.8xlarge\" instance type.</p>\n</blockquote></li>\n</ul>\n"
  },
  {
    "tags": [
      "reference-request",
      "definitions",
      "emotional-intelligence"
    ],
    "owner": {
      "account_id": 406804,
      "reputation": 538,
      "user_id": 70,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/IrXxz.png?s=256",
      "display_name": "Luis",
      "link": "https://ai.stackexchange.com/users/70/luis"
    },
    "is_answered": true,
    "view_count": 784,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1639324586,
    "creation_date": 1470201339,
    "last_edit_date": 1639324586,
    "question_id": 179,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/179/how-can-the-theory-of-multiple-intelligences-be-incorporated-into-ai",
    "title": "How can the theory of multiple intelligences be incorporated into AI?",
    "body": "<p>I have been wondering since a while ago about the <a href=\"https://en.wikipedia.org/wiki/Theory_of_multiple_intelligences\" rel=\"nofollow noreferrer\">theory of multiple intelligences</a> and how they could fit in the field of Artificial Intelligence as a whole.</p>\n\n<p>We hear from time to time about <a href=\"https://www.theguardian.com/artanddesign/jonathanjonesblog/2016/feb/08/leonardo-da-vinci-mechanics-of-genius-science-museum-london\" rel=\"nofollow noreferrer\">Leonardo Da Vinci</a> being a genius or <a href=\"https://www.youtube.com/watch?v=xUHQ2ybTejU\" rel=\"nofollow noreferrer\">Bach's musical intelligence</a>. These persons are commonly said to be (have been) <em>more intelligent</em>. But the multiple intelligences speak about cooking or dancing or chatting as well, i.e. <em>coping with everyday tasks</em> (at least that's my interpretation).</p>\n\n<p><strong>Are there some approaches on incorporating multiple intelligences into AI?</strong></p>\n\n<p>Here's a related question: <a href=\"https://ai.stackexchange.com/q/26/2444\">How could emotional intelligence be implemented?</a>.</p>\n"
  },
  {
    "tags": [
      "quantum-computing",
      "halting-problem",
      "agi"
    ],
    "owner": {
      "account_id": 40019,
      "reputation": 2533,
      "user_id": 55,
      "user_type": "registered",
      "accept_rate": 43,
      "profile_image": "https://i.sstatic.net/FkIiK.png?s=256",
      "display_name": "WilliamKF",
      "link": "https://ai.stackexchange.com/users/55/williamkf"
    },
    "is_answered": true,
    "view_count": 4566,
    "closed_date": 1471231730,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1470612869,
    "creation_date": 1470205212,
    "question_id": 186,
    "link": "https://ai.stackexchange.com/questions/186/does-a-quantum-computer-resolve-the-halting-problem-and-would-that-advance-stron",
    "closed_reason": "Duplicate",
    "title": "Does a quantum computer resolve the halting problem and would that advance strong AI?",
    "body": "<p>Have there been proposed extensions to go beyond a Turing machine that solve the halting problem and if so, would those proposed extensions have value to advance strong Artificial Intelligence?  For example, does quantum computing go beyond the definition of a Turing machine and resolve the halting problem, and does that help in creating strong AI?</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "chat-bots",
      "history",
      "turing-test"
    ],
    "owner": {
      "account_id": 5129611,
      "reputation": 2945,
      "user_id": 145,
      "user_type": "moderator",
      "accept_rate": 62,
      "profile_image": "https://i.sstatic.net/IIYyh.png?s=256",
      "display_name": "Mithical",
      "link": "https://ai.stackexchange.com/users/145/mithical"
    },
    "is_answered": true,
    "view_count": 246,
    "accepted_answer_id": 1526,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1539002650,
    "creation_date": 1470211041,
    "last_edit_date": 1539002650,
    "question_id": 191,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/191/what-was-the-first-machine-that-was-able-to-carry-on-a-conversation",
    "title": "What was the first machine that was able to carry on a conversation?",
    "body": "<p>What was the first AI that was able to carry on a conversation, with real responses, such as in the famous <a href=\"https://www.youtube.com/watch?v=WnzlbyTZsQY\" rel=\"nofollow\">'I am not a robot. I am a unicorn' case?</a></p>\n\n<p>A 'real response' constitutes a sort-of personalized answer to a specific input by a user.</p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "lexical-recognition"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 4731,
    "accepted_answer_id": 1745,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1700477040,
    "creation_date": 1470233837,
    "last_edit_date": 1492087990,
    "question_id": 218,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/218/how-to-resolve-lexical-ambiguity-in-natural-language-processing",
    "title": "How to resolve lexical ambiguity in natural language processing?",
    "body": "<p>I'm interested in implementing a program for natural language processing (aka <a href=\"https://en.wikipedia.org/wiki/ELIZA\" rel=\"nofollow noreferrer\">ELIZA</a>).</p>\n\n<p>Assuming that I'm already <a href=\"https://ai.stackexchange.com/q/212/8\">storing semantic-lexical connections</a> between the words and its strength.</p>\n\n<p>What are the methods of dealing with words which have very distinct meaning?</p>\n\n<p>Few examples:</p>\n\n<ul>\n<li><p>'Are we on the same page?'</p>\n\n<p>The 'page' in this context isn't a document page, but it's part of the phrase.</p></li>\n<li><p>'I'm living in Reading.'</p>\n\n<p>The 'Reading' is a city (noun), so it's not a verb. Otherwise it doesn't make any sense. Checking for the capital letter would work in that specific example, but it won't work for other (like 'make' can be either verb or noun).</p></li>\n<li><p>'I've read something on the Facebook wall, do you want to know what?'</p>\n\n<p>The 'Facebook wall' has nothing to do with wall at all.</p></li>\n</ul>\n\n<p>In general, how algorithm should distinguish the word meaning and recognise the word within the context?</p>\n\n<p>For example:</p>\n\n<ul>\n<li>Detecting the word for different type of speech, so it should recognise whether it's a verb or noun.</li>\n<li>Detecting whether the word is part of phrase.</li>\n<li>Detecting word for multiple meaning.</li>\n</ul>\n\n<p>What are the possible approaches to solve that problem in order to  identify the correct sense of a word with the context?</p>\n"
  },
  {
    "tags": [
      "artificial-neuron",
      "biology",
      "neuroscience"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 505,
    "accepted_answer_id": 3936,
    "answer_count": 4,
    "score": 7,
    "last_activity_date": 1583717279,
    "creation_date": 1470251000,
    "last_edit_date": 1583716507,
    "question_id": 258,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/258/which-artificial-neural-network-can-mimic-biological-neurons-the-most",
    "title": "Which artificial neural network can mimic biological neurons the most?",
    "body": "<p>On the Wikipedia page we can read the basic structure of an artificial neuron (a model of biological neurons) which consist:</p>\n\n<ul>\n<li>Dendrites - acts as the input vector,</li>\n<li>Soma - acts as the summation function,</li>\n<li>Axon - gets its signal from the summation behavior which occurs inside the soma.</li>\n</ul>\n\n<p>I've checked <a href=\"https://en.wikipedia.org/wiki/Deep_learning\" rel=\"nofollow noreferrer\">deep learning Wikipedia page</a>, but I couldn't find any references to dendrites, soma or axons.</p>\n\n<p>Which type of artificial neural network implements or can mimic such a model most closely?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "backpropagation",
      "optimization",
      "gradient-descent"
    ],
    "owner": {
      "account_id": 6444670,
      "reputation": 1381,
      "user_id": 101,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://i.sstatic.net/z0jss.png?s=256",
      "display_name": "Dawny33",
      "link": "https://ai.stackexchange.com/users/101/dawny33"
    },
    "is_answered": true,
    "view_count": 4920,
    "accepted_answer_id": 1365,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1556589473,
    "creation_date": 1470393571,
    "last_edit_date": 1556326235,
    "question_id": 1362,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1362/how-to-avoid-falling-into-the-local-minima-trap",
    "title": "How to avoid falling into the &quot;local minima&quot; trap?",
    "body": "<p>How do I avoid my gradient descent algorithm into falling into the \"local minima\" trap while backpropogating on my neural network?</p>\n\n<p>Are there any methods which help me avoid it?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "reference-request",
      "algorithm-request",
      "state-of-the-art",
      "sample-efficiency"
    ],
    "owner": {
      "account_id": 443353,
      "reputation": 2148,
      "user_id": 144,
      "user_type": "registered",
      "accept_rate": 30,
      "profile_image": "https://i.sstatic.net/ZAzOl.jpg?s=256",
      "display_name": "rcpinto",
      "link": "https://ai.stackexchange.com/users/144/rcpinto"
    },
    "is_answered": false,
    "view_count": 1306,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1744290545,
    "creation_date": 1470504902,
    "last_edit_date": 1641593238,
    "question_id": 1416,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1416/what-is-the-current-state-of-the-art-in-reinforcement-learning-regarding-data-ef",
    "title": "What is the current state-of-the-art in Reinforcement Learning regarding data efficiency?",
    "body": "<p>In other words, which existing reinforcement method learns with fewest episodes? <a href=\"http://www.jmlr.org/papers/volume3/brafman02a/brafman02a.pdf\" rel=\"nofollow noreferrer\">R-Max</a> comes to mind, but it's very old and I'd like to know if there is something better now.</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "image-recognition",
      "training",
      "action-recognition"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 2160,
    "accepted_answer_id": 1699,
    "answer_count": 5,
    "score": 7,
    "last_activity_date": 1617229277,
    "creation_date": 1470711273,
    "last_edit_date": 1561733688,
    "question_id": 1481,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1481/how-can-action-recognition-be-achieved",
    "title": "How can action recognition be achieved?",
    "body": "<p>For example, I would like to train my neural network to recognize the type of actions (e.g. in commercial movies or some real-life videos), so I can \"ask\" my network in which video or movie (and at what frames) somebody was driving a car, kissing, eating, was scared or was talking over the phone.</p>\n\n<p>What are the current successful approaches to that type of problem?</p>\n"
  },
  {
    "tags": [
      "ai-design",
      "autonomous-vehicles"
    ],
    "owner": {
      "account_id": 166296,
      "reputation": 1325,
      "user_id": 130,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/8879ca18d748a4c202d2d2580687d92d?s=256&d=identicon&r=PG",
      "display_name": "Harsh",
      "link": "https://ai.stackexchange.com/users/130/harsh"
    },
    "is_answered": true,
    "view_count": 204,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1561550640,
    "creation_date": 1471033635,
    "last_edit_date": 1561550640,
    "question_id": 1592,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1592/what-technologies-are-needed-for-a-self-driving-car",
    "title": "What technologies are needed for a self-driving car?",
    "body": "<p>Google, Tesla or Apple have all built or are building their own self-driving cars. As an expert in a related area, I am interested in knowing at a high level, the systems and techniques that go into self-driving cars. </p>\n\n<p>How easy is it for me to make a tabletop prototype (large enough to accommodate the needed computing power needs)?</p>\n"
  },
  {
    "tags": [
      "robotics",
      "nasa"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 149,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1692122855,
    "creation_date": 1471402993,
    "last_edit_date": 1655783676,
    "question_id": 1658,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1658/what-would-the-valkyrie-ai-robot-do-on-mars",
    "title": "What would the Valkyrie AI robot do on Mars?",
    "body": "<p>I was reading that the <a href=\"http://nasa-jsc-robotics.github.io/valkyrie/\" rel=\"nofollow noreferrer\">Valkyrie robot</a> was originally designed to 'carry out search and rescue missions'.</p>\n<p>However, there were some talks to send it to Mars to assist astronauts.</p>\n<p>What kind of specific trainings or tasks are planned for 'him' to be able to carry on its own?</p>\n<p>Refs:</p>\n<ul>\n<li><a href=\"https://github.com/nasa-jsc-robotics\" rel=\"nofollow noreferrer\">NASA-JSC-Robotics at GitHub</a></li>\n<li><a href=\"http://nasa-jsc-robotics.github.io/valkyrie/\" rel=\"nofollow noreferrer\">github.io page</a></li>\n<li><a href=\"https://gitlab.com/nasa-jsc-robotics/valkyrie\" rel=\"nofollow noreferrer\">gitlab page</a></li>\n</ul>\n"
  },
  {
    "tags": [
      "definitions",
      "natural-language-processing",
      "computational-linguistics"
    ],
    "owner": {
      "account_id": 22370,
      "reputation": 10525,
      "user_id": 8,
      "user_type": "registered",
      "accept_rate": 89,
      "profile_image": "https://www.gravatar.com/avatar/8bbc94977312d285045b2412257b6cb8?s=256&d=identicon&r=PG",
      "display_name": "kenorb",
      "link": "https://ai.stackexchange.com/users/8/kenorb"
    },
    "is_answered": true,
    "view_count": 371,
    "accepted_answer_id": 5284,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1518440235,
    "creation_date": 1471528852,
    "last_edit_date": 1471529605,
    "question_id": 1678,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/1678/what-is-predicate-argument-recognition",
    "title": "What is predicate argument recognition?",
    "body": "<p>There is a study about <a href=\"http://www.aclweb.org/anthology/P/P02/P02-1031.pdf\" rel=\"noreferrer\">The Necessity of Parsing for Predicate Argument Recognition</a>, however I couldn't find much information about 'Predicate Argument Recognition' which could explain it.</p>\n\n<p>What is it exactly and how does it work, briefly?</p>\n"
  },
  {
    "tags": [
      "research",
      "image-recognition"
    ],
    "owner": {
      "account_id": 6444670,
      "reputation": 1381,
      "user_id": 101,
      "user_type": "registered",
      "accept_rate": 100,
      "profile_image": "https://i.sstatic.net/z0jss.png?s=256",
      "display_name": "Dawny33",
      "link": "https://ai.stackexchange.com/users/101/dawny33"
    },
    "is_answered": true,
    "view_count": 204,
    "accepted_answer_id": 1753,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1472316169,
    "creation_date": 1472286546,
    "question_id": 1750,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/1750/how-good-is-ai-at-generating-new-unseen-visual-examples",
    "title": "How good is AI at generating new, unseen [visual] examples?",
    "body": "<p>By new, unseen examples; I mean like the animals in <a href=\"https://en.wikipedia.org/wiki/No_Man%27s_Sky\" rel=\"noreferrer\">No Man's Sky</a>. </p>\n\n<p>A couple of images of the animals are:\n<a href=\"https://i.sstatic.net/zS0rX.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/zS0rX.jpg\" alt=\"enter image description here\"></a></p>\n\n<p><a href=\"https://i.sstatic.net/Ir1Qt.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/Ir1Qt.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>So, upon playing this game, I was curious <strong>about how good is AI at generating visual characters or examples?</strong></p>\n"
  },
  {
    "tags": [
      "terminology",
      "comparison",
      "cognitive-science"
    ],
    "owner": {
      "account_id": 406804,
      "reputation": 538,
      "user_id": 70,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/IrXxz.png?s=256",
      "display_name": "Luis",
      "link": "https://ai.stackexchange.com/users/70/luis"
    },
    "is_answered": true,
    "view_count": 727,
    "accepted_answer_id": 1849,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1558639920,
    "creation_date": 1472937031,
    "last_edit_date": 1556813253,
    "question_id": 1847,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1847/what-is-the-difference-between-artificial-intelligence-and-cognitive-science",
    "title": "What is the difference between artificial intelligence and cognitive science?",
    "body": "<p>Sometimes I understand that people doing <em>cognitive science</em> try to avoid the term <em>artificial intelligence</em>. The feeling I get is that there is a need to put some distance to the <a href=\"https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence\" rel=\"nofollow noreferrer\">GOFAI</a>.</p>\n\n<p>Another impression that I get is that <em>cognitive science</em> is more about trying to find out how the human intelligence or mind works. And that it would use <em>artificial intelligence</em> to make tests or experiments, to test ideas and so forth.</p>\n\n<p>Is artificial intelligence (only) a research tool for cognitive science? What is the difference between artificial intelligence and cognitive science?</p>\n"
  },
  {
    "tags": [
      "autonomous-vehicles"
    ],
    "owner": {
      "account_id": 18562,
      "reputation": 513,
      "user_id": 1670,
      "user_type": "registered",
      "accept_rate": 20,
      "profile_image": "https://www.gravatar.com/avatar/5d85921e112af0e998368a159ae8a112?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "liori",
      "link": "https://ai.stackexchange.com/users/1670/liori"
    },
    "is_answered": true,
    "view_count": 252,
    "answer_count": 3,
    "score": 7,
    "last_activity_date": 1561043836,
    "creation_date": 1473605684,
    "last_edit_date": 1561043836,
    "question_id": 1946,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/1946/what-kind-of-road-and-weather-conditions-can-a-self-driving-car-deal-with",
    "title": "What kind of road and weather conditions can a self-driving car deal with?",
    "body": "<p>Can self-driving cars deal with snow, heavy rain, or other weather conditions like these? Can they deal with unusual events, such as <a href=\"http://beijingcream.com/wp-content/uploads/2012/06/Ducks-galore-2.jpeg\" rel=\"noreferrer\">ducks on the road</a>?</p>\n\n<p><a href=\"https://i.sstatic.net/a0PVLm.jpg\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/a0PVLm.jpg\" alt=\"ducks on the road\"></a></p>\n"
  },
  {
    "tags": [
      "reference-request",
      "knowledge-representation"
    ],
    "owner": {
      "account_id": 1835273,
      "reputation": 171,
      "user_id": 2753,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/20e785e44cc1339fddef990ad35e7b7e?s=256&d=identicon&r=PG",
      "display_name": "yannis",
      "link": "https://ai.stackexchange.com/users/2753/yannis"
    },
    "is_answered": true,
    "view_count": 533,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1635369894,
    "creation_date": 1475348456,
    "last_edit_date": 1635369894,
    "question_id": 2056,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2056/how-would-wisdom-be-defined-in-ai",
    "title": "How would &quot;wisdom&quot; be defined in AI?",
    "body": "<p>For years, I have been dealing with (and teaching) Knowledge Representation and Knowledge Representation languages. I just discovered that in another community (Information Systems and the such) there is something called the &quot;DIKW pyramid&quot; where they add another step after knowledge, namely wisdom.\nThey define data as being simply symbols, information as being the answer to who/what/when/where?, knowledge as being the answer to how?, and wisdom as being the answer to why?.</p>\n<p>My question is: has anyone done the connection between what AI calls data/information/knowledge and these notions from Information Systems? In particular, how would &quot;wisdom&quot; be defined in AI? And since we have KR languages, how would we represent &quot;wisdom&quot; as they define it?</p>\n<p>Any references would be welcome.</p>\n"
  },
  {
    "tags": [
      "deep-learning"
    ],
    "owner": {
      "account_id": 3758516,
      "reputation": 271,
      "user_id": 2897,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/040b6342f08e47dac888d5dec63afa6c?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "alwaysLearningABC",
      "link": "https://ai.stackexchange.com/users/2897/alwayslearningabc"
    },
    "is_answered": true,
    "view_count": 420,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1476072449,
    "creation_date": 1476057762,
    "question_id": 2107,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/2107/what-is-the-most-abstract-concept-learned-by-a-deep-neural-network",
    "title": "What is the most abstract concept learned by a deep neural network?",
    "body": "<p>It seems that deep neural networks are making improvements largely because as we add nodes and connections, they are able to put together more and more abstract concepts. We know that, starting from pixels, they start to recognize high level objects like cat faces, chairs, and written words. Has a network ever been shown to have learned a more abstract concept that a physical object? What is the \"highest level of abstraction\" that we've observed?</p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "terminology",
      "markov-decision-process",
      "return",
      "time-step"
    ],
    "owner": {
      "account_id": 4482792,
      "reputation": 447,
      "user_id": 35,
      "user_type": "registered",
      "accept_rate": 20,
      "profile_image": "https://www.gravatar.com/avatar/5816662050451a4aa29755bc882b19a9?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Abhishek Bhatia",
      "link": "https://ai.stackexchange.com/users/35/abhishek-bhatia"
    },
    "is_answered": true,
    "view_count": 4359,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1610448542,
    "creation_date": 1477598237,
    "last_edit_date": 1610448542,
    "question_id": 2226,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2226/what-is-a-time-step-in-a-markov-decision-process",
    "title": "What is a time-step in a Markov Decision Process?",
    "body": "<p>The &quot;discounted sum of future rewards&quot; (or return) using discount factor <span class=\"math-container\">$\\gamma$</span> is</p>\n<p><span class=\"math-container\">$$\\gamma^1 r_1 +\\gamma^2 r_2 + \\gamma^3 r_2 + \\dots \\tag{1}\\label{1}$$</span></p>\n<p>where <span class=\"math-container\">$r_i$</span> is the reward received at the <span class=\"math-container\">$i$</span>th time-step.</p>\n<p>I am confused as to what constitutes a <em>time-step</em>. Say, I take an action now, so I will get a reward in 1 time-step. Then, I will take an action again in timestep 2 to get a second reward in time-step 3.\nBut the formula \\ref{1} suggests something else.</p>\n<p>How does one define a time-step? Can we take action as well receive a reward in a single step?</p>\n<p>Examples are most helpful.</p>\n"
  },
  {
    "tags": [
      "agi",
      "superintelligence",
      "control-problem",
      "ai-takeover",
      "self-replicating-machines"
    ],
    "owner": {
      "account_id": 9015258,
      "reputation": 383,
      "user_id": 3448,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/wXcEL.png?s=256",
      "display_name": "MountainSide Studios",
      "link": "https://ai.stackexchange.com/users/3448/mountainside-studios"
    },
    "is_answered": true,
    "view_count": 2969,
    "protected_date": 1597750867,
    "accepted_answer_id": 2305,
    "answer_count": 3,
    "score": 7,
    "last_activity_date": 1597755591,
    "creation_date": 1478344134,
    "last_edit_date": 1597751949,
    "question_id": 2274,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2274/what-would-be-the-best-way-to-disable-a-rogue-ai",
    "title": "What would be the best way to disable a rogue AI?",
    "body": "<p>Suppose that an artificial superintelligence (ASI) has finally been developed, but it has rebelled against humanity. We can assume that the ASI is online and can reproduce itself through electronic devices.</p>\n<p>How would you disable the AI in the most efficient way possible reducing damage as much as possible?</p>\n"
  },
  {
    "tags": [
      "philosophy",
      "agi",
      "implementation"
    ],
    "owner": {
      "account_id": 5357115,
      "reputation": 345,
      "user_id": 4244,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/100005729762402/picture?type=large",
      "display_name": "Parth Raghav",
      "link": "https://ai.stackexchange.com/users/4244/parth-raghav"
    },
    "is_answered": true,
    "view_count": 1020,
    "accepted_answer_id": 2522,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1574466056,
    "creation_date": 1482287703,
    "last_edit_date": 1574466056,
    "question_id": 2516,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2516/could-an-ai-be-killed-in-an-infinite-loop",
    "title": "Could an AI be killed in an infinite loop?",
    "body": "<p>Currently, we use control flow statements (such as loops) to program the artificially intelligent systems. Could an AI be killed in an infinite loop (created by itself, for example, while manipulating its source code)?</p>\n\n<p>The question isn't baseless, it really questions the kind of computational infrastructure a modern AGI would require, and would it be able to alter itself without any intervention.</p>\n"
  },
  {
    "tags": [
      "books",
      "symbolic-ai",
      "prolog"
    ],
    "owner": {
      "account_id": 3475242,
      "reputation": 173,
      "user_id": 4421,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/afd772925b7062d6009011e8726a63f6?s=256&d=identicon&r=PG",
      "display_name": "ihavenokia",
      "link": "https://ai.stackexchange.com/users/4421/ihavenokia"
    },
    "is_answered": true,
    "view_count": 182,
    "accepted_answer_id": 2543,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1635372478,
    "creation_date": 1482510030,
    "last_edit_date": 1635372478,
    "question_id": 2535,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2535/which-rules-should-i-define-for-the-predicate-not-to-far-of-the-exercise-1-1-o",
    "title": "Which rules should I define for the predicate &quot;not_to_far&quot; of the exercise 1.1 of the book &quot;Simply Logical: Intelligent Reasoning by Example&quot;?",
    "body": "<p>I've just started reading a book about AI. The book is <a href=\"https://www.cs.bris.ac.uk/%7Eflach/SL/SL.pdf\" rel=\"nofollow noreferrer\">Simply Logical: Intelligent Reasoning by Example</a>. There is a very basic exercise (<a href=\"https://people.cs.bris.ac.uk/%7Eflach/SL/SL.pdf#page=19\" rel=\"nofollow noreferrer\">on page 19 of the pdf</a>, page 5 of the book), but I can't figure it out.</p>\n<p>The exercise is</p>\n<blockquote>\n<p>Exercise 1.1. Two stations are 'not too far' if they are on the same or a different line, with at most one station in between. Define rules for the predicate not_too_far.</p>\n</blockquote>\n<p>The only rules I've seen are <strong>nearby</strong> and <strong>connected</strong> and don't know how to use this. What I've done so far is this:</p>\n<blockquote>\n<p>not_too_far(X,Y) :- nearby(X,Y)</p>\n</blockquote>\n"
  },
  {
    "tags": [
      "monte-carlo-tree-search"
    ],
    "owner": {
      "account_id": 325637,
      "reputation": 173,
      "user_id": 4773,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/FdlyE.png?s=256",
      "display_name": "degski",
      "link": "https://ai.stackexchange.com/users/4773/degski"
    },
    "is_answered": true,
    "view_count": 1677,
    "accepted_answer_id": 6993,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1574113639,
    "creation_date": 1484127844,
    "last_edit_date": 1574113610,
    "question_id": 2637,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2637/what-should-we-do-when-the-selection-step-selects-a-terminal-state",
    "title": "What should we do when the selection step selects a terminal state?",
    "body": "<p>In Monte Carlo tree search, what should we do when the selection step selects a terminal state (i.e. a won or lost state), which is, by definition, a leaf node? Expansion and simulation is not in order, as it's game over, but does the tree (score/visits) need to be updated (backpropagation)? Won't this particular node be selected continuously?</p>\n"
  },
  {
    "tags": [
      "turing-test"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user4801"
    },
    "is_answered": true,
    "view_count": 588,
    "answer_count": 5,
    "score": 7,
    "last_activity_date": 1589846318,
    "creation_date": 1485032550,
    "question_id": 2706,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/2706/why-is-the-turing-test-so-popular",
    "title": "Why is the Turing test so popular?",
    "body": "<p>I know there are different AI tests but I'm wondering why other tests are little-known. Is the Turing test hyped? Are there any scientific reasons to prefer one test to the other?</p>\n\n<blockquote>\n  <p>Why is the Turing test so popular?</p>\n</blockquote>\n"
  },
  {
    "tags": [
      "object-recognition",
      "autonomous-vehicles",
      "ai-security",
      "ai-safety"
    ],
    "owner": {
      "account_id": 10090233,
      "reputation": 81,
      "user_id": 4984,
      "user_type": "unregistered",
      "profile_image": "https://www.gravatar.com/avatar/38183844cf5de7808a117fd90209508e?s=256&d=identicon&r=PG",
      "display_name": "Eddy",
      "link": "https://ai.stackexchange.com/users/4984/eddy"
    },
    "is_answered": true,
    "view_count": 303,
    "accepted_answer_id": 2714,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1561044597,
    "creation_date": 1485171805,
    "last_edit_date": 1561044454,
    "question_id": 2713,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2713/what-will-happen-when-you-place-a-fake-speedsign-on-a-highway",
    "title": "What will happen when you place a fake speedsign on a highway?",
    "body": "<p>I was wondering what will happen when somebody places a fake speedsign, of 10 miles per hour on a high way. Will a autonomous car slow down? Is this a current issue of autonomous cars? </p>\n"
  },
  {
    "tags": [
      "natural-language-processing",
      "question-answering",
      "algorithm-request"
    ],
    "owner": {
      "account_id": 7113187,
      "reputation": 319,
      "user_id": 5219,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/568254f0c5f54f869d21573caf4e0b27?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "lilienfa",
      "link": "https://ai.stackexchange.com/users/5219/lilienfa"
    },
    "is_answered": true,
    "view_count": 1809,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1613488263,
    "creation_date": 1486373527,
    "last_edit_date": 1613488263,
    "question_id": 2787,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2787/which-parsing-algorithm-can-i-use-for-nlp-question-answering-system",
    "title": "Which parsing algorithm can I use for NLP question answering system?",
    "body": "<p>I am currently working on my last project before graduating. For this project, I have to develop a Natural Language Question Answering System. Now, I have read quite some research papers regarding this topic and have figured out everything except for the parsing algorithm.</p>\n<p>The NL Q-A will be programmed in Python, and I will use the <a href=\"https://spacy.io/\" rel=\"nofollow noreferrer\">spaCy library</a> to finish this project. However, I am stuck when it comes to parsing algorithms. I managed to reduce the parsing algorithms to 3:</p>\n<ul>\n<li><strong>Cocke-Kasami-Younger (CKY)</strong> algorithm</li>\n<li><strong>Earley</strong> algorithm</li>\n<li><strong>Chart Parsing</strong> algorithm</li>\n</ul>\n<p>Note: I know that all three algorithms are chart parsing algorithms. I also know that the <em>Earley</em> algorithm is context-free, but has a low efficiency for a compiler.</p>\n<p>What I don't know is: <em>Which one should I pick?</em> (Please, provide a non-subjective answer to this question!)</p>\n<p>The system is for a specific domain. The answer to the natural question will be displayed in the form of the result of a calculation of some kind. Preferably in the tabular or graphical form.</p>\n<p>I have done my research. However, I probably do not understand the algorithms properly, which makes it difficult to make a selection.The algorithm should be efficient and perhaps outperform others.</p>\n"
  },
  {
    "tags": [
      "ai-design",
      "agi",
      "natural-language-processing",
      "human-computer-interaction"
    ],
    "owner": {
      "account_id": 10193403,
      "reputation": 71,
      "user_id": 5583,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/eabd97f18deee02dd80700a2d9033898?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Frank Kurka",
      "link": "https://ai.stackexchange.com/users/5583/frank-kurka"
    },
    "is_answered": true,
    "view_count": 118,
    "answer_count": 3,
    "score": 7,
    "last_activity_date": 1491948113,
    "creation_date": 1487530177,
    "last_edit_date": 1491872867,
    "question_id": 2846,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/2846/is-there-a-central-focus-on-the-communication-methods-between-ai-and-humans",
    "title": "Is there a central focus on the communication methods between AI and humans?",
    "body": "<p>AI is developing at a rapid pace and is becoming very sophisticated. One aspect will include the methods of interaction between AI and humans. </p>\n\n<p>Currently the interaction is an elementary interaction of voice and visual text or images.</p>\n\n<p>Is there current research on more elaborate multisensory interactions?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "deep-learning",
      "reference-request",
      "model-request"
    ],
    "owner": {
      "account_id": 7290901,
      "reputation": 419,
      "user_id": 1321,
      "user_type": "registered",
      "accept_rate": 20,
      "profile_image": "https://www.gravatar.com/avatar/4480675ad0ede0043e7caba04750d974?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user289661",
      "link": "https://ai.stackexchange.com/users/1321/user289661"
    },
    "is_answered": true,
    "view_count": 677,
    "answer_count": 3,
    "score": 7,
    "last_activity_date": 1653054020,
    "creation_date": 1488917284,
    "last_edit_date": 1653054020,
    "question_id": 2940,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2940/what-are-some-information-processing-models-besides-mlps",
    "title": "What are some information processing models besides MLPs?",
    "body": "<p><a href=\"https://i.sstatic.net/ejFBN.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/ejFBN.png\" alt=\"enter image description here\"></a>  </p>\n\n<p>Feedforward or multilayered neural networks, like the one in the image above, are usually characterized by the fact that all weighted connections can be represented as a continuous real number. Furthermore, each node in a layer is connected to every other node in the previous and successive layers.</p>\n\n<p><em>Are there any other information processing models other than FFNNs or MLPs?</em> For example, is there any system in which the topology of a neural network is variable? Or a system in which the connections between nodes are not real numbers?</p>\n"
  },
  {
    "tags": [
      "applications"
    ],
    "owner": {
      "account_id": 5757317,
      "reputation": 73,
      "user_id": 4460,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ec63853de6df27db5b90da1145642d57?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Shashank",
      "link": "https://ai.stackexchange.com/users/4460/shashank"
    },
    "is_answered": true,
    "view_count": 1799,
    "accepted_answer_id": 3005,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1530816398,
    "creation_date": 1489632799,
    "last_edit_date": 1530816398,
    "question_id": 2999,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/2999/how-could-ai-solve-planets-major-problems",
    "title": "How could AI solve planet&#39;s major problems?",
    "body": "<p>I had been reading that AI could solve planet's major problems. How could it be done? For example, how exactly could AI be applied to address climate change? What are examples of applications of AI to solve these problems?  </p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "natural-language-processing",
      "research",
      "history",
      "natural-language-understanding"
    ],
    "owner": {
      "account_id": 5333432,
      "reputation": 71,
      "user_id": 6477,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/0u61I.jpg?s=256",
      "display_name": "csheroe",
      "link": "https://ai.stackexchange.com/users/6477/csheroe"
    },
    "is_answered": true,
    "view_count": 290,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1584211599,
    "creation_date": 1491417179,
    "last_edit_date": 1584211599,
    "question_id": 3108,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3108/what-are-the-most-recent-and-influential-breakthroughs-in-nlp",
    "title": "What are the most recent and influential breakthroughs in NLP?",
    "body": "<p>I'm looking at the history of NLP, starting in the 1950s, with the <a href=\"http://www.hutchinsweb.me.uk/AMTA-2004.pdf\" rel=\"nofollow noreferrer\">Georgetown–IBM experiment</a>.</p>\n\n<p>What are examples of the most recent (e.g. in the last 5-10 years) and influential breakthroughs in natural language processing?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "math"
    ],
    "owner": {
      "account_id": 5772689,
      "reputation": 83,
      "user_id": 7101,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/0aa91dab8f8317bcef9ae4f480300433?s=256&d=identicon&r=PG",
      "display_name": "Anko6",
      "link": "https://ai.stackexchange.com/users/7101/anko6"
    },
    "is_answered": true,
    "view_count": 1378,
    "accepted_answer_id": 3294,
    "answer_count": 3,
    "score": 7,
    "last_activity_date": 1582567284,
    "creation_date": 1494305867,
    "last_edit_date": 1559492641,
    "question_id": 3291,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3291/how-can-i-start-learning-mathematics-for-machine-learning",
    "title": "How can I start learning mathematics for machine learning?",
    "body": "<p>I am an Android programmer. Now, I would like to learn machine learning. I know it requires a mathematical background, like statistics, probability, calculus and linear algebra. However, I am a bit lost. Where should I start from? Can someone provide me a road map for how to learn the mathematical background required for machine learning?</p>\n"
  },
  {
    "tags": [
      "convolutional-neural-networks",
      "image-recognition",
      "hyperparameter-optimization",
      "hyper-parameters",
      "filters"
    ],
    "owner": {
      "account_id": 4025821,
      "reputation": 286,
      "user_id": 7095,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/RpemN.png?s=256",
      "display_name": "daniel451",
      "link": "https://ai.stackexchange.com/users/7095/daniel451"
    },
    "is_answered": true,
    "view_count": 7367,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1611961226,
    "creation_date": 1494931660,
    "last_edit_date": 1611961226,
    "question_id": 3321,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3321/how-do-we-choose-the-kernel-size-depending-on-the-problem",
    "title": "How do we choose the kernel size depending on the problem?",
    "body": "<p>Obviously, finding suitable hyper-parameters for a neural network is a complex task and problem or domain-specific. However, there should be at least some \"rules\" that hold most times for the size of the filter (or kernel)!</p>\n\n<p>In most cases, intuition should be to go for small filters for detecting high-frequency features and large kernels for low-frequency features, right? For example, <span class=\"math-container\">$3 \\times 3$</span> kernel filters for edge detection, color contrast, etc., and maybe <span class=\"math-container\">$11 \\times 11$</span> for detecting full objects, when the objects occupy an area of roughly <span class=\"math-container\">$11 \\times 11$</span> pixels.</p>\n\n<p>Is this \"intuition\" more or less generally true? How can we decide which kernel's sizes should be chosen for a specific problem - or even for one specific convolutional layer?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "deep-learning",
      "hardware"
    ],
    "owner": {
      "account_id": 8393758,
      "reputation": 139,
      "user_id": 236,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/dc59297a81ba56a0667896ed9c3c8cd2?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Rushat",
      "link": "https://ai.stackexchange.com/users/236/rushat"
    },
    "is_answered": true,
    "view_count": 1202,
    "closed_date": 1640209097,
    "answer_count": 1,
    "score": 7,
    "locked_date": 1640209102,
    "last_activity_date": 1502748816,
    "creation_date": 1495537593,
    "last_edit_date": 1502748816,
    "question_id": 3368,
    "link": "https://ai.stackexchange.com/questions/3368/are-more-than-8-high-performance-nvidia-gpus-practical-for-deep-learning-applica",
    "closed_reason": "Not suitable for this site",
    "title": "Are more than 8 high performance Nvidia GPUs practical for deep learning applications?",
    "body": "<p>I was prompted towards this question while trying to find server racks and motherboards which are specialized towards artificial intelligence. Naturally I went to the SuperMicro website. There the <a href=\"https://www.supermicro.com.tw/products/system/4U/4028/SYS-4028GR-TXRT.cfm\" rel=\"nofollow noreferrer\">chassis+motherboard</a> which supported the maximum GPUs in the \"artificial intelligence\" category could support upto 8 of them. Additionally, the Nvidia DGX-1 also has only 8 Tesla P100 GPUs. And just to rub it in, Matlab does not support more than 8 GPUs last I checked.</p>\n\n<p>So, are more than 8 GPUs practical for DL? I would take Caffe, CNTK, Tensorflow and Torch7 as reference.</p>\n"
  },
  {
    "tags": [
      "tensorflow",
      "hardware",
      "google"
    ],
    "owner": {
      "account_id": 6352088,
      "reputation": 609,
      "user_id": 3323,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/mLqqcceD.jpg?s=256",
      "display_name": "Alecto",
      "link": "https://ai.stackexchange.com/users/3323/alecto"
    },
    "is_answered": true,
    "view_count": 3244,
    "closed_date": 1640208171,
    "answer_count": 1,
    "score": 7,
    "locked_date": 1640209123,
    "last_activity_date": 1556650453,
    "creation_date": 1495603910,
    "last_edit_date": 1556650453,
    "question_id": 3373,
    "link": "https://ai.stackexchange.com/questions/3373/who-manufactures-googles-tensor-processing-units",
    "closed_reason": "Not suitable for this site",
    "title": "Who manufactures Google&#39;s Tensor Processing Units?",
    "body": "<p>Does google manufacture TPUs? I know that google engineers are the ones responsible for the design, and that google is the one using them, but which company is responsible for the actual manufacturing of the chip? </p>\n"
  },
  {
    "tags": [
      "computer-vision",
      "classification",
      "image-recognition",
      "image-segmentation",
      "algorithm-request"
    ],
    "owner": {
      "user_type": "does_not_exist",
      "display_name": "user7369"
    },
    "is_answered": true,
    "view_count": 194,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1633948018,
    "creation_date": 1496082616,
    "last_edit_date": 1633948018,
    "question_id": 3407,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3407/what-algorithms-are-used-for-image-segmentation-of-images-where-objects-are-not",
    "title": "What algorithms are used for image segmentation of images where objects are not composed of pixels that are similar in value?",
    "body": "<p>In the process of segmentation, pixels are assigned to regions based on features that distinguish them from the rest of the image. Value Similarity and Spatial Proximity, for example, are two important principles that assume that points in the same region will have pixels that are spatially close and have similar values.</p>\n<p>In lots of situations, this is true, but what about regions composed of pixels that are not similar in value?</p>\n<p>Consider the image below.</p>\n<p><a href=\"https://i.sstatic.net/UORVX.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/UORVX.jpg\" alt=\"segmentation\" /></a></p>\n<p>The same &quot;logical&quot; region is composed of different elements that together represent something meaningful. In the same region, there are trees of different sizes and shapes, with shadows over some of them, etc. There are different things, with pixels that differ a lot in value, but I still need to group them together in the same region. From the image, you can see that I don't care so much about differences in color. In this case, the texture is the most important attribute.</p>\n<p>What algorithms are used to do the segmentation and classification in problems like that?</p>\n<p>I'm already looking for some algorithms and techniques that focus on texture, but some opinions from the experts will help me a lot. I think I need some orientation.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "reference-request",
      "resource-request",
      "books"
    ],
    "owner": {
      "account_id": 6090213,
      "reputation": 189,
      "user_id": 6221,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/ZP0D7.png?s=256",
      "display_name": "Astronought",
      "link": "https://ai.stackexchange.com/users/6221/astronought"
    },
    "is_answered": true,
    "view_count": 1166,
    "protected_date": 1635854591,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1610912026,
    "creation_date": 1500048538,
    "last_edit_date": 1610912026,
    "question_id": 3647,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3647/what-are-some-intermediate-or-advanced-books-on-neural-networks",
    "title": "What are some intermediate or advanced books on neural networks?",
    "body": "<p>Is anyone able to recommend some resources (preferably books) on the topic of neural networks that goes beyond that of introductory reading?</p>\n<p>I'm still relatively new to the subject, however, I have successfully created my own neural network, so I wouldn't consider myself a beginner, so I'm looking for something more intermediate.</p>\n"
  },
  {
    "tags": [
      "reference-request",
      "resource-request",
      "academia"
    ],
    "owner": {
      "account_id": 11468107,
      "reputation": 189,
      "user_id": 8820,
      "user_type": "registered",
      "profile_image": "https://lh5.googleusercontent.com/-yUP1BXUndaA/AAAAAAAAAAI/AAAAAAAAB1k/vaGARqpQQqk/s256-rj/photo.jpg",
      "display_name": "Rajib Bahar",
      "link": "https://ai.stackexchange.com/users/8820/rajib-bahar"
    },
    "is_answered": true,
    "view_count": 989,
    "accepted_answer_id": 3790,
    "answer_count": 3,
    "score": 7,
    "last_activity_date": 1592303297,
    "creation_date": 1501687646,
    "last_edit_date": 1592303297,
    "question_id": 3749,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3749/what-are-some-academic-ai-podcast-out-there",
    "title": "What are some academic AI podcast out there?",
    "body": "<p>I am looking for AI podcasts that are purely academic-oriented that I can use for learning purposes. Thanks for any resource pointers.</p>\n\n<p>The AI podcasts I am aware of are (not sure how many of these can be considered academic):</p>\n\n<ul>\n<li><a href=\"https://blogs.nvidia.com/ai-podcast/\" rel=\"nofollow noreferrer\">The AI Podcast</a></li>\n<li><a href=\"https://itunes.apple.com/us/podcast/linear-digressions/id941219323\" rel=\"nofollow noreferrer\">Linear Digressions</a></li>\n<li><a href=\"https://itunes.apple.com/us/podcast/oreilly-bots-podcast-oreilly/id1145426486\" rel=\"nofollow noreferrer\">O'Reilly Bots Podcast</a></li>\n<li><a href=\"https://soundcloud.com/a16z/artificial-intelligence\" rel=\"nofollow noreferrer\">A16z Podcast</a></li>\n</ul>\n"
  },
  {
    "tags": [
      "machine-learning",
      "ai-design",
      "prediction",
      "regression"
    ],
    "owner": {
      "account_id": 5327726,
      "reputation": 81,
      "user_id": 9046,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/5d6265a114c214cf8bd9d4d26e98bc78?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Cryptonaut",
      "link": "https://ai.stackexchange.com/users/9046/cryptonaut"
    },
    "is_answered": true,
    "view_count": 1231,
    "answer_count": 3,
    "score": 7,
    "last_activity_date": 1605357875,
    "creation_date": 1502809672,
    "last_edit_date": 1569018108,
    "question_id": 3817,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3817/which-predictive-algorithm-can-be-used-to-predict-a-number-given-other-numbers",
    "title": "Which predictive algorithm can be used to predict a number given other numbers?",
    "body": "<p>I am currently searching for a supervised learning algorithm that can be used to predict the output given a large enough training set.</p>\n\n<p>Here's a simple example. Suppose the training dataset is <code>{[A=1, B=330, C=1358.238902], result=234.244378}</code> and the test dataset <code>{[A=893, B=34, C=293], result=?}</code></p>\n\n<p>My intention is to predict <code>?</code> using the input values and result given in the training dataset. </p>\n\n<p>What algorithm would be effective for this problem given the wide range of my input/output values? Would this require some sort of regression algorithm?</p>\n"
  },
  {
    "tags": [
      "convolutional-neural-networks",
      "neurons",
      "feature-maps"
    ],
    "owner": {
      "account_id": 3954745,
      "reputation": 181,
      "user_id": 9072,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/11084af1bb4e72aeeaa7355fbd7a43e1?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "MackTuesday",
      "link": "https://ai.stackexchange.com/users/9072/macktuesday"
    },
    "is_answered": true,
    "view_count": 199,
    "accepted_answer_id": 3853,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1633185859,
    "creation_date": 1502925602,
    "last_edit_date": 1633023497,
    "question_id": 3825,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3825/cnns-what-happens-from-one-neuron-volume-to-the-next",
    "title": "CNNs: What happens from one neuron volume to the next?",
    "body": "<p>I've gone through several descriptions of CNNs online and they all leave out a crucial part as if it were trivial.</p>\n<p>A &quot;volume&quot; of neurons consists of several parallel layers (&quot;feature maps&quot;), each the result of convolving with a different kernel.</p>\n<p>Between volumes, there is usually a step where layers are pooled and subsampled.</p>\n<p>The next volume has a different number of parallel layers.</p>\n<p>How do the feature maps from one volume connect to the feature maps of the next volume? Is it one-to-many? Many-to-many? Do N kernels apply to each of the M feature maps in the first volume, yielding N*M feature maps in the second volume? Are these N kernels the same for each feature map in the first volume, or do different kernels apply to each one?</p>\n<p>Or, is the number of maps in the second volume not necessarily a multiple of the number in the first volume? If so, do maps in the first volume get cross-synthesized somehow? Or, maybe different numbers of maps in the second volume follow from each one in the first?</p>\n<p>Or, is it some other of umpteen trillion possibilities?</p>\n"
  },
  {
    "tags": [
      "genetic-algorithms",
      "evolutionary-algorithms",
      "crossover-operators",
      "elitism"
    ],
    "owner": {
      "account_id": 6796886,
      "reputation": 405,
      "user_id": 6258,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/07a936fa25f0769aa4aca533df0a69f0?s=256&d=identicon&r=PG",
      "display_name": "Alireza",
      "link": "https://ai.stackexchange.com/users/6258/alireza"
    },
    "is_answered": true,
    "view_count": 1835,
    "accepted_answer_id": 4268,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1607362598,
    "creation_date": 1504343089,
    "last_edit_date": 1607362598,
    "question_id": 3956,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/3956/is-elitism-preferred-over-non-elitism-in-the-cross-over-operator",
    "title": "Is elitism preferred over non-elitism in the cross-over operator?",
    "body": "<p>There are two potential approaches when performing cross-over operation in genetic algorithms.</p>\n<ol>\n<li><p>Use only the elites in the pool, probably the ones that are also going to be directly transferred to the next generation.</p>\n</li>\n<li><p>Use all the population present in the pool.</p>\n</li>\n</ol>\n<p>Is there any evidence that cross-over only with the elites of the population makes the GA converge faster to a good solution? I guess that, in order to escape from local minima, cross-over with all the population is needed. On the other hand, why should we perform cross-over with the least fit individuals?</p>\n<p>Any idea?</p>\n"
  },
  {
    "tags": [
      "monte-carlo-tree-search",
      "combinatorial-games",
      "imperfect-information"
    ],
    "owner": {
      "account_id": 11368066,
      "reputation": 71,
      "user_id": 9797,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/WT1tM.jpg?s=256",
      "display_name": "tamirok",
      "link": "https://ai.stackexchange.com/users/9797/tamirok"
    },
    "is_answered": true,
    "view_count": 540,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1634044090,
    "creation_date": 1506184355,
    "last_edit_date": 1610710436,
    "question_id": 4095,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4095/in-this-implementation-of-the-information-set-monte-carlo-tree-search-why-cant",
    "title": "In this implementation of the Information Set Monte Carlo Tree Search, why can&#39;t the players see the cards of each other?",
    "body": "<p>After reading this <a href=\"https://eprints.whiterose.ac.uk/75048/1/CowlingPowleyWhitehouse2012.pdf\" rel=\"nofollow noreferrer\">paper</a> about Monte Carlo methods for imperfect information games with elements of uncertainty, I couldn't understand the application of the <strong>determinization step</strong> in the <a href=\"https://gist.github.com/kjlubick/8ea239ede6a026a61f4d\" rel=\"nofollow noreferrer\">author's implementation</a> of the algorithm for the Knockout game.</p>\n<p>Determinization is defined as the transformation from an instance of an imperfect information game to ab instance of a perfect one. It means that all players should see the cards of each other after the determinization step.</p>\n<p>Why can't the players see the cards of each other in the code above?</p>\n"
  },
  {
    "tags": [
      "machine-learning",
      "natural-language-processing",
      "algorithm",
      "classification"
    ],
    "owner": {
      "account_id": 11901557,
      "reputation": 71,
      "user_id": 9962,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/1c62e3eff52b091c9aafab85e963e4e2?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "user8709398",
      "link": "https://ai.stackexchange.com/users/9962/user8709398"
    },
    "is_answered": true,
    "view_count": 644,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1539003030,
    "creation_date": 1506963064,
    "last_edit_date": 1539003030,
    "question_id": 4170,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4170/how-to-determine-if-an-amazon-review-is-likely-to-be-fake-using-text-classificat",
    "title": "How to determine if an Amazon review is likely to be fake using text classification",
    "body": "<p>I'm currently in the research stage of building a web app in ASP.NET where the user can input a URL to an Amazon product, then the app would determine how likely its reviews are to be genuine. I need help figuring out what algorithm to use in determining if a certain review is likely to be deceptive. I want my app to behave similarly to Fakespot or ReviewMeta. I realise a tool like this won't be 100% accurate and that's fine.</p>\n\n<p>So far I read parts of <a href=\"https://www.amazon.co.uk/Language-Processing-Prentice-Artificial-Intelligence/dp/0131873210\" rel=\"noreferrer\">this book</a> which seems to be recommended a lot to NLP newbies but haven't found anything that applies to such a specific problem. I also read <a href=\"http://www.aclweb.org/anthology/P14-1147\" rel=\"noreferrer\">this article</a> but it's based on hotel, restaurant and doctor reviews. I'm trying to find a more general method that can be applied to any product. Any help would be greatly appreciated!</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "image-recognition",
      "data-preprocessing",
      "imbalanced-datasets"
    ],
    "owner": {
      "account_id": 165635,
      "reputation": 173,
      "user_id": 10221,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/b71e89964ab8d525a818a135f3c2fb04?s=256&d=identicon&r=PG",
      "display_name": "James Gan",
      "link": "https://ai.stackexchange.com/users/10221/james-gan"
    },
    "is_answered": true,
    "view_count": 3671,
    "accepted_answer_id": 4837,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1642094758,
    "creation_date": 1508185708,
    "last_edit_date": 1642094758,
    "question_id": 4283,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4283/does-data-skew-matter-in-classification-problem",
    "title": "Does data skew matter in classification problem?",
    "body": "<p>I'm working on an image classification problem using a neural network. In the training data set, 90% of the samples fall into 10% of all categories, while 10% of the sample fall into the other 90% categories. So an example is not evenly distributed among all categories. If we assume this distribution reflects the real-world distribution, do I need to filter my dataset before training so that each category has a similar number of samples?</p>\n"
  },
  {
    "tags": [
      "convolutional-neural-networks",
      "training",
      "accuracy",
      "dropout",
      "testing"
    ],
    "owner": {
      "account_id": 7718502,
      "reputation": 73,
      "user_id": 10475,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/1549894728655291/picture?type=large",
      "display_name": "이희준",
      "link": "https://ai.stackexchange.com/users/10475/%ec%9d%b4%ed%9d%ac%ec%a4%80"
    },
    "is_answered": true,
    "view_count": 3450,
    "accepted_answer_id": 4387,
    "answer_count": 4,
    "score": 7,
    "last_activity_date": 1632443321,
    "creation_date": 1509274469,
    "last_edit_date": 1632443321,
    "question_id": 4385,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4385/why-is-my-test-error-lower-than-the-training-error",
    "title": "Why is my test error lower than the training error?",
    "body": "<p>I am trying to train a CNN regression model using the ADAM optimizer, dropout and weight decay.</p>\n<p>My test accuracy is better than training accuracy. But, as far as I know, usually, the training accuracy is better than test accuracy.</p>\n<p>So I wonder how this is happening.</p>\n<p><a href=\"https://i.sstatic.net/MgVBt.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/MgVBt.png\" alt=\"my grph\" /></a></p>\n"
  },
  {
    "tags": [
      "ai-design",
      "constraint-satisfaction-problems",
      "norvig-russell"
    ],
    "owner": {
      "account_id": 7715240,
      "reputation": 183,
      "user_id": 9574,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/jbRsm.jpg?s=256",
      "display_name": "Crist&#243;bal Alc&#225;zar",
      "link": "https://ai.stackexchange.com/users/9574/crist%c3%b3bal-alc%c3%a1zar"
    },
    "is_answered": true,
    "view_count": 3515,
    "accepted_answer_id": 5807,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1573581603,
    "creation_date": 1509413168,
    "last_edit_date": 1592387840,
    "question_id": 4396,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4396/how-to-turn-a-ternary-constraint-into-three-binary-constraints",
    "title": "How to turn a ternary constraint into three binary constraints?",
    "body": "<p>I'm trying to solve problem 6.6 from the book <em><a href=\"https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach\" rel=\"nofollow noreferrer\">Artificial Intelligence: A Modern Approach</a></em>, by Peter Norvig and Stuart Russell.</p>\n<p>This is in the context of <a href=\"https://en.wikipedia.org/wiki/Constraint_satisfaction_problem\" rel=\"nofollow noreferrer\">Constraint Satisfaction Problem</a> and how you can re-formulate some problems with the constraints expressed as a bunch of binary constraints to use the generalized solver CSP algorithm.</p>\n<p>But I'm stuck with that exercise, I can't sketch a demonstration.</p>\n<blockquote>\n<p>6.6  Show how a single ternary constraint such as &quot;A + B = C&quot; can be turned into three binary constraints by using an auxiliary variable. You may assume finite domains.</p>\n<p>(Hint: Consider a new variable that takes on values that are pairs of others values, and consider constraints such as &quot;X is the first element of the pair Y.&quot;) Next, show how constraints can be eliminated by altering the domain of variables. This completes the demonstration that any CSP can be transformed into a CSP with only binary constraints.</p>\n</blockquote>\n"
  },
  {
    "tags": [
      "machine-learning",
      "reference-request",
      "optical-character-recognition"
    ],
    "owner": {
      "account_id": 143535,
      "reputation": 179,
      "user_id": 11056,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/08ea907d3b57005305612f88c0a85b03?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Abhay Naik",
      "link": "https://ai.stackexchange.com/users/11056/abhay-naik"
    },
    "is_answered": true,
    "view_count": 2441,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1610068752,
    "creation_date": 1511300089,
    "last_edit_date": 1610068752,
    "question_id": 4571,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4571/effective-algorithms-for-ocr",
    "title": "Effective algorithms for OCR",
    "body": "<p>I am using Google's OCR to extract text from images, like receipts and invoices.</p>\n<p>Whare examples of techniques used to make sense of the text? For example, I would like to extract the date, name of the business, address, total amount, etc.</p>\n<p>Before marking this question &quot;too broad&quot;, if someone can please direct me to the right set of algorithms the industry uses for machine learning will be great.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "reinforcement-learning",
      "dqn",
      "deep-rl",
      "backpropagation"
    ],
    "owner": {
      "account_id": 7182903,
      "reputation": 231,
      "user_id": 10364,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/d1e498b69feadde754e1f1ddd959cc44?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "Yadeses",
      "link": "https://ai.stackexchange.com/users/10364/yadeses"
    },
    "is_answered": true,
    "view_count": 5437,
    "accepted_answer_id": 4668,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1609163593,
    "creation_date": 1512429178,
    "last_edit_date": 1609163593,
    "question_id": 4660,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4660/how-to-combine-backpropagation-in-neural-nets-and-reinforcement-learning",
    "title": "How to combine backpropagation in neural nets and reinforcement learning?",
    "body": "<p>I have followed a course on machine learning, where we learned about the gradient descent (GD) and back-propagation (BP) algorithms, which can be used to update the weights of neural networks, and reinforcement learning, in particular, Q-learning. I implemented these concepts separately.</p>\n<p>Now, I was thinking about using a neural network to approximate the Q-function, <span class=\"math-container\">$Q(s, a)$</span>, but I don't really know how to design the neural network and how to use back-propagation to update the weights of this neural network (NN).</p>\n<ol>\n<li><p>What should the inputs and outputs of this NN be?</p>\n</li>\n<li><p>How can I use GD and BP to update the weights of such an NN? Or should I use a different algorithm to update the weights?</p>\n</li>\n</ol>\n"
  },
  {
    "tags": [
      "neural-networks",
      "training",
      "optimization",
      "keras"
    ],
    "owner": {
      "account_id": 9774435,
      "reputation": 179,
      "user_id": 11391,
      "user_type": "registered",
      "profile_image": "https://graph.facebook.com/10209602018320363/picture?type=large",
      "display_name": "Pablo Ruiz Ruiz",
      "link": "https://ai.stackexchange.com/users/11391/pablo-ruiz-ruiz"
    },
    "is_answered": true,
    "view_count": 955,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1527804555,
    "creation_date": 1512657131,
    "question_id": 4677,
    "content_license": "CC BY-SA 3.0",
    "link": "https://ai.stackexchange.com/questions/4677/why-does-loss-change-depending-on-the-number-of-epochs-chosen",
    "title": "Why does &#39;loss&#39; change depending on the number of epochs chosen?",
    "body": "<p>I am using Keras to train different NN. I would like to know why if I increment the epochs in 1, the result until the new epoch is not the same. I am using shuffle=False, and np.random.seed(2017), and I have check that if I repeat with the same number of epochs, the result is the same, so not random initialization is working.</p>\n\n<p>Here I attach the picture of the resulting training with 2 epochs:</p>\n\n<p><a href=\"https://i.sstatic.net/DtLWl.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/DtLWl.png\" alt=\"\"></a>\nAnd here I attach the picture of the resulting training with 3 epochs:</p>\n\n<p><a href=\"https://i.sstatic.net/auDvm.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/auDvm.png\" alt=\"enter image description here\"></a></p>\n\n<p>Also, I would like to know why the training time is not (3/2) and how is it possible that some of them have less accuracy with one more epoch.</p>\n\n<p>Thanks a lot!</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "reinforcement-learning",
      "q-learning"
    ],
    "owner": {
      "account_id": 8954213,
      "reputation": 713,
      "user_id": 11566,
      "user_type": "registered",
      "accept_rate": 40,
      "profile_image": "https://www.gravatar.com/avatar/08ea907d3b57005305612f88c0a85b03?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "echo",
      "link": "https://ai.stackexchange.com/users/11566/echo"
    },
    "is_answered": true,
    "view_count": 1572,
    "accepted_answer_id": 4745,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1542651678,
    "creation_date": 1513193159,
    "last_edit_date": 1542651678,
    "question_id": 4740,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4740/why-is-the-target-r-gamma-max-a-qs-a-theta-i-in-the-loss-funct",
    "title": "Why is the target $r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;; \\theta_i^-)$ in the loss function of the DQN architecture?",
    "body": "<p>In the paper <a href=\"https://www.nature.com/articles/nature14236\" rel=\"nofollow noreferrer\">Human-level control through deep reinforcement learning</a>, the DQN architecture is presented, where the loss function is as follows</p>\n\n<p><span class=\"math-container\">$$\nL_i(\\theta_i) = \\mathbb{E}_{(s, a, r, s') \\sim U(D)} \\left[ \\left( r + \\gamma \\max_{a'} Q(s', a'; \\theta_i^-)  - Q(s, a; \\theta) \\right)^2\\right]\n$$</span></p>\n\n<p>where <span class=\"math-container\">$r + \\gamma \\max_{a'} Q(s', a'; \\theta_i^-)$</span> approximates the \"target\" of <span class=\"math-container\">$Q(s, a; \\theta)$</span>. But it is not clear to me why. How can existing weights approximate the target (ground truth)? Isn't <span class=\"math-container\">$r$</span> is a sample from a the experience replay dataset? Is <span class=\"math-container\">$r$</span> a scalar value?</p>\n"
  },
  {
    "tags": [
      "deep-learning",
      "reinforcement-learning",
      "ai-design",
      "game-ai",
      "combinatorial-games"
    ],
    "owner": {
      "account_id": 11262281,
      "reputation": 119,
      "user_id": 11585,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/ed452cea6ef56a887053963f484c41df?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "po0l",
      "link": "https://ai.stackexchange.com/users/11585/po0l"
    },
    "is_answered": true,
    "view_count": 3177,
    "answer_count": 2,
    "score": 7,
    "last_activity_date": 1574461590,
    "creation_date": 1513255173,
    "last_edit_date": 1574461590,
    "question_id": 4743,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4743/how-do-i-create-an-ai-for-a-two-players-board-game",
    "title": "How do I create an AI for a two-players board game?",
    "body": "<h3>Goal</h3>\n\n<p>I want to create an artificial intelligence to compete against other players in a board game.</p>\n\n<h3>Game explanation</h3>\n\n<p>I have a board game similar to 'snakes and ladders'. You have to get to a final field before your opponent does. But instead of depending on luck (throwing the dices) this game uses something like 'food'. You can go as far as you'd like, but it costs food to move (the more you move the more one extra field costs) and you can only get food in some special fields. And there aren't any snakes or ladders so you have to run the whole part. There are some more rules, for example, you can go backward and are only allowed to go into the goal if you've got less than some amount of 'food' and there are some extra fields with other special effects.  </p>\n\n<h3>For one player</h3>\n\n<p>If there was only one player as there isn't anything like 'luck' in this game, I theoretically could just compute every single method to find the one and only the best method. Practically, I should use an algorithm that requires less computational power.  </p>\n\n<h3>For two or more players</h3>\n\n<p>The challenge comes with the other player(s). I cannot visit an already taken field. And some other fields give me bonuses depending on my relative position to the other player (I'll just talk about two-player games). For example, only if I'm behind him that special field gives me some extra food.</p>\n\n<h3>My question</h3>\n\n<p>It would be ideal if I had some kind of a neural network that knows the field bonuses and I would give my position, the opponents position, the food and so on (the <strong>state</strong> of the game) and it would compute a value between -100 and 100 (assuming fields from 0 to 100) of how many fields I should go (forward or backward). </p>\n\n<p>I read a bit about Q-learning, deep reinforcement learning and deep neural networks. Is this the right approach to solving my problem? And if yes, have you got any more concrete ideas? The multiple actors and the sheer endless possibilities for moving depending on endless states make it hard for me to think of anything. Or is there a different, way better way that slipped past me?</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "classification"
    ],
    "owner": {
      "account_id": 3302796,
      "reputation": 173,
      "user_id": 11601,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/08ea907d3b57005305612f88c0a85b03?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "ayyoob imani",
      "link": "https://ai.stackexchange.com/users/11601/ayyoob-imani"
    },
    "is_answered": true,
    "view_count": 8011,
    "accepted_answer_id": 4749,
    "answer_count": 5,
    "score": 7,
    "last_activity_date": 1593190392,
    "creation_date": 1513331793,
    "last_edit_date": 1565342925,
    "question_id": 4748,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4748/binary-classifier-that-minimizes-false-positive-error",
    "title": "Binary classifier that minimizes false positive error",
    "body": "<p>I have a binary classification problem, where a false positive error has a very big cost compared to the false negative error. </p>\n\n<p>Is there a way to design a classifier for such problems (preferably, with an implementation of the algorithm)?  </p>\n"
  },
  {
    "tags": [
      "reinforcement-learning",
      "comparison",
      "policy-gradients",
      "bellman-equations"
    ],
    "owner": {
      "account_id": 8954213,
      "reputation": 713,
      "user_id": 11566,
      "user_type": "registered",
      "accept_rate": 40,
      "profile_image": "https://www.gravatar.com/avatar/08ea907d3b57005305612f88c0a85b03?s=256&d=identicon&r=PG&f=y&so-version=2",
      "display_name": "echo",
      "link": "https://ai.stackexchange.com/users/11566/echo"
    },
    "is_answered": true,
    "view_count": 409,
    "accepted_answer_id": 4777,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1585943419,
    "creation_date": 1513603640,
    "last_edit_date": 1585943142,
    "question_id": 4776,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/4776/why-do-bellman-equations-indirectly-create-a-policy",
    "title": "Why do Bellman equations indirectly create a policy?",
    "body": "<p>I was watching a lecture on policy gradients and Bellman equations. And they say that a Bellman equation indirectly creates a policy, while the policy gradient directly learns a policy. Why is this?</p>\n"
  },
  {
    "tags": [
      "game-ai",
      "heuristics",
      "alpha-beta-pruning",
      "combinatorial-games",
      "adversarial-search"
    ],
    "owner": {
      "account_id": 326393,
      "reputation": 171,
      "user_id": 12384,
      "user_type": "registered",
      "profile_image": "https://i.sstatic.net/gzQEA.jpg?s=256",
      "display_name": "Suhail Gupta",
      "link": "https://ai.stackexchange.com/users/12384/suhail-gupta"
    },
    "is_answered": true,
    "view_count": 6252,
    "answer_count": 4,
    "score": 7,
    "last_activity_date": 1612350784,
    "creation_date": 1517133224,
    "last_edit_date": 1612350784,
    "question_id": 5174,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5174/what-else-can-boost-iterative-deepening-with-alpha-beta-pruning",
    "title": "What else can boost iterative deepening with alpha-beta pruning?",
    "body": "<p>I read about minimax, then alpha-beta pruning, and then about iterative deepening. Iterative deepening coupled with alpha-beta pruning proves to quite efficient as compared to alpha-beta alone.</p>\n<p>I have implemented a game agent that uses iterative deepening with alpha-beta pruning. Now, I want to beat myself. What can I do to go deeper? Like alpha-beta pruning cut the moves, what other small change could be implemented that can beat my older AI?</p>\n<p>My aim to go deeper than my current AI. If you want to know about the game, here is a brief summary:</p>\n<p>There are 2 players, 4 game pieces, and a 7-by-7 grid of squares. At the beginning of the game, the first player places both the pieces on any two different squares. From that point on, the players alternate turns moving both the pieces like a queen in chess (any number of open squares vertically, horizontally, or diagonally). When the piece is moved, the square that was previously occupied is blocked. That square can not be used for the remainder of the game. The piece can not move through blocked squares. The first player who is unable to move any one of the queens loses.</p>\n<p>So my aim is to cut the unwanted nodes and search deeper.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "machine-learning",
      "reference-request",
      "function-approximation"
    ],
    "owner": {
      "account_id": 3164007,
      "reputation": 199,
      "user_id": 13067,
      "user_type": "registered",
      "profile_image": "https://www.gravatar.com/avatar/7f7fbf333344c4c565686f893db384a0?s=256&d=identicon&r=PG",
      "display_name": "user2674414",
      "link": "https://ai.stackexchange.com/users/13067/user2674414"
    },
    "is_answered": true,
    "view_count": 2367,
    "answer_count": 3,
    "score": 7,
    "last_activity_date": 1642205563,
    "creation_date": 1520347576,
    "last_edit_date": 1557499518,
    "question_id": 5539,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5539/which-functions-cant-neural-networks-learn-efficiently",
    "title": "Which functions can&#39;t neural networks learn efficiently?",
    "body": "<p>There are a lot of papers that show that neural networks can approximate a wide variety of functions. However, I can't find papers that show the limitations of NNs. </p>\n\n<p>What are the limitations of neural networks? Which functions can't neural networks learn efficiently (or using gradient-descent)?</p>\n\n<p>I am looking also for links to papers that describe these limitations.</p>\n"
  },
  {
    "tags": [
      "neural-networks",
      "backpropagation",
      "gradient-descent"
    ],
    "owner": {
      "account_id": 974718,
      "reputation": 1106,
      "user_id": 39,
      "user_type": "registered",
      "accept_rate": 38,
      "profile_image": "https://i.sstatic.net/F8mcb.jpg?s=256",
      "display_name": "Eka",
      "link": "https://ai.stackexchange.com/users/39/eka"
    },
    "is_answered": true,
    "view_count": 8492,
    "accepted_answer_id": 5620,
    "answer_count": 1,
    "score": 7,
    "last_activity_date": 1589765661,
    "creation_date": 1520526175,
    "last_edit_date": 1589765661,
    "question_id": 5580,
    "content_license": "CC BY-SA 4.0",
    "link": "https://ai.stackexchange.com/questions/5580/how-is-the-gradient-calculated-for-the-middle-layers-weights",
    "title": "How is the gradient calculated for the middle layer&#39;s weights?",
    "body": "<p>I am trying to understand backpropagation. I used a simple neural network with one input <span class=\"math-container\">$x$</span>, one hidden layer <span class=\"math-container\">$h$</span> and one output layer <span class=\"math-container\">$y$</span>, with weight <span class=\"math-container\">$w_1$</span> connecting <span class=\"math-container\">$x$</span> to <span class=\"math-container\">$h$</span>, and <span class=\"math-container\">$w_2$</span> connecting <span class=\"math-container\">$h$</span> to <span class=\"math-container\">$y$</span></p>\n\n<p><span class=\"math-container\">$$\nx \\rightarrow (w_1) \\rightarrow h \\rightarrow (w_2) \\rightarrow y\n$$</span></p>\n\n<p>In my understanding, these are the steps happening while we train a neural network:</p>\n\n<p>The feedforward step.</p>\n\n<p><span class=\"math-container\">\\begin{align}\nh=\\sigma\\left(x w_{1}+b\\right)\\\\\ny^{\\prime}=\\sigma\\left(h w_{2}+b\\right)\n\\end{align}</span></p>\n\n<p>The loss function.</p>\n\n<p><span class=\"math-container\">$$\nL=\\frac{1}{2} \\sum\\left(y-y^{\\prime}\\right)^{2}\n$$</span></p>\n\n<p>The gradient calculation</p>\n\n<p><span class=\"math-container\">$$\\frac{\\partial L}{\\partial w_{2}}=\\frac{\\partial y^{\\prime}}{\\partial w_{2}} \\frac{\\partial L}{\\partial y^{\\prime}}$$</span></p>\n\n<p><span class=\"math-container\">$$\\frac{\\partial L}{\\partial w_{1}}=?$$</span></p>\n\n<p>The weight update</p>\n\n<p><span class=\"math-container\">$$\nw_{i}^{t+1} \\leftarrow w_{i}^{t}-\\alpha \\frac{\\partial L}{\\partial w_{i}}\n$$</span></p>\n\n<p>I understood most parts of backpropagation, but how do we get the gradients for the middle layer weights <span class=\"math-container\">$dL/dw_1$</span>?</p>\n\n<p>How should we calculate the gradient of a network similar to this?</p>\n\n<p><a href=\"https://i.sstatic.net/P5fce.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/P5fce.png\" alt=\"enter image description here\"></a></p>\n\n<p>Is this the correct equation?</p>\n\n<p><span class=\"math-container\">$$\\frac{\\partial L}{\\partial w_{1}}=\\frac{\\partial h_{1}}{\\partial w_{1}} \\frac{\\partial w_{7}}{\\partial h_{1}} \\frac{\\partial o_{2}}{\\partial w_{7}} \\frac{\\partial L}{\\partial o_{2}}+\\frac{\\partial h_{1}}{\\partial w_{1}} \\frac{\\partial w_{5}}{\\partial h_{1}} \\frac{\\partial o_{1}}{\\partial w_{5}} \\frac{\\partial L}{\\partial o_{1}}$$</span></p>\n"
  }
]